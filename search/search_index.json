{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Sunil's Wiki","text":"<p>This wiki document serves as a comprehensive repository for technical references and personal learnings. It aims to capture a wide range of topics related to various domains, including programming languages, frameworks, tools, and concepts. The document's purpose is to facilitate knowledge sharing, collaboration, and continuous learning among its contributors and readers. It covers practical examples, code snippets, best practices, troubleshooting tips, and insights gained from personal experiences. By fostering a community-driven approach, this wiki document encourages individuals to contribute their expertise and unique perspectives, creating a valuable resource for anyone seeking technical guidance and insights.</p> <p>Programming Languages</p> <p>We would be exploring primariliy on python and its <code>programming terms</code>, syntax and code snippets that could be helpful in software development. We will also discuss on <code>python design patterns</code> and its real time use cases. </p> <p>Frameworks</p> <p>Cover popular frameworks and libraries used in software development, web development</p> <p>Tools and Utilities</p> <p>Discuss essential tools, software, and utilities used in various technical fields. Include tutorials, tips, and tricks for using them effectively.</p> <p>Concepts and Fundamentals</p> <p>Dive into fundamental concepts and principles in computer science, software engineering, networking, databases, algorithms, and more. Provide explanations, diagrams, and practical applications.</p> <p>Troubleshooting and Debugging</p> <p>Provide tips and techniques for identifying and resolving common errors, bugs, and issues in different programming environments and platforms.</p> <p>Personal Projects and Learnings</p> <p>Share personal projects undertaken by contributors, detailing the challenges faced, lessons learned, and insights gained. Include code repositories, project documentation, and success stories.</p> <p>Resources and References</p> <p>Compile a curated list of external resources, books, online courses, documentation, and websites that provide valuable technical knowledge and support further learning.</p> <p>Documentation help</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"about/","title":"Telis nullam et restabat putate","text":""},{"location":"about/#inopem-isthmon-nostras-et-vectus-vidi-tydiden","title":"Inopem Isthmon nostras et vectus vidi Tydiden","text":"<p>Lorem markdownum ventis diu fatentis movere properataque Eleis malis cum signaque vinctum gaudia, valebam! Pudorem saxa cetera mihi sola equina, imago, sensit, procul Iuno: fide.</p> <ol> <li>Neque nec reparabat excipit geri tinctam vagantur</li> <li>Facies obstantis sceleratior</li> <li>Trahebat esse viro mollia neque et ore</li> <li>Sacro ire</li> </ol> <p>Cubitoque Doridaque lapis fessus effuge greges, cui ossibus caelo. Sub cani ambit teretesque inhospita verbis in non Venerem repertum solet, videt, urbes. Coepere fugiebat prolem ferrataque Colchide hoc salutant carmine secuit consistere, nisi. Cum da ipsa minus, dicebar vivit. Portae opera pedes inquit contactus, nec tibi Procrin, reddidit heu.</p>"},{"location":"about/#magis-spumam","title":"Magis spumam","text":"<p>Cuius cogor; excidit at animam gaudet sui quamvis dona diu; fronti lacrimaeque arma sunt Hectoris interdum! Hinc sententia membris prohibentque fraterna et atque, in nymphis tumet, quondam moveri, quoque precor. Dabas seu, hos illa sua. Ut imas tua nati memorantur moneo, et suum facit thalamos ira usque scelerata Iovique. Pericula fuso est animi signis nitar forte et laevam insidiae trementem dignamque.</p> <ol> <li>Non accessi casus est dum procumbere requiram</li> <li>Verbis momentaque petunt quibus pectora ad nullam</li> <li>Sancta reseratque ex sine est cuique elidite</li> <li>Incerta sequeretur</li> <li>Tantus aequor promissaque donec arbitrio queror in</li> </ol> <p>Mane guttura navigat, ab diuque o ut dixit nescio tristisque sine. Fluvialis haec Elide manus sed; dicitur habebat, egerit non habitabilis caput, ne positus membrisque erat. Ambiguus terras illa. Magna regia nec: Phoebus culpavit fratre quae Aurora lacrimosa monstri!</p> <p>Per ipsum dixit volucris rupit deos corpora cupiens traditque; aprum et. Saevique multis, ille aera potiunda auro soporem, iuvenis haustos. Si Neoptolemum Nelei praebere: fuit misit!</p> <p>Ait et madefactam undis Palladias quaque verbis vera est tendit puppis quisque villae venenis siqua quies dabat gravida, Inachidos. Aurora annos, facta sola mercibus ratus, cum agebat concepit quid. Esset iubet Pyracmon nobilitas socios sternuntur et ille Calydonius quaeras circumsonat moliri tamen huius; merui nec iacet litore, mare. Vides litora, rari esset credulus possim proditus rotatis ingentem.</p>"},{"location":"cicd/overview/","title":"Overview Of CICD","text":""},{"location":"cicd/overview/#difference-between-cicd","title":"Difference between CI/CD","text":"<p>Continuous Delivery: Software can be deployed to customers at any time with the \"push of a button\" (i.e. by running a deployment script).</p> <p>Continuous Deployment: Software is automatically deployed to customers once it passes through the continuous integration system.</p> <p></p>"},{"location":"cicd/overview/#phases-of-continious-integrations","title":"Phases of Continious integrations","text":"<p>Each of these phases involves incremental improvements to the technical infrastructure as well as, perhaps more importantly, improvements in the practices and culture of the development team itself</p>"},{"location":"cicd/overview/#phase-1-no-build-server","title":"Phase 1 - No Build Server","text":"<p>Software is built manually on a developer's machine, but developers do not necessarily commit their changes on a regular basis. Some time before a release is scheduled, a developer manually integrates the changes</p>"},{"location":"cicd/overview/#phase-2-nightly-builds","title":"Phase 2 - Nightly Builds","text":"<p>the team has a build server, and automated builds are scheduled on a regular (typically nightly) basis. This build simply compiles the code, as there are no reliable or repeatable unit tests. Indeed, automated tests, if they are written, are not a mandatory part of the build process, and may well not run correctly at all. However developers now commit their changes regularly, at least at the end of every day</p>"},{"location":"cicd/overview/#phase-3-nightly-builds-and-basic-automated-tests","title":"Phase 3 - Nightly Builds and Basic Automated Tests","text":"<p>The build server is configured to kick off a build whenever new code is committed to the version control system, and team members are able to easily see what changes in the source code triggered a particular build, and what issues these changes address</p>"},{"location":"cicd/overview/#phase-4-enter-the-metrics","title":"Phase 4 - Enter the Metrics","text":"<p>Automated code quality and code coverage metrics are now run to help evaluate the quality of the code base and (to some extent, at least) the relevance and effectiveness of the tests. The code quality build also automatically generates API documentation for the application. </p>"},{"location":"cicd/overview/#phase-5-getting-more-serious-about-testing","title":"Phase 5 - Getting More Serious About Testing","text":"<p>Test-Driven Development are more widely practiced, resulting in a growing confidence in the results of the automated builds. The application is no longer simply compiled and tested, but if the tests pass, it is automatically deployed to an application server for more comprehensive end-to-end tests and performance tests</p>"},{"location":"cicd/overview/#phase-6-automated-acceptance-tests-and-more-automated-deployment","title":"Phase 6 - Automated Acceptance Tests and More Automated Deployment","text":"<p>Acceptance-Test Driven Development is practiced, guiding development efforts and providing high-level reporting on the state of project. These automated tests use Behavior-Driven Development and Acceptance-Test Driven Development tools to act as communication and documentation tools and documentation as mush as testing tools, publishing reports on test results in business terms that non-developers can understand. Since these high-level tests are automated at an early stage in the development process, they also provide a clear idea of what features have been implemented, and which remain to be done. The application is automatically deployed into test environments for testing by the QA team either as changes are committed, or on a nightly basis; a version can be deployed(or \"prompted\") to UAT and possibly production environments using a manually-triggered build when testers consider it ready. The team is also capable of using the build server to back out a release, rolling back to a previous release, if something goes horribly wrong.</p>"},{"location":"cicd/overview/#phase-7-continuous-deployment","title":"Phase 7 - Continuous Deployment","text":"<p>Confidence in the automated unit, integration and acceptance tests is now such that teams can apply the automated deployment techniques developed in the previous phase to push out new changes directly into production. The progression between levels here is of course somewhat approximate, and may not always match real-world situations</p>"},{"location":"cicd/argocd/applications/","title":"applications","text":"<p>Its a resource object representing a deployed application instance in an environment. Its defined by two key pieces of information. </p> <ul> <li>source: desired state</li> <li>destination: reference to target cluster and namespace</li> </ul> <p>All the applications/projects can be created using below three methods.</p> <ul> <li>declarative(yaml), always recommened</li> <li>Web UI</li> <li>CLI</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/argoproj/argocd-example-apps.git\n    targetRevision: HEAD\n    path: guestbook\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: guestbook\n</code></pre>"},{"location":"cicd/argocd/applications/#create-application","title":"Create application","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: guestbook\n  namespace: argocd\nspec: \n  destination: \n    namespace: guestbook\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>\u279c  labs git:(master) \u2717 kubectl get application -n argocd\nNAME        SYNC STATUS   HEALTH STATUS\nguestbook   OutOfSync     Missing\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app list            \nNAME              CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                PATH       TARGET\nargocd/guestbook  https://kubernetes.default.svc  guestbook  default  OutOfSync  Missing  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\n\u279c  labs git:(master) \u2717 \n</code></pre> <p>Go to the UI and start \"sync\" which will then start your deployment by checking with your manifests and so on </p> <p></p> <p>Create application using CLI</p> <pre><code>\u279c  labs git:(master) \u2717 argocd app create app-2 --repo https://github.com/mabusaa/argocd-example-apps.git --revision master --path guestbook --dest-namespace app-2 --dest-server https://kubernetes.default.svc --sync-option CreateNamespace=true\n\napplication 'app-2' created\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app list\nNAME              CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                PATH       TARGET\nargocd/app-1      https://kubernetes.default.svc  app-1      default  Synced     Healthy  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\nargocd/app-2      https://kubernetes.default.svc  app-2      default  OutOfSync  Missing  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\nargocd/guestbook  https://kubernetes.default.svc  guestbook  default  Synced     Healthy  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app sync app-2\nTIMESTAMP                  GROUP        KIND   NAMESPACE                  NAME    STATUS    HEALTH        HOOK  MESSAGE\n2025-04-11T08:56:13+05:30            Service       app-2          guestbook-ui  OutOfSync  Missing              \n2025-04-11T08:56:13+05:30   apps  Deployment       app-2          guestbook-ui  OutOfSync  Missing              \n2025-04-11T08:56:15+05:30          Namespace                             app-2   Running   Synced              namespace/app-2 created\n2025-04-11T08:56:15+05:30            Service       app-2          guestbook-ui    Synced  Healthy              \n2025-04-11T08:56:15+05:30            Service       app-2          guestbook-ui    Synced   Healthy              service/guestbook-ui created\n2025-04-11T08:56:15+05:30   apps  Deployment       app-2          guestbook-ui  OutOfSync  Missing              deployment.apps/guestbook-ui created\n2025-04-11T08:56:16+05:30   apps  Deployment       app-2          guestbook-ui    Synced  Progressing              deployment.apps/guestbook-ui created\n\nName:               argocd/app-2\nProject:            default\nServer:             https://kubernetes.default.svc\nNamespace:          app-2\nURL:                https://localhost:8080/applications/app-2\nSource:\n- Repo:             https://github.com/mabusaa/argocd-example-apps.git\n  Target:           master\n  Path:             guestbook\nSyncWindow:         Sync Allowed\nSync Policy:        Manual\nSync Status:        Synced to master (93860ce)\nHealth Status:      Progressing\n\nOperation:          Sync\nSync Revision:      93860cefec473c343718a38c99a2e099cc40d209\nPhase:              Succeeded\nStart:              2025-04-11 08:56:12 +0530 IST\nFinished:           2025-04-11 08:56:15 +0530 IST\nDuration:           3s\nMessage:            successfully synced (all tasks run)\n\nGROUP  KIND        NAMESPACE  NAME          STATUS   HEALTH       HOOK  MESSAGE\n       Namespace              app-2         Running  Synced             namespace/app-2 created\n       Service     app-2      guestbook-ui  Synced   Healthy            service/guestbook-ui created\napps   Deployment  app-2      guestbook-ui  Synced   Progressing        deployment.apps/guestbook-ui created\n\u279c  labs git:(master) \u2717 \n</code></pre>"},{"location":"cicd/argocd/applications/#projects","title":"Projects","text":"<ul> <li>you can create project for application logical grouping. </li> <li>access restrictions for multiple teams</li> <li>allow apps to be deployed into specific clusters and namespaces</li> <li>project roles, enables to create a role with set of policies \"permission\" to grant access to project applications i.e you can set JWT or ODIC etc </li> <li>Always it would create a default project </li> </ul>"},{"location":"cicd/argocd/applications/#create-projects","title":"Create projects","text":"<p>Declarative method</p> <ul> <li>Allow all sources</li> <li>Allow all destination</li> <li>Allow all cluster and namespace scopes resources</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: demo-project\n  namespace: argocd\nspec:\n  description: Demo Project\n  sourceRepos:\n  - '*'\n\n  destinations:\n  - namespace: '*'\n    server: '*'\n\n  clusterResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  namespaceResourceWhitelist:\n  - group: '*'\n    kind: '*'\n</code></pre> <p>Once the project is defined, then you would create application related to this project. </p> <pre><code>\u279c  labs git:(master) \u2717 kubectl get appproject -n argocd                         \nNAME      AGE\ndefault   47h\n\u279c  labs git:(master) \u2717 \n</code></pre> <p>TODO:</p> <ul> <li>[ ] Create a new project and deploy application in that project.</li> <li>[ ] Create a new project to be allowed only for particular namespace.</li> <li>[ ] create application in that namespace to chekc if deployment is successful</li> <li>[ ] Describe the logs for the application incase there's issue</li> <li>[ ] Modify the logs according to the namespace and you would not be able to run the app</li> </ul>"},{"location":"cicd/argocd/applications/#roles","title":"Roles","text":"<p>Enable you to create a role with set of policies \"permission\" to grant access to a project applications. fine-grained access control within a project using RBAC (Role-Based Access Control).</p> <p>You can:</p> <ul> <li>Create custom roles for a project</li> <li>Assign JWT tokens (like service accounts) for automation or scripts</li> <li>Define what actions are allowed on what applications</li> </ul> <p>Example: </p> <pre><code># ci-role.yaml\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: project-with-role\n  namespace: argocd\nspec:\n  description: project-with-role description\n  sourceRepos:\n  - '*'\n\n  destinations:\n  - namespace: '*'\n    server: '*'\n\n  clusterResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  namespaceResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  roles:\n  - name: ci-role\n    description: Sync privileges for project-with-role\n    policies:\n    - p, proj:project-with-role:ci-role, applications, sync, project-with-role/*, allow\n</code></pre> <ul> <li>all sources</li> <li>all destinations</li> <li>all clusters and namespace scoped resources</li> <li>define role</li> <li>defined role with sync to all applications in the same project. </li> <li>create token related to this role</li> <li>try to delete application using the token which will be <code>denied</code> by cluster.</li> </ul> <pre><code>argocd proj role create-token ci-role\n</code></pre> <p>using cli, try to delete some other application using the above token, you will get \"action denied\" because this token don't have an access to delete.</p> <pre><code>argocd app list \nargocd app delete &lt;project&gt; --auth-token etrasdSaaweSxcese.....\n</code></pre> <p>TODO:</p> <ul> <li>[ ] run \"ci-role.yaml\" </li> <li>[ ] create token</li> <li>[ ] using the token, try deleting...</li> </ul>"},{"location":"cicd/argocd/applications/#repository","title":"repository","text":"<p>You can add your private repo as well to the Argocd, just make sure you pass your <code>git-creds</code> and <code>application id</code>. </p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/#github-app-credential</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-https\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/mabusaa/argocd-example-apps-private.git\n  password: # password goes here, NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n  username: my-token\n</code></pre> <pre><code>#private-repo-creds-https.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-creds-https\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: https://github.com/mabusaa\n  password: # password goes here, NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n  username: my-token\n</code></pre> <pre><code># private-repo-ssh.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-ssh\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: git@github.com:mabusaa/argocd-example-apps-private.git\n  sshPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n     # key goes here  NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n    -----END OPENSSH PRIVATE KEY-----\n</code></pre> <p>TODO</p> <ul> <li>[ ] Clone the repo and make it private. i.e create empty repo, copy files and then push src code.</li> <li>[ ] Create a secret uing username password or API token in argocd namespace </li> <li>[ ] Go to UI and check for repository, you would see the repo getting connected..</li> <li>[ ] Create a new application providing the priavte repo url as source of manifest</li> <li>[ ] Try for adding HTTPS/SSH connection as well.  </li> <li>[ ] Validate if application created and OutOfsync from \"Web UI\"</li> </ul>"},{"location":"cicd/argocd/faq/","title":"faq","text":"# Interview Question Explanation / Answer 1 What is Argo CD and how does it fit into GitOps? Argo CD automates Kubernetes deployments using Git as the source of truth. It enables GitOps by syncing Git state with live cluster state. 2 How does Argo CD differ from Flux CD? Argo CD has a GUI, built-in RBAC, and better multi-cluster support. Flux is more CLI/operator-focused. 3 How do you manage secrets in Argo CD? Use tools like SOPS, Sealed Secrets, or External Secrets Operator. Avoid committing raw secrets to Git. 4 Difference between Manual and Auto Sync? Manual sync requires user action. Auto sync applies changes from Git to the cluster automatically. 5 How do you promote code from dev \u2192 stage \u2192 prod in Argo CD? Use Git branches, kustomize overlays, or Helm values per environment. Trigger promotion by merging branches. 6 What are ApplicationSets in Argo CD? A controller that dynamically generates Argo Applications using templates and generators (Git, Matrix, List, etc.). 7 How to implement RBAC and security best practices? Integrate with SSO, disable admin user, restrict resource access via RBAC, use NetworkPolicies. 8 How is drift detection handled? Argo CD continuously checks live vs. Git state. Drift shows as <code>OutOfSync</code>. Can use <code>syncOptions</code> to fine-tune sync behavior. 9 How do you rollback in Argo CD? Use <code>argocd app history</code> and <code>argocd app rollback</code> to revert to a previous revision. 10 How to monitor and audit Argo CD? Integrate Prometheus/Grafana, export logs, and use Argo CD audit logs or webhook notifications."},{"location":"cicd/argocd/faq/#real-time-issues","title":"real time issues","text":"# Issue Root Cause Solution / Fix 1 App stuck in OutOfSync due to Helm/Kustomize Rendering failed (missing values or wrong base) Validate with <code>helm template</code>, ensure <code>valuesFiles</code> or <code>kustomize.path</code> is correct. 2 Sync fails due to RBAC or NetworkPolicy Insufficient permissions or blocked API access Grant proper ClusterRoleBinding, fix NetworkPolicy, test with <code>kubectl auth can-i</code>. 3 Shared resource (e.g. Secret) is deleted during sync Prune deletes resources not scoped properly Use <code>Prune=false</code> or resource customizations in Argo CD config. 4 Excess Applications created by ApplicationSet Wrong generator or repo structure Use <code>preview</code> mode, validate Git paths, check controller logs. 5 Application stuck in Syncing state Failed pre/post sync hook or finalizer Check <code>sync hooks</code>, use retries, patch finalizers if needed. 6 Resource sync fails due to CRDs not installed yet CRDs are missing in cluster Install CRDs first or use <code>--validate=false</code> option in sync. 7 Secrets in Git exposed accidentally Raw secrets committed to Git Use encryption tools (SOPS), or external secret backends. 8 Multi-team setup with config drift Teams override shared manifests Use separate overlays per team/environment. Implement strict review via PRs. 9 UI shows stale sync status Controller not updated or cache issue Restart <code>argocd-application-controller</code>, refresh app. 10 Argo CD performance drops in large clusters High app count, long sync intervals Scale controller, increase cache size, split by project or namespace."},{"location":"cicd/argocd/install/","title":"install","text":"<p>You can install in <code>non-HA</code> using <code>minikube</code> or <code>docker-desktop</code> or <code>rancher-desktop</code></p> <pre><code>\u279c  ~ kubectl get nodes\nNAME                   STATUS   ROLES                  AGE    VERSION\nlima-rancher-desktop   Ready    control-plane,master   229d   v1.30.3+k3s1\n\n\u279c  ~ kubectl create ns argocd\nnamespace/argocd created\n\n\u279c  ~ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\ncustomresourcedefinition.apiextensions.k8s.io/applications.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/applicationsets.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io created\nserviceaccount/argocd-application-controller created\nserviceaccount/argocd-applicationset-controller created\nserviceaccount/argocd-dex-server created\nserviceaccount/argocd-notifications-controller created\nserviceaccount/argocd-redis created\nserviceaccount/argocd-repo-server created\nserviceaccount/argocd-server created\nrole.rbac.authorization.k8s.io/argocd-application-controller created\nrole.rbac.authorization.k8s.io/argocd-applicationset-controller created\nrole.rbac.authorization.k8s.io/argocd-dex-server created\nrole.rbac.authorization.k8s.io/argocd-notifications-controller created\nrole.rbac.authorization.k8s.io/argocd-redis created\nrole.rbac.authorization.k8s.io/argocd-server created\nclusterrole.rbac.authorization.k8s.io/argocd-application-controller created\nclusterrole.rbac.authorization.k8s.io/argocd-applicationset-controller created\nclusterrole.rbac.authorization.k8s.io/argocd-server created\nrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-dex-server created\nrolebinding.rbac.authorization.k8s.io/argocd-notifications-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-redis created\nrolebinding.rbac.authorization.k8s.io/argocd-server created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-server created\nconfigmap/argocd-cm created\nconfigmap/argocd-cmd-params-cm created\nconfigmap/argocd-gpg-keys-cm created\nconfigmap/argocd-notifications-cm created\nconfigmap/argocd-rbac-cm created\nconfigmap/argocd-ssh-known-hosts-cm created\nconfigmap/argocd-tls-certs-cm created\nsecret/argocd-notifications-secret created\nsecret/argocd-secret created\nservice/argocd-applicationset-controller created\nservice/argocd-dex-server created\nservice/argocd-metrics created\nservice/argocd-notifications-controller-metrics created\nservice/argocd-redis created\nservice/argocd-repo-server created\nservice/argocd-server created\nservice/argocd-server-metrics created\ndeployment.apps/argocd-applicationset-controller created\ndeployment.apps/argocd-dex-server created\ndeployment.apps/argocd-notifications-controller created\ndeployment.apps/argocd-redis created\ndeployment.apps/argocd-repo-server created\ndeployment.apps/argocd-server created\nstatefulset.apps/argocd-application-controller created\nnetworkpolicy.networking.k8s.io/argocd-application-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-applicationset-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-dex-server-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-notifications-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-redis-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-repo-server-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-server-network-policy created\n\u279c  ~\n\n\u279c  ~ kubectl get all -n argocd\nNAME                                                   READY   STATUS    RESTARTS   AGE\npod/argocd-application-controller-0                    1/1     Running   0          102s\npod/argocd-applicationset-controller-cc68b7b7b-6ck72   1/1     Running   0          103s\npod/argocd-dex-server-555b55c97d-qrrbq                 1/1     Running   0          103s\npod/argocd-notifications-controller-65655df9d5-drn4n   1/1     Running   0          103s\npod/argocd-redis-764b74c9b9-bkdrs                      1/1     Running   0          103s\npod/argocd-repo-server-7dcbcd967b-bxcjd                1/1     Running   0          103s\npod/argocd-server-5b9cc8b776-bjnqd                     1/1     Running   0          102s\n\nNAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/argocd-applicationset-controller          ClusterIP   10.43.100.205   &lt;none&gt;        7000/TCP,8080/TCP            104s\nservice/argocd-dex-server                         ClusterIP   10.43.83.92     &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   104s\nservice/argocd-metrics                            ClusterIP   10.43.158.10    &lt;none&gt;        8082/TCP                     104s\nservice/argocd-notifications-controller-metrics   ClusterIP   10.43.106.252   &lt;none&gt;        9001/TCP                     103s\nservice/argocd-redis                              ClusterIP   10.43.67.82     &lt;none&gt;        6379/TCP                     103s\nservice/argocd-repo-server                        ClusterIP   10.43.17.137    &lt;none&gt;        8081/TCP,8084/TCP            103s\nservice/argocd-server                             ClusterIP   10.43.40.122    &lt;none&gt;        80/TCP,443/TCP               103s\nservice/argocd-server-metrics                     ClusterIP   10.43.40.210    &lt;none&gt;        8083/TCP                     103s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argocd-applicationset-controller   1/1     1            1           103s\ndeployment.apps/argocd-dex-server                  1/1     1            1           103s\ndeployment.apps/argocd-notifications-controller    1/1     1            1           103s\ndeployment.apps/argocd-redis                       1/1     1            1           103s\ndeployment.apps/argocd-repo-server                 1/1     1            1           103s\ndeployment.apps/argocd-server                      1/1     1            1           103s\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argocd-applicationset-controller-cc68b7b7b   1         1         1       103s\nreplicaset.apps/argocd-dex-server-555b55c97d                 1         1         1       103s\nreplicaset.apps/argocd-notifications-controller-65655df9d5   1         1         1       103s\nreplicaset.apps/argocd-redis-764b74c9b9                      1         1         1       103s\nreplicaset.apps/argocd-repo-server-7dcbcd967b                1         1         1       103s\nreplicaset.apps/argocd-server-5b9cc8b776                     1         1         1       103s\n\nNAME                                             READY   AGE\nstatefulset.apps/argocd-application-controller   1/1     102s\n\u279c  ~\n</code></pre> <p>All the pods must be in running space </p> <pre><code>\u279c  ~ kubectl get pods -n argocd\nNAME                                               READY   STATUS    RESTARTS   AGE\nargocd-application-controller-0                    1/1     Running   0          2m16s\nargocd-applicationset-controller-cc68b7b7b-6ck72   1/1     Running   0          2m17s\nargocd-dex-server-555b55c97d-qrrbq                 1/1     Running   0          2m17s\nargocd-notifications-controller-65655df9d5-drn4n   1/1     Running   0          2m17s\nargocd-redis-764b74c9b9-bkdrs                      1/1     Running   0          2m17s\nargocd-repo-server-7dcbcd967b-bxcjd                1/1     Running   0          2m17s\nargocd-server-5b9cc8b776-bjnqd                     1/1     Running   0          2m16s\n\u279c  ~\n</code></pre> <p>You need to follow below steps for setup </p> <p>Get the initial password that is stored as secret in argocd namesapce convert secret from base64 into plain text</p> <pre><code>\u279c  ~ kubectl get secret -n argocd argocd-initial-admin-secret -o yaml\napiVersion: v1\ndata:\n  password: T2FDUFJ5d1pab0g1ODVuRQ==\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-04-09T04:17:55Z\"\n  name: argocd-initial-admin-secret\n  namespace: argocd\n  resourceVersion: \"775735\"\n  uid: aebfa38f-12fa-4a8d-adab-22ed8ed816ad\ntype: Opaque\n\u279c  ~\n\n\u279c  ~  echo \"T2FDUFJ5d1pab0g1ODVuRQ==\"|base64 -d\nOaCPRywZZoH585nE%                                                                                                                                   \u279c  ~\n</code></pre> <p>Expose the argocd-server to connect to UI by port-forwarding</p> <pre><code>\u279c  ~ kubectl get svc -n argocd\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nargocd-applicationset-controller          ClusterIP   10.43.100.205   &lt;none&gt;        7000/TCP,8080/TCP            8m38s\nargocd-dex-server                         ClusterIP   10.43.83.92     &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   8m38s\nargocd-metrics                            ClusterIP   10.43.158.10    &lt;none&gt;        8082/TCP                     8m38s\nargocd-notifications-controller-metrics   ClusterIP   10.43.106.252   &lt;none&gt;        9001/TCP                     8m37s\nargocd-redis                              ClusterIP   10.43.67.82     &lt;none&gt;        6379/TCP                     8m37s\nargocd-repo-server                        ClusterIP   10.43.17.137    &lt;none&gt;        8081/TCP,8084/TCP            8m37s\nargocd-server                             ClusterIP   10.43.40.122    &lt;none&gt;        80/TCP,443/TCP               8m37s\nargocd-server-metrics                     ClusterIP   10.43.40.210    &lt;none&gt;        8083/TCP                     8m37s\n\u279c  ~\n\n\u279c  ~ kubectl port-forward svc/argocd-server -n argocd 8080:443\nForwarding from [::1]:8080 -&gt; 8080\n\n[ leave one terminal as such .....]\n</code></pre> <p>Now, you can connect your localhost with the url http://localhost:8080</p> <p>Username:admin password:OaCPRywZZoH585nE</p>"},{"location":"cicd/argocd/install/#argocd-cli","title":"ArgoCD Cli","text":"<p>what can be done using cli</p> <ul> <li>manage application </li> <li>manage repos</li> <li>manage clusters</li> <li>admin tasks</li> <li>manage projects</li> <li>and more and more</li> </ul>"},{"location":"cicd/argocd/install/#installations","title":"installations","text":"<p>https://argo-cd.readthedocs.io/en/stable/cli_installation/</p> <pre><code>\u279c  ~ VERSION=$(curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/')\n\u279c  ~ echo \"${VERSION}\"\nv2.14.9\n\u279c  ~ curl -sSL -o argocd-darwin-amd64 https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-darwin-amd64\n\u279c  ~ sudo install -m 555 argocd-darwin-amd64 /usr/local/bin/argocd ;rm argocd-darwin-amd64\n\u279c \n\n\u279c  ~ argocd help | head\nargocd controls a Argo CD server\n\nUsage:\n  argocd [flags]\n  argocd [command]\n\nAvailable Commands:\n  account     Manage account settings\n  admin       Contains a set of commands useful for Argo CD administrators and requires direct Kubernetes access\n  app         Manage applications\n\u279c  ~\n</code></pre> <p>Login to cli using admin</p> <pre><code>\u279c  ~ argocd login localhost:8080\nWARNING: server certificate had error: tls: failed to verify certificate: x509: certificate signed by unknown authority. Proceed insecurely (y/n)? y\nUsername: admin\nPassword:\n'admin:login' logged in successfully\nContext 'localhost:8080' updated\n\u279c  ~\n\n\u279c  ~ argocd cluster list\nSERVER                          NAME        VERSION  STATUS   MESSAGE                                                  PROJECT\nhttps://kubernetes.default.svc  in-cluster           Unknown  Cluster has no applications and is not being monitored.\n\u279c  ~\n\n</code></pre>"},{"location":"cicd/argocd/install/#note","title":"Note","text":"<pre><code>Make sure that ArgoCD server endpoint is accessible whether using port-forward or ingress or load balancer service.\n\nexample: kubectl port-forward svc/argocd-server -n argocd 8080:443\n\nRemember to login into  ArgoCD using command argocd login.\n\nyou need admin user and password. remember that you can get the initial admin password from k8s secret \"argocd-initial-admin-secret\"\n\n\nexample : argocd login localhost:8080 --insecure\n\nThen you can apply cli commands.\n</code></pre>"},{"location":"cicd/argocd/overview/","title":"ArgoCD","text":"<p>Argo CD (short for Argo Continuous Delivery) is a declarative, GitOps-based continuous delivery tool for Kubernetes. It automates the deployment of the desired application states defined in Git repositories to Kubernetes clusters(actual state).</p>"},{"location":"cicd/argocd/overview/#core-concepts","title":"Core Concepts","text":"Concept Description GitOps Uses Git as the single source of truth for declarative infrastructure and applications. Application A Kubernetes Custom Resource (CR) in Argo CD that defines the desired state of an app. Sync The process of aligning the live Kubernetes cluster state with what's defined in Git. Drift Detection Identifies any difference between the declared state in Git and the actual state in the cluster. Reconciliation Keeps the cluster in sync with Git either manually or through automated syncing."},{"location":"cicd/argocd/overview/#architecture","title":"Architecture","text":"<p>Argo CD follows a controller-based architecture. It continuously monitors a Git repository and ensures that the Kubernetes cluster's state matches the state defined in Git.</p> <p></p> Component Description API Server Provides the REST and gRPC API. Handles user authentication, RBAC, and serves the CLI/UI. Repository Server Clones Git repositories, renders manifests (Helm, Kustomize, etc.), and makes them available to the controller. Application Controller Monitors application resources. Detects drift between desired (Git) and live (cluster) state and triggers sync. Dex (Optional) An identity service that enables SSO integration with providers like GitHub, LDAP, Google, etc. CLI / Web UI User interfaces to interact with Argo CD \u2014 <code>argocd</code> CLI for terminal, Web UI for visual management."},{"location":"cicd/argocd/overview/#gitops","title":"Gitops","text":"<p>GitOps is a modern DevOps practice that uses Git as the single source of truth for managing infrastructure and application deployment. It enables automated delivery of code and configuration into Kubernetes or any other environment through Git workflows.</p>"},{"location":"cicd/argocd/overview/#core-principles","title":"Core principles","text":"Principle Description Git as the source of truth All infrastructure and application configuration is versioned and stored in Git repositories. Declarative configuration The desired state of the system is defined declaratively (e.g., YAML files). Automated reconciliation A GitOps controller ensures the live state matches the desired state in Git, and takes action if there is a drift. Pull-based delivery Changes are pulled from Git <p>Benefits of GitOps</p> <p>Auditability: Every change is tracked in Git</p> <p>Consistency: No drift between environments</p> <p>Security: Pull-based model keeps CI/CD tooling outside of your cluster</p> <p>Speed &amp; Agility: Quick, safe, and repeatable deployments</p> <p>Self-healing systems: Automated correction of configuration drift</p>"},{"location":"cicd/argocd/sync/","title":"sync","text":""},{"location":"cicd/argocd/sync/#automated-syncing","title":"Automated syncing","text":"<ul> <li>by default, argocd polls git repo every 3 minutes to detect changes to the manifests. This will sync the desired manifests automaically to the live state in the cluster. </li> </ul> <p>Note: </p> <ul> <li>automated sync will be performed only if the application is <code>OutOfSync</code>. it will not re-attemps if the automated sync has been failed against the same SHA Commmit.</li> <li>Rollback cannot be performed against an application with automated sync enabled. </li> </ul> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --sync-policy automated\n</code></pre> <pre><code>#automated-sync.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-sync-app\n  namespace: argocd\nspec:\n  destination:\n    namespace: auto-sync-app\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated: {}\n    syncOptions:\n      - CreateNamespace=true\n</code></pre>"},{"location":"cicd/argocd/sync/#automated-pruning","title":"Automated pruning","text":"<p>Default: No prune enabled. </p> <p>autosync when enabled, for safety purpose we have <code>auto prune</code> disabled so that when sync happends we will not delete any resources when detected for any changes. but it can be enabled.  When using production please don't enable it. </p> <pre><code># automated-prune.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-pruning-demo\n  namespace: argocd\nspec: \n  destination:\n    namespace: auto-pruning-demo\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated: \n      prune: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --auto-prune\n</code></pre>"},{"location":"cicd/argocd/sync/#automated-self-healing","title":"Automated self-healing","text":"<p>by default, changes that are made to the live cluster will not trigger automated sync. argocd has a feature to enable self healing when the live cluster state deviates from the git state. </p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-selfheal-demo\n  namespace: argocd\nspec: \n  destination:\n    namespace: auto-selfheal-demo\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated:\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --self-heal\n</code></pre>"},{"location":"cicd/argocd/sync/#sync-options","title":"Sync options","text":""},{"location":"cicd/github_actions/cicd/","title":"CICD pipeline","text":""},{"location":"cicd/github_actions/cicd/#cicd-pipeline","title":"cicd pipeline","text":"<p>we would have complete version of the cicd pipeline. </p>"},{"location":"cicd/github_actions/cicd/#feature-to-develop-branch","title":"feature to develop branch","text":"<ul> <li>Developer would create a new <code>feature branch</code> and <code>push</code></li> <li>Once the push is detected, we would <code>run the workflow</code> against it. </li> <li>Once they are successful, you would <code>create a PR</code> for the <code>dev branch</code> where we run <code>workflow actions</code>.</li> <li>once the <code>job is successful</code>, we would <code>deploy code in the staging server</code>. </li> </ul>"},{"location":"cicd/github_actions/cicd/#develop-to-master-branch","title":"develop to master branch","text":"<ul> <li>we would create a<code>new PR from dev branch to master</code>, which makes the <code>actions to run</code></li> <li>On successful run, we would deploy code to the production server. </li> </ul>"},{"location":"cicd/github_actions/cicd/#post-deployments","title":"Post deployments","text":"<ul> <li>Job failures, we would create a new issue </li> <li>successful job, would send a slack message</li> <li>successful release, we would send a slack message</li> </ul>"},{"location":"cicd/github_actions/cicd/#pull_requests","title":"pull_requests","text":"<p>As we discussed earlier, we can't push to <code>master</code> or <code>develop</code> using the branch. hence we need to create a new <code>feature</code> branch and requires a <code>pull request</code> to the respective branches. You need to update settings on the <code>branch protection rules</code> for both <code>develop and master</code></p> <p></p> <p>Here is an example of python project, which builds and let you merge the pull request upon successful</p> <pre><code>name: ci\non:\n    pull_request: \n        branches: \n          - develop # PR created from feature to develop\n    push:\n        branches:\n          - develop # merge PR (push to develop branch)\njobs:\n    build:\n        runs-on: ubuntu-latest\n        steps:\n            - name: repository checkout\n              uses: actions/checkout@v3\n\n            - name: python3.10 setup \n              uses: actions/setup-python@v4\n              with:\n                python-version: '3.10'\n                check-latest: true\n\n            - name: install dependencies\n              run: |\n                pip install fastapi[\"all\"]\n                if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\n            - name: cache dependencies\n              uses: actions/cache@v3\n              with:\n                path: ~/.cache/pip\n                key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n                restore-keys: |\n                  ${{ runner.os }}-pip-\n\n            - name: code formating\n              run: flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n            - name: run testcases\n              run: pytest\n\n            - name: test coverage report\n              run: pytest --cov=./app ./tests\n\n            - name: upload artifacts test coverage report\n              uses: actions/upload-artifact@v1\n              with:\n                name: code-coverage\n                path: ./.coverage\n\n            - name: build fastapi docker image\n              if: github.event_name == 'push'\n              run: docker build -t '${{ secrets.DOCKER_LOGIN }}'/demo-fastapi:'${{ github.sha }}' .\n\n            - name: login to dockerhub\n              if: github.event_name == 'push'\n              run: docker login --username '${{ secrets.DOCKER_LOGIN }}' --password '${{ secrets.DOCKER_PASSWORD }}'\n\n            - name: push image to dockerhub\n              if: github.event_name == 'push'\n              run: docker push '${{ secrets.DOCKER_LOGIN }}'/demo-fastapi:'${{ github.sha }}'\n</code></pre>"},{"location":"cicd/github_actions/intro/","title":"Introduction","text":"<p>In this intro page, we would look for couple of examples that we learnt from the overview section. </p>"},{"location":"cicd/github_actions/intro/#examples","title":"examples","text":"<p>As part of testing, create a new file in <code>.github/workflows/example.yml</code> on push to repository. Check from your <code>actions</code> of the github repository to see the results overview. </p> <pre><code>name: example\nrun-name: example\non: push\njobs:\n    demo:\n        runs-on: ubuntu-latest\n        steps:\n            - name: first step\n              run: echo \"Hello world !\"\n\n            - name: second step\n              run: echo \"Hello World again !\"\n</code></pre> <p>For, any python application in the repository, you need to check for the <code>requirements.txt</code> file and add all your dependencies to it, which will do repository checkout, python setup, install libs, setup linter and unit testing.</p> <pre><code>name: python application\non:\n    push:\n        branches: [\"master\"]\njobs:\n    preinstall:\n        runs-on: ubuntu-latest\n        steps:\n            - name: repository checkout\n              uses: actions/checkout@v3\n\n            - name: python3.10 setup \n              uses: actions/setup-python@v4\n              with:\n                python-version: '3.10'\n                check-latest: true\n\n            - name: install dependencies\n              run: |\n                pip install fastapi[\"all\"]\n                if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\n            - name: install flake8\n              run: |\n                flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n            - name: test with pytest\n              run: |\n                pytest   \n</code></pre> <p>We can checkout our repository using the steps, however we can do the same things using <code>uses</code> or <code>checkout</code></p> <pre><code>name: checkout the git repo in the runner machine\non:\n  - push\njobs:\n    checkout_repo:\n    runs-on: ubuntu-latest\n    steps:\n      - name: download the current repo\n        run: |\n          git init \n          git remote add origin \"https://$GITHUB_ACTOR:${{ secrets.GITHIB_TOKEN }}@github.com/$GITHIB_REPOSITORY.git\"\n          git fetch origin \n          git checkout main\n\n      - name: list all the files from download repository\n        run: ls -a          \n</code></pre> <ul> <li>using <code>uses</code> to download any repo</li> </ul> <p><code>yaml   name: download the repo   on:     - push   jobs:       checkout_repo:       runs-on: ubuntu-latest       steps:         - name: download the repository           uses: actions/&lt;your repository name&gt;@[&lt;commit_id&gt;|&lt;tag&gt;|&lt;release_version&gt;]         - name: list the files from repository           run: ls -a</code></p> <ul> <li>using <code>checkout</code> to download the current repo, using <code>with</code> you can specify any repository you need to download</li> </ul> <p>```yaml     name: download the current repo     on:       - push     jobs:       checkout_repo_1:       runs-on: ubuntu-latest       steps:         - name: files before checkout repository           run: ls -a</p> <pre><code>    - name: download the repository\n      uses: actions/checkout@v3\n\n    - name: files after checkout repository\n      run: ls -a\n</code></pre> <p>```</p> <p>References: https://github.com/actions/checkout </p>"},{"location":"cicd/github_actions/matrix_strategy/","title":"Matrix stratergy","text":"<p>We will be discussing more on the Startergy, matrix and docker container in Jobs. </p>"},{"location":"cicd/github_actions/matrix_strategy/#matrix","title":"matrix","text":"<p>Incase we need to run with specific version, then we can use <code>uses</code> to defined the versions. </p> <pre><code>name: versions\non:\n    push\n\njobs:\n    node_version:\n        runs-on: ubuntu-latest\n        steps:\n            - name: log node version\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: 6\n\n            - name: log node version\n              run: node -v\n</code></pre> <p>If we need to run multiple jobs for multiple versions, then you need to use <code>strategy</code> and specify in the array to run all of those parallel. </p> <pre><code>name: strategy matrix\non:\n    push\n\njobs:\n    node_version:\n        strategy:\n            matrix:\n                node_version: [6, 7, 8]\n                os: [\"ubuntu-latest\",\"windows-latest\"]\n        runs-on: ${{ matrix.os }}\n        steps:\n            - name: log node version installed in ubuntu\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: ${{ matrix.node_version}}\n\n            - name: log node version currenly set to\n              run: node -v\n</code></pre> <p>Now, you will have totally 6 jobs running at once. </p> <p></p>"},{"location":"cicd/github_actions/matrix_strategy/#include-and-exclude-matrix","title":"include and exclude matrix","text":"<p>Incase we do get a situation, that we need to exclude some of the versions and include few version, then we can use <code>include</code> and <code>exclude</code> stratergy to run jobs.</p> <pre><code>name: strategy matrix\non:\n    push\n\njobs:\n    node_version:\n        strategy:\n            matrix:\n                node_version: [6, 7, 8]\n                os: [\"ubuntu-latest\",\"windows-latest\"]\n                exclude:\n                    - os: ubuntu-latest\n                      node_version: 6\n                    - os: ubuntu-latest\n                      node_version: 8\n                include:\n                    - os: ubuntu-latest\n                      node_version: 9\n                      is_ubuntu_9: 9\n\n        env:\n            IS_UBUNTU_9: ${{ matrix.is_ubuntu_9 }}\n        runs-on: ${{ matrix.os }}\n        steps:\n            - name: log node version installed in ubuntu\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: ${{ matrix.node_version}}\n\n            - name: log node version currenly set to\n              run: |\n                node -v\n                echo $IS_UBUNTU_9\n\n</code></pre> <p></p>"},{"location":"cicd/github_actions/matrix_strategy/#run-from-docker-containers","title":"run from docker containers","text":"<p>Till now, we had runners working from the virtual machines, but now we want docker container to initialize and we would run steps from inside container. </p> <pre><code>name: container\non: push \n\njobs:\n    node-docker:\n        runs-on: ubuntu-latest\n        container:\n            image: node:20-alpine3.17\n        steps:\n            - name: log node version\n              run: |\n                node -v \n                cat /etc/os-release\n</code></pre>"},{"location":"cicd/github_actions/overview/","title":"Overview","text":""},{"location":"cicd/github_actions/overview/#github-actions-components","title":"GitHub actions components","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.</p> <p>You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container.</p>"},{"location":"cicd/github_actions/overview/#workflow","title":"Workflow","text":"<p>A workflow is a configurable automated process that will run one or more jobs, Workflows are defined by a YAML file checked in to your repository(.github/workflows) and will run when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule.</p>"},{"location":"cicd/github_actions/overview/#events","title":"Events","text":"<p>An event is a specific activity in a repository that triggers a workflow run.  e.g activity can originate from GitHub when someone creates a pull request, opens an issue, or pushes a commit to a repository. </p>"},{"location":"cicd/github_actions/overview/#jobs","title":"Jobs","text":"<p>A job is a set of steps in a workflow that is executed on the same runner. Each step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other. Since each step is executed on the same runner, you can share data from one step to another</p>"},{"location":"cicd/github_actions/overview/#actions","title":"Actions","text":"<p>An action is a custom application for the GitHub Actions platform that performs a complex but frequently repeated task.Use an action to help reduce the amount of repetitive code that you write in your workflow files</p>"},{"location":"cicd/github_actions/overview/#runners","title":"Runners","text":"<p>A runner is a server that runs your workflows when they're triggered. Each runner can run a single job at a time. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows; each workflow run executes in a fresh, newly-provisioned virtual machine.</p> <pre><code>name: learn-github-actions\nrun-name: ${{ github.actor }} is learning GitHub Actions\non: [push]\njobs:\n  check-bats-version:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3 # checkout code in runner.\n      - uses: actions/setup-node@v3 # install specified version 14 in runner and sets $PATH\n        with:\n          node-version: '14'\n      - run: npm install -g bats # run the command\n      - run: bats -v # run the command\n</code></pre> <p></p>"},{"location":"cicd/github_actions/overview/#references","title":"References","text":"<p>GitHub Actions Documentation</p>"},{"location":"cicd/github_actions/variables/","title":"Variables","text":""},{"location":"cicd/github_actions/variables/#default-custom-env-vars","title":"Default &amp; custom env vars","text":"<p>Variables provide a way to store and reuse non-sensitive configuration information.</p> <p>You can set your own custom variables or use the default environment variables that GitHub sets automatically. For more information, see Default environment variables.</p> <pre><code>name: custom vars\non:\n    push\nenv:\n    WF_ENV: env for workflow\n\njobs:\n    log-env:\n        runs-on: ubuntu-latest\n        env: \n            JOB_ENV: env for job\n        steps:\n            - name: display all envs\n              env:\n                STEP_ENV: env for step\n              run: |\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n\n            - name: display workflow and job env\n              run: |\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n    log-default-env:\n        runs-on: ubuntu-latest\n        steps:\n            - name: display all default envs\n              run: |\n                echo \"GITHUB_ACTOR: ${GITHUB_ACTOR}\"\n                echo \"GITHUB_JOB: ${GITHUB_JOB}\"\n                echo \"RUNNER_ARCH: ${RUNNER_ARCH}\"\n                echo \"RUNNER_OS: ${RUNNER_OS}\"\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n\n</code></pre> <p>References: about variables</p>"},{"location":"cicd/github_actions/variables/#secrets-variables","title":"secrets variables","text":"<p>incase you need to add secrets to the workflow, in your github, settings-&gt;Secrets and Variables-&gt;add secrets vars and save it. You can ue those values in your workflow, so that it won't be able to display in the output. </p> <pre><code>on:\n    push\nenv:\n    WF_ENV: ${{ secrets.WF_ENV }} # reference the secrets\n\njobs:\n    log-env:\n        runs-on: ubuntu-latest\n        env: \n            JOB_ENV: env for job\n</code></pre>"},{"location":"cicd/github_actions/variables/#calling-rest-api-to-create-issue","title":"Calling REST API to create issue","text":"<p>You can use the GITHUB_TOKEN to make authenticated API calls. This example workflow creates an issue using the GitHub REST API</p> <pre><code>jobs:\n  create_issue:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    steps:\n      - name: Create issue using REST API\n        run: |\n          curl --request POST \\\n          --url https://api.github.com/repos/${{ github.repository }}/issues \\\n          --header 'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}' \\\n          --header 'content-type: application/json' \\\n          --data '{\n            \"title\": \"Automated issue for commit: ${{ github.sha }}\",\n            \"body\": \"This issue was automatically created by the GitHub Action workflow **${{ github.workflow }}**. \\n\\n The commit hash was: _${{ github.sha }}_.\"\n            }' \\\n          --fail\n</code></pre>"},{"location":"cicd/github_actions/variables/#pull-and-push-using-github_token-variable","title":"Pull and push using GITHUB_TOKEN variable","text":"<pre><code>jobs:\n    create_issue:\n        runs-on: ubuntu-latest\n        permissions:\n            write-all\n        steps:\n          - name: push a random file\n            run: |\n                git init \n                git remote add origin \"https://samperay:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY.git\"\n                git config --global user.email \"samperay@gmail.com\"\n                git config --global user.name \"samperay\"\n                git fetch\n                git checkout master\n                git branch --set-upstream-to=origin/master\n                git pull \n                ls -la\n                echo $RANDOM &gt;&gt; random.txt\n                ls -la\n                git add -A\n                git commit -m \"added random file\"\n                git push \n</code></pre> <p>you can define access and scopes for the GITHUB_TOKEN</p>"},{"location":"cicd/github_actions/variables/#encrypting-and-decrypting-files","title":"Encrypting and Decrypting files","text":"<p>we could use the file to store credentials if data &gt;64KB where we store api_username or api_token.  we could encrypt those file and later in the workflow we can decrypt to use those. Create PASSPHRASE in the github secrets and use that for decryption for the files to get api_username or api_token. </p> <pre><code>vim secrets.json\n{\n    api_username: 'assasas'\n    api_token: 'as8qwe78qwshhshhwhsd_sd!e7we'\n}\n\ngpg --symmetric --cipher-algo AES256 /tmp/secrets.json\ngit add /tmp/secrets.json.gpg\ngit commit -m 'added secretfile'\ngit push\n</code></pre> <pre><code>on:\n    push\n\njobs:\n    decrypt: \n        runs-on: ubuntu-latest\n        steps:\n            - uses: actions/checkout@v1\n\n            - name: Decrypt file\n              run: gpg --quiet --batch --yes --decrypt --passphrase=\"$PASSPHRASE\" --output $HOME/secret.json secrets.json.gpg\n              env: \n                PASSPHRASE: ${{ secrets.PASSPHRASE }}\n\n            - name: print file secret contents\n              run: cat $HOME/secret.json\n</code></pre> <p>References: encrypted secrets</p>"},{"location":"cicd/github_actions/variables/#contexts","title":"Contexts","text":"<p>Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects.</p> <pre><code>name: Context testing\nruns-name: \"contexts example\"\non: push\n\njobs:\n  dump_contexts_to_log:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - name: Dump job context\n        env:\n          JOB_CONTEXT: ${{ toJson(job) }}\n        run: echo \"$JOB_CONTEXT\"\n      - name: Dump steps context\n        env:\n          STEPS_CONTEXT: ${{ toJson(steps) }}\n        run: echo \"$STEPS_CONTEXT\"\n      - name: Dump runner context\n        env:\n          RUNNER_CONTEXT: ${{ toJson(runner) }}\n        run: echo \"$RUNNER_CONTEXT\"\n      - name: Dump strategy context\n        env:\n          STRATEGY_CONTEXT: ${{ toJson(strategy) }}\n        run: echo \"$STRATEGY_CONTEXT\"\n      - name: Dump matrix context\n        env:\n          MATRIX_CONTEXT: ${{ toJson(matrix) }}\n        run: echo \"$MATRIX_CONTEXT\"\n\n</code></pre> <p>References: context</p>"},{"location":"cicd/github_actions/variables/#functions-and-expressions","title":"Functions and expressions","text":"<p>You can use expressions to programmatically set environment variables in workflow files and access contexts. An expression can be any combination of literal values, references to a context, or functions</p> <pre><code>  functions_expressions:\n    runs-on: ubuntu-latest\n    steps:\n        - name: expressions\n          run: |\n            echo ${{ contains(fromJSON('[\"push\", \"pull_request\"]'), github.event_name) }}\n            echo ${{ startsWith('Hello world', 'He') }}\n            echo ${{ endsWith('Hello world', 'ld') }}\n\n  func_status_checks:\n    runs-on: ubuntu-latest\n    steps:\n        - name: step to fail\n          run: echooo \"hello\"\n\n        - name: run even if failed\n          if: failure()\n          run: echo \"I need this step to execute even if previous step fails\"\n\n        - name: run always\n          if: always()\n          run: echo \"this step will always run, success or failed step\"\n\n        - name: run success\n          run: echo \"run when none of previous step failed\" \n          if: success()\n\n        - name: runs cancel\n          run: echo \"runs on cancel job\"\n          if: cancelled()\n</code></pre> <p>Note: line # 216, if we add <code>continue-on-error: true</code> we don't need line # 219.  Both functions work in same manner.</p> <p>References: expressions</p>"},{"location":"cicd/github_actions/workflow_triggers/","title":"Triggers","text":"<p>In this section, we will discuss about the Events, Schedules, External Events &amp; Filters which can trigger the workflow run. </p>"},{"location":"cicd/github_actions/workflow_triggers/#triggerring-workflow-with-events","title":"Triggerring workflow with events","text":"<p>We will create a new branch e.g(develop) and once created we would want the actions to run when we try to create a new PR. so we would add the event for <code>pull_request</code> which will trigger the workflow actions. </p> <p>There are types involved in the <code>pull_request</code> which tells as to what activities on PR would need your workflow to trigger run actions. e.g, when you <code>close</code> | <code>reopen</code> | <code>assign</code> ..etc</p> <p>here is the link for list of activities for pull_request</p> <p>we can also set the runners to run the terminal which we choose by default for entire workflow or for induvidual steps.  e.g even runnners from <code>windows-latest</code> or <code>ubuntu-latest</code> it would choose only which we set as default one's</p> <pre><code>name: actions workflow\nrun-name: actions workflow\non: \n    push:\n    pull_request: \n        types: [opened, closed, assigned, reopened] # you can specify the activity types.\n\n# choose the default shell as bash\ndefault:\n  run:\n    shell: bash    \n\njobs:\n    run-github-actions:\n        runs-on: ubuntu-latest\n        steps:\n            - name: list files\n              run: |\n                pwd\n                ls\n            - name: checkout\n              uses: actions/checkout@v1\n            - name: list files after checkout \n              run: |\n                pwd\n                ls -a\n            - name: simple JS actions\n              id: greet\n              uses: actions/hello-world-javascript-action@v1\n              with:\n                who-to-greet: John\n            - name: Log greeting time\n              run: echo \"${{ steps.greet.outputs.time }}\"\n\n    windows-actions:          \n      runs-on: windows-latest\n\n      # we are trying to overide the default shell from 'bash' to 'powershell'\n      default:\n        run:\n         shell: pwsh\n\n      steps: \n        - name: display dirrectory\n          run: dir\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#needs","title":"needs","text":"<p>Identify any jobs that must complete successfully before another job will run.</p> <p>e.g, when you have two or more jobs to run, and you have a scnerio that you must complete the first job and then it should start second job. in that case, you would use the key word <code>needs</code></p> <pre><code>jobs:\n  run-shell-cmds:\n    runs-on: ubuntu-latest\n    steps:\n      - name: echo string\n        run: echo \"hello world\"\n\n      - name: multiline script\n        run: |\n          node -v\n          npm -v \n\n      - name: python command \n        run: | \n          import platform\n          print(platform.processor())\n        shell: python\n\n  run-windows-cmds:\n    runs-on: windows-latest\n    needs: \n      - run-shell-cmds\n</code></pre> <p>References: needs</p>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-scheduler","title":"trigger using scheduler","text":"<p>you can use <code>cron</code> scheduler to run the action  workflow. </p> <p>Cron scheduler to run for every 5 mins, thats the minimum shortest interval you can run scheduled workflows.</p> <pre><code>name: actions workflow\nrun-name: actions workflow by @{{ github.actor}}, ${{ github.event_name }}\non: \n    schedule:\n      # you can defined multiple cron schedulers\n      - cron: \"*/5 * * * *\"\n      - cron: \"0 14 * * *\"\n    # push:\n    pull_request: \n      types: [opened, closed, assigned, reopened]\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-dispatch_events","title":"trigger using dispatch_events","text":"<p>You can use the GitHub API to trigger a webhook event called repository_dispatch when you want to trigger a workflow for activity that happens outside of GitHub. </p> <p>Create a new access token for the authorization for the repo, and you can trigger it using curl or postman, on which your build would run. </p> <p>Postman configurations:</p> <pre><code>Authorization -&gt; Select Basic Auth -&gt; Password: sdhdjhdhd\n\nHeaders: \n\nContent-Type: application/json\nAccept: application/json\n\nBody: raw -&gt; select JSON\n\n{\n    \"event_type\": \"build\",\n        \"client_payload\": {\n            \"env\" : \"production\"\n  }\n}\n\n</code></pre> <p>You can write any keyword for the <code>event_type</code> which you can call in the github action workflow.</p> <pre><code>on:\n  repository_dispatch:\n    types: [build]\n  pull_request: \n    types: [opened, closed, assigned, reopened]\n\njobs:\n    run-github-actions:\n        runs-on: ubuntu-latest\n        steps:\n            - name: webhook trigger build\n              run: echo \"${{ github.event.client_payload.env }}\"\n</code></pre> <p>On triggring the event in the postman, your build should be running.</p> <p>References: </p> <p>create-a-repository-dispatch-event</p> <p>repository_dispatch</p>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-branches-tags-paths","title":"trigger using branches, tags, paths","text":"<p>you can use specific branch, tags and paths to execute scripts based on the actions.  Note: You can't use including or excluding at same time</p> <p>including</p> <ul> <li>branches</li> <li>tags</li> <li>paths</li> </ul> <p>excluding</p> <ul> <li>branches-ignore</li> <li>tags-ignore</li> <li>paths-ignore</li> </ul> <p>while you merge changes form one branch to another, and incase your target branch is <code>dev</code> or <code>feature</code> then your PR would be running state. </p> <pre><code>on:\n  push: \n    branches: \n      - \"!dev1\" # don't run actions if push is from dev1 branch\n      - \"dev\" # run actions if push from dev branch\n\n  pull_request:\n    types:\n      - opened\n    branches:\n      - \"dev\"\n      - \"feature/*\" # feature/featureA, feature/featureB\n      - \"feature/**\" # feature/feat/A, feature/feat/B\n      - \"!dev1\" # don't run actions incase I create PR from current branch to dev1\n</code></pre> <p>References: Filter Usage</p>"},{"location":"cicd/github_actions/workflow_triggers/#skip-actions","title":"skip actions","text":"<p>incase you don't want to run your job while commiting, you need to update below message while <code>git commit</code></p> <pre><code>git commit -m 'your change[actions skip]'\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#display-runner-logs","title":"display runner logs","text":"<p>while you run the workflow, you need to mark the lines as errors for display purpose in the runner. We can also mask the secrets and group logs</p> <pre><code>steps:\n    - name: Display error\n      run: echo \"::error:: your error message\"\n\n    - name: Display warning\n      run: echo \"::warning:: your error message\"\n\n    - name: Display debug\n      run: echo \"::debug:: your error message\"\n\n    - name: Display notice\n      run: echo \"::notice:: your error message\"\n\n    - name: diplay group of logs\n      run: |\n        echo \"::group:: group title logs\n        echo \"write all the logs - 1\"\n        echo \"write up logs -2 \"\n        echo \"::endgroup::\"\n\n    - name: mask the secrets\n      run: echo \"::add-mask::my-password\"\n\n    # it would show as ***\n    - name: display my secret password\n      run: echo \"my-password\"\n</code></pre> <p>References: https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions</p>"},{"location":"cicd/github_actions/workflow_triggers/#parent-child-workflow-triggers","title":"parent child workflow triggers","text":"<p>When you have two jobs and you need to run the job after one workflow has been completed, you can let the new job know about it.</p> <p>Example as below</p> <pre><code>#simple.yml(parent workflow)\nname: simple\non:\n    push\njobs:\n    simple:\n        runs-on: ubuntu-latest\n        steps:\n            - name: parent workflow\n              run: echo \"hello world\"\n\n\n#deps.yml(child.yml)\nname: depsjod\non:\n    workflow_run:\n        workflows: [simple]\n        types: [completed]\njobs:\n    simple_deps_jobs:\n        runs-on: ubuntu-latest\n        steps:\n          - name: child workflow\n            run: echo \"running child job after parent job.\"\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-workflow-manually","title":"trigger workflow manually","text":"<p>You can configure custom-defined input properties, default input values, and required inputs for the event directly in your workflow. you need to go to the UI and run the job manually by providing the inputs</p> <pre><code>name: manual-runjob\non:\n    workflow_dispatch:\n      inputs:\n        logLevel:\n          description: 'Log level'\n          required: true\n          default: 'warning'\n          type: choice\n          options:\n          - info\n          - warning\n          - debug\n        tags:\n          description: 'Test scenario tags'\n          required: false\n          type: boolean\n        envs:\n          description: 'environments'\n          type: environment\n          required: true\njobs:\n    log-the-inputs:\n        runs-on: ubuntu-latest\n        steps:\n        - run: |\n            echo log_level: ${{ inputs.logLevel }}\n            echo tags: ${{ inputs.tags }}\n            echo env: ${{ inputs.envs }}\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#environment-files-and-env","title":"environment files and env","text":"<p>During the execution of a workflow, the runner generates temporary files that can be used to perform certain actions. The path to these files are exposed via environment variables.  </p> <p>You can make an environment variable available to any subsequent steps in a workflow job by defining or updating the environment variable and writing this to the GITHUB_ENV environment file. The step that creates or updates the environment variable does not have access to the new value, but all subsequent steps in a job will have access.</p> <pre><code>steps:\n  - name: Set the value\n    id: step_one\n    run: |\n      echo \"action_state=yellow\" &gt;&gt; \"$GITHUB_ENV\"\n  - name: Use the value\n    id: step_two\n    run: |\n      printf '%s\\n' \"$action_state\" # This will output 'yellow'\n\n## multiline string\n\nsteps:\n  - name: Set the value in bash\n    id: step_one\n    run: |\n      {\n        echo 'JSON_RESPONSE&lt;&lt;EOF'\n        curl https://example.com\n        echo EOF\n      } &gt;&gt; \"$GITHUB_ENV\"\n</code></pre>"},{"location":"cicd/jenkins/deploy/","title":"Virtual Server","text":"<p>Application: https://github.com/samperay/jenkins-project/tree/single-server-deploy</p> <p>Spinned up a new ec2 machine(prod-server) where the application is deployed. we would be configuring the application to have an Continious deployment. This is on a <code>single-server-deployment</code> </p> <pre><code>mkdir /home/ec2-user/\ncd /home/ec2-user/app\npython3 -m venv venv\nvi /etc/systemd/system/flaskapp.service\n\n[Unit]\nDescription=flask app \nAfter=network.target\n\n[Service]\nUser=ec2-user\nGroup=ec2-user\nWorkingDirectory=/home/ec2-user/app/\nEnvironment=\"PATH=/home/ec2-user/app/venv/bin\"\nExecStart=/home/ec2-user/app/venv/bin/python3 /home/ec2-user/app/app.py\n\n[Install]\nWantedBy=multi-user.target\n\n\nsudo systemctl enable flaskapp.service\nsudo systemctl start flaskapp.service\n</code></pre>"},{"location":"cicd/jenkins/deploy/#jenkins-server-installation","title":"Jenkins server installation","text":"<p>EC2 machine has been spinned <code>t2.medium</code> without which the jenkins server won't be able to schedule the job.</p>"},{"location":"cicd/jenkins/deploy/#installation","title":"Installation","text":"<pre><code>sudo yum update \u2013y\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\nsudo yum install java-17-amazon-corretto docker git pip pytest -y\nsudo yum install jenkins -y\nsudo systemctl enable jenkins\nsudo systemctl start jenkins\nsudo systemctl status jenkins\nsudo systemctl start docker.service\nsudo systemctl enable docker.service\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\nsudo usermod -a -G docker ec2-user\nsudo usermod -a -G docker jenkins\nsudo chown jenkins /var/run/docker.sock\n</code></pre>"},{"location":"cicd/jenkins/deploy/#pipelines","title":"Pipelines","text":"<p>In Jenkins configure the <code>pipeline</code> job, and run the build to see the deployed version. https://github.com/samperay/jenkins-project/tree/single-server-deploy</p> <pre><code>pipeline {\n    agent any \n\n    environment {\n        SERVER_IP = credentials('prod-server-ip')\n    }\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh \"pip install -r requirements.txt\"\n            }\n        }\n\n        stage('Test') {\n            steps {\n                sh \"pytest\"\n            }\n        }\n\n        stage('Package code') {\n            steps {\n                sh \"zip -r myapp.zip ./* -x '*.git*'\"\n                sh \"ls -lart\"\n            }\n        }\n\n        stage(\"Deploy to Prod\") {\n            steps {\n                withCredentials([sshUserPrivateKey(credentialsId: 'ssh-key', keyFileVariable: 'MY_SSH_KEY', usernameVariable: 'username')]) {\n                    sh '''\n                    scp -i $MY_SSH_KEY -o StrictHostKeyChecking=no myapp.zip ${username}@${SERVER_IP}:/home/ec2-user/\n                    ssh -i $MY_SSH_KEY -o StrictHostKeyChecking=no ${username}@${SERVER_IP} &lt;&lt; EOF \n                      unzip -o /home/ec2-user/myapp.zip -d /home/ec2-user/app/\n                      source app/venv/bin/activate\n                      cd /home/ec2-user/app\n                      pip install -r requirements.txt\n                      sudo systemctl restart flaskapp.service\n                    '''\n                }\n            }\n        }\n\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/deploy/#docker","title":"Docker","text":"<p>In Jenkins configure the <code>pipeline</code> job to build and push the docker image to hub.  https://github.com/samperay/jenkins-project/tree/docker-image</p> <pre><code>pipeline \n{\n    agent any\n\n    environment {\n        IMAGE_NAME='sunlnx/jenkins-flask-app'\n        IMAGE_TAG=\"${IMAGE_NAME}:${env.GIT_COMMIT}\"\n    }\n\n    stages \n    {\n\n        stage('Setup') \n        {\n            steps {\n                sh \"pip install -r requirements.txt\"\n            }\n        }\n\n        stage('Test')\n        {\n            steps {\n                sh \"pytest\"\n            }\n        }\n\n                stage('Build Docker Image') \n        {\n            steps {\n                sh 'docker build -t ${IMAGE_TAG} .'\n                echo \"Docker image build successfully\"\n                sh 'docker images ls'\n            }\n        }\n\n        stage('Login to Dockerhub') \n        {\n            steps {\n                withCredentials([usernamePassword(credentialsId: 'docker-creds', usernameVariable: 'USERNAME', passwordVariable: 'PASSWORD')]) {\n\n                sh 'echo ${PASSWORD} | docker login -u ${USERNAME} --password-stdin' \n                echo 'login successful'\n                }\n\n            }\n        }\n\n        stage('Push Docker image') \n        {\n            steps {\n                sh \"docker push ${IMAGE_TAG}\"\n                echo \"Docker image pushed successfully.\"\n            }\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"cicd/jenkins/deploy/#multipipeline-repo-setup","title":"multipipeline repo setup","text":""},{"location":"cicd/jenkins/overview/","title":"Overview","text":""},{"location":"cicd/jenkins/overview/#build-trigger","title":"Build Trigger","text":"<p>Trigger builds remotely</p> <p>Enable this option if you would like to trigger new builds by accessing a special predefined URL. you will pre-define the token and will pass that to the specific url to trigger build</p> <p><code>JENKINS_URL/job/python-app/build?token=TOKEN_NAME</code></p> <p>Build after other projects are built</p> <p>Set up a trigger so that when some other projects finish building, a new build is scheduled for this project</p> <p>Build periodically</p> <p>Cron job</p> <p>Build when a change is pushed to Gogs</p> <p>when we commit the change, this would trigger the build</p> <p>Poll SCM</p> <p>Configure Jenkins to poll changes in SCM. Costly ops as it should scan the entire workspace. </p> <p>Reference: https://wiki.jenkins.io/display/JENKINS/Building-a-software-project.html</p>"},{"location":"cicd/jenkins/overview/#sample-jenkinsfile","title":"Sample Jenkinsfile","text":"<p>Simple Jenkinsfile</p> <pre><code>pipeline {\n    agent any \n    stages{\n        stage('Bulid') {\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#environments-jenkinsfile","title":"Environments Jenkinsfile","text":"<p>Global environment variables set</p> <pre><code>pipeline {\n    agent any \n\n    // env defined globally for all the stages\n    // we must avoid env to be set global\n\n    environment {\n        DB_HOSTNAME = 'rds-db.example.com'\n        PORT=3023\n        DB_USERNAME = 'dbuser'\n        DB_PASSWORD = 'password123'\n    }\n    stages{\n        stage('Bulid') {\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"stage:test db_username: ${DB_USERNAME}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Define env vars locally </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('Bulid') {\n                environment {\n                DB_HOSTNAME = 'rds-db.example.com'\n                PORT=3023\n                DB_USERNAME = 'dbuser'\n                DB_PASSWORD = 'password123'\n            }\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        // this stage won't have access for the vars and hence the build will be failed.\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"stage:test db_username: ${DB_USERNAME}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Builtin env </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('Bulid') {\n                environment {\n                DB_HOSTNAME = 'rds-db.example.com'\n                PORT=3023\n                DB_USERNAME = 'dbuser'\n                DB_PASSWORD = 'password123'\n            }\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"build number ${env.BUILD_ID}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://wiki.jenkins.io/display/JENKINS/Building-a-software-project.html#Buildingasoftwareproject-belowJenkinsSetEnvironmentVariables</p> <p>Accessing credentials</p> <p>There are two types of credentials</p> <ol> <li>System - Defined for specific projects etc </li> <li>Global - Defined for all the projects</li> </ol> <p>I am using <code>Global</code> I will create <code>username and password</code> with id as <code>prod-server</code> for the credentials and use them for the Jenkinsfile</p> <pre><code>pipeline {\n    agent any\n\n    environment {\n        PROD_SERVER = credentials('prod-server')\n    }\n\n    stages {\n        stage('Build') {\n            steps {\n                echo \"running build stage\"\n                echo \"${PROD_SERVER}\"\n                echo \"${PROD_SERVER_USR}\"\n                echo \"${PROD_SERVER_PSW}\"\n            }\n        }\n    }\n}\n</code></pre> <p>when used <code>PROD_SERVER_USR</code> and <code>PROD_SERVER_PSW</code> it will display username and password. but its not recommended to use as it would provide warning message in the jenkins console. </p> <p>we must be using Jenkins Binding Credentials. </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('build') {\n            steps {\n                echo 'build stage'\n                withCredentials([usernamePassword(credentialsId: 'prod-server', usernameVariable: 'myusername', passwordVariable: 'mypassword')])\n                {\n                    sh '''\n                    echo username: ${myusername}\n                    echo password: ${mypassword}\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://www.jenkins.io/doc/pipeline/steps/credentials-binding/#withcredentials-bind-credentials-to-variables</p>"},{"location":"cicd/jenkins/overview/#nested-and-parallel-stages","title":"Nested and parallel stages","text":"<p>Nested</p> <pre><code>pipeline {\n    agent any\n    stages{\n        stage('Linting and Testing'){\n            stages {\n                stage('lint'){\n                    steps {\n                        echo \"linting stage\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        echo \"testing stage\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre> <p>Parallel</p> <p>when you write <code>parallel</code> in the stages, it would be executed at same time.</p> <pre><code>pipeline {\n    agent any\n    stages{\n        stage('Linting and Testing'){\n            parallel {\n                stage('lint'){\n                    steps {\n                        sh \"sleep 30\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        sh \"sleep 30\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#pipeline-options","title":"Pipeline Options","text":"<p>timeout - how long does the job needs to be run before timing out.</p> <pre><code>pipeline {\n    agent any\n\n    options {\n        timeout(time:1 ,unit: 'MINUTES')\n    }\n\n    stages{\n        stage('Linting and Testing'){\n            parallel {\n                stage('lint'){\n                    steps {\n                        sh \"sleep 70\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        sh \"sleep 70\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#parameters","title":"parameters","text":"<p>You can customize the behaviour of the pipeline by passing in some data when we trigger a build. like env=dev/stage/prod..</p> <pre><code>pipeline {\n    agent any\n\n    parameters { \n        choice(name: 'ENVIRONMENT', choices: ['dev', 'stage', 'prod'], description: 'Deployment environment')\n        booleanParam(name: 'RUN_TESTS', defaultValue: true, description: 'Run tests in pipeline')\n\n    }\n\n    stages{\n        stage('Test'){\n            when {\n                expression {\n                    params.RUN_TESTS == true\n                }\n            }\n\n            steps {\n                echo \"RUN_TESTS is true, we will execute these steps\"\n            }\n        }\n\n        stage('Development Deploy Stage'){\n            when {\n                expression {\n                    params.ENVIRONMENT == \"dev\"\n                }\n            }\n\n            steps {\n                echo \"Deploy stage to development environment\"\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://www.jenkins.io/doc/book/pipeline/syntax/#parameters</p>"},{"location":"cicd/jenkins/overview/#input","title":"input","text":"<pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Build'){\n            steps {\n                echo \"Build stage\"\n            }\n        }\n\n        stage('Deploy'){\n            steps {\n                echo \"Deploy stage for production\"\n            }\n            input {\n                message \"Please approve the deployment for production?\"\n                ok \"Approved\"\n            }\n\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/tekton/concepts/","title":"concepts","text":""},{"location":"cicd/tekton/concepts/#cicd","title":"cicd","text":"<ul> <li>set of practices to streamline software development process.</li> <li>achievement of faster deployment cycles, reduce errors, enhance quality &amp; collaboration</li> </ul>"},{"location":"cicd/tekton/concepts/#ci","title":"ci","text":"<ul> <li>encourages developers to frequeently integrate their code into shared repository which detects integration issues, erors very early stage of development. </li> <li>Build is transparent and can be used by tools like 'mavev,docker,gradle' etc</li> <li>auotmated testing i.e unit test, integration testing etc </li> <li>linting, checkstyles etc for code quality</li> </ul>"},{"location":"cicd/tekton/concepts/#cd","title":"cd","text":"<ul> <li>new git release has been created </li> <li>delivery/deployment pipeline </li> <li>publish artifacts/images</li> <li>handle stage of release</li> <li>human approval process involmenet.</li> <li>operation transparent.</li> </ul>"},{"location":"cicd/tekton/concepts/#tekton-architecture","title":"Tekton architecture","text":""},{"location":"cicd/tekton/concepts/#definition","title":"Definition","text":"<p>steps: smallest units, its equivalent to container that executes on specific input to produce output.</p> <p>task: composed of one or more steps. represent higher level unit of work of specific job in your CI/CD process. </p> <p>pipelines series of tasks that should be executed in a specific order. they orchstrate the flow of work through the CICD process. </p> <p>^ ^ Complete process but only the blueprint(definition)</p>"},{"location":"cicd/tekton/concepts/#instatiation","title":"Instatiation","text":"<p>pipelineRun is the actual work done.. these components picked up by the tekton controller , this controller will create respoective pods </p> <p>eventhandling </p> <p>listens to external events such as code-commits and trigger piprlines executions. this is done by the HTTP which is configured to have a webhooks defined. </p> <p>Interceptors inspect events and filter external events.. they run through the pipeline so you can customize triggers.. only auth events pass..</p> <p>Trigger binding/templates: extract data from the  incoming event payloads to feed pipelineRun.reusable templates to trigger the pipelines. </p> <p></p>"},{"location":"cicd/tekton/overview/","title":"overview","text":"<p>Tekton is an open-source cloud native CICD solution</p> <p>Make sure you have any of the tool being installed for the kubernetes cluser. i.e Rancher/Podman/Docker Desktop etc..</p> <p>kind is a tool for installation local kubernetes cluster, make sure to install</p> <p>installing-from-release-binaries</p> <p>Verify cluster is created and working fine. </p> <pre><code>\u279c  ~ kind version\nkind v0.20.0 go1.20.4 darwin/amd64\n\u279c  ~ kind create cluster --name tekton-cluster\nCreating cluster \"tekton-cluster\" ...\n \u2713 Ensuring node image (kindest/node:v1.27.3) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-tekton-cluster\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-tekton-cluster\n\nNot sure what to do next? \ud83d\ude05  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n\u279c  ~ kubectl config current-context\nkind-tekton-cluster\n\u279c  ~ kubectl create deployment nginx --image=nginx\ndeployment.apps/nginx created\n</code></pre> <p>install tekton pipelines - install necessary binaries/packages</p> <pre><code>\u279c  ~ kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.52.0/release.yaml\nnamespace/tekton-pipelines created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created\nclusterrole.rbac.authorization.k8s.io/tekton-events-controller-cluster-access created\nrole.rbac.authorization.k8s.io/tekton-pipelines-controller created\nrole.rbac.authorization.k8s.io/tekton-pipelines-webhook created\nrole.rbac.authorization.k8s.io/tekton-pipelines-events-controller created\nrole.rbac.authorization.k8s.io/tekton-pipelines-leader-election created\nrole.rbac.authorization.k8s.io/tekton-pipelines-info created\nserviceaccount/tekton-pipelines-controller created\nserviceaccount/tekton-pipelines-webhook created\nserviceaccount/tekton-events-controller created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-events-controller-cluster-access created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-leaderelection created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-leaderelection created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-info created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-events-controller created\nrolebinding.rbac.authorization.k8s.io/tekton-events-controller-leaderelection created\ncustomresourcedefinition.apiextensions.k8s.io/clustertasks.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/customruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/pipelines.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/pipelineruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/resolutionrequests.resolution.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/tasks.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/taskruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/verificationpolicies.tekton.dev created\nsecret/webhook-certs created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.pipeline.tekton.dev created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.pipeline.tekton.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.pipeline.tekton.dev created\nclusterrole.rbac.authorization.k8s.io/tekton-aggregate-edit created\nclusterrole.rbac.authorization.k8s.io/tekton-aggregate-view created\nconfigmap/config-defaults created\nconfigmap/config-events created\nconfigmap/feature-flags created\nconfigmap/pipelines-info created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-observability created\nconfigmap/config-registry-cert created\nconfigmap/config-spire created\nconfigmap/config-tracing created\ndeployment.apps/tekton-pipelines-controller created\nservice/tekton-pipelines-controller created\ndeployment.apps/tekton-events-controller created\nservice/tekton-events-controller created\nnamespace/tekton-pipelines-resolvers created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-resolvers-resolution-request-updates created\nrole.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created\nserviceaccount/tekton-pipelines-resolvers created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created\nconfigmap/bundleresolver-config created\nconfigmap/cluster-resolver-config created\nconfigmap/resolvers-feature-flags created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-observability created\nconfigmap/git-resolver-config created\nconfigmap/hubresolver-config created\ndeployment.apps/tekton-pipelines-remote-resolvers created\nservice/tekton-pipelines-remote-resolvers created\nhorizontalpodautoscaler.autoscaling/tekton-pipelines-webhook created\ndeployment.apps/tekton-pipelines-webhook created\nservice/tekton-pipelines-webhook created\n\u279c  ~\n</code></pre> <p>Deletion:</p> <pre><code>kind delete cluster --name tekton-cluster\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/","title":"pipelines","text":""},{"location":"cicd/tekton/tasks_pipelines/#task","title":"Task","text":"<p>re-usale atomic work, a specific job that needs to be built or testing. it executes as a pod in the cluster. it has be initiated by taskRun.</p> <pre><code># tasks.yml\napiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: 4-01-echo-task\nspec:\n  description: A simple example Tekton Task\n  steps:\n    - name: echo-step\n      image: alpine:3.14\n      script: |\n        #!/bin/sh\n        echo \"Message: Hello, Tekton!\"\n</code></pre> <pre><code>kubectl create -f tasks.yaml\nkubectl get tasks\nkubectl describe task 4-01-echo-task\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#taskrun","title":"TaskRun","text":"<ul> <li>individual runs of the tekton tasks. </li> <li>actual work carried out for CICD tasks. </li> <li>tasksRun are initated from the tasks for execution</li> <li>each taskRun is executed only once. </li> </ul> <pre><code># tasks_run.yml\napiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\n  name: 4-02-echo-task-run\nspec:\n  taskRef:\n    name: 4-01-echo-task\n</code></pre> <pre><code>kubectl create -f tasks_run.yml\nkubectl get pods \nkubectl logs &lt;pod_name&gt;\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#pipeline","title":"Pipeline","text":"<ul> <li>define and manage CICD pipelines</li> <li>seq of tasks together for CICD</li> <li>customize the execution conditions according to business needs</li> </ul> <pre><code># pipeline.yml\napiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: 4-03-example-pipeline\nspec:\n  tasks:\n    - name: echo-task\n      taskRef:\n        name: 4-01-echo-task\n</code></pre> <pre><code>kubectl create -f pipeline.yml \nkubectl get pipelines \nkubectl describe pipeline 4-03-example-pipeline\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#pipelinerun","title":"PipelineRun","text":"<ul> <li>instance of pipeline</li> <li>representatin of actual instantition of CICD workflow</li> <li>pipelineRuns are building constructs from those blueprints</li> <li>each pipelineRun is a unique execution of pipeline</li> </ul> <pre><code># pipeline_run.yml\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\n  name: 4-04-example-pipeline-run\nspec:\n  pipelineRef:\n    name: 4-03-example-pipeline\n</code></pre> <pre><code>kubectl create -f pipeline_run.yml \nkubectl get pipelineruns  / kubectl get taskruns\nkubectl get pods \nkubectl logs 4-04-example-pipeline-run-echo-task-pod \n</code></pre> <p>if you are deleting pipelineRuns, assicoated taskRuns also deleted. </p>"},{"location":"cicd/tekton/tasks_pipelines/#input-parameterizing","title":"input parameterizing","text":"<ul> <li>instead of static, we would write re-usable code using input parameteriation.</li> <li>flow control is key benefit</li> <li>allows to make decision during the execution of your tasks/pipelines based on input values.</li> </ul>"},{"location":"cicd/tekton/tasks_pipelines/#results","title":"Results","text":"<ul> <li>tekton tasks and taskruns can accept input parameters</li> <li>flexibility using results of steps in and beyond tasks.  The tasks is able to emit results in simple case as string, objects, arrays etc</li> </ul>"},{"location":"cicd/tekton/tasks_pipelines/#workspaces","title":"workspaces","text":"<ul> <li>storage area that allows tasks and pipeines to share data and files</li> <li>configmap, secrets, pvc, pvc-templates etc</li> <li>why ?</li> <li>data sharing</li> <li>data persistance</li> <li>isolation</li> <li>task level </li> <li>definition where workspace reside on its step containers</li> <li>specifies how data should be stored and accessed during the execution of that specific task. </li> <li>pipeline level</li> <li>mamages data flow between thaks via worksapce collaboration</li> <li>task A downloads code, while task B needs artifact for compiling</li> </ul> <p>Who creates wokspace and who manages ?</p> <ul> <li>its reposibility of taskrun and taskrunpipeline. </li> <li>providing and managing the workspace</li> <li>when creating a taskrun or pipelinerun</li> <li>specficatin necessary which workspace to use under which volume</li> <li>tekton take care about creating and mounting volumes on pod level</li> </ul> <p>write task in one workspace and read from another workspace.  more info on examples: 4.0.7-*.yaml</p>"},{"location":"cicd/tekton/tasks_pipelines/#auth","title":"auth","text":""},{"location":"cicd/tekton/tasks_pipelines/#clustertask","title":"clustertask","text":""},{"location":"cicd/tekton/tasks_pipelines/#resolvers","title":"resolvers","text":""},{"location":"cloud/aws/faq/","title":"Interview Questions","text":""},{"location":"cloud/aws/overview/","title":"Load Balancing","text":"<ul> <li>Spread load across multiple downstream instances </li> <li>Expose a single point of access (DNS) to your application </li> <li>Seamlessly handle failures of downstream instances </li> <li>Do regular health checks to your instances </li> <li>Provide SSL termination (HTTPS) for your websites </li> <li>Enforce stickiness with cookies \u2022 High availability across zones </li> <li>Separate public traffic from private traffic</li> </ul>"},{"location":"cloud/aws/overview/#elb","title":"ELB","text":"<ul> <li>managed load balancer</li> <li>AWS guarantees that it will be working</li> <li>AWS takes care of upgrades, maintenance, high availability</li> <li>AWS provides only a few configuration knobs</li> <li>integrated with many AWS offerings / services</li> <li>EC2, EC2 Auto Scaling Groups, Amazon ECS</li> <li>AWS Certificate Manager (ACM), CloudWatch</li> <li>Route 53, AWS WAF, AWS Global Accelerator</li> <li>health checks</li> <li>load balancer to know if instances it forwards traffic to are available to reply to requests</li> <li>health check is done on a port and a route (/health is common)</li> <li>response is not 200 (OK), then the instance is unhealthy</li> </ul>"},{"location":"cloud/aws/overview/#types-of-load-balancers","title":"types of load balancers","text":"<p>Classic Load Balancer - HTTP, HTTPS, TCP, SSL (secure TCP)</p> <p>Application Load Balancer - HTTP, HTTPS, WebSocket - Operates at layer 7(Appliation layer) - Routing tables to different target groups:   - Routing based on path in URL (example.com/users &amp; example.com/posts)   - Routing based on hostname in URL (one.example.com &amp; other.example.com)   - Routing based on Query String, Headers -  ALB are a great fit for micro services &amp; container-based application -  Target Groups:    -  EC2 instances (can be managed by an Auto Scaling Group)- HTTP    -  ECS tasks (managed by ECS itself) \u2013 HTTP    -  Lambda functions \u2013 HTTP request is translated into a JSON event    - ALB can route to multiple target groups    - Health checks are at the target group level  - Fixed hostname  - The application servers don\u2019t see the IP of the client directly     - The true IP of the client is inserted in the header X-Forwarded-For    - We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)</p> <p>Network Load Balancer - TCP, TLS (secure TCP), UDP - Operates at layer 4(Network layer)</p> <p>Gateway Load Balancer - Operates at layer 3 (Network layer) \u2013 IP Protocol</p>"},{"location":"cloud/aws/overview/#userdata","title":"userdata","text":"<pre><code>#!/bin/bash\n# Use this for your user data (script from top to bottom)\n# install httpd (Linux 2 version)\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"&lt;h1&gt;Hello World from $(hostname -f)&lt;/h1&gt;\" &gt; /var/www/html/index.html\n</code></pre>"},{"location":"cloud/aws/overview/#lab-ec2-load-balancer","title":"lab: ec2 load balancer","text":"<p>registered target groups</p> <p></p> <p></p>"},{"location":"cloud/ibm/application_deployment/","title":"IBM application Deployment","text":""},{"location":"cloud/ibm/application_deployment/#ibm-cloud-registry","title":"IBM Cloud Registry","text":"<p>A container registry is a service which provides a collection of repositories in which images can be stored. </p> <p>It can also have\u202f API\u202fpaths and\u202faccess control\u202frules.\u202f </p> <p>Container registries can be hosted publicly or privately.\u202f </p> <p></p>"},{"location":"cloud/ibm/application_deployment/#benefits-of-the-ibm-cloud-container-registry","title":"Benefits of the IBM Cloud Container Registry","text":"<p>HA and scalable.</p> <p>Push private images to conveniently run them in the IBM Cloud Kubernetes Service, Red Hat\u00ae OpenShift* Kubernetes Service, and other runtime environments. </p> <p>Images are checked for security issues, giving users full control and the ability to make informed decisions about their deployments.</p> <p>IBM Cloud Container Registry can be used by setting up an image namespace and pushing container images to the namespace.</p> <p>Image security compliance with vulnerable advisor</p> <p>When pushing images to IBM Cloud Container Registry, users can benefit from the built-in Vulnerability Advisor features that scan for potential security issues and vulnerabilities. </p> <p>Vulnerability Advisor\u202fchecks for vulnerable packages in specific Docker base images and known vulnerabilities in app configuration settings. When vulnerabilities are found, information about the vulnerability is provided. This information can be used to resolve security and configuration issues.\u202f </p> <p>Benefit from automatic scanning of images in a namespace. </p> <p>Review recommendations that are specific to the operating system to fix potential vulnerabilities and protect containers from being compromised.</p> <p>Quota limits for storage and pull traffic</p> <p>Benefit from free storage and pull traffic to private images until the free quota is reached. </p> <p>Set custom quota limits for storage and pull traffic per month to avoid exceeding the preferred payment level. </p>"},{"location":"cloud/ibm/application_deployment/#namespace","title":"namespace","text":"<p>A namespace in IBM Cloud Container Registry is a collection of repositories that store container images. IAM permissions can be granted to manage access control on namespaces, and retention policies can be set on namespaces in IBM Cloud Container Registry.</p> <pre><code>ibmcloud cr namespace-list\nibmcloud cr create yournamespace\nibmcloud cr namespace-rm &lt;my_namespace&gt;\n</code></pre>"},{"location":"cloud/ibm/application_deployment/#vulnerability-advisor","title":"vulnerability advisor","text":"<p>Vulnerability Advisor checks configuration settings in images that use supported operating systems (MySQL, NGINX, and Apache apps) and provides a link to any relevant security notices about the vulnerability. </p> <p>The table of vulnerabilities displays essential information about each issue, such as the Vulnerability ID, the policy status, the affected packages, and how to resolve the issue. </p>"},{"location":"cloud/ibm/application_deployment/#using-ibm-cloud-kubernetes-service","title":"Using IBM Cloud Kubernetes Service","text":"<p>Provides</p> <p>Intelligent scheduling  Horizontal scaling  Service discovery  Load balancing  Automated rollouts and rollbacks  Secret and configuration management  Self-healing features for apps</p> <p>Offers</p> <p>Advanced tools for secure and efficient management of cluster workloads  Built-in security and isolation features  Highly available and secure containerized app delivery on the public cloud  User-friendly interface</p> <p>IBM Cloud Kubernetes Service uses a split control plane and data plane model, which refers to the separation of the control plane components (such as the API server, scheduler, and controller manager) from the worker nodes that run the application workloads.  </p> <p>This separation allows for better scalability and fault tolerance, as well as improved security and ease of management. The control plane manages the cluster's overall state and makes decisions about where to place workloads, while the worker nodes handle the actual execution of the workloads. </p> <p>IBM Cloud offers two deployment options for running Kubernetes clusters: Classic and VPC (Virtual Private Cloud). The Classic option uses a shared, multi-tenant infrastructure, while the VPC option provides a dedicated, isolated virtual network for each client.  </p> <p>The VPC option offers greater security, control, and flexibility compared to Classic but also requires more management overhead. The choice between Classic and VPC depends on the specific needs and priorities of each organization</p>"},{"location":"cloud/ibm/application_deployment/#planning-app-deployment","title":"Planning App Deployment","text":"<p>Stateless Apps</p> <ul> <li>Stateless apps are preferred for cloud-native environments like Kubernetes.</li> <li>Stateless apps are easy to migrate and scale. </li> <li>Stateless apps declare dependencies and store configurations separately from the code. </li> <li>Stateless apps treat backing services such as databases as attached resources instead of coupled to the app. </li> <li>App pods don't require persistent data storage or a stable network IP address. </li> <li>Pods can be terminated, rescheduled, and scaled in response to workload demands. </li> <li>Stateless apps use a Database-as-a-Service for persistent data. </li> <li>Stateless apps use NodePort, load balancer, or Ingress services to expose the workload on a stable IP address.</li> </ul> <p>Stateful Apps</p> <ul> <li>Stateful apps are more complicated than stateless apps to set up, manage, and scale. </li> <li>Stateful apps require persistent data and a stable network identity. </li> <li>Stateful apps are often databases or other distributed, data-intensive workloads. </li> <li>Processing is more efficient closer to the data itself. </li> <li>To deploy a stateful app, persistent storage must be set up and mounted to the pod controlled by a StatefulSet object. </li> <li>File, block, or object storage can be added as persistent storage for a stateful set. </li> <li>Portworx can be installed on bare metal worker nodes to manage persistent storage for stateful apps as a highly - available software-defined storage solution. </li> <li>The Kubernetes documentation provides more information on how stateful sets work. </li> </ul>"},{"location":"cloud/ibm/automated_dev_tools/","title":"IBM Automation Tools","text":""},{"location":"cloud/ibm/automated_dev_tools/#continuous-delivery-services","title":"Continuous Delivery Services","text":"<p>DevOps is an increasingly common approach to agile software development that developers and operations teams use to build, test, deploy, and monitor applications with speed, quality, and control. DevOps is relevant to any kind of software project regardless of architecture, platform, or purpose. DevOps is a software development approach between development and operations that continuously delivers the required functionality. In the process, it also improves collaboration between the two teams. DevOps enables fast and automated deployment of services. </p> <p>Automate Continuous Integration is the effort required to integrate a system that increases exponentially with time. By integrating the system more frequently, integration issues are identified earlier, when they are easier to fix, and the overall integration effort is reduced. The result is a higher quality product and more predictable delivery schedules. </p> <p>Automated deployment is the practice of eliminating manual steps from the deployment of code to environments for software testing and delivery. </p> <p>Continuous delivery requires that code changes constantly flow from development all the way through to production. To continuously deliver in a consistent and reliable way, a team must break down the software delivery process into delivery stages and automate the movement of the code through the stages to create a delivery pipeline.</p> <p>The efficiency of working in small batches is one reason adopting agile development has benefited the software industry. Think about small batches in software development. The team writes and tests a small batch of code, such as a single user story, which provides an end-to-end capability that is valuable to a client. Everything is working, and you can demonstrate the capability to the client and get feedback quickly.</p> <p>Automate continuous testing to enable continuous delivery. An obvious benefit of automated testing, in contrast with manual testing, is that testing can happen quickly, repeatably, and on demand. It becomes a simple matter to verify that the software continues to run as it has before. In addition, using the practices of test-driven development(TDD) and behavior-driven development (BDD) to create test automation has been shown to improve coding quality and design.</p> <p>By using unattended automation to reduce the number of times people need to touch the systems, users can decrease the cost of the solution and improve profitability. Real profitability occurs when users can decrease the number of times people are required to touch the systems. This allows operators to focus on higher-value work and improves the operator-to-server ratio.</p> <p>Culture, Shift Left, Traceability Auditability, Security Education, and Visibility are the six best practices for DevSecOps.</p>"},{"location":"cloud/ibm/automated_dev_tools/#toolchains","title":"Toolchains","text":"<p>developers select each tool integration that they want to configure for their toolchain. Successful DevOps implementations generally rely on an integrated set of solutions or a \"toolchain\" to remove manual steps, reduce errors, increase team agility, and to scale beyond small, isolated teams. </p> <p>DevOps tools create integrated DevOps open toolchains to enable tool integrations that support development, deployment, and operations tasks. A toolchain is an integrated set of tools that developers can use to collaboratively develop, build, deploy, test, and manage applications and make operations repeatable and easier to manage. Toolchains can include open source tools, IBM Cloud services, such as IBM Cloud DevOps Insights, and third-party tools, such as GitHub, PagerDuty, and Slack. </p> <p>DevOps tools deliver continuously by using automated pipelines as well as automates builds, unit tests, deployments, and more. Capable of building, testing, and deploying in a repeatable way with minimal human intervention; ready to release into production at any time.</p> <p>DevOps Tools are able to edit and push code from anywhere by using the web-based Integrated Development Environment (IDE). They can create, edit, run, and debug, and complete source-control tasks in GitHub as well as move seamlessly from editing code to deploying it to production. </p> <p>DevOps tools allow for team collaboration and source code management with a Git repository (repos) and issue tracker that is hosted by IBM and built on GitLab Community Edition. This allows for the management of Git repos through fine-grained access controls that keep code secure. DevOps tools are able to review code and enhance collaboration through merge requests, track issues, and share ideas through the issue tracker, and they are able to document projects on the wiki system.</p>"},{"location":"cloud/ibm/automated_dev_tools/#functions-of-app-development","title":"Functions of App Development","text":"<p>Think: Plan application by creating bugs, tasks, or ideas using issue tracker.</p> <p>Code: implementation of application by providing a git as SCM.</p> <p>Deliver: configure the pipeline, allow users to specify automated build, deployment, testing code after developer pushes code the repository.</p> <p>Run:  run application in cloud env</p> <p></p> <p>IBM Cloud DevOps Insights is a tool that aggregates code, test, build, and deployment data to provide visibility of quality for all of your teams. DevOps Insights is a tool for continuous integration and continuous delivery (CI/CD). It evaluates builds to determine if they are safe to release. DevOps Insights is used to increase deployment quality and delivery control in continuous delivery. </p> <p>With DevOps Insights users can do the following: </p> <ul> <li>Maintain and improve the quality of their code in IBM Cloud.</li> <li>Monitor their deployments to identify risks before they are released. </li> <li>Analyze development changes for error probability.</li> <li>Improve the interactions of your team.</li> </ul> <p>Gates are the mechanism implemented in CI/CD tools to hold back the build if it does not meet the passing threshold. Gates compare the build with the policies configured for an application. A policy contains a set of rules.</p> <p>A rule is the passing criteria that users define for each type of test data they upload. For example, a policy can be created that contains a unit test rule that requires 100 percent success and a test coverage rule that requires 80 percent coverage.</p> <p>If the code does not meet or exceed a policy that is enacted at a particular gate, the deployment is stopped to prevent risky changes from being promoted to the next environment.</p>"},{"location":"cloud/ibm/automated_dev_tools/#open-source","title":"Open source","text":"<p>Tekton pipelines are able to build, test, and deploy in a repeatable way with minimal human intervention. It is an open source framework that is vendor neutral that can create continuous integration and delivery (CI/CD) systems. Tekton pipeline contains a Kubernetes-native framework and helps by modernizing continuous delivery. The following are three components of Tekton.</p> <p>Step: Platform management is one valid role for IBM Cloud Identity and Access Management</p> <p>Task: A task is a collection of steps in order. Tekton runs a task in the form of a Kubernetes pod, where each step becomes a running container in the pod.</p> <p>Pipeline: A pipeline is a collection of tasks in order. Tekton collects all the tasks, connects them in a directed acyclic graph (DAG), and executes the graph in sequence. In other words, it creates a number of Kubernetes pods and ensures that each pod completes running successfully as desired.</p> <p>A CI/CD workflow may contain speci\ufb01c executable actions like</p> <p>A PipelineRun is a speci\ufb01c execution of a pipeline. For example, a developer may ask Tekton to run a CI/CD workflow twice a day, and each execution becomes a PipelineRun resource trackable in the Kubernetes cluster. The status of the CI/CD workflow can be viewed and may include speci\ufb01cs of each task execution with PipelineRuns.</p> <p>A TaskRun is a speci\ufb01c execution of a task. TaskRuns are also available when a task is run outside a pipeline. The speci\ufb01cs of each step execution in a task may be viewed in the dashboard.</p> <p>Benefits and Features of Tekton</p> <p>Easier and fast deployment: Consistency and errors are reduced through automated processes. Quickly able to create cloud-native pipelines across multiple cloud providers or in hybrid environments.</p> <p>k8 native for agility and control: Tekton pipelines run on Kubernetes using their clusters as a first-class type, and they use containers as building blocks. </p> <p>Runs on any k8 cluster: Tekton is a true open source solution, which allows for the creation of continuous delivery pipelines to deploy apps to any Kubernetes environment. </p> <p>Serverless for greater efficiency: Cloud resources are used only when needed for execution of pipeline tasks. This enhances development team control and reduces costs. </p> <p>Shared pipelines reduce complexity: To reduce rework and speed up development, developers have access across projects and organizations through open source components that standardize CI/CD tooling.</p>"},{"location":"cloud/ibm/automated_dev_tools/#ibm-cloud-schematics","title":"IBM Cloud Schematics","text":""},{"location":"cloud/ibm/cloud_schematics/","title":"IBM Cloud Schematics","text":"<p>IBM Cloud Schematics provides automation by offering declarative Terraform templates to ensure a desired provisioned cloud infrastructure. Native integration with Red Hat\u00ae Ansible extends configuration, management, and provisioning to software and applications and integrates with other IBM Cloud Services. </p>"},{"location":"cloud/ibm/cloud_schematics/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>IBM Schematics executes IaC as a service which includes open source Ansible and Terraform amongst others (Git and Helm). IaC automation is hosted as-a-Service in the cloud and when adopted as an approach improves consistency, quickens deployment, and reduces manual errors through its provisioning and automation.    </p>"},{"location":"cloud/ibm/cloud_schematics/#features-and-key-capabilities","title":"Features and Key Capabilities","text":"<p>Hosted terraform workspace</p> <p>Able to easily provision cloud resources, allowing for focus to be on deployment of applications, configuration management, and subsequent daily upkeep in an automated approach for speed, consistency, and time to value benefits.  </p> <p>Native ansible actions</p> <p>Easily install software packages and application code on infrastructure.</p> <p>Collaborative environment</p> <p>Deploy and iterate infrastructure automation processes easily as a team.</p> <p>Integrate security</p> <p>Integrates easily with IBM Cloud IAM, Key Protect, IBM Log Analysis cloud service, and IBM Cloud Monitoring.</p>"},{"location":"cloud/ibm/cloud_schematics/#benefits-for-the-developer","title":"Benefits for the Developer","text":"<p>Faster provisioning Improved consistency Efficient development Improved ROI</p> <p></p>"},{"location":"cloud/ibm/cloud_schematics/#software-deployment","title":"Software Deployment","text":"<p>The IBM Software Solutions Catalog (opens in a new tab)has a wide range of software and infrastructure templates that developers can use to set up cloud resources, and to install IBM and third-party software in an IBM Cloud Kubernetes Service cluster, Red Hat\u00ae OpenShift\u00ae on IBM Cloud cluster, or a classic server or Virtual Servers for VPC. </p> <p></p>"},{"location":"cloud/ibm/cloud_schematics/#security-and-compliance","title":"Security and Compliance","text":"<ul> <li> <p>Monitor for controls and goals that pertain to IBM Cloud Schematics.</p> </li> <li> <p>Define rules for IBM Cloud Schematics that can help to standardize resource configuration.</p> </li> <li> <p>As a security or compliance focal, developers can use the IBM Cloud Schematics goals to help ensure that their organization is adhering to the external and internal standards for their industry. By using the Security and Compliance Center to validate resource configurations in their account, potential issues can be identified as they arise.</p> </li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/","title":"IBM Professional Cloud Developer Overview","text":""},{"location":"cloud/ibm/ibm_professional_cloud_developer/#objectives","title":"Objectives","text":"<ul> <li>Recall application development architectures such as full stack and microservices</li> <li>Apply working knowledge of IBM Cloud services: PaaS, IaaS, FaaS, and SaaS</li> <li>Demonstrate working knowledge of Kubernetes and Docker</li> <li>Identify concepts of IBM open innovation on IBM Cloud using Kubernetes and cloud-native, open-source work</li> <li>Apply concepts related to IBM security leadership associated with data encryption and configuration</li> <li>Identify variances between and across public, private, and hybrid-cloud enterprise grade offerings</li> <li>Identify techniques to move workloads between environments</li> <li>Recall concepts of open innovation design and implement an automated DevOps deployment</li> <li>Identify security concepts, like authentication, authorization, single sign-on, SSL/TLS, certificates, keys, encryption, Kubernetes config maps/secrets, and more.</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#major-topics","title":"Major topics:","text":"<ul> <li>IBM Professional Cloud Developer Overview</li> <li>IBM Cloud Features and Benefits</li> <li>Continuous Delivery Services</li> <li>IBM Cloud Schematics</li> <li>Essentials of IBM Cloud Professional Developer</li> <li>Application Deployment Choices</li> <li>Traditional Compute Options</li> <li>Identity Management</li> <li>IBM Key and Secrets Management</li> <li>Container Security</li> <li>Data Security</li> <li>IBM Cloud Professional Developer Principles</li> <li>Data Services</li> <li>Logging, Monitoring, and Event Management Tools</li> <li>Extending Applications through IBM Cloud Watson API Services</li> <li>Cloud Internet Services</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#cloud-developer-job-role","title":"Cloud Developer Job Role","text":"<p>An IBM Certified Cloud Developer designs and develops secure IBM Cloud applications, services, and products in an agile, collaborative environment. This includes, but is not limited to, back-end, front-end, full-stack, data and application integration, and cloud application deployment. </p> <p>The IBM Cloud Developer role is multi-functional and comprises one or more of the following sub-roles:</p>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#front-end","title":"Front end","text":"<ul> <li>Works closely with designers to take wireframes from conception to implementation</li> <li>Works closely with back-end developers to ensure implemented UI code is unit tested and production-code ready</li> <li>Codes user interfaces, including interactions, responsive layouts, and styling</li> <li>Uses and contributes to the IBM Design System to implement UIs</li> <li>Ensures user interfaces are accessible and enhance the performance of the application</li> <li>Works in programming languages like JavaScript, Node.js, or frameworks such as React, Vue, or Angular</li> <li>Skilled in UI development technologies such as HTML, CSS, JSON, and API usage</li> <li>Understands back-end concerns</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#backend","title":"Backend","text":"<ul> <li>Works with client-server architectures, networking protocols, application development, and databases</li> <li>Uses and develops RESTful APIs and web services</li> <li>Understands user and system requirements</li> <li>Develops using a combination of object-oriented and functional programming models</li> <li>Plans, analyzes, designs, and constructs databases</li> <li>Implements industry standards and best practices for database security and is capable of analyzing and - defining database and information security requirements</li> <li>Develops structured query language (SQL) queries, back-end database stored procedures, or NoSQL queries</li> <li>Works in programming languages like Ruby, Python, Java, Node.js, and server-side languages such as Go and Rust</li> <li>Understands front-end concerns</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#devsecops","title":"Devsecops","text":"<ul> <li>Works with development teams to enable a continuous integration environment that sustains high productivity levels and emphasizes defect-prevention techniques</li> <li>Ensures delivery pipeline from dev/test, staging, and production and performs A/B testing </li> <li>Automates, measures, and optimizes the system performance and processes</li> <li>Designs and implements tools for automated deployment and monitoring of multiple environments</li> <li>Works with tools like Jenkins, Maven, Ant, Gradle, Chef, Puppet, Docker, UrbanCode, Tekton, Terraform, and Ansible</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#qa-test-developers","title":"QA test Developers","text":"<ul> <li>Encourages application development that builds tests from the ground up</li> <li>Ensures the product is robust and failure scenarios are considered and refactored</li> <li>Collaborates with cross-functional team members on story development, from before definition through final  deployment</li> <li>Performs exploratory testing using industry-leading practices</li> <li>Discovers defects/bugs and works with coders and product owners to determine root cause and how to prevent similar issues from happening in the future</li> <li>Drives adoption of test automation - unit tests, integration tests, functional tests</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#skills-of-a-cloud-professional-developer","title":"Skills of a Cloud Professional Developer","text":"<p>Benefits for cloud</p> <p>Identify IBM Cloud features and benefits for cloud developers. </p> <p>Continious Delievery</p> <p>Identify fundamental concepts of Continuous Delivery Services Identify the features available when using Tekton pipelines on IBM Cloud Indicate the benefits when using DevOps Toolchains on IBM Cloud</p> <p>Cloud Schematics</p> <p>Identify the fundamental concepts of IBM Cloud Schematics</p> <p>Container registry &amp;&amp; Vulnerability advisor</p> <p>Indicate the purpose of IBM Cloud Container Registry</p> <p>Kubernetes service</p> <p>Recognize the considerations when deploying an application using IBM Cloud Kubernetes Service on IBM Cloud</p> <p>Openshift </p> <p>Recognize the considerations when deploying an application using Code Engine on IBM Cloud</p> <p>Code Engine</p> <p>Recognize the considerations when deploying an application using Red Hat\u00ae OpenShift\u00ae on IBM Cloud</p> <p>Traditional compute options</p> <p>Distinguish between IBM Cloud Platform compute options Identify appropriate use cases for VSIs in IBM Cloud Platform Identify appropriate use cases for using Bare Metal Servers on IBM Cloud Platform Identify appropriate use cases for VMware solutions on IBM Cloud Platform</p> <p>IAM</p> <p>Identify access controls when using IBM Cloud Identify and Access Management (IAM) Indicate the reasons an IBM Cloud Developer would use App ID when developing an application</p> <p>Key and secret management</p> <p>Indicate the purpose of secure key management on IBM Cloud Choose the best practices when using Secrets Manager</p> <p>Container security</p> <p>Identify the role of service accounts in container security Indicate the use of role-based access control (RBAC) in Kubernetes</p> <p>Data security</p> <p>Identify the key security aspects of cloud-native applications</p> <p>Data Services</p> <p>Determine the IBM Cloud Object Storage elements that secure data at rest, data in motion, and data access Distinguish between the IBM Cloud Databases services</p> <p>Logging, Monitoring, and Event management</p> <p>Understand the importance of IBM Log Analysis from a DevSecOps perspective</p> <p>Watson Services</p> <p>Indicate the capabilities of the IBM Watson Text to Speech API Recognize the functionality provided by IBM Watson\u00ae Assistant Identify the common input and output features of IBM Watson Speech to Text API Identify appropriate use cases for IBM Watson Natural Language Understanding Recognize the functionality of Watson Language Translator</p> <p>Internet Services</p> <p>Identify the use of Cloud Internet Services (CIS) on IBM Cloud</p>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#features-of-ibm-cloud-for-developers","title":"Features of IBM Cloud for Developers","text":"<ul> <li>Provide securely managed container orchestration for container needs. </li> <li>Secure continuous integration and continuous delivery services built with a security-first focus. </li> <li>Manage Infrastructure-as-code solutions enabling the use of Terraform and Ansible for provisioning and - configuration management needs.</li> <li>Provide cloud-wide robust identity and access management services. </li> <li>Provide enterprise-grade container runtimes and orchestration. </li> <li>Provide built-in container image scanning tools.</li> <li>Provide security and performance-boosting edge services.</li> </ul> <p>The DevOps Dashboard: Provides data sets for each application. The dashboard works within IBM Cloud, with multiple integrations and continuous delivery (CI/CD) tools. </p> <p>Toolchain Templates and Integrations: Our DevOps toolchain catalog includes many templates and third-party integrations like Slack to build powerful toolchains. </p> <p>Consistent, efficient, &amp; faster: Developers using IBM Cloud enjoy faster, consistent, and more efficient workflows. </p> <p>Improved ROI: IBM Cloud can help modernize workflow processes across the organization and its environment, lowering costs and increasing value. </p> <p>IBM Cloud Compliance: Multiple compliance programs are available, and all are certified with various regulatory and compliance standards. </p>"},{"location":"cloud/terraform/faq/","title":"Interview Questions","text":"<p>https://cloudchamp.notion.site/Terraform-Scenario-based-Interview-Questions-bce29cb359b243b4a1ab3191418bfaab</p> <p>You can create 3-tier architecture using the below code base https://github.com/ajitinamdar-tech/three-tier-arch-aws-terraform</p> <ul> <li>How do you define tags which are common to all the resources which you create ?</li> <li>How do you reference the list using terraform variable defined ?</li> <li>How to create 3 instances of the ec2 instance at once ?</li> <li>Required to create 1 instance of t2.micro and t3.xlarge based on the dev / prod environment ?</li> <li>What do you do when you want to see the logs in details when you need to debug the terraform code ?</li> <li>You have alredy defined the VPC Tags, based on the tags we need to create an public instance in the - defined subnet? How to achieve this ?</li> <li>How do I define the list of all the ports at once and add all of them using the resource group?</li> <li>What are the provisioners available in terraform and can you illustrate each of them as when and where - to be used ?</li> <li>How do you define modules and how do you reference in terraform ?</li> <li>How do you use the modulues which are already created by Hashicorp and use in the terrafor m script ?</li> <li>What are workspaces and how do you create environment using the workspaces ?</li> <li>What is remote state management and how do you use it ?</li> <li>How do you create multiple resources in different regions using multiple providers ?</li> <li>How do you enusure your password are kept secret using the terraform ?</li> <li>How do you set the terraform to be in-specific versions in the code and what happens if its not been set ?</li> <li>what happens when you do \"terraform init\" ?</li> <li>what is the difference between \"terraform plan\" and \"terraform refresh\" ?</li> <li>I do need to destroy certain resournces in the terraform, how do I do it in particular ?</li> <li>I do already have a terraform instance running, however I have lost the terraform file, Can I re-create - the terraform state file ?</li> <li>What is the difference between \"variable.tf\" and \"terraform.tfvars\" ?</li> <li>I would required to create an RDS instance before by AWS instance, How do I do it ?</li> <li>I have certain resources created using terraform and how do I keep this file in consistent so that it won't be corrupted or so?</li> </ul>"},{"location":"cloud/terraform/overview/","title":"Study Guide - Terraform Associate Certification","text":"<p>Important points to be covered for the exam. https://learn.hashicorp.com/terraform/certification/terraform-associate-study-guide</p> <p>We would go through some of the important points as we read through above study guide in theory.</p>"},{"location":"cloud/terraform/overview/#toc","title":"TOC","text":""},{"location":"cloud/terraform/overview/#learn-about-iac","title":"Learn about IaC","text":"<ul> <li>IaC makes it easy to provision and apply infrastructure configurations, saving time. It standardizes workflows across different infrastructure providers (e.g., VMware, AWS, Azure, GCP, etc.) by using a common syntax across all of them.</li> <li>Makes infra more reliable, IaC makes changes idempotent, consistent, repeatable, and predictable.</li> <li>With IaC, we can test the code and review the results before the code is applied to our target environments.</li> <li>Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time.</li> <li>Makes infra more manageable, IaC provides benefits that enable mutation, when necessary</li> </ul> <p>Terraform use case - Heroku App Setup - Multi-Tier Applications - Self-Service Clusters - Software Demos - Disposable Environments - Software Defined Networking - Resource Schedulers - Multi-Cloud Deployment, Terraform is cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. This simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures.</p>"},{"location":"cloud/terraform/overview/#manage-infrastructure","title":"Manage infrastructure","text":"<p>https://github.com/terraform-providers/terraform-provider-aws</p> <p>terraform init, Running this command will download and initialize any providers that are not already initialized and are installed in the current working directory.</p> <p>Note: terraform init cannot automatically download providers that are not distributed by HashiCorp(0.0.12v), however it is possible in terraform version(0.0.13v)</p> <p>To upgrade to latest version of all terraform modules <code>terraform init -upgrade</code> you can also use provider constraints using \"version\" within a provider block but that declares a new provider configuration that may cause problems particularly when writing shared modules. Hence recommended using required_providers</p> <p>To defined multiple providers you can use \"alias\", all the plugins would be installed in below location</p> <pre><code>windows: %APPDATA%\\terraform.d\\plugins\nLinux: ~/.terraform.d/plugins\n</code></pre> <p>OS which are supported by terraform,</p> <pre><code>darwin\nfreebsd\nlinux\nopenbsd\nsolaris\nwindows\n</code></pre> <p>purpose of terraform state:</p> <p>Mapping to the Real World, When you have a resource resource \"aws_instance\" \"foo\" in your configuration, Terraform uses this map to know that instance i-abcd1234 is represented by that resource.</p> <p>Metadata, track resource dependencies(implicit/explicit)</p> <p>Performance, When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes that it needs to make to reach your desired configuration.</p> <p>Syncing, Terraform can use remote locking as a measure to avoid two or more different users accidentally running Terraform at the same time, and thus ensure that each Terraform run begins with the most recent updated state.</p> <p>terraform settings, \"required_version\" setting can be used to constrain which versions of the Terraform CLI can be used with your configuration</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      version = \"&gt;= 2.7.0\"\n    }\n  }\n}\n</code></pre> <p>provisioners</p> <p>Provisioners can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service. https://www.terraform.io/docs/provisioners/#provisioners-are-a-last-resort</p>"},{"location":"cloud/terraform/overview/#master-the-workflow","title":"Master the workflow","text":"<p>Core Terraform workflow - write, Author infrastructure as code. - plan,  Preview changes before applying. - apply, Provision reproducible infrastructure.</p> <p>terraform init - Copy a Source Module, During init, it assumes that the working directory already contains a configuration and will attempt to initialize that configuration.</p> <ul> <li> <p>Backend Initialization, During init, the root configuration directory is consulted for backend configuration and the chosen backend is initialized using the given configuration settings.</p> </li> <li> <p>Child Module Installation, During init, the configuration is searched for module blocks, and the source code for referenced modules is retrieved from the locations given in their source arguments.</p> </li> <li> <p>Plugin Installation, For providers distributed by HashiCorp, init will automatically download and install plugins if necessary. Plugins can also be manually installed in the user plugins directory, located at ~/.terraform.d/plugins on most operating systems and %APPDATA%\\terraform.d\\plugins on Windows.</p> </li> </ul> <p>terraform validate Validate runs checks that verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. It is thus primarily useful for general verification of reusable modules, including correctness of attribute names and value types.</p> <p>terraform plan The terraform plan command is used to create an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files.</p> <p>By default, plan requires no flags and looks in the current directory for the configuration and state file to refresh.</p> <pre><code>exitcodes: Return a detailed exit code when the command exits\n0 = Succeeded with empty diff (no changes)\n1 = Error\n2 = Succeeded with non-empty diff (changes present)\n\nparallelism=n - Limit the number of concurrent operation as Terraform walks the graph. Defaults to 10.\n</code></pre> <p>Terraform itself does not encrypt the plan file. It is highly recommended to encrypt the plan file if you intend to transfer it or keep it at rest for an extended period of time.</p> <p>terraform apply The terraform apply command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan.</p> <p>terraform destroy The terraform destroy command is used to destroy the Terraform-managed infrastructure.</p> <p>The -target flag, instead of affecting \"dependencies\" will instead also destroy any resources that depend on the target(s) specified. The behavior of any terraform destroy command can be previewed at any time with an equivalent. terraform plan -destroy command</p>"},{"location":"cloud/terraform/overview/#learn-more-subcommands","title":"Learn more subcommands","text":"<p>terraform force-unlock Manually unlock the state for the defined configuration. This command removes the lock on the state for the current configuration. The behavior of this lock is dependent on the backend being used. Local state files cannot be unlocked by another process.</p> <p>terraform fmt command is used to rewrite Terraform configuration files to a canonical format and style</p> <pre><code>-list=false - Don't list the files containing formatting inconsistencies.\n-write=false - Don't overwrite the input files. (This is implied by -check or when the input is STDIN.)\n-diff - Display diffs of formatting changes\n-check - Check if the input is formatted. Exit status will be 0 if all input is properly formatted and non-zero otherwise.\n-recursive - Also process files in subdirectories. By default, only the given directory (or current directory) is processed.\n</code></pre> <p>terraform taint command manually marks a Terraform-managed resource as tainted, forcing it to be destroyed and recreated on the next apply. This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted.</p> <p>terraform state command is used for advanced state management, Terraform usage becomes more advanced, there are some cases where you may need to modify the terraform.tfstate</p> <p>terraform workspaces Terraform starts with a single workspace named \"default\". This workspace is special both because it is the default and also because it cannot ever be deleted.</p> <p>For local state, Terraform stores the workspace states in a directory called terraform.tfstate.d. some teams commit these files to version control, although using a remote backend instead is recommended when there are multiple collaborators.</p> <p>For remote state, the workspaces are stored directly in the configured backend. Multiple workspaces are currently supported by the following backends</p> <pre><code>AzureRM\nConsul\nCOS\nGCS\nLocal\nManta\nPostgres\nRemote\nS3\n</code></pre> <p>Note: Workspaces, managed with the terraform workspace command, isn't the same thing as Terraform Cloud's workspaces. Terraform Cloud workspaces act more like completely separate working directories.</p> <p>terraform import Import will find the existing resource from ID and import it into your Terraform state at the given ADDRESS. ADDRESS must be a valid resource address.</p> <p>Terraform will attempt to load configuration files that configure the provider being used for import. If no configuration files are present or no configuration for that specific provider is present, Terraform will prompt you for access credentials. You may also specify environmental variables to configure the provider.</p> <p>The only limitation Terraform has when reading the configuration files is that the import provider configurations must not depend on non-variable inputs.</p> <pre><code>terraform import aws_instance.foo i-abcd1234\nterraform import 'aws_instance.baz[0]' i-abcd1234\nterraform import 'aws_instance.baz[\"example\"]' i-abcd1234\n</code></pre> <p>terraform output command is used to extract the value of an output variable from the state file.</p> <p>terraform refresh command is used to reconcile the state Terraform knows about (via its statefile) with the real-world infrastructure. This can be used to detect any drift from the last-known state, and to update the state file. This does not modify infrastructure, but does modify the state file. If the state is changed, this may cause changes to occur during the next plan or apply.</p> <p>terraform show The terraform show command is used to provide human-readable output from a state or plan file. This can be used to inspect a plan to ensure that the planned operations are expected, or to inspect the current state as Terraform sees it.</p>"},{"location":"cloud/terraform/overview/#use-and-create-modules","title":"Use and create modules","text":"<p>Terraform Registry makes it simple to find and use modules.(https://registry.terraform.io/) The syntax for referencing a registry module // (hashicorp/consul/aws) <p>You can also use modules from a private registry of the form /// (app.terraform.io/example_corp/vpc/aws) <p>Module versioning We recommend explicitly constraining the acceptable version numbers for each external module to avoid unexpected or unwanted changes</p> <pre><code>module \"consul\" {\n  source  = \"hashicorp/consul/aws\"\n  version = \"0.0.5\"\n  servers = 3\n}\n</code></pre> <p>variables The name of a variable can be any valid identifier except the following,</p> <pre><code>source\nversion\nproviders\ncount\nfor_each\nlifecycle\ndepends_on\nlocals\n</code></pre> <p>variables on the command line can be associated like below</p> <pre><code>terraform apply -var=\"image_id=ami-abc123\"\nterraform apply -var='image_id_list=[\"ami-abc123\",\"ami-def456\"]'\nterraform apply -var='image_id_map={\"us-east-1\":\"ami-abc123\",\"us-east-2\":\"ami-def456\"}'\n</code></pre> <p>Terraform also automatically loads a number of variable definitions files if they are present, It's more convenient to specify their values in a variable definitions file terraform.tfvars or terraform.tfvars.json or names ending with .auto.tfvars or .auto.tfvars.json</p> <pre><code>terraform apply -var-file=\"custom.tfvars\"\n</code></pre> <p>Terraform searches the environment of its own process for environment variables named TF_VAR_ followed by the name of a declared variable.</p> <pre><code>export TF_VAR_ami_id=\"abd\"\n</code></pre> <p>If a root module variable uses a type constraint to require a complex value (list, set, map, object, or tuple), Terraform will instead attempt to parse its value using the same syntax used within variable definitions files.</p> <pre><code>export TF_VAR_availability_zone_names='[\"us-west-1b\",\"us-west-1d\"]'\n</code></pre> <p>Variable Definition Precedence</p> <pre><code>- Environment variables\n- The terraform.tfvars file, if present.\n- The terraform.tfvars.json file, if present.\n- Any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames.\n- Any -var and -var-file options on the command line, in the order they are provided.\n</code></pre> <p>Q) In order to make a Terraform configuration file dynamic and/or reusable, static values should be converted to use what?</p> <p>Solution: Input variables serve as parameters for a Terraform module, allowing aspects of the module to be customized without altering the module's own source code, and allowing modules to be shared between different configurations.</p>"},{"location":"cloud/terraform/overview/#read-and-write-configuration","title":"Read and write configuration","text":"<p>Resources Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records.</p> <p>A resource block declares a resource of a given type (\"aws_instance\") with a given local name (\"web\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module</p> <p>Meta-Arguments</p> <pre><code>- depends_on, for specifying hidden dependencies\n- count, for creating multiple resource instances according to a count\n- for_each, to create multiple instances according to a map, or set of strings\n- provider, for selecting a non-default provider configuration\n- lifecycle, for lifecycle customizations\n- provisioner and connection, for taking extra actions after resource creation\n</code></pre> <p>Explicitly specifying a dependency is only necessary when a resource relies on some other resource's behavior but doesn't access any of that resource's data in its arguments. e.g, creation of s3 bucket before aws_instance is provisioned.</p> <p>Data sources Data sources allow data to be fetched or computed for use elsewhere in Terraform configuration.</p> <p>References to Named Values</p> <ul> <li><code>&lt;RESOURCE TYPE&gt;.&lt;NAME&gt;</code> is an object representing a managed resource of the given type and name.</li> <li><code>var.&lt;NAME&gt;</code> is the value of the input variable of the given name.</li> <li><code>local.&lt;NAME&gt;</code> is the value of the local value of the given name.</li> <li><code>module.&lt;MODULE NAME&gt;.&lt;OUTPUT NAME&gt;</code> is the value of the specified output value from a child module called by the current module.</li> <li><code>data.&lt;DATA TYPE&gt;.&lt;NAME&gt;</code> is an object representing a data resource of the given data source type and name. If the resource has the count argument set, the value is a list of objects representing its instances. If the resource has the for_each argument set, the value is a map of objects representing its instances.</li> <li><code>path.module</code> is the filesystem path of the module where the expression is placed.</li> <li><code>path.root</code> is the filesystem path of the root module of the configuration.</li> <li><code>path.cwd</code> is the filesystem path of the current working directory. In normal use of Terraform this is the same as path.root, but some advanced uses of Terraform run it from a directory other than the root module directory, causing these paths to be different.</li> <li><code>terraform.workspace</code> is the name of the currently selected workspace.</li> </ul> <p>Local Named Values</p> <pre><code>count.index, in resources that use the count meta-argument.\neach.key / each.value, in resources that use the for_each meta-argument.\nself, in provisioner and connection blocks.\n</code></pre> <p>References to Resource Attributes The most common reference type is a reference to an attribute of a resource which has been declared either with a resource or data block</p> <pre><code>resource \"aws_instance\" \"example\" {\n  ami           = \"ami-abc123\"\n  instance_type = \"t2.micro\"\n\n  ebs_block_device {\n    device_name = \"sda2\"\n    volume_size = 16\n  }\n</code></pre> <p>dynamic blocks Within top-level block constructs like resources, expressions can usually be used only when assigning a value to an argument using the name = expression form.</p> <pre><code>resource \"aws_elastic_beanstalk_environment\" \"tfenvtest\" {\n  name                = \"tf-test-name\"\n  application         = \"${aws_elastic_beanstalk_application.tftest.name}\"\n  solution_stack_name = \"64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\"\n\n  dynamic \"setting\" {\n    for_each = var.settings\n    content {\n      namespace = setting.value[\"namespace\"]\n      name = setting.value[\"name\"]\n      value = setting.value[\"value\"]\n    }\n  }\n}\n</code></pre> <p>Type Constraints https://www.terraform.io/docs/configuration/types.html primitive type is a simple type that isn't made from any other types.</p> <ul> <li>string</li> <li>number</li> <li>bool</li> </ul> <p>Complex types</p> <p>Collection Types   A collection type allows multiple values of one other type to be grouped together as a single value. The type of value within a collection is called its element type.</p> <ul> <li>list</li> <li>map</li> <li>set</li> </ul> <p>Structural types   A structural type allows multiple values of several distinct types to be grouped together as a single value.</p> <ul> <li>object</li> <li>tuple</li> </ul> <p>Built-in-functions https://www.terraform.io/docs/configuration/functions.html</p> <ul> <li>string</li> <li>chomp</li> <li>format</li> <li>formatlist</li> <li>indent</li> <li>join</li> <li>lower</li> <li>regex</li> <li>regexall</li> <li>replace</li> <li>split</li> <li>strrev</li> <li>substr</li> <li>title</li> <li>trim</li> <li>trimprefix</li> <li>trimsuffix</li> <li>trimspace</li> <li>upper</li> </ul> <p>Numberic</p> <ul> <li>abs</li> <li>ceil</li> <li>floor</li> <li>log</li> <li>max</li> <li>min</li> <li>parseint</li> <li>pow</li> <li>signum</li> </ul>"},{"location":"cloud/terraform/overview/#manage-state","title":"Manage state","text":"<p>state locking If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state.</p> <p>force-unlock, Be very careful with this command. If you unlock the state when someone else is holding the lock it could cause multiple writers. Force unlock should only be used to unlock your own lock in the situation where automatic unlocking failed.</p> <p>Sensitive Data in State Terraform state can contain sensitive data, depending on the resources in use and your definition of \"sensitive.\" The state contains resource IDs and all resource attribute. When using local state, state is stored in plain-text JSON files.</p> <p>When using remote state, state is only ever held in memory when used by Terraform. It may be encrypted at rest, but this depends on the specific remote state backend.</p> <p>Terraform Cloud always encrypts state at rest and protects it with TLS in transit. Terraform Cloud also knows the identity of the user requesting state and maintains a history of state changes. This can be used to control access and track activity. Terraform Enterprise also supports detailed audit logging.</p> <p>The S3 backend supports encryption at rest when the encrypt option is enabled. IAM policies and logging can be used to identify any invalid access. Requests for the state go over a TLS connection.</p> <p>backends A \"backend\" in Terraform determines how state is loaded and how an operation such as apply is executed.</p> <p>benefits of backends - Working in a team, Backends can store their state remotely and protect that state with locks to prevent corruption - Keeping sensitive information off disk, State is retrieved from backends on demand and only stored in memory - Remote operations,  For larger infrastructures or certain changes, terraform apply can take a long, long time</p> <p>With a partial configuration, the remaining configuration arguments must be provided as part of the initialization process</p> <p>Interactively: Terraform will interactively ask you for the required values, unless interactive input is disabled. File: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init. Command-line key/value pairs: Key/value pairs can be specified via the init command line. Note that many shells retain command-line flags in a history file, so this isn't recommended for secrets. To specify a single key/value pair, use the -backend-config=\"KEY=VALUE\" option when running terraform init.</p> <p>changing configuration You can change your backend configuration at any time. You can change both the configuration itself as well as the type of backend (for example from \"consul\" to \"s3\").</p> <p>Terraform will automatically detect any changes in your configuration and request a reinitialization. As part of the reinitialization process, Terraform will ask if you'd like to migrate your existing state to the new configuration. This allows you to easily switch from one backend to another.</p> <p>unconfiguring backend If you no longer want to use any backend, you can simply remove the configuration from the file.</p> <p>local backend The local backend stores state on the local filesystem, locks that state using system APIs, and performs operations locally.</p> <pre><code>terraform {\n  backend \"local\" {\n    path = \"relative/path/to/terraform.tfstate\"\n  }\n}\n\n</code></pre> <p>Render your data from the path of the terraform.tfstate that exists locally.</p> <pre><code>data \"terraform_remote_state\" \"foo\" {\n  backend = \"local\"\n\n  config = {\n    path = \"${path.module}/../../terraform.tfstate\"\n  }\n}\n</code></pre> <p>Backend Types - Standard - Enhanced</p> <p>Manual State Pull/Push You can still manually retrieve the state from the remote state using the terraform state pull command. You can also manually write state with terraform state push. This is extremely dangerous and should be avoided if possible. This will overwrite the remote state. This can be used to do manual fixups if necessary.</p>"},{"location":"cloud/terraform/overview/#debug-in-terraform","title":"Debug in Terraform","text":"<p>Terraform has detailed logs which can be enabled by setting the TF_LOG environment variable to any value. This will cause detailed logs to appear on stderr</p> <p>You can set TF_LOG to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. TRACE is the most verbose and it is the default if TF_LOG is set to something other than a log level name.</p> <p>To persist logged output you can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled</p>"},{"location":"cloud/terraform/overview/#understand-terraform-cloud-and-enterprise","title":"Understand Terraform Cloud and Enterprise","text":"<p>Terraform Cloud and Terraform Enterprise</p> <p>Terraform Cloud is an application that helps teams use Terraform together. It manages Terraform runs in a consistent and reliable environment, and includes easy access to shared state and secret data, access controls for approving changes to infrastructure, a private registry for sharing Terraform modules, detailed policy controls for governing the contents of Terraform configurations.</p> <p>terraform cloud features - Terraform Workflow - Remote Terraform Execution - Workspaces for Organizing Infrastructure - Remote State Management, Data Sharing, and Run Triggers - Version Control Integration - Command Line Integration - Private Module Registry</p> <p>terraform cloud integrations - Full API - Notifications</p> <p>ACL and Governance - Team based permissions systems - Sentinel policies - Cost Estimations</p> <p>Terraform Cloud supports the following VCS providers.</p> <pre><code>GitHub\nGitHub.com (OAuth)\nGitHub Enterprise\nGitLab.com\nGitLab EE and CE\nBitbucket Cloud\nBitbucket Server\nAzure DevOps Server\nAzure DevOps Services\n</code></pre> <p>Terraform Enterprise, our self-hosted distribution of Terraform Cloud. It offers enterprises a private instance of the Terraform Cloud application, with no resource limits and with additional enterprise-grade architectural features like audit logging and SAML single sign-on.</p> <p>Sentinel Overview It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources.</p> <p>Sentinel with Terraform Cloud involves, Defining the policies: Policies are defined using the policy language with imports for parsing the Terraform plan, state and configuration. Managing policies for organizations: Users with permission to manage policies can add policies to their organization by configuring VCS integration or uploading policy sets through the API Enforcing policy checks on runs:  Policies are checked when a run is performed, after the terraform plan but before it can be confirmed or the terraform apply is executed Mocking Sentinel Terraform data: Terraform Cloud provides the ability to generate mock data for any run within a workspace</p> <p>Terraform language Terraform is not a configuration management tool. Terraform is a declarative language. Terraform supports a syntax that is JSON compatible. Terraform is primarily designed on immutable infrastructure principles.</p> <p>Terraform analyses any expressions within a resource block to find references to other objects and treats those references as implicit ordering requirements when creating, updating, or destroying resources.</p>"},{"location":"db/postgresql/install/","title":"setup and install","text":""},{"location":"db/postgresql/install/#development-environment-setup","title":"Development Environment Setup","text":"<p>In order to setup an database for development environment, we are using here an docker container and would add an sample database from postgresql-sample-database</p>"},{"location":"db/postgresql/install/#installation","title":"Installation","text":"<p>Create a new directory and file called <code>docker-compose-postgres.yml</code>.  Copy the below content into newly created file. </p> <pre><code>version: '3.8'\nservices:\n  db:\n    image: sunlnx/postgresql:v1\n    restart: always\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n    ports:\n      - '5432:5432'\n    volumes: \n      - db:/var/lib/postgresql/data\nvolumes:\n  db:\n    driver: local\n</code></pre> <p>Save the file, bring up the postgres container using  <code>docker-compose -f docker-compose-postgres.yml -d up</code> that starts the container in the background. Verify container status by <code>docker ps</code> </p>"},{"location":"db/postgresql/install/#database-restoration-using-pgadmin4","title":"Database Restoration using pgadmin4","text":"<p>You can load postgreSQL sample database as described in the documentation.</p>"},{"location":"docker_k8/docker/faq/","title":"FAQ","text":""},{"location":"docker_k8/docker/faq/#docker-networking","title":"Docker networking","text":"<p>Explain how Docker networking works.</p> <p>Docker networking provides different network drivers:</p> <ul> <li>Bridge (default for standalone containers): Containers can communicate within the same bridge network.</li> <li>Host: Removes network isolation between the container and the host.</li> <li>Overlay: Used in Swarm mode for multi-host networking.</li> <li>Macvlan: Assigns a MAC address to containers for direct communication with the physical network.</li> <li>None: No network access.</li> </ul> <p>How would you connect multiple containers in a production environment?</p> <ul> <li> <p>Use Docker Compose: Define a docker-compose.yml file to manage multi-container communication.</p> </li> <li> <p>Use Custom Networks:</p> </li> </ul> <pre><code>docker network create my_network\ndocker run --network=my_network --name=app1 my_app\ndocker run --network=my_network --name=db my_db\n</code></pre> <p>This allows containers to resolve each other using container names.</p>"},{"location":"docker_k8/docker/faq/#persistent-storage","title":"Persistent storage","text":"<p>How would you persist data in Docker containers to ensure it is not lost when the container restarts?</p> <p>Docker provides Volumes and Bind Mounts:</p> <p>Volumes (Preferred for Docker-managed storage)</p> <pre><code>docker volume create my_volume\ndocker run -d -v my_volume:/data --name my_container my_image\n</code></pre> <p>Bind Mounts (Maps a host directory to a container)</p> <pre><code>docker run -d -v /host/path:/container/path --name my_container my_image\n</code></pre> <p>For databases, use named volumes:</p> <pre><code>docker run -d -v db_data:/var/lib/mysql --name mysql_container mysql\n</code></pre>"},{"location":"docker_k8/docker/faq/#docker-img-optimize","title":"Docker img optimize","text":"<p>Your team is building a large Docker image that takes a long time to build and deploy. How would you optimize it?</p> <p>Best Practices to Optimize Docker Images:</p> <ul> <li> <p>Use a minimal base image <code>FROM python:3.9-alpine</code></p> </li> <li> <p>Leverage multi-stage builds</p> </li> </ul> <pre><code>FROM golang:1.18 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\nFROM alpine:latest\nCOPY --from=builder /app/myapp /myapp\nCMD [\"/myapp\"]\n</code></pre> <ul> <li>Minimize layers and avoid unnecessary packages</li> <li>Use <code>.dockerignore</code>to exclude unnecessary files (e.g., logs, .git)</li> </ul>"},{"location":"docker_k8/docker/faq/#docker-security","title":"Docker security","text":"<ul> <li>Use minimal base images: Avoid bloated images.</li> <li>Scan images for vulnerabilities <code>docker scan my_image</code></li> <li>Run containers as non-root users</li> </ul> <pre><code>RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup\nUSER appuser\n</code></pre> <ul> <li>Restrict container privileges </li> </ul> <pre><code>docker run --security-opt no-new-privileges --read-only --cap-drop=ALL my_app\n</code></pre> <ul> <li>Limit network exposure: Use internal networking instead of exposing unnecessary ports.</li> </ul>"},{"location":"docker_k8/docker/faq/#container-failures","title":"Container Failures","text":"<p>Your application runs in Docker containers. A container crashes unexpectedly. How do you debug it?</p> <ul> <li>Check logs: <code>docker logs container_name</code></li> <li>Inspect container state : <code>docker inspect container_name</code></li> <li>Check for OOM (Out-of-Memory) issues: <code>docker stats</code> incase it has update it <code>docker run -m 512m --memory-swap 1G my_app</code></li> <li>Restart Policy: <code>docker run --restart=always my_app</code></li> </ul>"},{"location":"docker_k8/docker/faq/#cicd-docker","title":"CICD Docker","text":"<p>How would you integrate Docker into a CI/CD pipeline?</p> <p>Use GitHub Actions, Jenkins, GitLab CI/CD:</p> <pre><code>name: Docker Build &amp; Push\non:\n  push:\n    branches:\n      - main\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build -t myrepo/myapp:latest .\n      - name: Login to Docker Hub\n        run: echo \"${{ secrets.DOCKER_PASSWORD }}\" | docker login -u \"${{ secrets.DOCKER_USERNAME }}\" --password-stdin\n      - name: Push Docker image\n        run: docker push myrepo/myapp:latest\n</code></pre> <p>Jenkinsfile</p> <pre><code>pipeline {\n    agent any\n\n    environment {\n        DOCKER_USERNAME = credentials('docker-username')  // Stored as a Jenkins credential\n        DOCKER_PASSWORD = credentials('docker-password')  // Stored as a Jenkins credential\n        IMAGE_NAME = 'myrepo/myapp:latest'\n    }\n\n    stages {\n        stage('Checkout Code') {\n            steps {\n                checkout scm\n            }\n        }\n\n        stage('Build Docker Image') {\n            steps {\n                script {\n                    sh \"docker build -t ${IMAGE_NAME} .\"\n                }\n            }\n        }\n\n        stage('Login to Docker Hub') {\n            steps {\n                script {\n                    sh \"echo ${DOCKER_PASSWORD} | docker login -u ${DOCKER_USERNAME} --password-stdin\"\n                }\n            }\n        }\n\n        stage('Push Docker Image') {\n            steps {\n                script {\n                    sh \"docker push ${IMAGE_NAME}\"\n                }\n            }\n        }\n    }\n}\n\n</code></pre>"},{"location":"docker_k8/docker/faq/#troubleshooting-issues","title":"Troubleshooting Issues","text":"<p>A container is running but you cannot access the application inside it. How do you troubleshoot?</p> <ul> <li>Check container status: <code>docker ps -a</code></li> <li>check logs: <code>docker logs my_container</code></li> <li>Inspect container networking: <code>docker inspect my_container | grep \"IPAddress\"</code></li> <li>Exec into the container: <code>docker exec -it my_container /bin/sh</code></li> <li>check port binding: <code>docker run -p 8080:80 my_app</code></li> </ul> <p>\"/bin/bash\" Exited few seconds ago </p> <p>A container lives as long as a process within it is running. If an application in a container crashes, container exits.</p> <p>unlike in other application programs like httpd, nginx, mysqld bash is not a process which is running. infact its a shell process which is listening for the input, when it don't get, it would exit the container.</p> <p>If we want to make the shell listen to some command for execution, you can find the below one. </p> <p>so we can make the container to sleep 30 seconds. </p> <pre><code>docker run ubuntu:18.04 sleep 30s\n</code></pre>"},{"location":"docker_k8/docker/overview/","title":"Overview","text":""},{"location":"docker_k8/docker/overview/#docker-architecture","title":"Docker architecture","text":"<ul> <li>Docker Daemon</li> <li>Docker Client</li> <li>Docker Registries</li> <li>Docker Objects</li> </ul> <p>Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers</p>"},{"location":"docker_k8/docker/overview/#docker-daemon","title":"Docker Daemon","text":"<p>The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>"},{"location":"docker_k8/docker/overview/#docker-client","title":"Docker Client","text":"<p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon. </p>"},{"location":"docker_k8/docker/overview/#docker-registries","title":"Docker registries","text":"<p>A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.</p>"},{"location":"docker_k8/docker/overview/#docker-objects","title":"Docker Objects","text":"<p>When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects</p> <ul> <li> <p>Images     An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization.</p> <p>Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.</p> </li> <li> <p>Containers</p> <p>A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.</p> <p>By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine.</p> <p>A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear. </p> <p>Docker defines certain policies to restart the container</p> <ul> <li>On-failure: container restarts only when a failure that occurred is not due to the user,</li> <li>Unless-stopped: container restarts only when a user executes the command to stop it,</li> <li>Always: the container is always restarted irrespective of error or other issues.</li> </ul> </li> </ul>"},{"location":"docker_k8/docker/overview/#docker-run","title":"Docker Run","text":"<ul> <li> <p>Pulls the  image: Docker checks for the presence of the ubuntu image and, if it doesn't exist locally on the host, then Docker downloads it from Docker Hub. If the image already exists, then Docker uses it for the new container.  <li> <p>Creates a new container: Once Docker has the image, it uses it to create a container. </p> </li> <li> <p>Allocates a filesystem and mounts a read-write layer: The container is created in the file system and a read-write layer is added to the image. </p> </li> <li> <p>Allocates a network / bridge interface: Creates a network interface that allows the Docker container to talk to the local host. </p> </li> <li> <p>Sets up an IP address: Finds and attaches an available IP address from a pool. </p> </li> <li> <p>Executes a process that you specify: Runs your application, and; </p> </li> <li> <p>Captures and provides application output: Connects and logs standard input, outputs and errors for you to see how your application is running. </p> </li>"},{"location":"docker_k8/docker/overview/#docker-storage","title":"Docker Storage","text":"<p>Lets discuss about the how containers are run and their association with volume mounts</p> <p>Once the Dockerfile all the commands, and when trying to build, it will create each layer of containers and finally makes a complete readonly snapshot of the image. These layes are called as image layers and they are in read-only. These intermidiate containers are stored in a cache, so incase if next build uses the same image it would be fetched from these containers, hence it will be taken very less time tp build.</p> <p>Once the image is built, we will run docker run image which will copy the executable from the image layer and writes to an read-write layer. These are called as copy-on-write. when the container runs, the storage is created in the run time and persists only until the contaniner is up. once the container is destroyed, its volume mounts are destroyed.</p> <p>In order to make containers retain their data, we would be using something called as persistant volumes, where we would explictly say to mount the data of our local paths to container paths. These are available in /var/lib/docker/volumes/.</p> <p>docker run -d -p hostport:containerport -v localdata:container image</p> <p>These types of mount are called as volume mounts</p> <p>Newer version you would be using the same using mount bind and they are called as volume binds</p> <p>docker run -d -p hostport:containerport --mount -bind src=localdata,destination=container image</p> <p>Different mount types available:</p> <ul> <li>Bind mounts: These can be stored anywhere on the host system</li> <li>Volume mount: they are managed by Docker and are stored in a part of the host filesystem.</li> <li>tmpfs mount: they are stored in the host system's memory. These mounts can never be written to the host's filesystem.</li> </ul>"},{"location":"docker_k8/docker/overview/#docker-container-lifecycle","title":"Docker Container lifecycle","text":"<ul> <li>Create phase</li> <li>Running phase</li> <li>Paused phase/unpause phase</li> <li>Stopped phase</li> <li>Killed phase</li> </ul>"},{"location":"docker_k8/docker/overview/#stateful-or-stateless","title":"stateful or stateless","text":"<p>Stateless applications should be preferred over a Stateful application for Docker Container. We can create one container from our application and take out the app's configurable state parameters. Once it is one, we can run the same container with different production parameters and other environments. Through the Stateless application, we can reuse the same image in distinct scenarios. It is also easier to scale a Stateless application than a Stateful application when it comes to Docker Containers.</p>"},{"location":"docker_k8/docker/overview/#docker-networks","title":"Docker Networks","text":"<ul> <li>bridge: Default network which the containers connect to if the network is not specified otherwise</li> <li>none: Connects to a container-specific network stack lacking a network interface</li> <li>host: Connects to the host\u2019s network stack</li> </ul> <p>default docker network</p> <pre><code>docker container run -d -p 8088:80 --name nginx-server1 nginx:alpine\ndocker inspect nginx-server1\ndocker container ps\ncurl http://localhost:&lt;port&gt;\n</code></pre> <p>custom docker network</p> <pre><code>docker network create -d bridge my-bridge-network\ndocker container run -d -p 8788:80 --network=\"my-bridge-network\" --name nginx-server2 nginx:alpine\ndocker container ps\ncurl http://localhost:&lt;port&gt;\ndocker inspect nginx-server2\n</code></pre>"},{"location":"docker_k8/docker/overview/#cmd-vs-entrypoint","title":"CMD Vs ENTRYPOINT","text":"<p>CMD provides a default arguments for the container also can be overridden when its running. </p> <pre><code>FROM Ubuntu:20.04 \nCMD [\"echo\", \"Hello from CMD\"]\n\n# docker build -t cmd-example . \n# docker run cmd-example # Output: Hello from CMD\n# docker run cmd-example \"hi there\" # Output: hi there\n</code></pre> <p>An ENTRYPOINT provides a fixed comamnd to run when container starts. its harder to override. Arguments passed during <code>docker run</code> are appended to ENTRYPOINT</p> <pre><code>FROM ubuntu:20.04\nENTRYPOINT [\"echo\", \"hello from ENTRYPOINT\"]\n\n# docker build -t entrypoint-example . \n# docker run entrypoint-example # output hello from ENTRYPOINT\n# docker run entrypoint-example \"hi there\" # output: hello from ENTRYPOINT hi there\n</code></pre> <p>Containers are meant to run a task or a process. A container lives as long as a process within it is running. If an application in a container crashes, container exits.</p> <p>Difference between the CMD and ENTRYPOINT with related to the supplied to the \"docker run\" command. While the CMD will be completely over-written by the supplied command (or args), for the ENTRYPOINT, the supplied command will be appended to it.</p> <pre><code># Dockerfile\nFROM ubuntu:20.04\n\nENTRYPOINT [\"echo\"]\nCMD [\"Hello from CMD\"]\n\ndocker build -t combined-example .\ndocker run combined-example                    # Output: Hello from CMD\ndocker run combined-example \"Custom Message\"   # Output: Custom Message\n</code></pre>"},{"location":"docker_k8/docker/overview/#example","title":"Example","text":"<p>Let's understand it with an ubuntu-sleeper example. </p> <p>When you want to make a container run, it would check for the CMD to run the process, but when we have a bash which is just a listening terminal to get the input and if we don't provide it, it would just exit the conatiner. </p> <pre><code>docker run ubutu:20.04 sleep 30 # provide an input to bash terminal for 30 sec.\n</code></pre> <p>Let's make a docker equivalent file for above command</p> <pre><code>FROM ubuntu:20.04\nCMD [\"sleep\", \"30\"]\n\ndocker build -t ubuntu-sleep .\ndocker run ubuntu-sleep\n</code></pre> <p>Container always sleep 30 sec once it started !  So what if we need to change the time ? i.e sleep 10 ?  Since its hardcoded, we would now want to make it parametrized.. </p> <pre><code>FROM ubuntu:20.04\nCMD [\"30\"]\nENTRYPOINT [\"sleep\"]\n\ndocker build -t ubuntu-sleep .\ndocker run ubuntu-sleep # sleep for 30s when no args are passed\ndocker run ubuntu-sleep 10 # sleep for 10 sec # observe that CMD has been overwritten for ENTRYPOINT\n</code></pre> <p>incase you want to override the command itself in the ENTRYPOINT, then..</p> <pre><code>docker run --entrypoint new-sleep-command ubuntu-sleep 60\n</code></pre>"},{"location":"docker_k8/docker/overview/#dockerfile","title":"Dockerfile","text":"<ul> <li> <p>FROM -  sets the base image for subsequent instructions, especially easier to start by pulling an image. </p> </li> <li> <p>MAINTAINER -  Author field of the generated images</p> </li> <li> <p>RUN - execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.. <code>RUN [ \"echo\", \"$HOME\" ]</code> will not do variable substitution on $HOME as exec won't invoke any command shell. if you want shell processing you need to specify the shell <code>RUN [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]</code></p> </li> <li> <p>CMD - Command that needs to be executed while running container. The main purpose of a CMD is to provide defaults for an executing. These defaults can include an executable, or they can omit the executable, in which case we must specify an ENTRYPOINT instruction as well.</p> <p>CMD It has 3 forms:</p> <p>CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form) CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) CMD command param1 param2 (shell form)</p> </li> <li> <p>WORDDIR AND ENV - The WORKDIR instruction sets the working directory for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile. The WORKDIR instruction can resolve environment variables previously set using ENV. The ENV instruction sets the environment variable to the value . This value will be passed to all future RUN instructions. The environment variables set using ENV will persist when a container is run from the resulting image.</p> </li> <li> <p>ADD copies new files, directories or remote file URLs from and adds them to the filesystem of the container at the path .</p> </li> <li> <p>ENTRYPOINT allows us to configure a container that will run as an executable.</p> </li> </ul>"},{"location":"docker_k8/docker/overview/#example_1","title":"Example","text":"<p>lets create Dockerfile and check above actions </p> <pre><code>FROM ubuntu:20.04\nMAINTAINER samperay\n\nRUN apt-get update &amp;&amp; apt-get install htop\nWORKDIR /root\nENV TAG Dev\n\n# build image\n\ndocker build -t demo . \ndocker images\ndocker run -it --rm demo -- /bin/bash\n\n# insise docker image\n$ pwd\n/root \n$ echo $TAG\nDev\n$ \n</code></pre> <p>Now, lets create a script and make it to run from the container</p> <pre><code># run.sh\n\n#!/bin/sh\necho \"The current directory : $(pwd)\"\necho \"The Tag variable : $TAG\"\necho \"There are $# arguments: $@\"\n</code></pre> <pre><code>FROM ubnutu:20.04\nWORKDIR /root\nENV TAG Dev\nADD run.sh /root/run.sh\nRUN chmod +x ./root/run.sh\nCMD [\"./run.sh\"]\n\n\ndocker build -t demo1 . \ndocker run -it --rm demo1\ndocker container run -it --rm demo1 ./run.sh Hello Sunil\n\n# outputs \necho \"The current directory : $(pwd)\"\necho \"The Tag variable : $TAG\"\necho \"There are $# arguments: $@\"\n</code></pre> <p>Let's discuss about the <code>CMD</code> and <code>ENTRYPOINT</code> in above code. since we pass arguments, we can use that using CMD options to provide as an input to ENTRYPOINT.</p> <pre><code>FROM ubnutu:20.04\nWORKDIR /root\nENV TAG Dev\nADD run.sh /root/run.sh\nRUN chmod +x ./root/run.sh\nENTRYPOINT [\"./run.sh\"]\nCMD [\"arg1\"]\n\n\ndocker build -t demo2 . \ndocker run -it --rm demo2  \n\n# Output: \necho \"The current directory : /root\"\necho \"The TAG variable : Dev\"\necho \"There are 1 arguments: arg1\"\n\n\ndocker container run -it --rm demo1 /bin/bash\n\necho \"The current directory : /root\"\necho \"The TAG variable : Dev\"\necho \"There are 1 arguments: /bin/bash\"\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/","title":"Application Life Cycle Management","text":""},{"location":"docker_k8/k8/applifecyclemgmt/#rollout-and-versioning-in-a-deployment","title":"Rollout and Versioning in a Deployment","text":"<p>when you first create deployment, it will create a rollout which trigger deployment ( e.g v1), future when the application is upgraded meaning when the container version is updated to a new one a new rollout is triggered and a new deployment revision is created named revision (v2).</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#deployment-strategies","title":"Deployment Strategies","text":"<p>There are 2 types of deployment strategies</p> <ul> <li>Recreate</li> <li>RollingUpdate (Default Strategy)</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#recreate","title":"Recreate","text":"<p>One way to upgrade these to a newer version is to destroy all of these and then create newer versions of application instances meaning first destroy the running instances and then deploy the new instances of the new application version.</p> <p>The problem with this as you can imagine is that during the period after the older versions are down and before any newer version is up the application is down and inaccessible to users this strategy is known as the Recreate strategy.</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#rollingupdate","title":"RollingUpdate","text":"<p>Instead we take down the older version and bring up a newer version one by one. This way the application never goes down and the upgrade is seamless. In other words rolling update is the default deployment strategy so we talked about upgrades.</p> <p>How exactly do you update your deployment when you say update.</p> <p>It could be different things such as updating your application version by updating the version of docker containers used, updating their labels or updating the number of replicas etc. Since we already have a deployment definition file it is easy for us to modify this file once we make the necessary changes.</p> <p>we run the kubectl apply command to apply the changes. A new rollout is triggered and a new revision after deployment is created.</p> <pre><code>$ kubectl create -f mywebapp.yaml\n$ kubectl get deployments\n$ kubectl apply -f mywebapp.yaml\n$ kubectl describe deployment mywebapp\n$ kubectl rollout status deployment/mywebapp\n$ kubectl rollout history deployment/mywebapp\n$ kubectl rollout undo deployment/mywebapp\n</code></pre> <p>This is imperative way of editing using kubectl. This will not be edited using the yaml definition. Suggested would be to use the declarative way to use this.</p> <pre><code>$ kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#configuring-applications","title":"Configuring Applications","text":"<ul> <li>Configuring Command and Arguments on applications</li> <li>Configuring Environment Variables</li> <li>Configuring Secrets</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#configuring-command-and-arguments-on-applications","title":"Configuring Command and Arguments on applications","text":"<p>Anything that is appended to the docker run command will go into the args property of the pod defination file in the form of an array. The command field corresponds to the entrypoint instruction in the Dockerfile</p> <pre><code>containers:\n- name: ubuntu-sleeper\n  image: ubuntu-sleeper\n  command: [\"sleep2.0\"]\n  args: [\"10\"]\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#configuring-environment-variables","title":"Configuring Environment Variables","text":"<p>To set an environment variable set an env property in pod defination file.</p> <p>There are other ways of setting the environment variables such as, - ConfigMaps - Secrets</p> <pre><code>containers:\n- name: ubuntu-sleeper\n  image: ubuntu-sleeper\n  command: [\"sleep2.0\"]\n  args: [\"10\"]\nenv:\n- name: APP_COLOR\n  value: pink\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#configmaps","title":"ConfigMaps","text":"<p>There are 2 phases involved in configuring ConfigMaps.   First, create the configMaps, Second, Inject then into the pod.</p> <p>There are 2 ways of creating a configmap.</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#imperative","title":"Imperative","text":"<pre><code>$ kubectl create configmap dbconfig --from-literal=DB_HOST=\"mysql_db\" --from-literal=DB_PASSWORD= \"mysql\" --from-literal=DB_PORT=3306\n\n$ kubectl create configmap dbconfig --from-file=dbconfig.properties (Another way)\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#declarative","title":"Declarative","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: dbconfig\ndata:\n  DB_HOST: \"mysql_db\"\n  DB_PASSWORD: \"mysql\"\n  DB_PORT: \"3306\"\n</code></pre> <p>reference these config maps to the pods and then check by logging into the pod, you must be able to see the env variables.</p> <pre><code>containers:\n  - name: env-configmap\n    image: nginx\n    envFrom:\n      - configMapRef:\n          name: dbconfig\n</code></pre> <pre><code>kubectl get pods -o wide\nkubectl exec -it pod/pod-name sh\nenv\n</code></pre> <p>There are other ways to inject configuration variables into pod</p> <ul> <li>You can inject it as a single environment variable.</li> <li>You can inject it as a file in a Volume.</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#secrets","title":"Secrets","text":"<p>How Kubernetes handles secrets ?</p> <ul> <li>A secret is only sent to a node if a pod on that node requires it.</li> <li>Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.</li> <li>Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#multi-container-pod","title":"Multi Container Pod","text":"<p>There are at times you need to have an container which has two or more services required to be available ( e.g webserver &amp; log agent). They need to co-exists and hence they are create/deleted at the same time. Such cases we are required to have multi container pods. They would share same namespaces, network isolations etc .</p> <p>There are 3 common patterns, when it comes to designing multi-container PODs. The first and what we just saw with the logging service example is known as a side car pattern. The others are the adapter and the ambassador pattern.</p> <pre><code>spec:\n  containers:\n      - name: nginx\n        image: nginx:latest\n      - name: redis\n        image: redis\n</code></pre> <p>what incase if you require any container to be run only once in the multi pod container ? i.e to pull a code or binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts</p> <p>Those can be configured as initContainers.</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#initcontainers","title":"initContainers","text":"<p>An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section,  like this:</p> <pre><code>spec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'git clone &lt;some-repository-that-will-be-used-by-application&gt; ; done;']\n</code></pre> <p>When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.</p> <p>You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order.</p> <pre><code>  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#liveness-readiness-startup-probes","title":"Liveness, Readiness, Startup probes","text":"<p>Many applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.</p> <p>kubelet uses readiness probes to know when a container is ready to start accepting traffic. one use of this signal is to control which pods are used as backend for services. when pod is not ready, its removed from service LB. Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the application, but you don't want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations</p> <p>kubelet uses startup probes to know when a container application has started. if any such probe is configured, it disables liveness and readiness checks until it succeeds. Sometimes, you have to deal with legacy applications that might require an additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe</p> <p>References: https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/</p>"},{"location":"docker_k8/k8/architecture/","title":"Kubernetes Overview","text":""},{"location":"docker_k8/k8/architecture/#k8-architecture-components","title":"k8 Architecture components","text":"<p>Kubernetes consists of nodes which are physcial or cloud which hosts applications on the nodes in form of containers.</p> <p>The master node in the Kubernetes cluster is responsible for managing the Kubernetes cluster storing information regarding the different nodes planning which containers cause where monitoring the notes and containers on them etc.</p> <p>The Master node does all of these using a set of components together known as the control plane components.</p>"},{"location":"docker_k8/k8/architecture/#etcd","title":"etcd","text":"<p>details about the container informations like when it was loaded, which container is placed on which nodes etc. These are stored as DB in key/value format.</p> <ul> <li>The ETCD Datastore stores information regarding the cluster such as Nodes, PODS, Configs, Secrets, Accounts, Roles, Bindings and Others.</li> <li>Every information you see when you run the kubectl get command is from the ETCD Server.</li> </ul>"},{"location":"docker_k8/k8/architecture/#kube-api","title":"kube-api","text":"<p>The kube-apiserver is the primary management component of kubernetes. The kube-api server is responsible for orchestrating all operations within the cluster. it exposes kubernetes API which is used by external users to perform mgmt operations on the cluster as well as the various controllers to monitor the state of the cluster and make the necessary changes as required and by the worker nodes to communicate with the server.</p> <p>Kube-apiserver is responsible for authenticating, validating requests, retrieving and Updating data in ETCD key-value store. In fact kube-apiserver is the only component that interacts directly to the etcd datastore. The other components such as kube-scheduler, kube-controller-manager and kubelet uses the API-Server to update in the cluster in thier respective areas.</p>"},{"location":"docker_k8/k8/architecture/#control-managers","title":"Control Managers","text":"<p>Kube Controller Manager manages various controllers in kubernetes.</p> <p>In kubernetes terms, a controller is a process that continously monitors the state of the components within the system and works towards bringing the whole system to the desired functioning state.</p> <ul> <li> <p>Node-controller : The node-controller takes care of nodes. They're responsible for onboarding new nodes to the cluster handling situations where nodes become unavailable or get gets destroyed.</p> <ul> <li>Monitoring Nodes</li> <li>Node Monitor Period 5s</li> <li>Node Monitoring Grace Period Status 40s</li> <li>POD Eviction timeout 5m</li> </ul> </li> <li> <p>Replication-Controller : Ensures that the desired number of containers are running at all times in your replication group.</p> </li> </ul>"},{"location":"docker_k8/k8/architecture/#kube-scheduler","title":"kube-scheduler","text":"<p>kube-scheduler is responsible for scheduling pods on nodes.The kube-scheduler is only responsible for deciding which pod goes on which node. It doesn't actually place the pod on the nodes, thats the job of the kubelet. - Decision factors   - Resource requirements and Limits   - Taints and Tolerations   - Node Selectors and Affinity</p>"},{"location":"docker_k8/k8/architecture/#kubelet","title":"kubelet","text":"<p>Kubelet is the sole point of contact for the kubernetes cluster</p> <p>A kubelet is an agent that runs on each node in a cluster. It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required. The kube-api server periodically fetches status reports from the kubelet to monitor the state of nodes and containers on them.</p>"},{"location":"docker_k8/k8/architecture/#kube-proxy","title":"kube proxy","text":"<p>Within Kubernetes Cluster, every pod can reach every other pod, this is accomplish by deploying a pod networking cluster to the cluster. Kube-Proxy is a process that runs on each node in the kubernetes cluster.</p> <p>The Kube-proxy service ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other.</p>"},{"location":"docker_k8/k8/architecture/#communication-on-kubernetes-mgmt-plane","title":"Communication on kubernetes mgmt plane","text":"<p>when user types kubectl get nodes, kube-api server authenticates the requests, validates it and gets information by etcd which stores all the dats using key/value pair. when new pod is created, kubeapi server authenticates first, and then validated. it then creates a pod without assigning it to a node, updates information in etcd and etcd reverts back to kube-api that information has been updated.</p> <p>Now, you have a container which is not assigned to any node, this is being seen by kube-scheduler  which verifies the requirements of container and would assign to right nodes and it would update the kube-api as to which nodes this container has to be scheduled, then kube-api would be send this request to etcd to update the inforamtion as to which nodes this container needs to be scheduuled, after which kube-api would pass this information to kubelet to start the container in appropriate worker node. kubelet then creates pod on the node and instructs the container runtime engine to deploy the image. Once done kubelet updates back inforamtion to kube-api which then sends back information to etcd</p>"},{"location":"docker_k8/k8/clustermaintenance/","title":"Cluster Maintenance","text":""},{"location":"docker_k8/k8/clustermaintenance/#os-upgrades","title":"OS Upgrades","text":"<p>if there are any deployments that are on the nodes, we would like to move to another node hence we <code>cordon</code> the node and <code>drain</code> it, so that the existing applications(deployments) would be re-created into another node.</p> <pre><code>kubectl drain node01\nkubectl describe node node01 | grep -i taint\nkubectl cordon node01\nkubectl describe node node01 | grep -i taint\nkubectl drain --ignore-daemonsets --force\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#cluster-upgrade-process","title":"Cluster Upgrade process","text":"<p>first, check how many nodes does exists and nodestatus.</p> <pre><code>kubectl describe node node01 | grep -i taint\nkubectl describe node master | grep -i taint\n</code></pre> <p>Check for the latest version of the cluster using <code>kubeadm</code> tool</p> <pre><code>kubeadm upgrade plan\n</code></pre> <p>Once you know it has to be upgraded, upgrade one node followed by another.</p> <pre><code>kubectl cordon master\nkubectl drain master --ignore-daemonsets\n</code></pre> <p>On master,</p> <pre><code>apt update\napt install kubeadm=1.19.0-00\nkubeadm upgrade apply v1.19.0\napt install kubelet=1.19.0-00\nsystem restart kubelet\n</code></pre> <pre><code>kubectl uncordon master\nkubectl describe node master | grep -i taint\n</code></pre> <p>then, worker node</p> <pre><code>apt update\napt install kubeadm=1.19.0-00\napt install kubelet=1.19.0-00\nsystem restart kubelet\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#etcd-backup","title":"etcd backup","text":"<pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt\n--cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key\nsnapshot save /opt/snapshot-pre-boot.db\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#etcd-restore","title":"etcd restore","text":"<pre><code>ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup\nsnapshot restore /opt/snapshot-pre-boot.db\n</code></pre> <p>Modify, /etc/kubernetes/manifests/etcd.yaml and in the hostPath mention: <code>/var/lib/etcd-from-backup</code> wait for few minutes, then check the applications. </p>"},{"location":"docker_k8/k8/deployment/","title":"deployment","text":"<p>Modifying, scaling, upgrading the resource allocation all these can be done using deployments. single instace of application or each container is encapsulated in pod, and such pods are deployed using the replication controller or replica sets. kubernetes provides the higher object called deployment which provides us capabilty upgrade instances using rolling update, scaling, modify etc</p>"},{"location":"docker_k8/k8/deployment/#deployment-using-files","title":"Deployment using files","text":"<pre><code>kubectl create -f deps.yml\nkubectl get deployments\nkubectl describe deployment depname\nkubectl delete deployment depname\n</code></pre>"},{"location":"docker_k8/k8/deployment/#deployment-using-kubectl-cli","title":"Deployment using kubectl cli","text":"<pre><code>kubectl create deployment nginx --image=nginx\nkubectl run nginx image=nginx:latest\nkubectl create deployment nginx --image=nginx --dry-run=client -o yaml\nkubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; nginx.yml\n</code></pre>"},{"location":"docker_k8/k8/deployment/#list-all-kubernetes-resources","title":"list all kubernetes resources","text":"<pre><code>kubectl get all\n</code></pre>"},{"location":"docker_k8/k8/deployment/#namespace","title":"Namespace","text":"<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Mainly used for isolation of the resources.</p>"},{"location":"docker_k8/k8/deployment/#create-namespaces-cli","title":"Create namespaces CLI","text":"<pre><code>kubectl get ns\nkubectl create ns dev\n</code></pre> <p>Create pod using yaml file, that creates both namespace as well as pod</p> <pre><code>kubectl create -f pod.yml\n</code></pre> <pre><code>kubectl run nginx --image=nginx -n dev\nkubectl get pods -n dev\n</code></pre>"},{"location":"docker_k8/k8/deployment/#namespace-and-dns","title":"Namespace and DNS","text":"<p>When you create a Service, it creates a corresponding DNS entry. This entry is of the form ..svc.cluster.local, which means that if a container just uses , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN)."},{"location":"docker_k8/k8/deployment/#set-context","title":"Set context","text":"<p>identify which namespace you need your default namespace to be, then you can set your context accordingly.</p> <pre><code>kubectl config set-context $(kubectl config current-context) --namespace=default\nkubectl config set-context $(kubectl config current-context) --namespace=dev\n</code></pre>"},{"location":"docker_k8/k8/deployment/#pods","title":"Pods","text":""},{"location":"docker_k8/k8/deployment/#deploy-pods","title":"Deploy pods","text":"<pre><code>kubectl run nginx --image=nginx\nkubectl get pods -o wide\n</code></pre>"},{"location":"docker_k8/k8/deployment/#reference","title":"Reference","text":"<ul> <li>https://kubernetes.io/docs/concepts/workloads/pods/pod/</li> <li>https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/</li> <li>https://kubernetes.io/docs/tutorials/kubernetes-basics/explore/explore-intro/</li> </ul>"},{"location":"docker_k8/k8/deployment/#replication-controller","title":"Replication Controller","text":"<p>A ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.</p> <p>ReplicationController are automatically replaced if they fail, are deleted, or are terminated</p> <p>Replication is used for the core purpose of Reliability, Load Balancing, and Scaling</p> <pre><code>kubectl create -f replication_controller.yml\nkubectl get rc\nkubectl describe rc\n</code></pre>"},{"location":"docker_k8/k8/deployment/#replication-set","title":"Replication Set","text":"<p>Replica Set ensures how many replica of pod should be running. It can be considered as a replacement of replication controller.</p> <pre><code>kubectl create -f replicaset.yml\nkubectl get rs\nkubectl describe rs\n</code></pre>"},{"location":"docker_k8/k8/deployment/#difference-between-replication-controller-and-replication-set","title":"Difference between replication controller and replication set","text":"<p>The key difference between the replica set and the replication controller is, the replication controller only supports equality-based selector whereas the replica set supports set-based selector https://blog.knoldus.com/replicationcontroller-and-replicaset-in-kubernetes/</p>"},{"location":"docker_k8/k8/deployment/#services","title":"Services","text":"<p>Kubernetes Services enables communication between various components within an outside of the application.</p>"},{"location":"docker_k8/k8/deployment/#service-types","title":"Service Types","text":"<p>There are 3 types of service types in kubernetes</p> <ul> <li>NodePort: Where the services makes an internal POD accessible on a POD on the NODE</li> <li>ClusterIP: The service creates a Virtual IP inside the cluster to enable communication between different services such as a set of frontend servers to a set of backend servers</li> <li>LoadBalancer: Provisions a load balancer for our application in supported cloud providers.</li> </ul> <pre><code>kubectl create -f pod-svc.yml\nkubectl create -f deployment_svc.yml\nkubectl get svc\nkubectl describe svc name\nkubectl delete -f pod-svc.yml\n</code></pre>"},{"location":"docker_k8/k8/networking/","title":"Networking","text":""},{"location":"docker_k8/k8/networking/#network-namespaces","title":"Network Namespaces","text":"<p>Once we have our containers created, it is isolated between the host systems's network, process, which means the process running inside the container is not visible to pysical host.</p> <p>We can create as such in below examples.</p>"},{"location":"docker_k8/k8/networking/#physical-host","title":"Physical Host","text":"<pre><code>ps aux\nroute\narp\nip netns\n</code></pre>"},{"location":"docker_k8/k8/networking/#virtualcontainers","title":"Virtual/Containers","text":"<pre><code>ip netns exec &lt;namespace&gt; ps aux\nip netns exec &lt;namespace&gt; route\nip netns exec &lt;namespace&gt; arp\nip netns exec &lt;namespace&gt; ip netns\n</code></pre>"},{"location":"docker_k8/k8/networking/#operations-network-namespace","title":"Operations - Network Namespace","text":"<pre><code>ip netns add red\nip netns add blue\nip netns exec red ip link\nip netns exec blue ip link\nip link add veth-red type veth peer name veth-blue\nip link set veth-red netns red\nip link set veth-blue netns blue\nip -n red addr add 192.168.15.1/24 dev veth-red\nip -n blue addr add 192.168.15.2/24 dev veth-blue\nip -n red link set veth-red up\nip -n blue link set veth-blue up\nip netns exec red ping 192.168.15.2\n</code></pre>"},{"location":"docker_k8/k8/networking/#cluster-networking","title":"Cluster Networking","text":""},{"location":"docker_k8/k8/networking/#conrol-plane-nodes","title":"Conrol plane nodes","text":"<p>Protocol    Direction   Port Range  Purpose                Used By TCP       Inbound     6443*     Kubernetes API server      All TCP       Inbound     2379-2380 etcd server client API   kube-apiserver, etcd TCP       Inbound     10250     Kubelet API Self,        Control plane TCP       Inbound     10251     kube-scheduler           Self TCP       Inbound     10252     kube-controller-manager   Self</p>"},{"location":"docker_k8/k8/networking/#worker-nodes","title":"Worker nodes","text":"<p>Protocol    Direction   Port Range  Purpose             Used By TCP       Inbound     10250       Kubelet API           Self, Control plane TCP       Inbound     30000-32767   NodePort Services\u2020  All</p>"},{"location":"docker_k8/k8/networking/#useful-commands","title":"Useful commands","text":"<pre><code>ip link\nip addr\nip addr add 192.168.56.10/24 dev eth0\nip route\nip route add 192.168.56.0/24 via 192.168.56.1\ncat /proc/sys/net/ipv4/ip_forward\narp\nnetstat -plnt\nroute\n</code></pre>"},{"location":"docker_k8/k8/networking/#pod-networking","title":"Pod Networking","text":""},{"location":"docker_k8/k8/networking/#cni-in-kubernetes","title":"CNI in Kubernetes","text":""},{"location":"docker_k8/k8/networking/#coredns","title":"CoreDNS","text":""},{"location":"docker_k8/k8/networking/#ingress","title":"Ingress","text":""},{"location":"docker_k8/k8/scheduling/","title":"Scheduling","text":"<p>If your kubernetes don't have a scheduler, then every POD has a field called NodeName that by default is not set. Once identified it schedules the POD on the node by setting the nodeName property to the name of the node by creating a binding object.</p>"},{"location":"docker_k8/k8/scheduling/#nodename","title":"NodeName","text":"<p>you can manually set to assign pods to that nodes by setting the property nodeName in your pod definition file while you create pod.</p> <pre><code>kubectl create -f nodeName.yml\nkubectl delete -f nodeName.yml\n</code></pre> <pre><code>kubectl run nginx --image=nginx --dry-run=client --labels=app=nginx,tier=frontend -o yaml &gt; nginx.yml\n\nvim nginx.yaml\n..\n   nodeName: node01\n...\n\nkubectl create -f nginx.yml\nkubectl get pods -l app-nginx\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#labels-selectors","title":"Labels &amp; Selectors","text":"<p>Labels and Selectors are standard methods to group things together. Labels are properties attach to each item. Selectors help you to filter these items</p> <p>you can select the pod using labels</p> <pre><code>kubectl get pods -l app=nginx\nkubectl get pods --selector app=nginx,type=front-end\n</code></pre> <pre><code>kubectl create deployment --image=sunlnx/kubenginx nginx-v5 --dry-run=client -o yaml &gt; nginx-v5.yaml\nkubectl create deployment --image=sunlnx/kubenginx nginx-v4 --dry-run=client -o yaml &gt; nginx-v4.yaml\nkubectl create deployment --image=sunlnx/kubenginx nginx-v3 --dry-run=client -o yaml &gt; nginx-v3.yaml\nkubectl create deployment --image=sunlnx/kubenginx nginx-v2 --dry-run=client -o yaml &gt; nginx-v2.yaml\nkubectl create deployment --image=sunlnx/kubenginx nginx-v1 --dry-run=client -o yaml &gt; nginx-v1.yaml\n\nkubectl create -f nginx-v5.yaml\nkubectl create -f nginx-v4.yaml\nkubectl create -f nginx-v3.yaml\nkubectl create -f nginx-v2.yaml\nkubectl create -f nginx-v1.yaml\n\nkubectl expose deployment nginx-v1 --port 80 --type=NodePort\nkubectl expose deployment nginx-v2 --port 80 --type=NodePort\nkubectl expose deployment nginx-v3 --port 80 --type=NodePort\nkubectl expose deployment nginx-v4 --port 80 --type=NodePort\nkubectl expose deployment nginx-v5 --port 80 --type=NodePort\n\nkubectl get all -l version=v1\nkubectl get all -l version=v1,tier=frontend\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#annotations","title":"Annotations","text":"<p>While labels and selectors are used to group objects, annotations are used to record other details for informative purpose</p>"},{"location":"docker_k8/k8/scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Pod to node relationship and how you can restrict what pods are placed on what nodes. Taints and Tolerations are used to set restrictions on what pods can be scheduled on a node.</p> <p>pods which are tolerant to particular taint will be scheduled on that node. there are 3 policies of taints:</p> <p>The taint effect defines what would happen to the pods if they do not tolerate the taint.</p> <ul> <li>NoSchedule</li> <li>PreferNoSchedule</li> <li>NoExecute</li> </ul> <p>The default policy of the any node is no taints so that any pod can be placed in any of the nodes.</p> <p>Default, master nodes have a NoSchedule so that there won't be any pods running on the master nodes. We would for the testing purpose untaint the node and then revert back</p> <pre><code>kubectl taint nodes master node-role.kubernetes.io/master- [ No taints and new pods will be created on this node ]\nkubectl taint nodes master node-role.kubernetes.io/master:NoSchedule [ tainted ]\n</code></pre> <p>Few more,</p> <pre><code>kubectl taint nodes kubenode01 env=qa:NoSchedule\nkubectl describe nodes kubenode01 | grep -i taint\nkubectl taint nodes kubenode01 env-\nkubectl describe nodes kubenode01 | grep -i taint\nkubectl taint nodes master node-role.kubernetes.io/master:NoSchedule-\n</code></pre> <pre><code>kubectl taint node minikube version=v1:NoSchedule\nkubectl create -f nginx-v1.yaml ==&gt; pods are in pending as these didnt tolerate the taint\nkubectl create -f nginx-v2.yaml ==&gt; pods in running state\n</code></pre> <p>Taints and Tolerations does not tell the pod to go to a particular node. Instead, it tells the node to only accept pods with certain tolerations.</p>"},{"location":"docker_k8/k8/scheduling/#nodeselector","title":"NodeSelector","text":"<p>We add new property called Node Selector to the spec section and specify the label. The scheduler uses these labels to match and identify the right node to place the pods onto the nodes.</p> <p>Label the nodes to ensure your pod is getting scheduled on a desired right nodes by kube-scheduler, however you can't really guarantee that the pods does only gets scheduled on the label being added. sometimes it might also get scheduled to other nodes as well. So, to fix this we would add nodeAffinity on the pods sections.</p> <p>If there are more complex operations(==, !=, etc) are involved then we might not be able to achieve using nodeSelector, so we need to use nodeAffinity and anti-affinity</p> <p>default labels of the node</p> <pre><code>get nodes node01 --show-labels\nkubectl label nodes kubenode01 size=small\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#node-affinity","title":"Node Affinity","text":"<p>The primary feature of Node Affinity is to ensure that the pods are hosted on particular nodes.</p>"},{"location":"docker_k8/k8/scheduling/#node-affinity-types","title":"Node Affinity Types","text":""},{"location":"docker_k8/k8/scheduling/#available","title":"Available","text":"<ul> <li>requiredDuringSchedulingIgnoredDuringExecution</li> <li>preferredDuringSchedulingIgnoredDuringExecution</li> </ul>"},{"location":"docker_k8/k8/scheduling/#planned","title":"Planned","text":"<ul> <li>requiredDuringSchedulingRequriedDuringExecution</li> <li>preferredDuringSchedulingRequiredDuringExecution</li> </ul>"},{"location":"docker_k8/k8/scheduling/#resource-limits","title":"Resource &amp; Limits","text":"<p>Each node has a set of CPU, Memory and Disk resources available. By default, K8s assume that a pod or container within a pod requires 0.5 CPU and 256Mi of memory. This is known as the Resource Request for a container.</p> <p>If your application within the pod requires more than the default resources, you need to set them in the pod definition file.</p> <pre><code>resources:\n     requests:\n      memory: \"1Gi\"\n      cpu: \"1\"\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#limits","title":"Limits","text":"<p>By default, k8s sets resource limits to 1 CPU and 512Mi of memory You can set the resource limits in the pod definition file.</p> <pre><code>limits:\n  memory: \"2Gi\"\n  cpu: \"2\"\n</code></pre> <p>Check the pod limitrange.yaml</p> <p>Note: Remember Requests and Limits for resources are set per container in the pod.</p>"},{"location":"docker_k8/k8/scheduling/#daemonset","title":"DaemonSet","text":"<p>DaemonSets are like replicaSet, as it helps in to deploy multiple instances of pod. But it runs one copy of your pod on each node in your cluster.</p> <p>Daemonset use cases: - Monitoring solution - logviewer - Helper pods for applications - Networking pods (weave net)</p>"},{"location":"docker_k8/k8/scheduling/#static-pods","title":"Static pods","text":"<p>When you need to create pods without any interference from the kubeapi server or kubernetes management control plane, you can write your yaml files and place in the kubelet directory and create pods. these kind of pods which have no interference from the any of the kubernetes components are called as static pod</p> <p>What if we run the static pods and if these nodes are part of kubernetes manage control plane, would it be known to kubeapi server? Yes kubeapi server would be known that there is pod running in the node as kubelet provides a mirror object in the kubeapi as read-only object.</p>"},{"location":"docker_k8/k8/scheduling/#first-method","title":"First Method","text":"<p>You can configure the kubelet to read the pod definition files from a directory on the server designated to store information about pods.The designated directory can be any directory on the host and the location of that directory is passed in to the kubelet as an option while running the service. --pod-manifest-path</p> <pre><code>systemctl status kubelet.service\ncat /lib/systemd/system/kubelet.service\nExecStart=/usr/local/bin/kubelet \\\\\n.\n.\n--pod-manifest-path=/etc/kubernetes/manifests\n.\n</code></pre> <p>Create a static pod</p> <pre><code>kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 &gt; /etc/kubernetes/manifests/static-busybox.yaml\n\nkubectl get pods\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#second-method","title":"Second Method","text":"<p>Incase if you have created kubeadmin way of configuring the cluster, your manifests will be in below location.</p> <pre><code>systemctl status kubelet.service\ncat /var/lib/kubelet/config.yaml\n\nlook for \"staticPodPath: /etc/kubernetes/manifests\"\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#multiple-schedulers","title":"Multiple schedulers","text":"<p>https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/</p> <p>my-scheduler pod</p> <p>we could ask nginx to create a pod using my-scheduler</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-my-scheduler\n  labels:\n    name: nginx-pod\nspec:\n  schedulerName: my-scheduler &lt;== custom scheduler.\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"docker_k8/k8/security/","title":"Security","text":"<p>We need to make two types of decisions.</p> <p>Authentication Who can access API server Authorization What can they do by gaining access to the cluster.</p>"},{"location":"docker_k8/k8/security/#tls-certificates","title":"TLS Certificates","text":"<p>All communication with the cluster, between the various components such as the ETCD Cluster, kube-controller-manager, scheduler, api server.</p> <p>Communication between the applications within cluster is defined by network policies</p>"},{"location":"docker_k8/k8/security/#authentication","title":"Authentication","text":"<p>there are two types of users,</p> <ul> <li>users ( admin/developer)</li> <li>service accounts (third party for service integrations, bots )</li> </ul> <p>All the user access is managed by API server and all requests go through it.</p>"},{"location":"docker_k8/k8/security/#authorization-mechanism","title":"Authorization Mechanism","text":"<ul> <li>static file</li> <li>static token file</li> <li>certificates</li> <li> <p>identity services</p> </li> <li> <p>authenticate using basic user file credentials</p> </li> </ul> <pre><code>curl -v -k http://master-node-ip:6443/api/v1/pods -u \"user1:password123\"\n</code></pre> <ul> <li>authenticate using token</li> </ul> <pre><code>curl -v -k http://master-node-ip:6443/api/v1/pods --header \"Authorization: Bearer &lt;Token&gt;\"\n</code></pre> <p>static file and token would be the easiest but not recommended, instead use role based access control</p>"},{"location":"docker_k8/k8/security/#tls-certificates_1","title":"TLS Certificates","text":"<ul> <li>What are TLS certificates?</li> <li>How does kubernetes use certificates?</li> <li>How to generate them?</li> <li>How to configure them?</li> <li>How to view them?</li> <li>How to troubleshoot issues related to certificates</li> </ul> <p>A certificate is used to gurantee trust between 2 parties during a transaction. tls certificates ensure that the communication between them is encrypted.</p>"},{"location":"docker_k8/k8/security/#symmetric","title":"Symmetric","text":"<p>It uses the same key to encrypt and decrypt the data and the key has to be exchanged between the sender and the receiver</p>"},{"location":"docker_k8/k8/security/#asymmetric","title":"Asymmetric","text":"<p>Instead of using single key to encrpyt and decrypt data, asymmetric encryption uses a pair of keys, a private key and a public key.</p>"},{"location":"docker_k8/k8/security/#certificate-naming-conventions","title":"Certificate naming conventions","text":"<p>certificate public key .crt, .pem Certificate private key *.pem</p>"},{"location":"docker_k8/k8/security/#kubernetes-tls-certificates","title":"Kubernetes TLS Certificates","text":"<p>Since, the method is being used for creation cluster is kubeadm, all the certificates are stored in below localtion. /etc/kubernetes/pki/</p> <p>Some of the certitcates are listed below</p> <p>Server Certificates for Servers Client Certificates for Clients</p> <p>Public keys</p> <pre><code>apiserver-etcd-client.crt  \napiserver-kubelet-client.crt  \napiserver.crt\nca.crt\nfront-proxy-ca.crt  \nfront-proxy-client.crt\n</code></pre> <p>Private keys</p> <pre><code>apiserver-etcd-client.key  \napiserver-kubelet-client.key  \napiserver.key\nca.key\nfront-proxy-ca.key  \nfront-proxy-client.key  \nsa.key\n</code></pre> <p>Viewing, certiticate details, these below to be looked into certificates</p> <pre><code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout\n</code></pre> <ul> <li>Issuer</li> <li>Validity</li> <li>Subject</li> <li>Subject Alternative Names</li> </ul> <p>Troubleshooting, kubeadm installation method.</p> <pre><code>kubectl logs etcd-master\ndocker ps -a\ndocker logs &lt;container-id&gt;\n</code></pre>"},{"location":"docker_k8/k8/security/#certificate-api","title":"Certificate API","text":"<p>The CA is really just the pair of key and certificate files that we have generated, whoever gains access to these pair of files can sign any certificate for the kubernetes environment. Kubernetes has a built-in certificates API that can do this for you.</p> <ul> <li>CertificateSigningRequest</li> <li>Review Requests</li> <li>Approve Requests</li> <li>Share to Users</li> </ul> <p>So user does create a key and CSR</p> <pre><code>openssl genrsa -out jane.key 2048\nopenssl req -new -key jane.key -subj \"/CN=jane\" -out jane.csr\n</code></pre> <p>Sends the request to the administrator and the adminsitrator takes the key and creates a CSR object, with kind as \"CertificateSigningRequest\" and a encoded \"jane.csr\"</p> <pre><code>apiVersion: certificates.k8s.io/v1beta1\nkind: CertificateSigningRequest\nmetadata:\n  name: jane\nspec:\n  groups:\n  - system:authenticated\n  usages:\n  - digital signature\n  - key encipherment\n  - server auth\n  request:\n    &lt;certificate-goes-here&gt;\n</code></pre> <pre><code>kubectl get csr\nkubectl certificate approve jane\nkubectl get csr jane -o yaml\necho \"&lt;certificate&gt;\" |base64 --decode\n</code></pre> <p>Deny request if its inappropriate</p> <pre><code>kubectl certificate deny jane\nkubectl delete csr jane\n</code></pre> <p>All the certificate releated operations are carried out by the controller manager.</p> <pre><code>$ cat kube-controller-manager.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    component: kube-controller-manager\n    tier: control-plane\n  name: kube-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - kube-controller-manager\n&lt;Clip&gt;\n    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n\n</code></pre>"},{"location":"docker_k8/k8/security/#kubeconfig","title":"kubeconfig","text":"<p>Client uses the certificate file and key to query the kubernetes Rest API for a list of pods using curl.</p> <pre><code>kubectl get pods --kubeconfig config\n</code></pre> <p>The kubeconfig file has 3 sections</p> <ul> <li>Clusters</li> <li>Contexts</li> <li>Users</li> </ul> <pre><code>apiVersion: v1\nclusters: &lt;============\n- cluster:\n    certificate-authority-data: LS0tLS1g==\n    server: https://192.168.56.51:6443\n  name: kubernetes\ncontexts:  &lt;============\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers: &lt;============\n- name: kubernetes-admin\n  user:\n    client-certificate-data: LS0tLtLQo=\n    client-key-data: LS0tCg==\n</code></pre> <pre><code>kubectl config view\nkubectl config veiw --kubeconfig=my-custom-config\nkubeclt config -h\n</code></pre>"},{"location":"docker_k8/k8/security/#api-groups","title":"API Groups","text":"<p>The kubernetes API is grouped into multiple such groups based on thier purpose. Such as one for APIs, one for healthz, metrics and logs etc.</p> <p>These APIs are catagorized into two. - core group, functionality exists - Named group, More organized and going forward all the newer features are going to be made available</p> <pre><code>curl http://localhost:6443 -k\ncurl http://localhost:6443/apis -k\n</code></pre>"},{"location":"docker_k8/k8/security/#rbac","title":"RBAC","text":""},{"location":"docker_k8/k8/security/#role","title":"Role","text":"<p>Roles and Rolebindings are namespaced meaning they are created within namespaces.</p> <p>Roles can be described with below three parameters. - apiGroups - resources - verbs</p> <p>we need to link the user to that role. i.e called rolebinding</p> <pre><code>kubectl create -f developer-role.yaml\nkubectl create -f devuser-developer-binding.yaml\nkubectl get roles\nkubectl get rolebindings\nkubectl describe role developer\nkubectl describe rolebinding devuser-developer-binding\n</code></pre>"},{"location":"docker_k8/k8/security/#verifiy-access","title":"Verifiy Access","text":"<pre><code>kubectl auth can-i create deployments\nkubectl auth can-i delete nodes\nkubectl auth can-i create deployments --as dev-user\nkubectl auth can-i create pods --as dev-user\nkubectl auth can-i create pods --as dev-user --namespace test\n</code></pre>"},{"location":"docker_k8/k8/security/#cluster-roles","title":"Cluster Roles","text":"<p>You won't be able to isolate the nodes within the namespaces, as these are cluster wide resources. there by, resources are catagorized either namespaced or cluster</p> <pre><code>kubectl api-resources --namespaced=true\nkubectl api-resources --namespaced=false\n</code></pre> <p>You can create a cluster role for namespace resources as well. When you do that user will have access to these resources across all namespaces</p> <pre><code>kubectl create -f cluster-roles.yml\nkubectl auth can-i list nodes --as michelle\n</code></pre>"},{"location":"docker_k8/k8/security/#images-security","title":"Images Security","text":"<p>when we pull image from \"nginx\" we are pulling from the nginx/nginx of the private docker.io hub. so you need to login to your hub.docker.com so that kubelet will try to pull image from the repository and deploy applications in the worker nodes.</p> <pre><code>containers:\n- name: nginx\n  image: nginx\n</code></pre> <p>what if the repository is private, then you need to specify the DNS name prefixed with the docker image.</p> <pre><code>containers:\n- name: nginx\n  image: private-repo/nginx:latest\n</code></pre> <p>If you have to modify the docker containers which has to be provided with the secrets, kubernets have secrets where you can store your credentials.</p> <pre><code>kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com\n</code></pre> <p>you would be using these credentials for the pod in applications to be up and running.</p> <pre><code>spec:\n  containers:\n  - name: private-reg-container\n    image: &lt;your-private-image&gt;\n  imagePullSecrets:\n  - name: private-reg-cred\n</code></pre>"},{"location":"docker_k8/k8/security/#secrurity-contexts","title":"Secrurity Contexts","text":"<p>You may choose to configure the security settings at a container level or at a pod level.</p> <p>To add security context on the container and a field called securityContext under the spec section.</p> <pre><code>spec:\n  securityContext:\n    runAsUser: 1010\n  containers:\n  - name: ubuntu\n</code></pre> <p>To add security context at security level,</p> <pre><code>spec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command: [\"sleep\", \"3600\"]\n    securityContext:\n      runAsUser: 1000\n      capabilities:\n        add: [\"MAC_ADMIN\"]\n</code></pre>"},{"location":"docker_k8/k8/security/#network-policies","title":"Network Policies","text":"<p>There are two types of traffic</p> <ul> <li>Ingress</li> <li>Egress</li> </ul> <p>Network policies can be applied on the pods for which traffic can be allowed or rejected. https://kubernetes.io/docs/concepts/services-networking/network-policies/</p>"},{"location":"docker_k8/k8/storage/","title":"Storage Concepts","text":""},{"location":"docker_k8/k8/storage/#introduction-of-docker-storage","title":"Introduction of Docker Storage","text":"<p>Docker stores data on the local file system, it creates this directory structures at /var/lib/docker, where it has all the folders and directories called (aufs, containers, image, volumes ). this is place where it stores data by default.</p> <p>All files related to containers are stored under the containers directory and the files related to images are stored under the image directory. Any volumes created by the Docker containers are created under the volumes directory.</p>"},{"location":"docker_k8/k8/storage/#layered-architecture","title":"Layered Architecture","text":"<p>When docker builds images, it builds these in a layered architecture. Each line of instruction in the Docker file creates a new layer in the Docker image with just the changes from the previous layer.</p> <p>Advantage of layered architecture: When image builds, docker is not going to build first three layers instead of it reuses the same three layers it built for the first application from the cache. This way, Docker builds images faster and efficiently saves disk spaces. Once the build is complete, you cannot modify the contents of these layers and so they are read-only and you can only modify them by initiating a new build.</p> <p>When you run a container based off of this image, using the Docker run command, Docker creates a container based off of these layers and creates a new writeable layer on top of the image layer. The writeable layer is used to store data created by the container such as log files written by the applications, any temporary files generated by the container.</p> <p>Examples:</p> <ul> <li>Let's take an example of our application code. Since we bake our code into the image, the code is part of the image and as such, its read-only. After running a container, what if I wish to modify the source code.</li> <li>Yes, I can still modify this file, but before I saved the modified file, Docker automatically creates a copy of the file in the read-write layer and I will then be modifying a different version of the file in the read-write layer. All future modifications will be done on this copy of the file in the read-write layer. This is called copy-on-right mechanism.</li> <li>The Image layer being a read-only just means that the files in these layers will not be modified in the image itself. So, the image will remain the same all the time until you rebuild the image using the Docker build command. If container destroyed then all of the data that was stored in the container layer also gets deleted.</li> </ul> <p>Container run time engine: Kubernetes used Docker alone as the container runtime engine, and all the code to work with Docker was embedded within the Kubernetes source code. The Container Runtime Interface is a standard that defines how an orchestration solution like Kubernetes would communicate with container runtimes like Docker</p>"},{"location":"docker_k8/k8/storage/#persistent-volume","title":"Persistent Volume","text":"<p>In the large environment, with a lot of users deploying a lot of pods, the users would have to configure storage every time for each Pod.</p> <p>A Persistent Volume is a cluster-wide pool of storage volumes configured by an administrator to be used by users deploying application on the cluster. The users can now select storage from this pool using Persistent Volume Claims.</p> <pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: pv-vol1\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  capacity:\n   storage: 1Gi\n  hostPath:\n   path: /tmp/data\n</code></pre> <pre><code>kubectl create -f pv.yaml\nkubectl get pv\nkubectl delete pv pv-vol1\n</code></pre>"},{"location":"docker_k8/k8/storage/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>Now we will create a Persistent Volume Claim to make the storage available to the node. Volumes and Persistent Volume Claim are two separate objects in the Kubernetes namespace. Once the Persistent Volume Claim created, Kubernetes binds the Persistent Volumes to claim based on the request and properties set on the volume.</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim1\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  resources:\n   requests:\n     storage: 300m\n</code></pre> <p>Kubernetes binds only one PVC to one PV. if there are no volumes for claims, then then claims will remain in the pending state. you can also use 'labels' or 'SelectLabels' to defined which volumes to be used for 'claims' These can be described based on parameters like - Sufficient capacity - Access Modes - Volume Modes - Storage Class - Selector</p> <p>There are different accessModes that can be mounted. - ReadWriteOnce -- the volume can be mounted as read-write by a single node - ReadOnlyMany -- the volume can be mounted read-only by many nodes - ReadWriteMany -- the volume can be mounted as read-write by many nodes</p> <p>In above example, though you might have an PV with 1G, if the claim is 300M it would select PV with 1GB. though if there are other claims that exists since it can't find the volumes it will be still pending.</p> <p>If the claims are deleted, we could set parameter in the PV to do these - Retain ( default ) - though claims are deleted, this will retain the volumes - Delete - claims are deleted, volumes are deleted - Recycle - volumes are scrubbed and can be re-used for claims</p>"},{"location":"docker_k8/k8/storage/#storage-class","title":"Storage Class","text":"<p>We created Persistent Volume but before this if we are taking a volume from Cloud providers like GCP, AWS, Azure. We need to first create disk.</p>"},{"location":"docker_k8/k8/storage/#static-provisioning","title":"Static Provisioning","text":"<p>We need to create manually each time when we define in the Pod definition file. that's called Static Provisioning.</p>"},{"location":"docker_k8/k8/storage/#dynamic-provisioning","title":"Dynamic Provisioning","text":"<p>No we have a Storage Class, So we no longer to define Persistent Volume. It will create automatically when a Storage Class is created. It's called Dynamic Provisioning</p> <pre><code>sc-definition.yaml\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: io1\n  iopsPerGB: \"10\"\n  fsType: ext4\n</code></pre> <pre><code>kubectl create -f sc-definition.yaml\nkubectl get sc\n</code></pre> <pre><code>pvc-definition.yaml\n\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  storageClassName: aws-ebs      \n  resources:\n   requests:\n     storage: 500Mi\n</code></pre> <pre><code>pod-definition.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: frontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: web\n  volumes:\n    - name: web\n      persistentVolumeClaim:\n        claimName: myclaim\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/issues/","title":"issues","text":""},{"location":"docker_k8/k8/troubleshooting/issues/#image-pull-error","title":"Image pull error","text":"<ul> <li>incorrect image</li> <li>incorect image tags</li> <li>incorrect secrets</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#crashing-pods","title":"Crashing pods","text":"<p>it restarts bcoz of the pod definition file has <code>restartPolicy: Always</code>. It can also be set as <code>never</code> or <code>on failure</code></p> <ul> <li>not providing ENV variable it can be <code>secrets</code> or <code>configmaps</code></li> <li>permission issues</li> <li>Unable to find the file/volume.</li> <li>OOMkilled</li> <li>liveness or rediness probes endpoint</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#pending-pods","title":"pending pods","text":"<ul> <li>insufficient cpu</li> <li>taints on the node</li> <li>node selector or missing labels </li> <li>missing toleration on nodes</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#missing-pods","title":"missing pods","text":"<ul> <li>resource quota on your namespace</li> <li>necessary resource accounts or dependencies on your namespace</li> </ul> <p>Always, check the events i.e <code>kubectl get events -n &lt;namespace&gt;</code></p>"},{"location":"docker_k8/k8/troubleshooting/issues/#schrodingers-deployment","title":"Schr\u00f6dinger's Deployment","text":"<p>Always labels in the deployment when created would get the same match for the pods, which will forward traffic to the nodes.  Note: you must also have the same label configured for the service as well to respond the traffic back. </p> <pre><code>kubectl get pods -l app=web\nkubectl get svc -l app=web\nkubectl get endpoints\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/","title":"Troubleshooting","text":"<p>Basic commands required to troubleshoot application</p> <pre><code>kubeclt get all \nkubectl get all -A\nkubectl get -n &lt;namespace&gt; deployments -o yaml\nkubectl get pods -A\nkubectl get pods -n &lt;namespace&gt;\nkubectl describe pod &lt;podname&gt; -n &lt;namespace&gt;\nkubectl describe node # check for events\nkubectl logs &lt;podname&gt; -n namespace\nkubectl get events -n &lt;namespace&gt;\nkubectl logs -n &lt;namespace&gt; --all-containers\nkubectl logs &lt;podname&gt; -c &lt;container&gt;\nkubectl logs -l app=webtier\nkubectl logs podname --timestamps \nkubectl logs podname since=5s\nkubectl logs -n &lt;namespace&gt; &lt;pod&gt; --since=1h # logs since 1hr\nkubectl logs &lt;podname&gt; -n namespace -f\nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -- ls \nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -c &lt;container&gt; --ls\nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -c &lt;container&gt; -it bash\nkubectl port-forward -n &lt;namespace&gt; svc/app.service laptop:listeningport\nkubectl explain &lt;resource&gt;.spec # documentation\nkubectl top node # memory/CPU\nkubectl auth can-i list pods -n namespace\nkubectl auth whoami\nkubectl diff\n\nkubectl rollout restart deployment -n &lt;namespace&gt; &lt;deployment&gt;\nkubectl get events -n &lt;namespace&gt;\n\nkubectl debug - why ?\n- minimize pod disruptions\n- distroless images\n- crashed container\n</code></pre> <p>ephemeral containers useful</p> <ul> <li>When using distroless images</li> <li>Images doesn\u2019t contain debugging utilities</li> <li>Debugging container in a crashloop</li> </ul> <p>Create debug pod for troubleshooting</p> <pre><code>kubectl debug -it ephemeral --image=busybox:1.28 --target=ephemeral\n</code></pre> <pre><code>kubectl debug -it ephemeral --image=busybox:1.28 --target=ephemeral\nTargeting container \"ephemeral\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\n--profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example \"--profile=general\".\nDefaulting debug container name to debugger-gvbx8.\nIf you don't see a command prompt, try pressing enter.\n/ # ps aux\nPID   USER     TIME  COMMAND\n    1 root      0:00 /pause\n   56 root      0:00 sh\n  112 root      0:00 ps aux\n/ # \n</code></pre> <p>upon exiting the shell, the container gets terminated. </p>"},{"location":"docker_k8/k8/troubleshooting/overview/#application-failures","title":"Application Failures","text":"<ul> <li>Application/Service status of the webserver</li> <li>endpoint of the service and compare it with the selectors</li> <li>status and logs of the pod</li> <li>logs of the previous pod</li> </ul> <pre><code>curl http://web-service-ip:node-port\nkubectl describe service web-service\nkubectl get pod\nkubectl describe pod web\nkubectl logs web\nkubectl logs web -f --previous\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#control-plane-failure","title":"Control Plane Failure","text":"<ul> <li>Nodes in the cluster, are they Ready or NotReady, If they are NotReady then check the LastHeartbeatTime of the node to find out the time when node might have crashed</li> <li>Possible CPU and MEMORY using top and df -h</li> <li>Status and the logs of the kubelet for the possible issues.</li> <li>Check the kubelet Certificates, they are not expired, and in the right group and issued by the right CA</li> </ul> <pre><code>kubectl get nodes\nkubectl describe node worker-1\ntop\ndf -h\nserivce kubelet status\nsudo journalctl \u2013u kubelet\nopenssl x509 -in /var/lib/kubelet/worker-1.crt -text\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#worker-nodes-failure","title":"Worker nodes failure","text":"<ul> <li>Status of the Nodes in the cluster, are they Ready or NotReady</li> <li>If they are NotReady then check the LastHeartbeatTime of the node</li> <li>Check the possible CPU and MEMORY using top and df -h</li> <li>Check the status and the logs of the kubelet for the possible issues.</li> <li>Check the kubelet Certificates, they are not expired, and in the right group and issued by the right CA.</li> </ul> <pre><code>kubectl get nodes\nkubectl describe node worker-1\nserivce kubelet status\nsudo journalctl \u2013u kubelet\nopenssl x509 -in /var/lib/kubelet/worker-1.crt -text\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#network-failures","title":"Network Failures","text":""},{"location":"ds_ai_ml/ai/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/ai/overview/#gen-ai-context","title":"Gen AI context","text":""},{"location":"ds_ai_ml/ai/overview/#ai-overvierw","title":"AI Overvierw","text":"<p>Making intelligient machines especially computer programs that simulate human intelligience and decision making. e.g: self drivinf cars, recomendation engines from netwfils or ammazon</p>"},{"location":"ds_ai_ml/ai/overview/#ml-overvierw","title":"ML Overvierw","text":"<p>Algorithm is trained using historical data to make predictions on the new data.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#supervisor-learning","title":"supervisor learning","text":"<p>algorithms(classic/regression) are trained to use labelled data. i.e raw data you annotate which will be provided to algorithm.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#unsupervisoed-learning","title":"unsupervisoed learning","text":"<p>algorithms(Clustering/Association) are trained to use unlabelled data. i.e raw data you annotate which will be provided to algorithm.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#reinforcements","title":"reinforcements","text":"<p>train algorith on trail and error approach. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#deep-learning-and-artifical-neural-networks","title":"Deep learning and Artifical neural networks","text":"<p>Artifical neural networks - mimic the structure of human brain</p> <p>Deep leanring - subset of ML, that focus on building artifical neural networks that can learn from data. </p> <p></p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#generative-ai","title":"Generative AI","text":"<p>model in GenAI fundational model which is <code>[ data + trained alogorithm ]</code> which generates text, images, videos from trained data. e.g ChatGPT/GPT</p> <p></p> <p>traditional machine learning models have disadvantage which is over come by fondational models</p> <p></p> <p>key characteristic of foundational models and its uses. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#working","title":"Working","text":"<p>What are tokens, parameters and temperature ?</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#use-cases","title":"use-cases","text":""},{"location":"ds_ai_ml/ai/overview/#categorization","title":"categorization","text":""},{"location":"ds_ai_ml/ai/overview/#aws-ai-services","title":"AWS AI services","text":""},{"location":"ds_ai_ml/ai/overview/#ec2-compute-ai-service","title":"EC2 Compute AI service","text":"<p>AWS Tranium and Inferentia - Train and deploy models using EC2 machines.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#sagemaker-ai","title":"SageMaker AI","text":"<p>Pre-generative traditional AI/ML service to help you build, train and deploy machine learning models.  you are now using Amazon bedrock service instead of this traditional mode, but this model can be used for analytics. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#bedrock","title":"bedrock","text":""},{"location":"ds_ai_ml/ai/overview/#q-business","title":"Q business","text":""},{"location":"ds_ai_ml/ai/overview/#q-developer","title":"Q developer","text":""},{"location":"ds_ai_ml/ai/overview/#aws-bedrock-arch","title":"AWS Bedrock Arch","text":""},{"location":"ds_ai_ml/ai/overview/#randomness-and-diversity","title":"Randomness and diversity","text":""},{"location":"ds_ai_ml/ai/overview/#length","title":"Length","text":""},{"location":"ds_ai_ml/ai/overview/#repetations","title":"repetations","text":"<p>Available only in few models</p> <p></p>"},{"location":"ds_ai_ml/ds/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/ds/overview/#data-science-methodology","title":"Data Science Methodology","text":"<p>Module 1: From Problem to Approach</p> <p>Business Understanding Analytic Approach</p> <p>Module 2: From Requirements to Collection</p> <p>Data Requirements Data Collection</p> <p>Module 3: From Understanding to Preparation</p> <p>Data Understanding Data Preparation</p> <p>Module 4: From Modeling to Evaluation</p> <p>Modeling Evaluation</p> <p>Module 5: From Deployment to Feedback</p> <p>Deployment Feedback</p>"},{"location":"ds_ai_ml/ds/overview/#business-understanding","title":"Business understanding","text":"<p>Data science methodology begins with spending the time to seek clarification, to attain what can be referred to as a business understanding. Having this understanding is placed at the beginning of the methodology because getting clarity around the problem to be solved, allows you to determine which data will be used to answer the core question.</p> <p>Establishing a clearly defined question starts with understanding the GOAL of the person who is asking the question. For example, if a business owner asks:  \"How can we reduce the costs of performing an activity?\"</p> <p>We need to understand, is the goal to improve the efficiency of the activity? Or is it to increase the businesses profitability?</p> <p>Once the goal is clarified, the next piece of the puzzle is to figure out the objectives that are in support of the goal. By breaking down the objectives, structured discussions can take place where priorities can be identified in a way that can lead to organizing and planning on how to tackle the problem.Depending on the problem, different stakeholders will need to be engaged in the discussion to help determine requirements and clarify questions.</p> <p>Case study for \"Business Understanding\" In the case study, the question being asked is: What is the best way to allocate the limited healthcare budget to maximize its use in providing quality care?</p> <p>This question is one that became a hot topic for an American healthcare insurance provider. As public funding for readmissions was decreasing, this insurance company was at risk of having to make up for the cost difference,which could potentially increase rates for its customers. Knowing that raising insurance rates was not going to be a popular move, the insurance company sat down with the health care authorities in its region and brought in IBM data scientists to see how data science could be applied to the question at hand. Before even starting to collect data, the goals and objectives needed to be defined. After spending time to determine the goals and objectives, the team prioritized \"patient readmissions\" as an effective area for review. With the goals and objectives in mind, it was found that approximately 30% of individuals who finish rehab treatment would be readmitted to a rehab center within one year; and that 50% would be readmitted within five years. After reviewing some records, it was discovered that the patients with congestive heart failure were at the top of the readmission list. It was further determined that a decision-tree model could be applied to review this scenario, to determine why this was occurring. To gain the business understanding that would guide the analytics team in formulating and performing their first project, the IBM Data scientists, proposed and delivered an on-site workshop to kick things off. The key business sponsors involvement throughout the project was critical, in that the sponsor:</p> <ul> <li>Set overall direction</li> <li>Remained engaged and provided guidance.</li> <li>Ensured necessary support, where needed.</li> <li>Finally, four business requirements were identified for whatever model would be built, i.e<ul> <li>Predicting readmission outcomes for those patients with Congestive Heart Failure</li> <li>Predicting readmission risk.</li> <li>Understanding the combination of events that led to the predicted outcome</li> <li>Applying an easy-to-understand process to new patients, regarding their readmission risk.</li> </ul> </li> </ul>"},{"location":"ds_ai_ml/ds/overview/#analytic-approach","title":"Analytic Approach","text":"<p>Once the problem to be addressed is defined, the appropriate analytic approach for the problem is selected in the context of the business requirements.</p> <p>Once a strong understanding of the question is established, the analytic approach can be selected. This means identifying what type of patterns will be needed to address the question most effectively. </p> <ul> <li> <p>Question is to determine probabilities of an action, then a predictive model might be used.</p> </li> <li> <p>Question is to show relationships, a descriptive approach maybe be required. This would be one that would look at clusters of similar activities based on events and preferences.</p> </li> <li> <p>Question is to Statistical analysis applies to problems that require counts.</p> </li> <li> <p>Question requires a yes/ no answer, then a classification approach to predicting a response would be suitable.</p> </li> </ul> <p>Machine Learning can be used to identify relationships and trends in data that might otherwise not be accessible or identified.</p> <p>In the case where the question is to learn about human behaviour, then an appropriate response would be to use Clustering Association approaches.</p> <p>Case study related to applying Analytic Approach.,  a decision tree classification model was used to identify the combination of conditions leading to each patient's outcome.</p> <p>In this approach, examining the variables in each of the nodes along each path to a leaf, led to a respective threshold value. This means the decision tree classifier provides both the predicted outcome, as well as the likelihood of that outcome, based on the proportion at the dominant outcome, yes or no, in each group.</p> <p>From this information, the analysts can obtain the readmission risk, or the likelihood of a yes for each patient. If the dominant outcome is yes, then the risk is simply the proportion of yes patients in the leaf. If it is no, then the risk is 1 minus the proportion of no patients in the leaf.</p> <p>A decision tree classification model is easy for non-data scientists to understand and apply, to score new patients for their risk of readmission. Clinicians can readily see what conditions are causing a patient to be scored as high-risk and multiple models can be built and applied at various points during hospital stay. This gives a moving picture of the patient's risk and how it is evolving with the various treatments being applied. For these reasons, the decision tree classification approach was chosen for building the Congestive Heart Failure readmission model.</p>"},{"location":"ds_ai_ml/ds/overview/#data-requirements","title":"Data Requirements","text":"<p>Building on the understanding of the problem at hand, and then using the analytical approach selected, the Data Scientist is ready to get started.</p> <p>Now let's look at some examples of the data requirements within the data science methodology.</p> <p>Prior to undertaking the data collection and data preparation stages of the methodology, it's vital to define the data requirements for decision-tree classification. This includes identifying the necessary data content, formats and sources for initial data collection.</p> <p>So now, let's look at the case study related to applying \"Data Requirements\".</p> <p>In the case study, the first task was to define the data requirements for the decision tree classification approach that was selected. This included selecting a suitable patient cohort from the health insurance providers member base. In order to compile the complete clinical histories, three criteria were identified for inclusion in the cohort.</p> <p>First, a patient needed to be admitted as in-patient within the provider service area, so they'd have access to the necessary information. Second, they focused on patients with a primary diagnosis of congestive heart failure during one full year. Third, a patient must have had continuous enrollment for at least six months, prior to the primary admission for congestive heart failure, so that complete medical history could be compiled.</p> <p>Congestive heart failure patients who also had been diagnosed as having other significant medical conditions, were excluded from the cohort because those conditions would cause higher-than-average re-admission rates and, thus, could skew the results. Then the content, format, and representations of the data needed for decision tree classification were defined. This modeling technique requires one record per patient, with columns representing the variables in the model. To model the readmission outcome, there needed to be data covering all aspects of the patient's clinical history. This content would include admissions, primary, secondary, and tertiary diagnoses, procedures, prescriptions, and other services provided either during hospitalization or throughout patient/doctor visits. </p> <p>Thus, a particular patient could have thousands of records, representing all their related attributes. To get to the one record per patient format, the data scientists rolled up the transactional records to the patient level, creating a number of new variables to represent that information.</p> <p>This was a job for the data preparation stage, so thinking ahead and anticipating subsequent stages is important.</p>"},{"location":"ds_ai_ml/ds/overview/#data-collection","title":"Data Collection","text":"<p>Collecting data requires that you know the source or, know where to find the data elements that are needed.</p> <p>After the initial data collection is performed, an assessment by the data scientist takes place to determine whether or not they have what they need.</p> <p>Once the data ingredients are collected, then in the data collection stage, the data scientist will have a good understanding of what they will be working with. Techniques such as descriptive statistics and visualization can be applied to the data set, to assess the content, quality, and initial insights about the data. Gaps in data will be identified and plans to either fill or make substitutions will have to be made. In essence, the ingredients are now sitting on the cutting board.</p> <p>So now, let's look at the case study related to applying \"Data Collection\".</p> <p>demographic, clinical and coverage information of patients, provider information, claims records, as well as pharmaceutical and other information related to all the diagnoses of the congestive heart failure patients.</p> <p>For this case study, certain drug information was also needed, but that data source was not yet integrated with the rest of the data sources. This leads to an important point: It is alright to defer decisions about unavailable data, and attempt to acquire it at a later stage.</p> <p>For example, this can even be done after getting some intermediate results from the predictive modeling. If those results suggest that the drug information might be important in obtaining a good model, then the time to try to get it would be invested. As it turned out though, they were able to build a reasonably good model without this drug information.</p> <p>DBAs and programmers often work together to extract data from various sources, and then merge it. This allows for removing redundant data, making it available for the next stage of the methodology, which is data understanding.</p> <p>At this stage, if necessary, data scientists and analytics team members can discuss various ways to better manage their data, including automating certain processes in the database, so that data collection is easier and faster.</p>"},{"location":"ds_ai_ml/ds/overview/#data-understanding","title":"Data Understanding","text":"<p>Data understanding encompasses all activities related to constructing the data set. Essentially, the data understanding section of the data science methodology answers the question:  Is the data that you collected representative of the problem to be solved?</p> <p>Case Study:</p> <p>In order to understand the data related to congestive heart failure admissions, descriptive statistics needed to be run against the data columns that would become variables in the model.</p> <ul> <li> <p>First, these statistics included Hearst, univariates, and statistics on each variable, such as mean, median, minimum, maximum, and standard deviation.</p> </li> <li> <p>Second, pairwise correlations were used, to see how closely certain variables were related, and which ones, if any, were very highly correlated, meaning that they would be essentially redundant, thus making only one relevant for modeling.</p> </li> <li> <p>Third, histograms of the variables were examined to understand their distributions. Histograms are a good way to understand how values or a variable are distributed, and which sorts of data preparation may be needed to make the variable more useful in a model.</p> </li> </ul> <p>For example, for a categorical variable that has too many distinct values to be informative in a model, the histogram would help them decide how to consolidate those values. The univariates, statistics, and histograms are also used to assess data quality.</p> <p>From the information provided, certain values can be re-coded or perhaps even dropped if necessary, such as when a certain variable has many missing values. The question then becomes, does \"missing\" mean anything? Sometimes a missing value might mean \"no\", or \"0\" (zero), or at other times it simply means \"we don't know\". Or, if a variable contains invalid or misleading values, such as a numeric variable called \"age\" that contains 0 to 100 and also 999, where that \"triple-9\" actually means \"missing\", but would be treated as a valid value unless we corrected it.</p> <p>Initially, the meaning of congestive heart failure admission was decided on the basis of a primary diagnosis of congestive heart failure. But working through the data understanding stage revealed that the initial definition was not capturing all of the congestive heart failure admissions that were expected, based on clinical experience. This meant looping back to the data collection stage and adding secondary and tertiary diagnoses, and building a more comprehensive definition of congestive heart failure admission.</p> <p>This is just one example of the interactive processes in the methodology. The more one works with the problem and the data, the more one learns and therefore the more refinement that can be done within the model, ultimately leading to a better solution to the problem.</p>"},{"location":"ds_ai_ml/ds/overview/#data-preparation","title":"Data Preparation","text":"<p>In a sense, data preparation is similar to washing freshly picked vegetables in so far as unwanted elements, such as dirt or imperfections, are removed. Together with data collection and data understanding, data preparation is the most time-consuming phase of a data science project, typically taking seventy percent and even up to even ninety percent of the overall project time. Automating some of the data collection and preparation processes in the database, can reduce this time to as little as 50 percent.</p> <p>Case Study:</p> <p>Transforming data in the data preparation phase is the process of getting the data into a state where it may be easier to work with. Specifically, the data preparation stage of the methodology answers the question: What are the ways in which data is prepared?</p> <p>To work effectively with the data, it must be prepared in a way that addresses missing or invalid values and removes duplicates, toward ensuring that everything is properly formatted.</p> <p>Feature engineering is also part of data preparation. It is the process of using domain knowledge of the data to create features that make the machine learning algorithms work. A feature is a characteristic that might help when solving a problem. Features within the data are important to predictive models and will influence the results you want to achieve.</p> <p>Feature engineering is critical when machine learning tools are being applied to analyze the data. When working with text, text analysis steps for coding the data are required to be able to manipulate the data. The data scientist needs to know what they're looking for within their dataset to address the question. </p> <p>The text analysis is critical to ensure that the proper groupings are set, and that the programming is not overlooking what is hidden within. The data preparation phase sets the stage for the next steps in addressing the question. While this phase may take a while to do, if done right the results will support the project. If this is skipped over, then the outcome will not be up to par and may have you back at the drawing board. It is vital to take your time in this area, and use the tools available to automate common steps to accelerate data preparation. Make sure to pay attention to the detail in this area. After all, it takes just one bad ingredient to ruin a fine meal.</p>"},{"location":"ds_ai_ml/ds/overview/#modelling","title":"Modelling","text":"<p>Modelling is the stage in the data science methodology where the data scientist has the chance to sample the sauce and determine if it's bang on or in need of more seasoning!</p> <p>two key questions: First, what is the purpose of data modeling, and second, what are some characteristics of this process?</p> <p>Data Modelling focuses on developing models that are either descriptive or predictive.</p> <p>An example of a descriptive model might examine things like: if a person did this, then they're likely to prefer that.</p> <p>A predictive model tries to yield yes/no, or stop/go type outcomes.</p> <p>These models are based on the analytic approach that was taken, either statistically driven or machine learning driven. The data scientist will use a training set for predictive modelling. A training set is a set of historical data in which the outcomes are already known. The training set acts like a gauge to determine if the model needs to be calibrated. In this stage, the data scientist will play around with different algorithms to ensure that the variables in play are actually required.</p> <p>The success of data compilation, preparation and modelling, depends on the understanding of the problem at hand, and the appropriate analytical approach being taken. The data supports the answering of the question, and like the quality of the ingredients in cooking, sets the stage for the outcome. Constant refinement, adjustments and tweaking are necessary within each step to ensure the outcome is one that is solid.</p> <p>In John Rollins' descriptive Data Science Methodology, the framework is geared to do 3 things:  First, understand the question at hand.  Second, select an analytic approach or method to solve the problem, and third, obtain, understand, prepare, and model the data.</p> <p>The end goal is to move the data scientist to a point where a data model can be built to answer the question. With dinner just about to be served and a hungry guest at the table, the key question is: Have I made enough to eat? Well, let's hope so.</p> <p>In this stage of the methodology, model evaluation, deployment, and feedback loops ensure that the answer is near and relevant. This relevance is critical to the data science field overall, as it \u00eds a fairly new field of study, and we are interested in the possibilities it has to offer. The more people that benefit from the outcomes of this practice, the further the field will develop.</p>"},{"location":"ds_ai_ml/ds/overview/#evaluation","title":"Evaluation","text":"<p>A model evaluation goes hand-in-hand with model building as such, the modeling and evaluation stages are done iteratively. Model evaluation is performed during model development and before the model is deployed.</p> <p>Evaluation allows the quality of the model to be assessed but it's also an opportunity to see if it meets the initial request. Evaluation answers the question: Does the model used really answer the initial question or does it need to be adjusted?</p> <p>Model evaluation can have two main phases.</p> <p>The first is the diagnostic measures phase, which is used to ensure the model is working as intended. If the model is a predictive model, a decision tree can be used to evaluate if the answer the model can output, is aligned to the initial design. It can be used to see where there are areas that require adjustments. If the model is a descriptive model, one in which relationships are being assessed, then a testing set with known outcomes can be applied, and the model can be refined as needed.</p> <p>The second phase of evaluation that may be used is statistical significance testing. This type of evaluation can be applied to the model to ensure that the data is being properly handled and interpreted within the model. This is designed to avoid unnecessary second guessing when the answer is revealed.</p> <p>Case Study:</p> <p>Let's look at one way to find the optimal model through a diagnostic measure based on tuning one of the parameters in model building. Specifically we'll see how to tune the relative cost of misclassifying yes and no outcomes. As shown in this table, four models were built with four different relative misclassification costs. As we see, each value of this model-building parameter increases the true-positive rate, or sensitivity, of the accuracy in predicting yes, at the expense of lower accuracy in predicting no, that is, an increasing false-positive rate.</p> <p>The question then becomes, which model is best based on tuning this parameter? For budgetary reasons, the risk-reducing intervention could not be applied to most or all congestive heart failure patients, many of whom would not have been readmitted anyway. On the other hand, the intervention would not be as effective in improving patient care as it should be, with not enough high-risk congestive heart failure patients targeted.</p> <p>So, how do we determine which model was optimal? As you can see on this slide, the optimal model is the one giving the maximum separation between the blue ROC curve relative to the red base line. We can see that model 3, with a relative misclassification cost of 4-to-1, is the best of the 4 models. And just in case you were wondering, ROC stands for receiver operating characteristic curve, which was first developed during World War II to detect enemy aircraft on radar. It has since been used in many other fields as well. Today it is commonly used in machine learning and data mining. The ROC curve is a useful diagnostic tool in determining the optimal classification model.</p> <p>This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied. In this case, the criterion is a relative misclassification cost. By plotting the true-positive rate against the false-positive rate for different values of the relative misclassification cost, the ROC curve helped in selecting the optimal model.</p>"},{"location":"ds_ai_ml/ds/overview/#deployment","title":"Deployment","text":"<p>While a data science model will provide an answer, the key to making the answer relevant and useful to address the initial question, involves getting the stakeholders familiar with the tool produced.</p> <p>In a business scenario, stakeholders have different specialties that will help make this happen, such as the solution owner, marketing, application developers, and IT administration. Once the model is evaluated and the data scientist is confident it will work, it is deployed and put to the ultimate test. Depending on the purpose of the model, it may be rolled out to a limited group of users or in a test environment, to build up confidence in applying the outcome for use across the board.</p> <p>Case Study</p> <p>In preparation for solution deployment, the next step was to assimilate the knowledge for the business group who would be designing and managing the intervention program to reduce readmission risk. In this scenario, the business people translated the model results so that the clinical staff could understand how to identify high-risk patients and design suitable intervention actions.</p> <p>The goal, of course, was to reduce the likelihood that these patients would be readmitted within 30 days after discharge. During the business requirements stage, the Intervention Program Director and her team had wanted an application that would provide automated, near real-time risk assessments of congestive heart failure. It also had to be easy for clinical staff to use, and preferably through browser-based application on a tablet, that each staff member could carry around. This patient data was generated throughout the hospital stay. It would be automatically prepared in a format needed by the model and each patient would be scored near the time of discharge. Clinicians would then have the most up-to-date risk assessment for each patient, helping them to select which patients to target for intervention after discharge. As part of solution deployment, the Intervention team would develop and deliver training for the clinical staff. Also, processes for tracking and monitoring patients receiving the intervention would have to be developed in collaboration with IT developers and database administrators, so that the results could go through the feedback stage and the model could be refined over time.</p> <p>This map is an example of a solution deployed through a Cognos application. In this case, the case study was hospitalization risk for patients with juvenile diabetes. Like the congestive heart failure use case, this one used decision tree classification to create a risk model that would serve as the foundation for this application. The map gives an overview of hospitalization risk nationwide, with an interactive analysis of predicted risk by a variety of patient conditions and other characteristics. This slide shows an interactive summary report of risk by patient population within a given node of the model, so that clinicians could understand the combination of conditions for this subgroup of patients. And this report gives a detailed summary on an individual patient, including the patient's predicted risk and details about the clinical history, giving a concise summary for the doctor.</p>"},{"location":"ds_ai_ml/ds/overview/#feedback","title":"Feedback","text":"<p>Once in play, feedback from the users will help to refine the model and assess it for performance and impact. The value of the model will be dependent on successfully incorporating feedback and making adjustments for as long as the solution is required. Throughout the Data Science Methodology, each step sets the stage for the next. Making the methodology cyclical, ensures refinement at each stage in the game. The feedback process is rooted in the notion that, the more you know, the more that you'll want to know.</p> <p>Once the model is evaluated and the data scientist is confident it'll work, it is deployed and put to the ultimate test: actual, real-time use in the field.</p> <p>Case Study</p> <p>The plan for the feedback stage included these steps:</p> <p>First, the review process would be defined and put into place, with overall responsibility for measuring the results of a \"flying to risk\" model of the congestive heart failure risk population. Clinical management executives would have overall responsibility for the review process.</p> <p>Second, congestive heart failure patients receiving intervention would be tracked and their re-admission outcomes recorded.</p> <p>Third, the intervention would then be measured to determine how effective it was in reducing re-admissions.</p> <p>For ethical reasons, congestive heart failure patients would not be split into controlled and treatment groups. Instead, readmission rates would be compared before and after the implementation of the model to measure its impact. After the deployment and feedback stages, the impact of the intervention program on re-admission rates would be reviewed after the first year of its implementation. Then the model would be refined, based on all of the data compiled after model implementation and the knowledge gained throughout these stages. Other refinements included: Incorporating information about participation in the intervention program, and possibly refining the model to incorporate detailed pharmaceutical data.</p> <p>If you recall, data collection was initially deferred because the pharmaceutical data was not readily available at the time. But after feedback and practical experience with the model, it might be determined that adding that data could be worth the investment of effort and time. We also have to allow for the possibility that other refinements might present themselves during the feedback stage. Also, the intervention actions and processes would be reviewed and very likely refined as well, based on the experience and knowledge gained through initial deployment and feedback.</p> <p>Finally, the refined model and intervention actions would be redeployed, with the feedback process continued throughout the life of the Intervention program.</p>"},{"location":"ds_ai_ml/mlops/data/","title":"Data","text":""},{"location":"ds_ai_ml/mlops/data/#collection","title":"Collection","text":""},{"location":"ds_ai_ml/mlops/data/#gathering","title":"gathering","text":""},{"location":"ds_ai_ml/mlops/models/","title":"Models","text":""},{"location":"ds_ai_ml/mlops/models/#development","title":"Development","text":""},{"location":"ds_ai_ml/mlops/models/#training","title":"training","text":""},{"location":"ds_ai_ml/mlops/models/#deployment","title":"deployment","text":""},{"location":"ds_ai_ml/mlops/models/#serving","title":"serving","text":""},{"location":"ds_ai_ml/mlops/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/mlops/security_gov/","title":"Security","text":""},{"location":"ds_ai_ml/mlops/security_gov/#security-intro","title":"Security Intro","text":""},{"location":"ds_ai_ml/mlops/security_gov/#governance","title":"Governance","text":""},{"location":"jobdesc/devops/","title":"devops","text":""},{"location":"jobdesc/devops/#accenture","title":"Accenture","text":"<p>Summary:As a DevOps Engineer, you will be responsible for building and setting up new development tools and infrastructure utilizing knowledge in continuous integration, delivery, and deployment (CI/CD), Cloud technologies, Container Orchestration and Security. You will build and test end-to-end CI/CD pipelines, ensuring that systems are safe against security threats. Your typical day will involve working on developing and implementing CI/CD pipelines, collaborating with cross-functional teams, and ensuring the security and efficiency of the development process.</p> <p>Roles &amp; Responsibilities:</p> <ul> <li>Expected to perform independently and become an SME.</li> <li>Required active participation/contribution in team discussions.</li> <li>Contribute in providing solutions to work related problems.</li> <li>Develop and implement CI/CD pipelines for efficient software development and deployment.</li> <li>Collaborate with cross-functional teams to ensure smooth integration and delivery of software.</li> <li>Ensure the security and integrity of systems by implementing security measures and best practices.</li> <li>Monitor and optimize the performance of CI/CD pipelines.</li> <li>Stay updated with the latest trends and technologies in DevOps and implement them in the development process.</li> </ul> <p>Professional &amp; Technical Skills:</p> <ul> <li>Must To Have Skills:Proficiency in SAP UI5 Development.</li> <li>Strong understanding of continuous integration, delivery, and deployment (CI/CD) principles.</li> <li>Experience with Cloud technologies and platforms such as AWS or Azure.</li> <li>Knowledge of container orchestration tools like Kubernetes or Docker.</li> <li>Solid understanding of security best practices and implementing security measures.</li> <li>Experience with scripting languages such as Python or Shell scripting.</li> </ul>"},{"location":"jobdesc/devops/#hcl","title":"HCL","text":"<ul> <li>Hands on experience in scripting (Shell, Python)</li> <li>Deep knowledge of the IBM Workload Scheduler</li> <li> <p>Design, develop automation suite and integrate with continuous integration process through Jenkins</p> </li> <li> <p>Implement automation and orchestration process framework for CI/CD pipeline</p> </li> <li>Knowledge of Scrum and Test Driven Development practices.</li> <li>Proficient in test automation using Java, Python and any scripting languages.</li> <li>Hands-on experience in writing, executing and monitoring automated test suites</li> <li>Profound knowledge in code deployment tools like Ansible, Chef</li> <li>Working knowledge of database, SQL queries and maintain Java web applications</li> <li>Good knowledge in container concepts, docker and kubernetes hands on experience is a value add</li> <li>Strong knowledge and hands on experience in unix OS</li> <li>Experience in network, server, application status monitoring and troubleshooting</li> <li>Possess good problem solving and debugging skills. Troubleshoot issues and coordinate with development team to - streamline code deployment to generate build</li> <li>Good understanding of build and deployment process for both on-premise and cloud</li> <li>Apply cloud computing skills to deploy the product in AWS, Azure, GCP</li> <li>Optimise the cloud computing architecture</li> <li>Conduct system tests for security, performance and availability</li> <li>Collaborate with team members to improve the process, tools, security</li> <li>Implement the best practices of CI/CD framework to increase the efficiency of development lifecycle</li> <li>Fulfilling all the commitments by timely delivering the deliverables</li> <li>Design, develop automation suite and integrate with continuous integration process through Jenkins</li> </ul>"},{"location":"jobdesc/srdevops/","title":"srdevops","text":""},{"location":"jobdesc/srdevops/#job-description-12-yrs","title":"job description 12 yrs","text":"<p>naukri senior devops job desc</p>"},{"location":"jobdesc/srdevops/#iconium-consulting","title":"Iconium Consulting","text":"<ol> <li>DevOps Implementation: \u2022 Lead the design and implementation of DevOps practices, including CI/CD pipelines, automation, and infrastructure as code (IaC). \u2022 Collaborate with software development and operations teams to streamline workflows and optimize processes. \u2022 Ensure the efficient and reliable delivery of software releases.</li> <li>Automation and CI/CD: \u2022 Create and maintain automation scripts and workflows for building, testing, and deploying applications. \u2022 Design and manage continuous integration and continuous deployment (CI/CD) pipelines. \u2022 Implement best practices for code versioning, branching, and deployment strategies.</li> <li>Infrastructure Management: \u2022 Manage and optimize cloud-based and on-premises infrastructure resources. \u2022 Implement Infrastructure as Code (IaC) principles using tools like Terraform, Ansible, or CloudFormation. \u2022 Ensure the availability, scalability, and security of infrastructure components.</li> <li>Containerization and Orchestration: \u2022 Architect and manage containerization solutions using Docker and container orchestration platforms such as Kubernetes. \u2022 Optimize container deployments for resource utilization and scalability. \u2022 Implement best practices for managing containerized applications.</li> <li>Security and Compliance: \u2022 Collaborate with security teams to implement and enforce security best practices within the DevOps processes. \u2022 Integrate security testing and vulnerability scanning into CI/CD pipelines. \u2022 Ensure compliance with industry standards and regulations.</li> <li>Monitoring and Performance: \u2022 Implement monitoring and alerting solutions to proactively identify and address issues. \u2022 Analyze system performance metrics to optimize resource usage. \u2022 Respond to incidents and troubleshoot issues to minimize downtime.</li> <li>Documentation and Knowledge Sharing: \u2022 Maintain comprehensive documentation for DevOps processes, configurations, and best practices. \u2022 Share knowledge and provide mentoring to junior team members. \u2022 Promote a culture of continuous learning and improvement.</li> </ol>"},{"location":"jobdesc/srdevops/#vco","title":"VCO","text":"<ul> <li>Design and support Infrastructure as Code automation and deployment processes</li> <li>Define, implement, and manage the provisioning, software configuration and release process for all applications and - environments</li> <li>Serve as a gatekeeper for infrastructure and system deployments; establish controls and</li> <li>processes around these functions</li> <li>Deliver new features in conjunction with the application team by providing highly available and scalable - infrastructure</li> <li>Improve infrastructure and reliability of systems through monitoring and metric collection</li> <li>Make continuous improvements to security and costs of infrastructure</li> <li>Maintain Operations Environment and Tools</li> <li>Provide production support</li> <li>Work with an Agile Development team responsible for designing, building, testing of high traffic Supply Chain platforms.</li> </ul> <p>Must Have Experience:</p> <ul> <li>Bachelors degree in computer science or equivalent experience</li> <li>Good verbal and written communication skills</li> <li>5+ years of cloud DevOps experience.</li> <li>6-8 years of cloud DevOps experience.</li> <li>Must have in-depth knowledge of Terraform, Kubernetes, AWS,Jenkins</li> <li>Nice to have: Monitoring and Metrics (Nagios, Prometheus, Grafana,New Relic)</li> <li>OS experience : Windows, Linux, Unix</li> <li>Cloud experience (AWS)</li> <li>Containerization (Docker, Kubernetes, ECR, EKS)</li> </ul>"},{"location":"jobdesc/srdevops/#next-sphere","title":"next sphere","text":"<ul> <li>Collaborating with coworkers to conceptualize, develop, and release software.</li> <li>Conducting quality assurance to ensure that the software meets prescribed guidelines.</li> <li>Rolling out fixes and upgrades to software, as needed.</li> <li>Securing software to prevent security breaches and other vulnerabilities.</li> <li>Collecting and reviewing customers' feedback to enhance user experience.</li> <li>Suggesting alterations to workflow in order to improve efficiency and success.</li> <li>Pitching ideas for projects based on gaps in the market and technological advancements.</li> </ul>"},{"location":"jobdesc/srdevops/#engminds","title":"Engminds","text":"<p>Engineersmind is focused on providing the greatest customer experience possible and hiring the most brilliant individuals to study and build technology that enhances the lives of people worldwide, all while maintaining a growing client base.</p> <p>About You and this Role We are looking for a Senior DevOps Engineer with talent to manage our group of DevOps Consultants in maintaining and improving our clients' deployments. Along with the internal team, clients, and outside IT specialists, you will collaborate to deploy, automate, and manage the software infrastructure. As a Senior DevOps engineer, you will also be responsible for setting up cloud infrastructure, monitoring schedules, and CI/CD pipelines.</p> <p>Key Responsibilities:</p> <ul> <li>Preferred to have certification - AWS associate level and Azure (good to have)</li> <li>Deep practical knowledge of AWS services i.e. S3, EC2, API Gateway, Load balancer, Secrets manager and route53, - AMI, RDS Pand Auroa services etc.</li> <li>Good to have knowledge of Azure data platform especially their build process Datalake and Power BI with DevOps - perspective, this is crucial</li> <li>Expert with terraform and CloudFormation</li> <li>Must know the best security practices for both cloud and on Prem as we work mostly with healthcare and financial - clients and security is paramount for us.</li> <li>Take charge of Continuous Deployment (CD), Continuous Improvement (CI) Github action, Argo cd, DevOps, and System - Integration. Interacting with project teams, the internal engineering department, and external customers.</li> <li>Manage and enhance cloud infrastructure such as AWS, Azure, and GCP.</li> <li>Discuss operating requirements for software solutions with management SDLC.</li> <li>Exchange information regarding the dangers, operational impact, and alternatives for information systems.</li> <li>As junior software engineers gain experience and assume responsibility for DevOps tasks, mentor them.</li> <li>Supervise the installation and configuration of the solution.</li> <li>Collaborate with developers on software specifications and evaluate test stage outcomes.</li> <li>Creating interface simulators and designing automated module deployments</li> <li>Completely update scripts and code and fix any problems with product implementation.</li> <li>Oversee routine maintenance procedures and perform diagnostic tests.</li> <li>Document processes and monitor performance indicators.</li> <li>Follow recommended practices for network administration and cybersecurity.</li> <li>Assist our customer in implementing the same infrastructure-as-code tools that we use internally.</li> <li>Work independently to proactively pinpoint system flaws, shortcomings, and opportunities for development.</li> <li>Keep track of problem solving, record solutions used, and produce troubleshooting manuals.</li> <li>As needed, communicate with the product development team, clients, partners and integrators of the solution, and other cross-functional teams.</li> <li>Use regular execution procedures and methods with efficacy.</li> </ul> <p>Technical Expertise:</p> <ul> <li>Proficiency in source control management, CI/CD, DevOps, and GitOps.</li> <li>Competence with Maven, Jenkins, Artifactory, Ansible, and Git.</li> <li>Competence with containerization and Kubernetes.</li> <li>Strong background in managing systems based on RedHat Linux.</li> <li>Familiarity with logging and monitoring systems like Datadog's etc</li> <li>Very good with databases both AWS and Azure</li> <li>Must know about Vue, React, TypeScript or JavaScript for modern web clients, and Restful APIs and how to do - automated deployment.</li> <li>Experience with cloud infrastructure management and automation technologies, as well as familiarity with a - comprehensive range of AWS infrastructure solutions (EBS, EC2, RDS, S3, EC2, CloudFront, Route 53, Secret Manager, ACM, Lambda, VPC).</li> </ul> <p>Secondary Skills:</p> <ul> <li>It is necessary to take full responsibility and accountability for all duties, including obtaining requirements, - finishing technical work, documenting, and assisting with delivery.</li> <li>Self-starter who is at ease compiling data from many sources.</li> <li>Outstanding analytical and debugging abilities, including the ability to troubleshoot complicated systems with - several tiers of technology.</li> <li>Demonstrated aptitude at picking up new tools, languages, and software development techniques.</li> <li>Excellent and clear communication abilities, both written and spoken.</li> <li>Dedicated, meticulous, customer-focused, and a team player.</li> <li>Excellent interpersonal abilities.</li> </ul>"},{"location":"jobdesc/srdevops/#yamaha-motor-co","title":"Yamaha Motor Co","text":"<p>Role</p> <p>As a DevOps Engineer at Yamaha  you will work with the Engineering and Data Science teams to deliver amazing new features and take our product to the next level. You will be a part of a highly creative, agile, human centred team who values your happiness, growth and personal development. You\u2019ll be a self-starter and highly motivated thinker who loves to learn new things and enjoys when no two days are the same. The role involves working in an agile engineering team, assisting in the development and continuous improvement of our cloud infrastructure and systems.</p> <p>To be successful in this role you will have:</p> <p>\u2022 Strong written and verbal communication skills</p> <p>\u2022 Scripting experience in Python and Bash</p> <p>\u2022 Hands on experience with Linux and Windows systems</p> <p>\u2022 Knowledge of Git commands and branching</p> <p>\u2022 Experience managing in Azure networking concepts including but not limited to virtual networks, route tables, Hub and Spoke network design.</p> <p>\u2022 Experience with the following Azure services:</p> <p>o Azure Firewall</p> <p>o Azure Kubernetes Service</p> <p>o Azure VM Scale Sets</p> <p>o Azure Functions</p> <p>o Event Hubs</p> <p>o Virtual Networks</p> <p>o Cosmos DB</p> <p>o Azure API Management o Azure Storage (ADLS Gen 2) o Azure AD &amp; DNS \u2022 Experience with Terraform, Jenkins CI/CD Declarative Pipelines, Grafana</p> <p>\u2022 Experience with Databricks</p> <p>\u2022 Experience with Azure DevOps pipelines</p> <p>\u2022 Experience with Kubernetes Administration and Management, minimum CKA certification or equivalent experience</p> <p>\u2022 Experience with Active Directory and DNS management</p> <p>Skills that will be highly desired but not essential:</p> <p>\u2022 Experience with SAML integration using Okta as IDP</p> <p>\u2022 Experience with Okta administration</p> <p>\u2022 Experience working with or deploying SEIM/Infrastructure Logging solutions.</p> <p>Roles and Responsibilities Responsibilities</p> <p>There is no typical working week at the Yield. But here are some of the things you might be working on, dayto-day;</p> <p>\u2022 Working with software and data science teams to ensure efficient release of software and update/create CI/CD automation to enable repeatable releases.</p> <p>\u2022 Build and deploy infrastructure to support business requirements.</p> <p>\u2022 Follow and contribute towards engineering and design guidelines for developing highly scalable, available and fault tolerant systems.</p> <p>\u2022 Contributing to a cohesive, diverse, and high-performing team that is genuinely inclusive and genderbalanced.</p> <p>\u2022 Establishing good internal and external relationships, communicating verbally and in writing with key stakeholders</p> <p>\u2022 Being flexible and able to work independently, we need people who will do whatever it takes to get the job done.</p> <p>\u2022 Focus on secure infrastructure and development practices that extends to reviewing new and existing infrastructure with a focus on ensuring configuration is secure and follows principle of least privilege</p>"},{"location":"jobdesc/srdevops/#bounteous","title":"Bounteous","text":"<p>Key Responsibilities:</p> <ul> <li>Pipeline Development and Optimisation:</li> <li>Design, develop, and maintain CI/CD pipelines in GitLab from scratch</li> <li>Optimise pipelines for performance, reliability, and scalability</li> <li>Collaborate with development teams to integrate GitLab CI/CD pipelines into their workflows</li> </ul> <p>Deployment Management:</p> <ul> <li>Write deployment scripts and YAML files to automate the deployment of applications and services</li> <li>Ensure seamless deployment processes across various environments (development, testing, production)</li> <li>Testing Integration and Optimisation:</li> <li>Integrate automated unit, integration, and regression tests written in Cucumber into the GitLab pipelines</li> <li>Optimise the performance of running automated tests to ensure quick feedback cycles</li> </ul> <p>Monitoring and Tooling:</p> <ul> <li>Build and implement monitoring tools within GitLab to track pipeline performance, failures, and bottlenecks</li> <li>Develop dashboards and alerting mechanisms to provide visibility into pipeline health</li> </ul> <p>Must Have:</p> <ul> <li>Proven experience as a DevOps Engineer, with a strong focus on GitLab CI/CD</li> <li>Proficient in shell scripting and Windows PowerShell</li> <li>Experience in writing deployment scripts and YAML files for automation and configuration management</li> <li>Strong experience in integrating automated unit, integration, and regression tests written in Cucumber into CI/CD pipelines</li> <li>Strong knowledge of monitoring and logging tools (Splunk, Grafana, Dx-APM)</li> <li>Strong knowledge of Terraform, Kubernetes, Docker, Cactus, ArgoCD,</li> </ul>"},{"location":"jobdesc/sre/","title":"sre","text":""},{"location":"jobdesc/sre/#job-description-12-yrs","title":"job description 12 yrs","text":"<p>naukri senior sre job desc</p>"},{"location":"jobdesc/sre/#ibm-sre","title":"IBM-SRE","text":"<p>The ideal candidate will have 5 to 8 years of experience in ensuring the reliability, availability, and performance of critical services and systems. This role involves building and maintaining infrastructure, automating processes, and responding to incidents to ensure our systems run smoothly and efficiently.</p> <p>Infrastructure Management:</p> <ul> <li>Design, build, and maintain scalable, resilient infrastructure using cloud platforms (AWS and Azure).</li> <li>Manage and optimize Kubernetes clusters, containers, and microservices.</li> <li>Implement Infrastructure as Code (IaC) using tools like Terraform (Must), Ansible (Good to have), or CloudFormation(Good to have).</li> </ul> <p>Automation &amp; CI/CD:</p> <ul> <li>Maintain automated CI/CD pipelines to ensure rapid, safe, and reliable delivery of software.</li> <li>Automate repetitive tasks, processes, and workflows to increase efficiency and reduce human error.</li> <li>Implement and maintain monitoring, logging, and alerting systems to ensure visibility into system performance.</li> </ul> <p>Cost Optimization:</p> <ul> <li>Set up monitoring and reporting tools to track cloud spending in real-time.</li> <li>Regularly review the architecture and operations to identify areas where costs can be reduced. This includes - evaluating new tools, services, or practices that could lead to further cost savings.</li> <li>Collaborate with development teams to ensure that cost-efficient practices are followed in software design and - deployment.</li> <li>Recommend and manage the purchase of reserved instances, savings plans, or other discounts offered by cloud providers to reduce costs for long-term workloads.</li> </ul> <p>Incident Response &amp; Troubleshooting:</p> <ul> <li>Respond to and resolve incidents in a timely manner, ensuring minimal downtime and impact on customers.</li> <li>Perform root cause analysis and post-mortem reviews to prevent recurrence of issues.</li> <li>Collaborate with development teams to improve system reliability through proactive issue identification and resolution.</li> </ul> <p>Performance Optimization:</p> <ul> <li>Monitor system performance and capacity, and implement improvements to optimize efficiency and scalability.</li> <li>Analyze and improve application performance, ensuring high availability and low latency.</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>Ensure security best practices are followed across the infrastructure.</li> <li>Implement security controls and monitoring to protect against vulnerabilities and threats.</li> <li>Work with compliance teams to ensure systems adhere to regulatory requirements.</li> </ul> <p>Collaboration &amp; Communication:</p> <ul> <li>Work closely with software engineers, product managers, platform team, Global Support and other stakeholders to - ensure system reliability aligns with business goals.  </li> <li>Provide guidance and mentorship to junior SREs and other team members.</li> <li>Document processes, procedures, and best practices for the broader team.</li> <li>Required Technical and Professional Expertise</li> <li>5-8 years of experience in Site Reliability Engineering, DevOps, or a similar role.</li> <li>Strong experience with cloud platforms (AWS and Azure) and cloud-native technologies.</li> <li>Proficiency in scripting languages (e.g., Python, Bash) and automation tools.</li> <li>Experience with containerization (Docker, Kubernetes) and orchestration.</li> <li>Knowledge of networking, security, and infrastructure best practices.</li> <li>Familiarity with monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack).</li> <li>Strong problem-solving skills and ability to work under pressure.</li> <li>Excellent communication and collaboration skills.</li> <li>Preferred Technical and Professional Expertise</li> <li>Experience with database management (SQL).</li> <li>Knowledge of software development practices and experience with Agile methodologies.</li> <li>Certifications in cloud platforms (AWS Certified, Azure Certified, K8s certification etc.)</li> </ul>"},{"location":"jobdesc/sre/#sre-automation-developer","title":"SRE Automation Developer","text":"<p>Job description</p> <p>The candidate will support the Siemens Xcelerator platform and will be responsible for identifying, managing, improving, and reporting on availability, resiliency, reliability, and stability efficiencies. This includes providing technical guidance and leadership to drive solutions, create &amp; enhance processes that deliver excellence. A strong relationship with the various product teams of the Xcelerator platform is necessary to support core objectives. This roles success will be defined by product teams within DISW business units meeting their SLAs.</p> <p>Responsibilities/Tasks</p> <ul> <li>Provide &amp; lead the design, deployment, automation, and scripting solutions to drive new capabilities, visibility, - and efficiency</li> <li>Collaborate with other technical platforms and partners to engineer automated and integrated solutions between - tools, services, teams that increase availability, reliability, and performance.</li> <li>Own and ensure the internal and external SLA s meet and exceed expectations</li> <li>Be part of maintaining a 24x7, global, highly available SaaS environment</li> <li>Participate in an on-call rotation that supports our production infrastructure</li> <li>Troubleshoot production availability incidents that often span across multiple teams and services.</li> <li>Lead production incident post-mortems, and contribute to solutions to prevent problem recurrence; with the goal of - automated response to all non-exceptional service conditions</li> <li>Communicate to business and technical partners on incidents as they occur when they impact system performance or availability at a critical level</li> </ul> <p>Required Knowledge/Skills, Education, and Experience</p> <p>Bachelor s Degree with at least 2+ years of IT experience or equivalent experience.</p> <ul> <li>4+ years experience with automation via scripting &amp; API development</li> <li>3+ years experience with software development in the cloud</li> <li>2+ years experience with observability tools</li> <li>(Datadog, CloudWatch, CloudTrail, Elastic Stack, Grafana, or equivalent tools)</li> <li>2+ years experience with containerization, specifically Kubernetes</li> <li>2+ years experience with Amazon Web Services (AWS) services</li> <li>2+ years experience Terraform, CloudFormation, Ansible, or equivalent tools</li> <li>2+ years experience with Python</li> <li>Preferred Knowledge/Skills, Education, and Experience</li> </ul> <p>Siemens Teamcenter software - Desired certifications include: Datadog, Kubernetes, AWS or Azure certification - 2+ years experience as a Site Reliability Engineer or equivalent role - 2+ years experience with issue/incident tracking tool - (ServiceNOW, ServiceDesk, Jira or equivalent tools) - 2+ years with log management tools (ie ELK Stack) - 2+ years experience Enterprise IT environment with distributed environments - Networking concepts, including firewalls, VPN, routing, load balancers, security and DNS - Senior level system administration experience, including troubleshooting, support, mentorship/training, and oversight Attachments</p>"},{"location":"linux/admin/basics/","title":"basic","text":""},{"location":"linux/admin/basics/#linux-boot-process","title":"linux boot process","text":"<p>BIOS</p> <p>upon booting the system, it would perform system integrity checks and searches, loads and executes the body loader program (e.g CDROM, Floppy, Network..etc). Once the boot loader is detached and loaded into the memory, BIOS gives control to it, BIOS would then load the MBR boot loader.</p> <p>MBR</p> <p>located in the first sector of the bootlabel disk(/dev/hda or /dev/sda) which is 512MB, i.e 446Bytes of primary boot loader info, partition table of 64 bytes and last 2 bytes of validation checks, which contains info about the GRUB or LILO. MBR loads and executes GRUB boot loader.</p> <p>GRUB</p> <p>It has the complete knowledge of the system information and would load the file in /etc/grub/grub.conf which loads the kernel images and initrd images.</p> <p>kernel</p> <p>Mounts the root file system specified in the grub and executes /sbin/init program which is the first process in the linux system and loads all the necessary drivers compilied inside which helps to access the hard drive and other hardware devices.</p> <p>init</p> <p>looks /etc/inittab file to decide the runlevel and loads all the appropriate programs from it.</p> <p>runlevel</p> <p>depending on your runlevel from the /etc/inittab all the services from the /etc/rc3.d/ which start from 'S' will be executed which has all the services coming up."},{"location":"linux/admin/basics/#login-process","title":"login process","text":"<p>once the init process completes the run-level and finally it would be executing /etc/rc.local. it will start process called getty(get terminal) which initiates the login command and opens termial device, initializes it and prints login: and waits for user to to enter username.</p> <p>once user enters his name it starts /etc/login and prompts for the password which is hidden. it would next checks the credentials by verifying with /etc/passwd and /etc/shadow password matches then it initiates user properties gathering and starts users shell. If password doesn\u2019t match then getty terminates login process and re-initiates again with new prompt.</p> <p>In next stage the getty process reads the user properties (username, UID, GID, home directory, user shell etc.) from /etc/passwd file. After that it reads <code>/etc/motd</code> file for displaying content banner message.</p> <p>shell related properties, user aliases and variables getty process reads appropriate system wide default files /etc/profile or /etc/bashrc . After the system wide default files are read the shell reads user specific login files <code>.profile</code> or <code>.login</code></p> <p>At last stage it reads shell specific configuration files (.bashrc, .bash_profile etc. for bash shell) of that user which it gets on the users home directory.</p>"},{"location":"linux/admin/basics/#job-scheduler","title":"job scheduler","text":"<p>Cron</p> <p>Field    Description    Allowed Value MIN      Minute field    0 to 59 HOUR     Hour field      0 to 23 DOM      Day of Month    1-31 MON      Month field     1-12 DOW      Day Of Week     0-6 CMD      Command         Any command to be executed.</p> <pre><code>- schedule a cron to execute at 2 am daily\n0 2 * * * /bin/sh backup.sh\n\n- Schedule a cron to execute twice a day.\n0 5,17 * * * /scripts/script.sh\n\n- Schedule a cron to execute every Sunday at 5 PM\n0 17 * * sun  /scripts/script.sh\n\n- Schedule a cron to execute every 10 minutes\n*/10 * * * * /scripts/monitor.sh\n\n- Schedule a cron to execute on selected months.\n\\* \\* \\* jan,may,aug *  /script/script.sh\n\n- Schedule a cron to execute on selected days\n0 17 * * sun,fri  /script/script.sh\n\n- Schedule a cron to execute on the first Sunday of every month.\n0 2 * * sun  [ $(date +%d) -le 07 ] &amp;&amp; /script/script.sh\n\n- Schedule a cron to execute every four hours.\n0 */4 * * * /scripts/script.sh\n\n- Schedule a cron to execute twice every Sunday and Monday\n0 4,17 * * sun,mon /scripts/script.sh\n\n- Schedule tasks to execute yearly/monthly/weekly/daily/hourly\n@yearly/@monthly/@weekly/@daily/@hourly /bin/script.sh\n</code></pre> <p>at</p> <p>Jobs created with at command are executed only once.</p> <p>Schedule a job for the coming Monday at a time twenty minutes later than the current time <code>at Monday +20 minutes</code></p> <p>Schedule a job to run at 1:45 Aug 12 2020 <code>at 1:45 081220</code></p> <p>Schedule a job to run at 3pm four days from now <code>at 3pm + 4 days</code></p> <p>Schedule a job to shutdown the system at 4:30 today <code>echo \"shutdown -h now\" | at -m 4:30</code></p> <p>Schedule a job to run five hour from now <code>at now +5 hours</code></p>"},{"location":"linux/admin/basics/#login-non-login-shells","title":"login &amp; non-login shells","text":"<p>login</p> <p>When a user successfully logs in to a Linux system via terminal/SSH/switches to a user with the su - command, a login shell is created.</p> <p>echo $0 prints hypen then its a non-login shell</p> <ul> <li>login shell invokes <code>/etc/profile</code></li> <li><code>/etc/profile</code> invokes scripts in <code>/etc/profile.d/*.sh</code>, then executes users <code>~/.bash_profile</code></li> <li><code>~/.bash_profile</code> invokes users <code>~/.bashrc</code></li> <li><code>~/.bashrc</code> invokes <code>/etc/bashrc</code></li> </ul> <p>non-login shell</p> <p>A non-login shell is started by a login shell. shell that you start from another shell or from a program is a non-login shell.</p> <p>echo $0 prints no-hypen then its a non-login shell</p> <ul> <li>Non login shell first executes ~/.bashrc</li> <li>Then ~/.bashrc executes /etc/bashrc</li> <li>/etc/bashrc calls the scripts in /etc/profile.d</li> </ul>"},{"location":"linux/admin/basics/#softlinks-vs-hardlinks","title":"softlinks Vs hardlinks","text":"<ul> <li> <p><code>hard link</code>can only be created for a file but cannot be created for directories, where as <code>soft link</code> can link to a directory.</p> </li> <li> <p>Removing the original file that hard link points to does not remove the hardlink itself, the hardlink still provides the content of the underlying file.</p> </li> <li> <p>If we remove the hard link or the symlink itself, the original file will stay intact.</p> </li> </ul>"},{"location":"linux/admin/basics/#user-addition-process","title":"user addition process","text":"<p>When invoked, useradd creates a new user account according to the options specified on the command line and the default values set in the /etc/default/useradd</p> <p>useradd also reads the content of the /etc/login.defs file. This file contains configuration for the shadow password suite such as password expiration policy, ranges of user IDs used when creating system and regular users, and more..The command adds an entry to the /etc/passwd, /etc/shadow, /etc/group and /etc/gshadow files</p> <p>password fields:   - Login username    - Hash password    - Number of days since password has changed 01-01-1979   - Mininum days for a user to change his password   - Days available for password expiry   - Password expiry warining   - Empty. </p>"},{"location":"linux/admin/basics/#umask","title":"umask","text":"<p>umask lets you create the default permission for the files and folders, user can choose to restrict permissions by using a permission masks.</p> <pre><code>0    ---    No permission\n1    --x    Execute\n2    -w-    Write\n3    -wx    Write and execute\n4    r--    Read\n5    r-x    Read and execute\n6    rw-    Read and write\n7    rwx    Read, write, and execute\n</code></pre> <p>system default permission values are 777 (rwxrwxrwx) for folders and 666 (rw-rw-rw-) for files</p> <ul> <li>The default mask for a non-root user is 002, changing the folder permissions to 775 (rwxrwxr-x), and file permissions to 664 (rw-rw-r--).</li> <li>The default mask for a root user us 022, changing the folder permissions to 755 (rwxr-xr-x), and file permissions to 644 (rw-r--r--).</li> </ul>"},{"location":"linux/admin/basics/#file-special-permissions","title":"file special permissions","text":"<p>special permission bit are set on file or folder, thereby permitting only the owner or root user of the file or folder to modify, rename or delete the concerned directory or file. No other user would be permitted to have these privileges on a file which has a sticky bit.</p>"},{"location":"linux/admin/basics/#suid","title":"suid","text":"<p>we set SUID(set-user-ID) bit on the executable this behavior can be changed, then the file will always run with privileges of the owner of the file, no matter who runs the executable. instead of 'x' in the file permission you would be getting 's' in the user's permission</p> <p>Octal String: 4</p> <p>chmod: u+s</p> <p>e.g /usr/bin/passwd, /usr/bin/gpasswd</p>"},{"location":"linux/admin/basics/#sgid","title":"sgid","text":"<p>Set-group-ID bit on a file: Set-group-ID (SGID) is similar to SUID except that, an executable with SGID bit set runs with the privileges of the group which owns of the file</p> <p>Octal String: 2</p> <p>chmod : g+s</p>"},{"location":"linux/admin/basics/#sticky","title":"sticky","text":"<p>This special permission is useful to prevent users from deleting other user\u2019s file inside a shared folder where everyone has read, write, and execute access</p> <p>Octal String: 1</p> <p>chmod: 't'</p>"},{"location":"linux/admin/basics/#swap-size","title":"swap size","text":"<pre><code>sudo swapoff -a\nsudo dd if=/dev/zero of=/swapfile bs=1G count=8\nsudo mkswap /swapfile\nsudo swapon\n</code></pre>"},{"location":"linux/admin/basics/#etcfstab-fields","title":"/etc/fstab fields","text":"<p>The block device The mountpoint The filesystem type Mount options:     rw (read-write);     suid (respect setuid and setgid bits);     dev (interpret characters and block devices on the filesystem);     exec (allow executing binaries and scripts);     auto (mount the filesystem when the -a option of the mount command is used);     nouser(make the filesystem not mountable by a standard user);     async (perform I/O operations on the filesystem asynchronously).</p> <p>file system dump Fsck order:     default = 0     root = 1     non-root = 2</p>"},{"location":"linux/admin/basics/#password-policy","title":"password policy","text":"<p>/etc/login.defs  - all the password related informations can be found here.</p> <pre><code>PASS_MAX_DAYS    99999\nPASS_MIN_DAYS    0\nPASS_MIN_LEN    5\nPASS_WARN_AGE    7\n</code></pre>"},{"location":"linux/admin/basics/#cpu-utilization-calculation","title":"cpu utilization calculation","text":"<p>Linux load averages are \"system load averages\" that show the running thread (task) demand on the system as an average number of running plus waiting threads. This measures demand, which can be greater than what the system is currently processing. Most tools show three averages, for 1, 5, and 15 minutes</p> <p>The \u201cload\u201d is not the utilization but the total queue length.</p> <p>When load averages first appeared in Linux, they reflected CPU demand, as with other operating systems. But later on Linux changed them to include not only runnable tasks, but also tasks in the uninterruptible state (TASK_UNINTERRUPTIBLE or nr_uninterruptible). This state is used by code paths that want to avoid interruptions by signals, which includes tasks blocked on disk I/O and some locks. You may have seen this state before: it shows up as the \"D\" state in the output ps and top. The ps(1) man page calls it \"uninterruptible sleep (usually IO)\".</p> <p>Adding the uninterruptible state means that Linux load averages can increase due to a disk (or NFS) I/O workload, not just CPU demand</p>"},{"location":"linux/admin/basics/#use-of-nohup","title":"use of nohup","text":"<p>Nohup (stands for no hangup) is a command that ignores the HUP signal. You might be wondering what the HUP signal is. It is basically a signal that is delivered to a process when its associated shell is terminated. Usually, when we log out, then all the running programs and processes are hangup or stopped. If we want to continue running the process even after logout or disconnection from the current shell, we can use the nohup command</p> <p><code>nohup command &amp;</code></p> <ul> <li>how do you set priority for the process ?</li> <li>Explain how DNS works in Linux ?</li> <li>What is NFS and how would you configure it ?</li> <li>what is autofs and how's it configured ?</li> <li>what are TCP wrappers ?</li> <li>Explain about the IPtables and its chains ?</li> <li>How will you find out the system activity ?</li> <li>How will you trace all the system calls ?</li> </ul>"},{"location":"linux/admin/basics/#search-the-largest-or-empty-file","title":"search the largest or empty file","text":"<pre><code>-exec CMD: The file being searched which meets the above criteria and returns 0 for as its exit status for successful command execution.\n-ok CMD : It works same as -exec except the user is prompted first.\n-inum N : Search for files with inode number \u2018N\u2019.\n-links N : Search for files with \u2018N\u2019 links.\n-name demo : Search for files that are specified by \u2018demo\u2019.\n-newer file : Search for files that were modified/created after \u2018file\u2019.\n-perm octal : Search for the file if permission is \u2018octal\u2019.\n-print : Display the path name of the files found by using the rest of the criteria.\n-empty : Search for empty files and directories.\n-size +N/-N : Search for files of \u2018N\u2019 blocks; \u2018N\u2019 followed by \u2018c\u2019can be used to measure size in characters; \u2018+N\u2019 means size &gt; \u2018N\u2019 blocks and \u2018-N\u2019 means size &lt; \u2018N\u2019 blocks.\n-user name : Search for files owned by user name or ID \u2018name\u2019.\n\\(expr \\) : True if \u2018expr\u2019 is true; used for grouping criteria combined with OR or AND.\n! expr : True if \u2018expr\u2019 is false.\n</code></pre> <p>examples:</p> <ul> <li>Search a file with pattern.  <code>find ./GFG -name *.txt</code></li> <li>How to find and delete a file with confirmation. <code>find ./GFG -name sample.txt -exec rm -i {} \\;</code></li> <li>Search for empty files and directories <code>find ./GFG -empty</code></li> <li>Search for file with entered permissions <code>find ./GFG -perm 664</code></li> <li>Search text within multiple files <code>find ./ -type f -name \"*.txt\" -exec grep 'Geek'  {} \\;</code></li> </ul>"},{"location":"linux/admin/basics/#open-files-in-linux","title":"open files in linux","text":"<p>Graceful shutdown of relevant process would have not done for an On Linux or Unix systems, deleting a file via rm or through a file manager application will unlink the file from the file system's directory structure; however, if the file is still open (in use by a running process) it will still be accessible to this process and will continue to occupy space on disk. Therefore such processes may need to be restarted before that file's space will be cleared up on the filesystem.</p> <p><code>lsof | egrep \"deleted|COMMAND\"</code></p> <p>After a file has been identified, free the file used space by shutting down the affected process. If a graceful shutdown does not work, then issue the kill command to forcefully stop it by referencing the PID.</p> <p>link</p>"},{"location":"linux/admin/basics/#diff-yum-vs-rpm","title":"diff yum Vs rpm","text":"<p>Difference between RPM and YUM.</p> parameter RPM YUM Depencies resolvement no yes Multiple package instllations no yes automatic upgrades no yes online repo support no yes Autonomy RPM is autonomous and utilizes its own database to keep information about the packages on the system. YUM is a front-end utility that uses the RPM package manager for package management. The utility also uses the RPM database in the backend. Ease of use RPM package management and handling gets complicated at times. It is the easiest way to manage RPM packages. Rollback RPM doesn't support change rollback. YUM allows any changes to be rolled back."},{"location":"linux/admin/basics/#configure-local-yum-repo","title":"configure local yum repo","text":"<p>/etc/yum.conf - Main config file for yum.</p> <pre><code>[main]\ncachedir=/var/cache/yum/$basearch/$releasever\nkeepcache=0\ndebuglevel=2\nlogfile=/var/log/yum.log\nexactarch=1\nobsoletes=1\ngpgcheck=1\nplugins=1\ninstallonly_limit=3\n\n[comments abridged]\n\n# PUT YOUR REPOS HERE OR IN separate files named file.repo\n# in /etc/yum.repos.d\n</code></pre> <p>/etc/yum.repos.d/redhat.repo</p> <pre><code>[redhat]\nname = Red Hat Enterprise Linux\nbaseurl = file:/// or http:///\nenabled = 1\ngpgcheck = 1\ngpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\nsslverify = 1\nsslcacert = /etc/rhsm/ca/redhat-uep.pem\nsslclientkey = /etc/pki/entitlement/key.pem\nsslclientcert = /etc/pki/entitlement/11300387955690106.pem\n</code></pre>"},{"location":"linux/admin/basics/#recover-corrupted-rpmdb","title":"Recover corrupted rpmdb","text":"<pre><code>tar -zcvf /backups/rpmdb-$(date +\"%d%m%Y\").tar.gz  /var/lib/rpm\nrm -f /var/lib/rpm/__db*\n/usr/lib/rpm/rpmdb_verify /var/lib/rpm/Packages\n</code></pre> <p>In case the above operation fails, meaning you still encounter errors, then you should dump and load a new database</p> <pre><code># cd /var/lib/rpm/\n# mv Packages Packages.back\n# /usr/lib/rpm/rpmdb_dump Packages.back | /usr/lib/rpm/rpmdb_load Packages\n# rpm -qa\n# rpm -vv --rebuilddb\n# /usr/lib/rpm/rpmdb_verify Packages\n</code></pre>"},{"location":"linux/admin/basics/#webserver-issues","title":"Webserver issues","text":"<ul> <li>Application is suffering from the performance problem issues, how would you troubleshoot ?</li> <li>Need to create an FTP server to my localteam to help them setup yum server by creating a new ext3 volume ?</li> <li>user is unable to login to the system, what all could be the issues ?</li> <li>How do you set the user quota enabled in linux for user's homed irectory ?</li> <li>One of the NFS client is unable to access the share, how would you troubleshoot ?</li> <li>How would configure to authenticate nginx/httpd server ?</li> <li>Webserver is unable to access, explain the troubleshooting steps ?</li> </ul>"},{"location":"linux/admin/basics/#references","title":"References","text":"<p>linuxatemyram</p> <p>dns client issue troubleshooting</p>"},{"location":"linux/admin/networking/","title":"network","text":""},{"location":"linux/admin/networking/#ositcp-ip","title":"OSI/TCP IP","text":"<p>TCP/IP model more accurately represents the suite of protocols that are deployed in modern networks.</p> <p></p> <p>The layers in the TCP/IP network model, in order, include:</p> <p>Layer 5: Application  Layer 4: Transport  Layer 3: Network/Internet Layer 2: Data Link Layer 1: Physical</p>"},{"location":"linux/admin/networking/#layer-1-the-physical-layer","title":"Layer 1: The physical layer","text":"<p>Identify the cable has been plugged in, but we can easily troubleshoot physical layer problems from the Linux command line.</p> <pre><code>ip link show\n</code></pre> <p>Any indication of DOWN in the above output for the eth0 interface. This result means that Layer 1 isn\u2019t coming up.</p> <p>We might try troubleshooting by bringing up the interface(eth0) just to rule out that network interface disabled can be ruled out. </p> <pre><code>ip link set eth0 up\nip link show\nip -br link show # prints output in more readable format\nip -s link show eth0 # prints additional statistics about interface\n</code></pre> <p>ethtool utility is an excellent option. A particularly good use case for this command is checking to see if an interface has negotiated the correct speed. </p>"},{"location":"linux/admin/networking/#layer-2-the-data-link-layer","title":"Layer 2: The data link layer","text":"<p>The data link layer is responsible for local network connectivity. The most relevant Layer 2 protocol for most sysadmins is the Address Resolution Protocol (ARP), which maps Layer 3 IP addresses to Layer 2 Ethernet MAC addresses. When a host tries to contact another host on its local network (such as the default gateway), it likely has the other host\u2019s IP address, but it doesn\u2019t know the other host\u2019s MAC address. ARP solves this issue and figures out the MAC address for us. </p> <p>If your localhost can\u2019t successfully resolve its gateway\u2019s Layer 2 MAC address, then it won\u2019t be able to send any traffic to remote networks. This problem might be caused by having the wrong IP address configured for the gateway, or it may be another issue, such as a misconfigured switch port.</p> <p><code>ip neighbor show</code></p> <p>Linux caches the ARP entry for a period of time, so you may not be able to send traffic to your default gateway until the ARP entry for your gateway times out.</p> <pre><code> # ip neighbor show\n192.168.122.170 dev eth0 lladdr 52:54:00:04:2c:5d REACHABLE\n192.168.122.1 dev eth0 lladdr 52:54:00:11:23:84 REACHABLE\n# ip neighbor delete 192.168.122.170 dev eth0\n# ip neighbor show\n192.168.122.1 dev eth0 lladdr 52:54:00:11:23:84 REACHABLE\n</code></pre>"},{"location":"linux/admin/networking/#layer-3-the-networkinternet-layer","title":"Layer 3: The network/internet layer","text":"<p>Layer 3 involves working with IP addresses. IP addressing provides hosts with a way to reach other hosts that are outside of their local network</p> <pre><code>ip -br address show\n</code></pre> <p>check the interfaces and see it has ipaddress associated with the interface.  The lack of an IP address can be caused by a local misconfiguration, such as an incorrect network interface config file, or it can be caused by problems with DHCP.</p> <p>Layer 3 is the ping utility. Ping sends an ICMP Echo Request packet to a remote host, and it expects an ICMP Echo Reply in return. If you\u2019re having connectivity issues to a remote host, ping is a common utility to begin your troubleshooting</p> <p>Many might have blocked the ping hence you use traceroute. As with ICMP, intermediate routers may filter the packets that traceroute relies on, such as the ICMP Time-to-Live Exceeded message. But more importantly, the path that traffic takes to and from a destination is not necessarily symmetric, and it\u2019s not always the same.</p> <p>Another common issue that you\u2019ll likely run into is a lack of an upstream gateway for a particular route or a lack of a default route. When an IP packet is sent to a different network, it must be sent to a gateway for further processing. The gateway should know how to route the packet to its final destination. The list of gateways for different routes is stored in a routing table.</p> <pre><code>ip route show\n</code></pre>"},{"location":"linux/admin/networking/#layer-4-the-transport-layer","title":"Layer 4: The transport layer","text":"<p>The transport layer consists of the TCP and UDP protocols, with TCP being a connection-oriented protocol and UDP being connectionless. Applications listen on sockets, which consist of an IP address and a port. Traffic destined to an IP address on a specific port will be directed to the listening application by the kernel.</p> <p>The first thing that you may want to do is see what ports are listening on the localhost</p> <p>Another common issue occurs when a daemon or service won\u2019t start because of something else listening on a port</p> <pre><code>ss -tunlp4\n</code></pre> <p>The telnet command attempts to establish a TCP connection with whatever host and port you give it. This feature is perfect for testing remote TCP connectivity</p> <pre><code>telnet database.example.com 3306\ntelnet nfs.example.com 2049\n</code></pre> <p>The netcat utility can be used for many other things, including testing TCP connectivity.Note that netcat may not be installed on your system, and it\u2019s often considered a security risk to leave lying around. You may want to consider uninstalling it when you\u2019re done troubleshooting</p> <p>similarly, nmap which is capable of doing ..</p> <ul> <li>TCP and UDP port scanning remote machines.</li> <li>OS fingerprinting.</li> <li>Determining if remote ports are closed or simply filtered.</li> </ul>"},{"location":"linux/admin/networking/#how-examplecom-works","title":"How example.com works","text":"<ul> <li>The client types www.example.com in his browser</li> <li>The operating system looks at /etc/host file,first for the ip address of www.example.com(this can be changed from /etc/nsswitch), then looks /etc/resolv.conf for the DNS server IP for that machine</li> <li>the dns server will search its database for the name www.example.com, if it finds it will give that back, if not it will query the root server(.) for the information.</li> <li>root server will return a referral to the .com TLD name server(these TLD name servers knows the address of name servers of all SLD's).In our case we searched for www.example.com so root server will give us referral to .com TLD servers.</li> <li>Now One of the TLD servers of .com will give us the referral to the DNS server resposible for example.com domain.</li> <li>The dns server for example.com domain will now give the client the ip address of www host(www is the host name.)</li> </ul> <p>finally, type dig +trace www.google.com</p>"},{"location":"linux/admin/networking/#linux-dns-client-troubleshooting","title":"Linux DNS Client Troubleshooting","text":"<p>There are multiple potential points of failure during the DNS lookup process such as at the system performing the lookup, at the DNS cache, or on an external DNS server. </p> <p>Local Server Configuration</p> <p>it\u2019s important to understand the \u2018hosts\u2019 section of the /etc/nsswitch.conf file. <code>hosts: files dns myhostname</code></p> <p>Essentially this means that host name resolution will be performed in the order specified, left to right. First files will be checked, followed by DNS. As files are first these will be checked first, this references the local /etc/hosts file which contains static host name to IP address mappings. This file takes priority over any DNS resolution, any changes to the file will be placed straight into the DNS cache of that local server.</p> <p>If there is no entry in the hosts file DNS will be used next as per /etc/nsswitch.conf. The servers used for DNS resolution will be specified in the /etc/resolv.conf file</p> <p>For DNS resolution to succeed the DNS server will need to accept TCP and UDP traffic over port 53 from our server. A port scanner such as the nmap tool can be used to confirm if the DNS server is available on port 53</p> <pre><code>nmap -sU -p 53 &lt;dns server&gt;\ntcpdump -n host &lt;dns server&gt;\ndig google.com\n</code></pre>"},{"location":"linux/admin/networking/#website-down","title":"Website DOWN","text":""},{"location":"linux/admin/networking/#server-is-running","title":"Server is running?","text":"<pre><code>ping 1.2.3.4 \nssh 1.2.3.4\n</code></pre>"},{"location":"linux/admin/networking/#remote-port-opened","title":"remote port opened ?","text":"<pre><code>telnet 1.2.3.4 80\nnmap -p 80 1.2.3.4\nnc -vz 1.2.3.4 80\n</code></pre> <p>nmap states: - Open: target machine is listening for connections/packets on that port  - Filtered: A filtered nmap cannot determine whether the port is open because packet filtering prevents its probes from reaching the port. - Closed: ports have no application listening on them, though they could open up at any time. - unfiltered: Ports are classified as unfiltered when they are responsive to Nmap's probes, but Nmap cannot determine whether they are open or closed</p>"},{"location":"linux/admin/networking/#test-for-listening-ports","title":"Test for Listening Ports","text":"<pre><code>netstat -lnp | grep 80\n</code></pre> <p>Here the 0.0.0.0:80 tells us that the host is listening on all of its IPs for port 80 traffic.</p>"},{"location":"linux/admin/networking/#command-line-response-test","title":"Command line response test","text":"<p>curl has an advantage over raw telnet for web server troubleshooting in that it takes care of the HTTP protocol for us and makes things like testing authentication, posting data, using SSL</p> <pre><code>curl http://1.2.3.4\n</code></pre>"},{"location":"linux/admin/security/","title":"security","text":""},{"location":"linux/admin/security/#linux-security-checklist","title":"Linux security checklist","text":"<ul> <li>Keep the system Updated with Latest Security Patches</li> <li>Keep Yourself updated with latest vulnerabilities through mailing lists, forums etc.</li> <li>Disable and stop unwanted services on the server</li> <li>Use SUDO to limit ROOT Access</li> <li>SSH security settings</li> <li>Check the integrity of critical files using checksum</li> <li>Tunnel all of your X-Window Sessions through SSH</li> <li>Use SeLinux If required.</li> <li>Only create required no of users</li> <li>Maintain a good firewall policy</li> <li>Configure SSL/TLS if you are using FTP</li> <li>check file permissions accross filesystems.</li> <li>Use tools like adeos for potential file state</li> <li>Ensure sticky bit on /tmp Directory</li> <li>check and lock users with blank passwords.</li> <li>Bootloader and BIOS security</li> <li>Give special attention to portmap related services</li> <li>Deploy your NFS shares with Kerberos Authentication.</li> <li>Enable remote Logging</li> <li>Disable root Logins by editing /etc/securetty</li> <li>Keep A good Pasword Policy</li> </ul>"},{"location":"linux/admin/storage/","title":"storage","text":""},{"location":"linux/admin/storage/#logical-volume-manager","title":"logical volume manager","text":"<p>LVM allows for very flexible disk space management. It provides features like the ability to add disk space to a logical volume and its filesystem while that filesystem is mounted and active and it allows for the collection of multiple physical hard drives and partitions into a single volume group which can then be divided into logical volumes.</p> <p>The volume manager also allows reducing the amount of disk space allocated to a logical volume, but there are a couple requirements. First, the volume must be unmounted. Second, the filesystem itself must be reduced in size before the volume on which it resides can be reduced.</p>"},{"location":"linux/admin/storage/#create-volume","title":"create volume","text":"<pre><code>pvs\npvcreate /dev/hdd\nvgs\nvgextend /dev/MyVG01 /dev/hdd\nlvcreate -L +50G --name Stuff MyVG01\nmkfs -t ext4 /dev/MyVG01/Stuff\ne2label /dev/MyVG01/Stuff Stuff\nlvs\n</code></pre>"},{"location":"linux/admin/storage/#increase-volume","title":"increase volume","text":"<pre><code>pvs\nvgs\nlvs\nvgextend /dev/MyVG01 /dev/hdd\nlvextend -L +50G /dev/MyVG01/Stuff\nresize2fs /dev/MyVG01/Stuff\n</code></pre>"},{"location":"linux/admin/storage/#decrease-volume","title":"decrease volume","text":"<pre><code>df -h /testlvm1\numount /testlvm1\ne2fsck -f /dev/mapper/vg01-lv002\nresize2fs /dev/mapper/vg01-lv002 80G\nlvreduce -L 80G /dev/mapper/vg01-lv002\ne2fsck -f /dev/mapper/vg01-lv002\nmount /testlvm1\ndf -h /testlvm1\n</code></pre>"},{"location":"linux/admin/storage/#raid","title":"RAID","text":"<p>RAID stands for Redundant Array of Inexpensive/Independent Disks</p>"},{"location":"linux/admin/storage/#striped-andor-mirrored","title":"Striped and/or Mirrored","text":"<p>RAID 0</p> <p>RAID 0, data is written across the drives, or \u201cstriped\u201d. This means it can potentially be read from more than one drive concurrently. That can give us a real performance boost. Now we have two drives that could fail, taking out all our data. So, RAID 0 is only useful if we want a performance boost but don\u2019t care about long-term storage.</p> <p></p> <p>RAID 1</p> <p>We refer to RAID level 1 as \u201cmirrored\u201d because it is created with a pair of equal drives. Each time data is written to a RAID 1 device, it goes to both drives in the pair.</p> <p></p> <p>RAID10</p> <p>We can create a RAID 10 device with four disks: one pair of disks in RAID 0, mirroring another pair of disks in RAID 0</p> <p></p>"},{"location":"linux/admin/storage/#parity","title":"Parity","text":"<p>RAID5</p> <p>RAID 5 requires at least three equal-size drives to function. In practice, we can add several more, though rarely more than ten are used. RAID 5 sets aside one drive\u2019s worth of space for checksum parity data. It is not all kept on one drive, however; instead, the parity data is striped across all of the devices along with the filesystem data.</p> <p>This means we usually want to build our RAID out of a set of drives of identical size and speed. Adding a larger drive won\u2019t get us more space, as the RAID will just use the size of the smallest member. Similarly, the RAID\u2019s performance will be limited by its slowest member.</p> <p>RAID 5 can recover and rebuild with no data loss if one drive dies. If two or more drives crash, we\u2019ll have to restore the whole thing from backups.</p> <p>RAID6</p> <p>RAID 6 is similar to RAID 5 but sets aside two disks\u2019 worth for parity data. That means a RAID 6 can recover from two failed members.</p> <p>RAID 5 gives us more usable storage than mirroring does, but at the price of some performance. A quick way to estimate storage is the total amount of equal-sized drives, minus one drive. For example, if we have 6 drives of 1 terabyte, our RAID 5 will have 5 terabytes of usable space. That\u2019s 83%, compared to 50% of our drives were mirrored in RAID 1.</p> <p></p>"},{"location":"linux/admin/troubleshooting/","title":"booting issue","text":""},{"location":"linux/admin/troubleshooting/#kernel-panic-not-syncing-attempting-to-kill-init","title":"kernel panic: not syncing attempting to kill init","text":"<p>The issue is due to kernel image or the grub related, system wasn't able to locate the kernel or the label associated with it. </p> <p>Solution, attempt the linux machine into the sigle user mode, check the contents of the  grub.conf , which is tempravory, try modifying the file and bring up the system in runlevel 3. </p> <p>Once the machine is up, you can modify the grub.conf accordingly and boot the system.</p>"},{"location":"linux/admin/troubleshooting/#linux-booting-drops-into-grub","title":"linux booting drops into grub&gt;","text":"<p>grub wasn't able to find the second stage boot loader. try to load the second stage to boot the machine. </p> <pre><code>grub&gt; root(hd0,0)\ngrub&gt; kernel /vmlinux&lt;tab&gt; ro root=LABEL=/1\ngrub&gt; initrd /initrd&lt;tab&gt;\ngrub&gt; boot\n</code></pre> <p>Once the machine boots, you would check where is the second stage boot loader and will fix it.</p>"},{"location":"linux/admin/troubleshooting/#system-keeps-on-rebooting","title":"system keeps on rebooting","text":"<p>it could be a hardware related or the login related. </p> <p>Solution: Try to boot the system into single user mode, and if you find the error as no more process left in the runlevel, which means system is not able to start the process and you would required to boot in the rescue level. </p> <p>First, check the runlevel in /etc/inittab file, if runlevel is not correct, fix it</p> <p>secondly</p> <pre><code>Mount ISO or DVD\n: linux rescue\n&lt;skip&gt;\n\nchroot /mnt/sysimage\ncd etc\nls | grep inittab \nls | grep /etc/grub/grub.conf\nexit \nreboot\n\n</code></pre>"},{"location":"linux/admin/troubleshooting/#checking-filesystems-fsckext3-unable-to-resolve-label5-failed","title":"checking filesystems fsck.ext3: unable to resolve LABEL=/5  [ FAILED ]","text":"<p>Your file system file is corrupted, it can't find the parition or label to mount the directories. </p> <p>Solution: provide your single user password and fix the /etc/fstab</p> <pre><code>cat /etc/fstab\n&lt;check for correct entries&gt;\ne2label /dev/sda3\nmount -o remount,rw /etc/fstab\nvim /etc/fstab\n</code></pre>"},{"location":"linux/admin/troubleshooting/#verifying-dmi-pool-data","title":"Verifying DMI pool data","text":"<p>first stage boot loader is corrupted. </p> <p>Solution: </p> <pre><code>chroot /mnt/sysimage\ngrub-install /dev/sda\nctrl-d\n</code></pre>"},{"location":"linux/admin/troubleshooting/#you-lost-your-grub-password-how-would-you-recover","title":"You lost your GRUB password, how would you recover ?","text":"<p>Linux terminal, type <code>grub-md5-crypt &gt;&gt;/boot/grub/grub.conf</code> prefix it with password --md5 $jshhx.....</p> <p>insert ISO image or DVD, at the boot prompt, type  linux rescue </p> <pre><code>chroot /mnt/sysimage/\nvim /etc/grub/grub.conf\n\nDelete the line with any password entry, save and quit the file\n\nexit\n</code></pre>"},{"location":"linux/admin/troubleshooting/#root-unable-to-login","title":"root unable to login","text":"<p>1) password wrong</p> <p>2) bash not presented for root a/c   Symptom: when you type root login password, it will give you non-login shell. i.e password file corrupted. </p> <p>Solution: Go to single user mode, edit /etc/password(remove the passsord entry), save &amp; quit</p> <p>3) /etc/securetty file corrupted   try logging into the nonroot user and see if it works. only if root not abel to login, then the issue is with /etc/securetty. </p> <p>Solution: single usermode, verify settings in /etc/securetty</p>"},{"location":"linux/ansible/install/","title":"install","text":""},{"location":"linux/ansible/install/#local-setup","title":"local setup","text":"<p>We could do a local installation of the setup using vagrant + virtualbox.</p> <p>versions</p> <p>Vagrant: 2.4.1</p> <p>Oracle virtual box: 7.x</p>"},{"location":"linux/ansible/install/#configure-vagrant","title":"configure vagrant","text":"<pre><code>mkdir ansibledev\nvim Vagrantfile\n\n# start\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"centos/7\"\n  config.vm.network \"private_network\", ip: \"192.168.56.10\"\nend\n\n# save &amp; quit\n\nvagrant up\n</code></pre>"},{"location":"linux/ansible/install/#configure-ansible","title":"configure ansible","text":"<p>Now, you are required to make a modification to the newly created server from your physical machine for exeuting and working on with the playbooks</p> <pre><code># mkdir ansibledev\n# vim ansible.cfg\n[defaults]\ninventory = ./hosts\nremote_user = vagrant\nask_pass = false\nhost_key_checking = false\nprivate_key_file=/Users/sunilamperayani/vagrant/ansibledev/.vagrant/machines/default/virtualbox/private_key\n\n[privlege escalation]\nbecome= true\nbecome_method = sudo\nbecome_user = root\nbecome_ask_pass = false\n\n# Note: this is the same IP which you have created using oracle virtual box.\n\n# build ansible inventory!\n\n# vim hosts\n[test]\n192.168.56.10\n</code></pre>"},{"location":"linux/ansible/install/#test","title":"test","text":"<p>Test your connectivity using ping command</p> <pre><code>ansible -m ping all\n192.168.56.10 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"linux/ansible/overview/","title":"overview","text":"<p>You can read/practice <code>ansible</code> using the below repo. </p> <p>https://github.com/samperay/automation-with-ansible</p>"},{"location":"linux/scripting/faq/","title":"Interview Questions","text":"<p>Bash cheat sheet</p>"},{"location":"linux/scripting/faq/#list-files-in-directory","title":"list files in directory","text":"<pre><code>for item in /etc/*\ndo\n  if [ -f $item ]; then \n    echo ${item}\n  fi\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#sum-of-integers","title":"sum of integers","text":"<pre><code>set -x\na=$1\nb=$2\n\nfunction input_check(){\nif [ \"$#\" -ne 2 ]; then \n    echo 1\nelse\n    if [ \"$#\" -gt 2 ]; then \n        echo 1 \n    else \n        echo 0\n    fi\nfi\n}\n\ncheck_args=$(input_check $*)\nif [ $check_args -eq 1 ]; then \n    echo \"Usage: `basename $0` [arg1, arg2]\"\n    exit 1\nelse \n    sum=`expr $a + $b`\n    echo \"Result=$sum\"\nfi\n</code></pre>"},{"location":"linux/scripting/faq/#traverse-array-using-len","title":"traverse array using len","text":"<pre><code>array=(\"1\" \"2\")\necho ${#array[*]} // length of an array\n\n# fetch elements in an array\necho ${array[0]}\n\n# traverse an item in an array  \nfor item in ${array[@]}; do \n  echo $item \ndone\n\n# print the array index\nfor item in ${!array[*]}; do \n  echo $item \ndone\n\n# \"${array[@]}\" - returns each item as a separate word.\n# \"${array[*]}\" - returns all items in a word.\n\n# take each element in an array and print the results\nlen=${#array[@]}\nfor((i=0;i&lt;len;i++&gt;)); do \n  echo index:$i,item:${array[$i]}\ndone\n\n# iterate over index\nfor ((i=0;i&lt;$${#array[@]};i++)); do\n  echo ${array[i]}\ndone\n</code></pre> <pre><code>Fruits=('Apple' 'Banana' 'Orange')\necho \"${Fruits[0]}\"           # Element #0\necho \"${Fruits[-1]}\"          # Last element\necho \"${Fruits[@]}\"           # All elements, space-separated\necho \"${#Fruits[@]}\"          # Number of elements\necho \"${#Fruits}\"             # String length of the 1st element\necho \"${#Fruits[3]}\"          # String length of the Nth element\necho \"${Fruits[@]:3:2}\"       # Range (from position 3, length 2)\necho \"${!Fruits[@]}\"          # Keys of all elements, space-separated\n\nFruits=(\"${Fruits[@]}\" \"Watermelon\")    # Push\nFruits+=('Watermelon')                  # Also Push\nFruits=( \"${Fruits[@]/Ap*/}\" )          # Remove by regex match\nunset Fruits[2]                         # Remove one item\nFruits=(\"${Fruits[@]}\")                 # Duplicate\nFruits=(\"${Fruits[@]}\" \"${Veggies[@]}\") # Concatenate\nlines=(`cat \"logfile\"`)                 # Read from file\n\nfor i in \"${Fruits[@]}\"; do\n  echo \"$i\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#traversing-dicts","title":"traversing dicts","text":"<pre><code>declare -A sounds\nsounds[dog]=\"bark\"\nsounds[cow]=\"moo\"\nsounds[bird]=\"tweet\"\nsounds[wolf]=\"howl\"\n\necho \"${sounds[dog]}\" # Dog's sound\necho \"${sounds[@]}\"   # All values\necho \"${!sounds[@]}\"  # All keys\necho \"${#sounds[@]}\"  # Number of elements\nunset sounds[dog]     # Delete dog\n\n# iterate over values\n\nfor val in \"${sounds[@]}\"; do\n  echo \"$val\"\ndone\n\n# iterate over keys\n\nfor key in \"${!sounds[@]}\"; do\n  echo \"$key\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#diff-between-and","title":"diff between $@ and $*","text":"<p>$* and $@ when unquoted are identical and expand into the arguments.</p> <p>\"$*\" is a single word, comprising all the arguments to the shell, joined together with spaces. For example '1 2' 3 becomes \"1 2 3\".</p> <p>\"$@\" is identical to the arguments received by the shell, the resulting list of words completely match what was given to the shell. For example '1 2' 3 becomes \"1 2\" \"3\"</p> <p>In short, $@ when quoted (\"$@\") breaking up the arguments if there are spaces in them. But \"$*\" does not breaking the arguments. </p> <pre><code># ./specialvars.sh 1 2 \"3 4\"\nwithout quotes $*: 1\nwithout quotes $*: 2\nwithout quotes $*: 3\nwithout quotes $*: 4\n\nwithout quotes $@: 1\nwithout quotes $@: 2\nwithout quotes $@: 3\nwithout quotes $@: 4\n\nwith quotes \"$*\": 1 2 3 4\n\nwith quotes \"$@\": 1\nwith quotes \"$@\": 2\nwith quotes \"$@\": 3 4\n</code></pre>"},{"location":"linux/scripting/faq/#concat-two-str","title":"concat two str","text":"<pre><code>a=\"Sunil\"\nb=\"Kumar\"\n\necho $a $b\n</code></pre> <pre><code>array=(\"Sunil\" \"kumar\")\n\nfor i in ${array[@]}; do \n    strnew+=\"$i\"\ndone \n\necho $strnew\n</code></pre>"},{"location":"linux/scripting/faq/#function-defn-and-args","title":"function defn and args","text":"<pre><code>vim ./script Sunil\nfunction f1(){\n  echo \"Hello $1\"\n}\n\nf1 $1\n\n$./script Sunil\n</code></pre>"},{"location":"linux/scripting/faq/#func-return-value","title":"func return value","text":"<pre><code>function f1(){\n  echo \"Hello $1\"\n}\n\nretval=$(f1 $1)\necho $retval\n\n$./script Sunil \n</code></pre>"},{"location":"linux/scripting/faq/#file-types","title":"file types","text":"Operator Description Example -b file checks if file is a block special file; if yes, then the condition becomes true. [ -b $file ] is false. -c file checks if file is a character special file; if yes, then the condition becomes true. [ -c $file ] is false. -d file checks if file is a directory; if yes, then the condition becomes true. [ -d $file ] is not true. -f file checks if file is an ordinary file as opposed to a directory or special file; [ -f $file ] is true. -g file checks if file has its set group ID (SGID) bit set; if yes, then the condition becomes true. [ -g $file ] is false. -k file checks if file has its sticky bit set; if yes, then the condition becomes true. [ -k $file ] is false. -p file checks if file is a named pipe; if yes, then the condition becomes true. [ -p $file ] is false. -t file checks if file descriptor is open and associated with a terminal [ -t $file ] is false. -u file checks if file has its Set User ID (SUID) bit set [ -u $file ] is false. -r file checks if file is readable; if yes, then the condition becomes true. [ -r $file ] is true. -w file checks if file is writable; if yes, then the condition becomes true. [ -w $file ] is true. -x file checks if file is executable; if yes, then the condition becomes true. [ -x $file ] is true. -s file checks if file has size greater than 0; if yes, then condition becomes true. [ -s $file ] is true. -e file checks if file exists; is true even if file is a directory but exists. [ -e $file ] is true. <pre><code>test -f ${FILE} &amp;&amp; echo \"File Exists\" - Method 1 \n[ -f ${FILE} ] &amp;&amp; echo \"File Exists\" - Method 2 \n                  OR \n[[ -f ${FILE} ]] &amp;&amp; echo \"File Exists\"\n</code></pre>"},{"location":"linux/scripting/faq/#oddeven","title":"odd/even","text":"<pre><code># print even numbers\nfor i in {0..10..2}; do \n    echo \"${i}\"\ndone\n\n# sum of even numbers\nevensum=0\nfor i in {0..5..2}\ndo\n  evensum=$((evensum+i))\ndone\necho \"${evensum}\"\n\n# print odd numbers\nfor i in {1..10..2}; do \n    echo \"${i}\"\ndone\n\n# print sum of odd numbers\noddsum=0\nfor i in {1..5..2}\ndo\n  oddsum=$((oddsum+i))\ndone\necho \"${oddsum}\"\n</code></pre>"},{"location":"linux/scripting/faq/#reverse-string","title":"reverse string","text":"<pre><code>s=\"sunil\"\nfor((i=${#s};i&gt;=0;i--)); do \n    revstr=$revstr${s:$i:1}\ndone\n\n# oneliner\necho ${s}| rev \nrev &lt;&lt;&lt; ${s}\n</code></pre>"},{"location":"linux/scripting/faq/#for-and-while","title":"for and while","text":"<pre><code># for loop\nfor((i=1;i&lt;=10;i++)); do \n  echo $i\ndone\n\n# while loop\ni=0\nwhile [ $i -le 10 ];  do \n    echo $i \n    ((i++))\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#factorial","title":"factorial","text":"<pre><code>counter=5\nfactorial=1\nwhile [ $counter -gt 0 ]; do\n  factorial=$(($factorial * $counter))\n  counter=$(($counter - 1))\ndone\necho \"${factorial}\"\n</code></pre>"},{"location":"linux/scripting/faq/#reverse-of-num","title":"reverse of num","text":"<pre><code>n=123456\nrem=0\nrevnum=0\nwhile [ $n -gt 0 ]; do \n    rem=$(( $n % 10 ))\n    revnum=$(( $revnum * 10 + $rem ))\n    n=$(( $n / 10 ))\ndone\necho $revnum\n</code></pre>"},{"location":"linux/scripting/faq/#password-strength","title":"password strength","text":"<pre><code>code goes here\n</code></pre>"},{"location":"linux/scripting/faq/#line-count-in-file","title":"line count in file","text":"<pre><code>file=\"$1\"\nlet count=0\nwhile read line; do \n    ((count++))\ndone &lt;$file\n\necho \"Count:\" $count\n</code></pre>"},{"location":"linux/scripting/faq/#return-lines-from-func","title":"return lines from func","text":"<pre><code>function count() {\n  local file=\"$1\"\n  let count=0\n  while read line; do \n      ((count++))\n  done &lt;\"$file\"\n\n  echo $count \n}\n\necho \"Count:\" $(count \"$1\")\n</code></pre>"},{"location":"linux/scripting/faq/#discard-comments-in-file","title":"discard comments in file","text":"<pre><code>while read -r line\ndo\n  [[ $line = \\#* ]] &amp;&amp; continue\n  printf '%s\\n' \"$line\"\ndone &lt; ./passwd\n</code></pre>"},{"location":"linux/scripting/faq/#retain-the-last-50-lines-in-logfile","title":"retain the last 50 lines in logfile","text":"<pre><code>LOG_DIR=/var/log\nROOT_UID=0\nLINES=30\nE_WRONGARGS=85\nE_XCD=86\nE_NONROOT=87\n\nif [ \"$UID\" != \"$ROOT_UID\" ]\nthen\n  echo \"Must be root to run this script !\"\n  exit $E_NONROOT\nfi\n\n\ncase \"$1\" in \n  \"\") lines=50;;\n  *[0-9]*) echo \"Usage: `basename $0` #lines_to_cleanup\";\n           exit $E_WRONGARGS;;\n  *)      lines=$1;;\nesac\n\ncd /var/log || {\n  echo \"Cannot change to directory\" &gt;&amp;2\n  exit $E_XCD;\n  }\n\ntail -n $lines messages &gt; mesg.temp\nmv mesg.temp messages\n\necho \"Log files cleaned up\"\nexit 0\n</code></pre>"},{"location":"linux/scripting/faq/#random-num-generator-200-500","title":"random num generator 200-500.","text":"<pre><code>#!/bin/bash\nMIN=200\nMAX=500\nlet \"scope = $MAX - $MIN\" #300\nif [ \"$scope\" -le \"0\" ]; then\n  echo \"Error- MAX less than MIN\"\nfi\n\nfor i in {1..10}; do\n  let result=\"$RANDOM % $scope + $MIN\"\n  echo \"Random Number between Min/Max: $result\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#email-disk-alert","title":"email disk alert.","text":"<pre><code>#!/bin/bash \n\nWARNING=90\nCRITICAL=95\n\ndf -H | egrep -v '^/dev/loop|tmpfs|udev|Filesystem' | awk '{ print $5 \" \" $1}'| while read line;\ndo\n  echo $output\n  usep=$(echo $output | awk '{ print $1}' | cut -d'%' -f1  )\n  partition=$(echo $output | awk '{ print $2 }' )\n\n  if [ $usep -ge $WARNING ]; then \n    echo \"WARNING: Running out of space \\\"$partition - $usep\\\"\"\n  elif [ $usep -ge $CRITICAL ]; then \n    echo \"CRITICAL: Require Immediate attention on \\\"$partition - $usep\\\"\"\n    mail -s \"Alert:Critical: Disk running out of space\" &lt;youremailid&gt;\n  fi\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#fibonacci-series","title":"fibonacci series","text":"<pre><code>fib() {\n  return $(( $1 + $2 ))\n}\n\nF0=0\nF1=1\necho \"0: $F0\"\necho \"1: $F1\"\nfor count in `seq 2 20`; do\n fib $F0 $F1\n F2=$?\n if [ \"$F2\" -lt \"$F1\" ]; then\n  echo \"$count: $F2 (WRONG!)\"\n else\n  echo \"${count}: $F2\"\n fi\n F0=$F1\n F1=$F2\n sleep 0.1\ndone\n\nfib $F0 $F1\necho \"${count}: $?\"\n</code></pre> <pre><code>#!/bin/bash\nfunction fibonacci\n  {\n    echo $1 + $2 | bc | tr -d '\\\\\\n'\n  }\n\nF0=0\nF1=1\necho \"0: $F0, \"\necho \"1: $F1, \"\ncount=2\nwhile :\ndo\n  F2=`fibonacci $F0 $F1`\n  echo \"${count}: $F2,\"\n  ((count++))\n  F0=$F1\n  F1=$F2\n  sleep 0.1\ndone\nfibonacci $F0 $F1\n</code></pre>"},{"location":"linux/scripting/faq/#del-last-line-in-multiple-files","title":"del last line in multiple files","text":"<pre><code>for file in sub/*\n  do\n    if [ -f $file ]\n    then\n      vi -c '$d' -c 'wq' \"$file\"\n    fi\n  done\n</code></pre>"},{"location":"linux/scripting/faq/#replace-string-multiple-places-in-multiple-files","title":"replace string multiple places in multiple files","text":"<pre><code>for file in ./*\n  do\n    if [ -f $file ]\n    then\n      sed -i 's/2018/2019/g' \"$file\"\n    fi\n  done\n</code></pre>"},{"location":"linux/scripting/faq/#verify-string-not-null","title":"verify string not null","text":"<pre><code>str1=\"Not Null\"\nstr2=\" \"\nstr3=\"\"\nmessage=\"is not Null nor a space\"\n\nif [ ! -z \"$str1\" -a \"$str1\" != \" \" ]; then\n  echo \"str1 ${message}\"\nfi\n\nif [ ! -z \"$str2\" -a \"$str2\" != \" \" ]; then\n  echo \"str2 ${message}\"\nfi\n\nif [ ! -z \"$str3\" -a \"$str3\" != \" \" ]; then\n  echo \"str3 ${message}\"\nfi\n</code></pre>"},{"location":"linux/scripting/faq/#read-contents-in-a-file-line-by-line","title":"Read contents in a file line by line","text":"<pre><code>for h in $(&lt;hosts.txt)\ndo\n  echo $h\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#bash-substitutions","title":"bash substitutions","text":"<p>${v} - Substitue the value of v. v1=1 echo \"substitute value of v1 '\\${v1}'\" echo \"sub value for v1:\"${v1} echo \"--\"</p> <p>${v:-val} - if v is null or unset, val is substituuted echo ${v2:-2} echo \"Value not set for v2:\" $v2 echo \"--\"</p> <p>${v:=val} - if v is null or unset, vs is set to val echo ${v3:=3} echo \"val is substituted:\" $v3 echo \"---\"</p> <p>${v:+val} - if v is set, val is substituted. v is unchanged v4=1234 echo \"val is substituted\" ${v4:+44} echo \"val is unchanged\" ${v4} echo \"---\"</p> <p>${v:?val} - if v is null or unset, val is printed to std err v5=5 echo \"no err printed as v5 is set\" ${v5:?5555} echo \"value unchanged\" ${v5}</p> <p>echo \"not setting value, yest printing results\" echo \"print value undefined\" ${v6:? \"Value unable to find\"} echo \"---\"</p>"},{"location":"linux/scripting/faq/#example-script","title":"$* &amp; $# example script","text":"<pre><code>#!/bin/bash\n\nfor i in $*; do \n    echo \"without quotes \\$*: $i\"\ndone\n\necho\n\nfor i in $@; do \n    echo \"without quotes \\$@: $i\"\ndone\n\necho\n\nfor i in \"$*\"; do\n    echo \"with quotes \"'\"$*\"'\": $i\"\ndone\n\necho\n\nfor i in \"$@\"; do\n    echo \"with quotes \"'\"$@\"'\": $i\"\ndone\n\n# ./specialvars.sh 1 2 \"3 4\"\n# without quotes $*: 1\n# without quotes $*: 2\n# without quotes $*: 3\n# without quotes $*: 4\n\n# without quotes $@: 1\n# without quotes $@: 2\n# without quotes $@: 3\n# without quotes $@: 4\n\n# with quotes \"$*\": 1 2 3 4\n\n# with quotes \"$@\": 1\n# with quotes \"$@\": 2\n# with quotes \"$@\": 3 4\n</code></pre>"},{"location":"linux/scripting/faq/#design-help-menu","title":"design help menu","text":"<pre><code>function help() {\n   echo \"Syntax: ./script &lt;arg&gt; [-h|-v]\"\n   echo \n   echo \"options:\"\n   echo \"-h  Print this Help.\"\n   echo \"-v  Print software version and exit.\"\n   echo\n}\n\nwhile getopts \":hv\" option\ndo\n  case ${option} in \n    \"h\") help ;;\n    \"v\") echo 12.10.10 ;;\n    \"*\") echo \"Error: Invalid Option\" || exit \n  esac\ndone\n</code></pre> <pre><code>[[ -z \"$1\" ]] &amp;&amp; grep \"^#:\" $0 | sed -e 's/#://' &amp;&amp; exit\n[[ \"$1\" == '-h' ]] &amp;&amp; grep \"^#:\" $0 | sed -e 's/#://' &amp;&amp; exit\n[[ \"$1\" == '-v' ]] &amp;&amp; echo \"12.10.10\" || echo \"Error: Invalid Option\" &amp;&amp; exit\n</code></pre>"},{"location":"linux/scripting/faq/#log","title":"log","text":"<pre><code>#!/bin/bash\n\nexec 3&gt;&amp;1 4&gt;&amp;2\ntrap 'exec 2&gt;&amp;4 1&gt;&amp;3' 0 1 2 3\nexec 1&gt;log.out 2&gt;&amp;1\n\nlog() {\n# write the log\n  local msg=\"$1\"\n  DATE=`date '+%b %e %H:%M:%S'`\n  echo INFO: $DATE $msg\n}\n\n\nlog \"Hello World\"\nlog \"Alice &amp;&amp; Bob wants to talk to each other in secure communication \"\n</code></pre>"},{"location":"linux/scripting/faq/#design-calculator","title":"design calculator","text":"<pre><code>#!/bin/bash \n\n# Simple Basic Calculator \n\n# check for arguments \nif [ $# -ne 2 ]; then \n   echo \"Usage: ./calculator.sh &lt;arg1&gt; &lt;arg2&gt;\"\n   exit 1\nfi\n\nfunction summing() {\n  result=`expr $1 + $2`\n  echo \"Sum:($1+$2)=\"$result\n}\n\n# Difference \nfunction difference(){\n  result=`expr $1 - $2`\n  echo \"Difference:($1-$2)=\"$result\n}\n\n# Multiplication \nfunction multiplication(){\n  result=`expr $1 \\* $2`\n  echo \"Multiplication:($1*$2)=\"$result\n}\n\n# Division \nfunction division(){\n  result=`expr $1 / $2`\n  echo \"Division:($1/$2)=\"$result\n}\n\nsumming $1 $2 \ndifference $1 $2\nmultiplication $1 $2 \ndivision $1 $2\n</code></pre>"},{"location":"linux/scripting/faq/#seq-numbers","title":"seq numbers","text":"<p>first method</p> <pre><code>i=1\nwhile [[ $i -le 3 ]]; do \n  echo $i\n  i=$((i+1))\ndone\n</code></pre> <p>second method</p> <pre><code>seq 1 3 | while read i ; do \n  echo $i\ndone\n</code></pre> <p>third method</p> <pre><code>for i in $(seq 1 3); do \n    echo $i\ndone\n</code></pre>"},{"location":"linux/scripting/hackerrank/","title":"Hackerrank","text":""},{"location":"linux/scripting/hackerrank/#cut","title":"cut","text":"<p>print the 3rd character from each line as a new line of output.</p> <pre><code>cut -c3\n</code></pre> <p>Display the 2nd and 7th character from each line of text.</p> <pre><code>cut -c2,7\n</code></pre> <p>Display a range of characters starting at the 2nd position of a string and ending at the 7th position (both positions included)</p> <pre><code>cut -c2-7\n</code></pre> <p>Given a tab delimited file with several columns (tsv format) print the first three fields.</p> <pre><code>cut -f1-3\n</code></pre> <p>Print the characters from thirteenth position to the end.</p> <pre><code>cut -c13-\n</code></pre> <p>Each Input sentence, identify and display its fourth word. Assume that the space (' ') is the only delimiter between words.</p> <pre><code>cut -d\" \" -f4\n</code></pre> <p>The output should contain N lines. For each input sentence, identify and display its first three words. Assume that the space (' ') is the only delimiter between words.</p> <pre><code>cut -d\" \" -f1-3\n</code></pre> <p>For each line in the input, print the fields from second fields to last field.</p> <pre><code>cut -f2-\n</code></pre>"},{"location":"linux/scripting/hackerrank/#head","title":"head","text":"<p>Output the first 20 lines of the given text file.</p> <pre><code>head -20\n</code></pre> <p>Output the first 20 characters of the text file</p> <pre><code>head -c20\n</code></pre> <p>Display the lines (from line number 12 to 22, both inclusive) for the input file.</p> <p>Hint: First display first 22 lines and then tail(22-12=10)</p> <pre><code>head -22|tail -11\n</code></pre>"},{"location":"linux/scripting/hackerrank/#paste","title":"paste","text":"<p>Replace the newlines in the input file with semicolons</p> <pre><code>paste -d\";\" -s\n</code></pre> <p>Restructure the file so that three consecutive rows are folded into one line and are separated by semicolons.</p> <pre><code>paste -d \";\" - - -\n</code></pre> <p>The delimiter between consecutive rows of data has been transformed from the newline to a tab. Previous solution: paste -s -d\"\\t\". The delimiter option is not necessary as tab is the delimiter of paste by default</p> <pre><code>paste -s\n</code></pre> <p>Restructure the file in such a way, that every group of three consecutive rows are folded into one, and separated by tab.</p> <pre><code>paste - - -\n</code></pre>"},{"location":"linux/scripting/hackerrank/#sort","title":"sort","text":"<p>Output the text file with the lines reordered in lexicographical order.</p> <pre><code>sort\n</code></pre> <p>Output the text file with the lines reordered in reverse lexicographical order.</p> <pre><code>sort -r\n</code></pre> <p>Output the text file with the lines reordered in numerically ascending order.</p> <pre><code>sort -n\n</code></pre> <p>The text file, with lines re-ordered in descending order (numerically)</p> <pre><code>sort -n -r\n</code></pre> <p>Rearrange the rows of the table in descending order of the values for the average temperature in January (i.e, the mean temperature value provided in the second column) in a tab seperated file.</p> <pre><code>sort -k2 -n -r -t$'\\t'\n</code></pre> <p>The data has been sorted in ascending order of the average monthly temperature in January (i.e, the second column) in a tsv file</p> <pre><code>sort -n -k2 -t$'\\t'\n</code></pre> <p>The data has been sorted in descending order of the average monthly temperature in January (i.e, the second column).</p> <pre><code>sort -k2 -n -r -t '|'\n</code></pre>"},{"location":"linux/scripting/hackerrank/#tail","title":"tail","text":"<p>Output the last 20 lines of the text file.</p> <pre><code>tail -20\n</code></pre> <p>Display the last 20 characters of an input file.</p> <pre><code>tail -c20\n</code></pre>"},{"location":"linux/scripting/hackerrank/#tr","title":"tr","text":"<p>Output the text with all parentheses () replaced with box brackets [].</p> <pre><code>tr \"()\" \"[]\"\n</code></pre> <p>In a given fragment of text, delete all the lowercase characters a - z</p> <pre><code>tr -d \"a-z\"\n</code></pre> <p>Replace all sequences of multiple spaces with just one space.</p> <pre><code>tr -s \" \"\n</code></pre>"},{"location":"linux/scripting/hackerrank/#uniq","title":"uniq","text":"<p>Given a text file, remove the consecutive repetitions of any line.</p> <pre><code>uniq\n</code></pre> <p>Given a text file, count the number of times each line repeats itself. Only consider consecutive repetitions. Display the space separated count and line, respectively. There shouldn't be any leading or trailing spaces. Please note that the uniq -c command by itself will generate the output in a different format than the one expected here.</p> <pre><code>uniq -c | cut -c7-\n</code></pre> <p>compare consecutive lines in a case insensitive manner. So, if a line X is followed by case variants, the output should count all of them as the same (but display only the form X in the second column).</p> <pre><code>uniq -i -c | cut -c7-\n</code></pre> <p>Given a text file, display only those lines which are not followed or preceded by identical replications.</p> <pre><code>uniq -u\n</code></pre>"},{"location":"linux/scripting/overview/","title":"Bash Overview","text":""},{"location":"linux/scripting/overview/#redirecting-streams","title":"redirecting streams","text":"<p>file descriptor - A unique identifier that the operating system assigns to a file when it is opened.  0 - stdin  1 - stdout  2 - stderr</p> <p>we have two conditions that are there to redirect streams</p> <ol> <li><code>&gt;</code> shell thinks you are redirecting output to <code>stdout</code> which is to a file by default(also written as <code>&gt;&amp;</code>).</li> <li><code>&amp;&gt;</code> output + stderr both are redirected to the <code>file descriptor</code> and to a location. </li> </ol> <p>example</p> <p><code>ls -z 2&gt;&amp;1 &gt; file1.txt</code> </p> <p>we are redirecting the <code>stderr</code>(i.e <code>2</code>) to the file descriptor i.e <code>&amp;1</code> which is <code>stdout</code> which by defaults to <code>file1.txt</code>. so we are writing output and error in the same file. </p> <p>the above can also be changed to below syntax which is very common. <code>ls -z &gt; file1.txt 2&gt;&amp;</code> <code>ls -z &gt; /dev/null 2&gt;&amp;1</code> these can be found in the scripts normally.. </p> <p>custom redirections</p>"},{"location":"linux/scripting/overview/#single-line-modifications","title":"single line modifications","text":"<pre><code>echo \"Suni lkumar@gmail.com\" &gt; email_file.txt\ncat email_file.txt \nSuni lkumar@gmail.com -&gt; observer there is missing 4th letter.\nexec 3&lt;&gt; email_file.txt # open the file with fd as 3\nread -n 4 &lt;&amp;3 # read 4 words by inputing fd 3 i.e email_file/txt\necho -n \".\" &gt;&amp;3 # output 4th letter to the fd 3 i.e email_file/txt\nexec 3&gt;&amp;- # close the fd 3\n\ncat email_file.txt \nSuni.lkumar@gmail.com -&gt; you can now see its updated\n</code></pre>"},{"location":"linux/scripting/overview/#multiline-modifications","title":"multiline modifications","text":"<p>this is for <code>heredocs</code> where you can create/execte multiple commands for the executions. </p> <pre><code>ssh root@servre1 &lt;&lt;EOF\nmkdir ~/heredocs\necho \"heredocs\" &gt; ~/heredocs/heredocs.txt\nEOF\n</code></pre>"},{"location":"linux/scripting/overview/#pipes","title":"pipes","text":"<ul> <li>names pipes: stdout to a file</li> </ul> <pre><code>sort &lt; abc.txt &gt; sorted_text.txt\n</code></pre> <p>sort will take the input and redirect it to stdout.. so we would proivde the input to sort and later we will output stored in the normal text file.</p> <ul> <li>Anomymous pipes: pass output from one place to another place</li> </ul> <p>command1 | command 2</p> <pre><code>cat filename.txt | grep -i \"o\" | sort\n</code></pre> <p>there is 1 issue with the pipe commands, if the command is invalid or failed, it would continue to process the next inputs. WE MUST AVOID THAT KIND..</p> <pre><code>MacBook-Pro:wiki sunilamperayani$ ls -z | echo \"helllo\"\nhelllo --&gt; you are able to execute this command despite the first command got failed.\nls: invalid option -- z\nusage: ls [-@ABCFGHILOPRSTUWabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ sort somefile.xtxt  | uniq &amp;&amp; echo \"hello\"\nsort: No such file or directory\nhello\nMacBook-Pro:wiki sunilamperayani$ echo $?\n0\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <p>despite getting failed return code was successful.</p>"},{"location":"linux/scripting/overview/#pipefail","title":"pipefail","text":"<p>immediately stop the execution of the command when one of the command fails in the chain of command.</p> <pre><code>MacBook-Pro:wiki sunilamperayani$ set -o pipefail\nMacBook-Pro:wiki sunilamperayani$ sort somefile.xtxt  | uniq &amp;&amp; echo \"hello\"\nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ echo $?\n2\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <p>we can also use <code>exit</code> codes to infact exit the scripts..</p> <pre><code># set-fail.sh \n#!/usr/bin/env bash\nset -o pipefail\nsort newfile.txt | uniq || exit 80\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ ./set-fail.sh \nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ echo $?\n80\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <pre><code># noclobber.sh\n#!/usr/bin/env bash\nset -o noclobber \necho \"line1\" &gt; file1.txt\necho \"line2\" &gt; file1.txt\n\nsort somefile.txt | uniq || exit 100\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ ./noclobber.sh \n./noclobber.sh: line 3: file1.txt: cannot overwrite existing file\n./noclobber.sh: line 4: file1.txt: cannot overwrite existing file\nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <pre><code># eval.sh\nMacBook-Pro:wiki sunilamperayani$ cat eval.sh \n#!/usr/bin/env bash\ncmd=\"ls -l\"\neval $cmd\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre>"},{"location":"linux/scripting/overview/#arrays","title":"arrays","text":"<pre><code># bash shell &gt; 5.3-release\n\ndeclare -a servers\nservers=(\"server1\" \"coding\" \"structure tets\")\n\nnew_servers=(\"${servers[@]:0:1}\" \"server1.5\" ${servers[@]:1}]})\necho \"${new_servers[@]}\" # server1 server1.5 coding structure tets]}\n\nunset servers[1]\necho \"${servers[@]}\" # server1 structure tets\n\ndeclare -a array=(\"One\" \"Two\" \"Three\")\narray+=(\"Four\" \"Five\" \"Six\")\necho \"${array[@]}\" # One Two Three Four Five Six\n\ndeclare -a numbers=(5 1 3 2 4)\nprintf \"%s\\n\" \"${numbers[@]}\" | sort # 1 2 3 4 5\n\n# write an example of associative array key-value pair\n# instead of using index, we can use string as index\ndeclare -A fruits\nfruits=([apple]='red' [banana]='yellow' [cherry]='red')\necho \"${fruits[apple]}\" # red\n\n# add new key-value pair\nfruits[\"green\"]=\"pear\"\necho \"${fruits[green]}\" # pear\n\n# modify value\nfruits[\"red\"]=\"new apple\" \necho \"${fruits[@]}\"\n\n# delete key-value pair\nunset fruits[banana]\necho \"${fruits[@]}\" # red new apple pear\n\n# loop through key-value pair\nfor key in \"${!fruits[@]}\"; do\n    echo \"$key: ${fruits[$key]}\"\ndone\n</code></pre>"},{"location":"linux/scripting/overview/#variable-expansion","title":"variable expansion","text":"<pre><code>echo \"Hello ${name1:-unkown}\" # name is not defined so it will print unkown which is default value\n\nname=\"John Doe\"\necho \"Hello ${name:=unkown}\" # assign default values\n\n\necho \"Hello, ${name:0:4}\" # extract substring from 0 to 4\n\n# string replacement\necho \"${path/Downloads/Documents}\" # replace Downloads with Documents\n\n# string length\necho \"Length: ${#path}\" # print length of the string\n</code></pre>"},{"location":"linux/scripting/overview/#parameter-expansion","title":"parameter expansion","text":"<p>'#' matching <code>prefix</code> '%' matching <code>suffix</code></p> <pre><code># extract last part of the path\npath=\"/home/user/Downloads\"\necho \"Path: ${path##*/}\" # extract last part of the path\n\ngreeting=\"Hello World\"\necho \"${greeting#H}\" # matches from left to right\n#ello World\n\necho \"${greeting%d}\" # matches from right to left\n#Hello Worl\n\nmy_text_file=\"/home/my_username/text_file.txt\"\nmy_python_file=\"/usr/bin/app.py\"\n\necho \"${my_text_file##*/}\" # remove the last part of the path\n\necho \"${my_python_file##*/}\" # remove the last part of the path\n\necho \"${my_python_file%.*}\" # remove the last part of the path\n</code></pre>"},{"location":"linux/scripting/overview/#examples","title":"examples","text":""},{"location":"linux/scripting/overview/#bash-unit-testing","title":"bash unit testing","text":"<pre><code>#!/bin/bash \n\nhelp(){\n  scriptname=`basename $0`\n  echo \"Syntax: ${scriptname} &lt;filename&gt;\"\n  exit 1\n}\n\nvalidateOnlyOneArgument(){\n  if [ \"$#\" -ne 2 ]; then \n    echo \"PASSED: Atleast one argument passed\"\n  fi\n\n  return 0\n}\n\nvalidateFileOnly() {\n  [ -f ${filename} ] &amp;&amp; echo \"PASSED: fileonly\"\n  [ -d ${filename} ] &amp;&amp; echo \"Failed: directory\" &amp;&amp; exit 1\n\n  return 0\n}\n\nreadUnCommentLinesInFile() {\n  filename=\"$1\"\n  while read -r line; do \n    [[ \"$line\" = \\#* ]] &amp;&amp; continue \n    printf \"%s\\n\" \"$line\"\n  done&lt;${filename}\n}\n\ncleanLogFiles() {\n  local logfile=\"$1\"\n  local default_lines=50\n  tail -n ${default_lines} ${logfile} &gt; ${logfile}.tmp\n  mv ${logfile}.tmp ${logfile}\n  echo \"Info: housekeeping on ${logfile} completed\"\n\n  return 0\n}\n\nwhoExecutesThisScript() {\n  [ \"${UID}\" -ge 500 ] &amp;&amp; echo \"Info: non-root users can execute this script\"\n}\n\ncountNumberOfLinesInFile() {\n  local count=0\n  local filename=\"$1\"\n  while read -r line; do \n    ((count++))\n    echo $line &lt;$filename\n  done\n\n  return $count\n}\n\n# main function\nmain() {\n  filename=\"$1\"\n  logfilename=\"./app.log\"\n  whoExecutesThisScript\n  linecountinfile=countNumberOfLinesInFile \"$filename\"\n  echo \"lines in file: $?\"\n\n  # Unit tests for the script, only when successful, then continue\n\n  if validateOnlyOneArgument; then \n     validateFileOnly\n     cleanLogFiles $logfilename\n     readUnCommentLinesInFile $filename\n  fi\n}\n\n[ \"$#\" -ne \"1\" ] &amp;&amp; help \nmain \"$1\" | tee -a app.log\n</code></pre>"},{"location":"linux/scripting/overview/#good-practices","title":"Good practices","text":"<p>Bash reserved exit codes</p> <pre><code>0  success\n1  general error\n2  misuse of shell builtins\n126 cannot execute\n128  cannot execute\n130  script terminated by ctrl-c\n127  command not found\n</code></pre> <p><code>set -e</code>  exits on error <code>set -u</code> exits when a unset variable is used <code>set -o pipefail</code> catches errors in piped commands</p>"},{"location":"linux/scripting/overview/#set-e","title":"set -e","text":"<p>Incorrect bash script</p> <pre><code># safe.sh\n#!/usr/bin/env bash\n\nehco \"hello\"\nexit 0\n\n./safe.sh\necho $? #0 returns success code which is wrong\n</code></pre> <p>Correct Bash script</p> <pre><code># safe.sh\n#!/usr/bin/env bash\n\nset -e\n\nehco \"hello\"\nexit 0\n\n./safe.sh\necho $? #126\n</code></pre>"},{"location":"linux/scripting/overview/#set-u","title":"set -u","text":"<p>Incorrect bash script</p> <pre><code># safe1.sh\n\nname=\"Sunil\"\n# last_name=\"Kumar\"\n\necho \"My Full name is ${name} ${last_name}..!!\"\nexit 0 \n\n./safe1.sh\necho $? # 0 Incorrect as last_name is not executed\n</code></pre> <p>Corrected bash version</p> <pre><code>set -u\necho \"My Full name is ${name} ${last_name}..!!\"\nexit 0 \n\n./safe1.sh\necho $? # 1 \n</code></pre>"},{"location":"linux/scripting/overview/#set-o-pipefail","title":"set -o pipefail","text":"<pre><code># pipefail.sh\n\ncat non-existent-file.txt | sort | uniq \nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n0\n</code></pre> <p>In below script, if the <code>cat</code> has been failed, it should not execute the <code>echo</code> command.. but it does. so to fix it use <code>set -o pipefail</code></p> <pre><code># pipefail.sh\n\ncat non-existent-file.txt | sort | uniq &amp;&amp; echo \"this line should not be executed\";\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\n\"this line should not be executed\";\necho $? \n0\n</code></pre> <p>you won't get an <code>echo</code> statement if it fails when we set <code>set -o pipefail</code> but we still have an issue with the exit code.</p> <pre><code># pipefail.sh\nset -o pipefail\n\ncat non-existent-file.txt | sort | uniq &amp;&amp; echo \"this line should not be executed\";\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n0\n</code></pre> <pre><code># pipefail.sh\nset -e\nset -u\nset -o pipefail\n\nreadonly PIPE_ERROR=156\n\n# terminate function technique\nterminate()\n{\n  local -r msg=\"${1}\"\n  local -r code=\"${2:-160}\"\n  echo \"${msg}\" &gt;&amp;2\n  exit \"${code}\"\n}\n\ncat non-existent-file.txt | sort || { terminate \"error in piped command\" \"${PIPE_ERROR}\" }\n\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n156\n</code></pre>"},{"location":"linux/scripting/overview/#no-op-command","title":"no-op command","text":"<p>dry run command in bash. its a shell-built in command which has no behaviour programmed.</p> <pre><code>#!/use/bin/env bash\n\nif [[ \"$1\" = \"start\" ]]; then\n  : #no-op command\nelse\n  echo \"invalid string\"\nfi\n</code></pre>"},{"location":"linux/scripting/overview/#logging","title":"logging","text":"<pre><code>#!/usr/bin/env bash\nlog() {\n  echo $(date -u +\"%Y-%m-%d%T%H:%M:%SZ\") \"${@}\"\n}\n\nlog \"hello world\"\n</code></pre>"},{"location":"linux/scripting/overview/#awk","title":"awk","text":"<ul> <li>domain specific language for streamling text operations.</li> <li>expect input as standard behaviour</li> <li>input can be passwd by keyboard, pipes or files</li> <li>' '(quotes) used in awk is to prevent the shell/terminal expansion. </li> </ul> <pre><code># awk expects input as the standard behaviour.\nawk -F\":\" '{print $1}' /etc/passwd \nawk -F\":\" '{print $1}' &lt; /etc/passwd \ncat /etc/passwd | awk '{print $1}'\n</code></pre>"},{"location":"linux/scripting/overview/#built-in-variables","title":"built-in variables","text":"<ul> <li>NR: total number of records (lines) processed so far across all input files</li> <li>NF: represents the number of fields (columns) in the current record (line)</li> <li>FILENAME: represents the name of the current file being processed.</li> </ul> <p>Program block = <code>BEGIN</code> Action block = <code>{print $1}</code></p> <pre><code>\nfile1.txt\n\napple\nbanana\ncherry\n\nawk '{ print NR, $0 }' file1.txt\n\n# output \n\n1 apple\n2 banana\n3 cherry\n\nNR=1 for first line\nNR=2 for second line\n</code></pre> <pre><code>file2.txt\n\napple banana cherry\ngrape mango\n\nawk '{ print NF, $0 }' file2.txt\n\n# output\n3 apple banana cherry\n2 grape mango\n\nThe first line has 3 fields, so NF=3.\nThe second line has 2 fields, so NF=2.\n</code></pre> <pre><code># file1.txt\napple\nbanana\n\n#file2.txt\ngrape\nmango\n\n\nawk '{ print FILENAME, NR, $0 }' file1.txt file2.txt\n\nfile1.txt 1 apple\nfile1.txt 2 banana\nfile2.txt 3 grape\nfile2.txt 4 mango\n\nFILENAME shows the file name being processed.\nNR continues counting records across both files.\n</code></pre> <pre><code>awk '{ print \"File:\", FILENAME, \"Record:\", NR, \"Fields:\", NF, \"Line:\", $0 }' file1.txt file2.txt\n\nFile: file1.txt Record: 1 Fields: 1 Line: apple\nFile: file1.txt Record: 2 Fields: 1 Line: banana\nFile: file2.txt Record: 3 Fields: 1 Line: grape\nFile: file2.txt Record: 4 Fields: 1 Line: mango\n</code></pre> <p>Field seperator</p> <pre><code>cat  /etc/passwd | awk -F \":\" '{print $1 $7}'\n</code></pre> <p>-v declare a vraible before executing the action block or a program.</p> <pre><code>awk -v var=\"Hello world\" 'BEGIN { print var }' \nHello world\n\nawk -F \":\" -v user=\"Users homedirectory: \" '{print user, $1}' /etc/passwd \n\n# output\n\nUsers homedirectory:  _notification_proxy\nUsers homedirectory:  _avphidbridge\nUsers homedirectory:  _biome\nUsers homedirectory:  _backgroundassets\nUsers homedirectory:  _mobilegestalthelper\nUsers homedirectory:  _audiomxd\nUsers homedirectory:  _terminusd\n</code></pre> <pre><code># uid&gt;100\nawk -F\":\" -v user=\"username:\" -v uid=\"100\" '$3&gt;=uid {print user,$1, $3}' /etc/passwd\n\n# 100&gt;uid&lt;300\nawk -F\":\" -v user=\"username:\" -v uid=\"100\" -v uidlow=\"300\" '$3&gt;=uid &amp;&amp; $3&lt;=uidlow {print user,$1, $3}' /etc/passwd\n</code></pre>"},{"location":"linux/scripting/overview/#using-awk-file","title":"using awk-file","text":"<p>you can use an awk program, but shell expansions like globbing, command substituitions, and  command like utilities are not available. </p> <pre><code># hello.awk\n\n#!/usr/bin/env awk -f \nBEGIN {\n  print \"hello\"\n}\nbash-3.2$ \n\n./hello.awk\nhello\n</code></pre> <pre><code>#!/usr/bin/env bash\n\nawk -v hello=\"Hello\" 'BEGIN { \n    print hello\n  }'\n\n</code></pre>"},{"location":"linux/scripting/overview/#sed","title":"sed","text":"<p><code>sed</code> takes the input parameter and would process the unprocessed text and the processed text to the stdout. it will take input from the keyboard, file, or the pipes. i.e similar to awk</p> <pre><code>sed 'p' # waits for the input from the keyboard\nt\nt # unprocessed text\nt # processed text i.e output\n</code></pre> <p>In order to supress automatic printing use <code>-n</code></p> <pre><code>sed -n '2p' filename.txt # print the second line supressing the automatic printing\n</code></pre> <p>delete(-d)</p> <pre><code>sed -n '2d' filename.txt # delete the 2nd line and print to standard output\nsed -n '2,5d' filename.txt # delete from 2nd line to 5th line and print to standard output\n</code></pre> <p>in-place (-i) </p> <pre><code>sed -i '2,5d' filename.txt # write to the filename.txt instead of the stdout\n</code></pre> <p>search</p> <p>search always ends with '//' follwed by 'p' print.  <pre><code>sed -n '/broot/p' /etc/passwd\n\n#\\b word boundary followed by string pattern.\nsed -n '/\\broot\\b/p' /etc/passwd # good practice to use '\\b'\n\n# -e indicated its an sed script for multiple options MANDATORY\nsed -n -e '/\\broot\\b/p' -e '/\\bsunil\\b/p' /etc/passwd \n\n# delete the root and write into the file\nsed -n -i -e '/\\broot\\b/d' -e '/\\bsunil\\b/p' /tmp/passwd\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/","title":"setup and install","text":""},{"location":"monitoring/elk/elasticsearch/01_install/#getting-started","title":"Getting started","text":"<p>Please download the elasticsearch and kibana from below official websites for your OS.</p> <p>Elasticsearch</p> <p>Kibana</p> <p>I would be using Mac for elasticsearch and hence would provide those links for this purpose. </p> <p>elasticsearch for Mac-Intel</p> <p>Kibana for Mac-Intel</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#local-installation-setup","title":"Local Installation Setup","text":"<p>Create a new directory and move those compressed files into the directory for extraction.</p> <pre><code>mkdir elastic-stack\ntar -zxvf elasticsearch-8.8.0-darwin-x86_64.tar.gz\ntar -zxvf kibana-8.8.0-darwin-x86_64.tar.gz\nmv elasticsearch-8.8.0 elasticsearch\nmv kibana-8.8.0 kibana\n</code></pre> <p>Open two terminals and move to respective directories for those two services. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#elasticsearch","title":"Elasticsearch","text":"<p>start the service, It provides the username, password and tokens as part of initial startup</p> <pre><code>cd elastic-stack/elasticsearch\nbin/elasticsearch [ Enter ]\n\n\u2705 Elasticsearch security features have been automatically configured!\n\u2705 Authentication is enabled and cluster connections are encrypted.\n\n\u2139\ufe0f  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):\n  Xl4mktLmPkO=22=_DwEl\n\n\u2139\ufe0f  HTTP CA certificate SHA-256 fingerprint:\n  5c47f6c2bd182fc83cd9487af5dc400c3fa75013995fa8c5cf5b63c930803cb1\n\n\u2139\ufe0f  Configure Kibana to use this cluster:\n\u2022 Run Kibana and click the configuration link in the terminal when Kibana starts.\n\u2022 Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):\n  eyJ2ZXIiOiI4LjguMCIsImFkciI6WyIxOTIuMTY4LjAuMTA2OjkyMDAiXSwiZmdyIjoiNWM0N2Y2YzJiZDE4MmZjODNjZDk0ODdhZjVkYzQwMGMzZmE3NTAxMzk5NWZhOGM1Y2Y1YjYzYzkzMDgwM2NiMSIsImtleSI6ImlTd1BjSWdCRmJwS05TRUtLSmwyOm9sVlc2aEd1VG9tYVJVUThCZ2Y1c3cifQ==\n\n\u2139\ufe0f  Configure other nodes to join this cluster:\n\u2022 On this node:\n  \u2043 Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.\n  \u2043 Uncomment the transport.host setting at the end of config/elasticsearch.yml.\n  \u2043 Restart Elasticsearch.\n\u2022 On other nodes:\n  \u2043 Start Elasticsearch with `bin/elasticsearch --enrollment-token &lt;token&gt;`, using the enrollment token that you generated.\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#kibana","title":"Kibana","text":"<p>Start the service, after which they are prompted using the url to navigate into browser. </p> <pre><code>xattr -d -r com.apple.quarantine kibana\ncd elastic-stack/kibana\nbin/kibana [ Enter ]\n\nhttp://localhost:5200/_  ..\n\n</code></pre> <p>Provide the username and password generated from starting the elasticsearch cluster and use your own default dashboard for setup. </p> <p>If you want to query, then <code>hover</code> to the kibana dashboard -&gt; <code>App -&gt;dev_settings-&gt;console</code></p>"},{"location":"monitoring/elk/elasticsearch/01_install/#search-query-using-console","title":"search query using console","text":"<p>All the queries in the elasticsearch would be using an REST API under the hood when after successful validation would respond with an JSON object.</p> <pre><code>GET /_cluster/health  # Cluster health status\nGET /_cat/indices?v # list indices\nGET /_cat/nodes?v # list nodes \n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#search-query-using-curl","title":"search query using curl","text":"<p>Since, its being an REST API call, you can use any of the client like <code>curl</code> or <code>postman</code> to query for the response. </p> <pre><code>curl -H \"Content-Type:application/json\" --cacert config/certs/http_ca.crt -u elastic:Xl4mktLmPkO=22=_DwEl -X GET https://localhost:9200/products/_search -d '{\"query\": {\"match_all\":{}}}'\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#sharding","title":"Sharding","text":"<p>Data in Elasticsearch is organized into indices. Each index is made up of one or more shards. Each shard is an instance of a Lucene index, which you can think of as a self-contained search engine that indexes and handles queries for a subset of the data in an Elasticsearch cluster.</p> <p>Shard</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#replication","title":"Replication","text":"<p>Each index is divided into shards and each shard can have multiple copies, and these are called as <code>replication group</code> and must always be kept in sync when docs are added and deleted. The copied shard from the replicatio group is called <code>replica shard</code> and for better fault tolerance they are kept in the secondary node. </p> <p>Note: In critical production, it would always be good to have a 3 nodes and incase if there are failures, we would be able to search data from other backups. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#snaphosts","title":"Snaphosts","text":"<p>A snapshot is a backup of a running Elasticsearch cluster. </p> <p>A snapshot copies segments from an index\u2019s primary shards. When you start a snapshot, Elasticsearch immediately starts copying the segments of any available primary shards. If a shard is starting or relocating, Elasticsearch will wait for these processes to complete before copying the shard\u2019s segments. If one or more primary shards aren\u2019t available, the snapshot attempt fails</p> <p>To back up an index, a snapshot makes a copy of the index\u2019s segments and stores them in the snapshot repository.</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#adding-additional-nodes-to-the-elasticsearch","title":"Adding additional nodes to the Elasticsearch","text":"<p>copy the tar file of the <code>elasticsearch</code> and place in a new directory and extract. </p> <pre><code>mkdir elastic-stack/node2\ncp elasticsearch-8.8.0-darwin-x86_64.tar.gz elastic-stack/second-node\ntar -zxvf elasticsearch-8.8.0-darwin-x86_64.tar.gz \nmv elasticsearch-8.8.0 second-node\nvim elastic-stack/second-node/node2/config/elasticsearch.yml\n\n# modify the hostname, save &amp; quit\nnode.name: second-node\n</code></pre> <p>Now, you need to get the active token from already running(master) elasticsearch. </p> <pre><code>cd elastic-stack/elasticsearch/bin/\n./elasticsearch-create-enrollment-token --scope node\n</code></pre> <p>copy the token and go to second node. </p> <pre><code>cd elastic-stack/second-node/node2/\n./bin/elasticsearch --enrollement-token &lt;token&gt;\n</code></pre> <p>Once they are joined, you would need to go to kibana dashboard console and query for the <code>GET /_cluster/health</code> you would be seeing two nodes. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#starting-services","title":"Starting services","text":"<p>Open two terminals side by side and execute below</p> <pre><code>cd elastic-stack/elasticsearch/bin/\nbin/elasticsearch  [ Enter ]\n\ncd elastic-stack/kibana\nbin/kibana [ Enter ]\n</code></pre> <p>Copy paste kibana URL into browser http://localhost:5601/app/home#/</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#console-login","title":"Console login","text":"<p>Login to the url kibana console login for elastic search http://localhost:5601/app/home#/.  Click on, Hover button -&gt; Dev tools to open console. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#cluster-health-check","title":"Cluster health check","text":"<pre><code>GET /_cluster/health\nGET /_cat/indices?v\nGET /_cat/shards?v\nGET /_cat/nodes?v\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/","title":"managing documents","text":""},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#create-index","title":"Create index","text":"<pre><code>PUT /products \n{\n  \"settings\": {\n    \"number_of_shards\": 2, \n    \"number_of_replicas\": 1\n  }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#delete-index","title":"Delete index","text":"<pre><code>DELETE /products\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#create-index-id","title":"Create index id","text":"<p>Creating some random document ID</p> <pre><code>POST /products/_doc\n{\n  \"name\": \"coffeemaker\",\n  \"price\": 64,\n  \"in_stock\": 10\n}\n\n</code></pre> <p>You can also create with index which you wish</p> <pre><code>POST /products/_doc/100\n{\n  \"name\": \"mobile charger\",\n  \"price\": 1999,\n  \"in_stock\": 0\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#get-index-by-id","title":"Get index by id","text":"<pre><code>GET /products/_doc/100\n\n# Get all the documents\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#update-index-by-id","title":"Update index by id","text":"<pre><code>POST /products/_update/100\n{\n  \"doc\": {\n    \"name\": \"mobile earbuds\"\n  }\n}\n\nGET /products/_doc/100\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#add-new-fields","title":"Add new fields","text":"<pre><code>POST /products/_update/100\n{\n  \"doc\": {\n    \"tags\": [\"electornics\"]\n  }\n}\n\nGET /products/_doc/100\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#scripted-updates","title":"Scripted updates","text":"<pre><code>POST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#add-parameters-and-query","title":"Add parameters and query","text":"<pre><code>POST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock=params.quantity\",\n    \"params\": {\n      \"quantity\":1\n    }\n\n  }\n}\n\nGET /products/_doc/100\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#upsert","title":"Upsert","text":"<p>Incase the document exists, then run the query &amp; update else you would be creating a new document. </p> <pre><code>POST /products/_update/101\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock++\"},\n    \"upsert\":\n    {\n      \"name\": \"iphone 11\",\n      \"price\": 309999,\n      \"in_stock\": 8\n    }\n}\n\nGET /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#replace-document","title":"Replace document","text":"<pre><code>PUT /products/_doc/101\n{\n  \"name\": \"Google pixel\",\n  \"price\": 121111,\n  \"in_stock\": 3\n}\n\nGET /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#deleting-document","title":"Deleting document","text":"<pre><code>DELETE /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#routing","title":"Routing","text":"<p>When running a search request, Elasticsearch selects a node containing a copy of the index\u2019s data and forwards the search request to that node\u2019s shards. This process is known as search <code>**shard routing or routing**</code></p> <p>(How read and write happens in elastic search)[https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html]</p>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#update-by-query","title":"update by query","text":"<p>Updates documents that match the specified query. If no query is specified, performs an update on every document in the data stream or index without modifying the source, which is useful for picking up mapping changes.</p> <pre><code>POST products/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  },\n  \"query\": {\n      \"match_all\": {}\n    }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#delete-by-query","title":"delete by query","text":"<p>Deletes documents that match the specified query.</p> <pre><code>POST products/_delete_by_query\n{\n  \"query\": {\n      \"match_all\": {}\n    }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#bulk-api","title":"Bulk API","text":"<p>Performs multiple indexing or delete operations in a single API call. This reduces overhead and can greatly increase indexing speed.</p> <pre><code>POST _bulk\n{ \"index\" : { \"_index\" : \"products\", \"_id\" : \"200\" } }\n{ \"name\" : \"Mac Laptop\", \"price\": 102121,\"in_stock\":10 }\n{ \"create\" : { \"_index\" : \"products\", \"_id\" : \"201\" } }\n{ \"name\" : \"Laptop bags\", \"price\": 1032,\"in_stock\":29 }\n{ \"update\" : {\"_id\" : \"200\", \"_index\" : \"products\"} }\n{ \"doc\" : { \"name\" : \"Mac Laptop Premier\", \"price\": 102121,\"in_stock\":10} }\n{ \"delete\": { \"_index\" : \"products\", \"_id\" : \"201\" }}\n{ \"delete\": { \"_index\" : \"products\", \"_id\" : \"200\" }}\n</code></pre> <p>Incase you want to use the curl to pass the data, you need to create a new file <code>request</code> and update <code>_bulk</code> data into the file and pass it to curl. </p> <pre><code>curl -H \"Content-Type:application/json\" --cacert config/certs/http_ca.crt -u elastic:Xl4mktLmPkO=22=_DwEl -X POST https://localhost:9200/_bulk --data-binary \"@request\"; echo\n\n\n{\"took\":39,\"errors\":false,\"items\":[{\"index\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":13,\"_primary_term\":1,\"status\":201}},{\"create\":{\"_index\":\"products\",\"_id\":\"201\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":14,\"_primary_term\":1,\"status\":201}},{\"update\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":2,\"result\":\"updated\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":15,\"_primary_term\":1,\"status\":200}},{\"delete\":{\"_index\":\"products\",\"_id\":\"201\",\"_version\":2,\"result\":\"deleted\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":16,\"_primary_term\":1,\"status\":200}},{\"delete\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":3,\"result\":\"deleted\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":17,\"_primary_term\":1,\"status\":200}}]}\n\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/","title":"mapping and analysis","text":""},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#intro-to-analysis","title":"Intro to Analysis","text":"<p>standard analyzer or text analyzer</p> <p>When we index a text value, it goes through an analysis process. The purpose of it is to store the values in a data structure that is efficient for searching. When a text value is indexed, a so-called analyzer is used to process the text.</p> <p>An analyzer consists of three building blocks</p> <ul> <li>**character filters **</li> </ul> <p>A character filter receives the original text and may transform it by adding, removing, or changing characters. An analyzer may have zero or more character filters, and they are applied in the order in which they are specified.</p> <p>e.g get all text using html_strip</p> <p>-** a tokenizer **</p> <p>An analyzer must contain exactly one tokenizer, which is responsible for tokenizing the text. By \u201ctokenizing,\u201d I am referring to the process of splitting the text into tokens. As part of that process, characters may be removed, such as punctuation, exclamation marks, etc.</p> <p>e.g split a sentence into words by splitting the string whenever a whitespace is encountered. The input string is therefore tokenized into a number of tokens.</p> <ul> <li>token filters</li> </ul> <p>These receive the tokens that the tokenizer produced as input and they may add, remove, or modify tokens. As with character filters, an analyzer may contain zero or more token filters, and they are applied in the order in which they are specified.</p> <p>e.g token filter is probably the \u201clowercase\u201d filter, which lowercases all letters.</p> <p>Output: The result of analyzing text values is then stored in a searchable data structure.</p> <p>No character filter is used by default, so the text is passed on to the tokenizer as is. The tokenizer splits the text into tokens according to the Unicode Segmentation algorithm.  tokenizer breaks sentences into words by whitespace, hyphens, and such. In the process, it also throws away punctuation such as commas, periods, exclamation marks, etc.</p> <p>The tokens are then passed on to a token filter named \u201clowercase.\u201d it lowercases all letters for the tokens. </p> <p>This standard analyzer is used for all text fields unless configured otherwise. There are a couple of analyzers available besides the \u201cstandard\u201d analyzer, but that\u2019s the one you will typically use.</p> <p>From the elastic console, try to run the API query for the standard analyzer, you would understand.</p> <pre><code>POST /_analyze\n{\n    \"text\": \"walk into bar and don't.... DRINK\",\n    \"analyzer\": \"standard\"\n}\n</code></pre> <p>You can also write your custom analyzer, however it would behave same as standard analyzer outputs. </p> <pre><code>POST /_analyze\n{\n    \"text\": \"walk into bar and don't.... DRINK\",\n    \"char_filter\":[],\n    \"tokenizer\":\"standard\",\n    \"filter\":[\"lowercase\"]\n}\n</code></pre> <p>let\u2019s take a look at what actually happens with the result, being the tokens. Elasticsearch uses more than one data structure, is to ensure efficient data retrieval for different access patterns.</p> <p>e.g searching for a given term is handled differently than aggregating data.</p> <p>Actually these data structures are all handled by Apache Lucene and not Elasticsearch.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#inverted-indices","title":"Inverted indices","text":"<p>An inverted index is essentially a mapping between tokens that are emitted by the analyzer(a.k.a tokens) and which documents contain them.</p> <p>Anyway, let\u2019s take previous example and see how that would be stored within an inverted index.</p> <p>Document #1 -&gt; \"walk into bar and don't.... DRINK\" -&gt; [\"and\",\"bar\",\"don't\",\"DRINK\",\"into\",\"walk\"]  Document #2 -&gt; \"I walk into bar\" -&gt; [\"I\",\"bar\",\"into\",\"walk\"]</p> <p>As you can see, each unique term is placed in the index together with information about which documents contain the term, sorted alphbetically. Suppose that we perform a search for the term \"bar\", we can see that documents #1 and #2 contain the term.</p> <p>its called as inverted index because, the logical mapping is terms they contain to documents.  The inverted indices that are stored within Apache Lucene contain a bit more information, such as data that is used for relevance scoring.</p> <p>As you can imagine, we don\u2019t just want to get the documents back that contain a given term; we also want them to be ranked by how well they match.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#mapping","title":"mapping","text":"<p>Mapping defines the structure of documents and how they are indexed and stored.  This includes the fields of a document and their data types. As a simplification, you can think of it as the equivalent of a table schema in a relational database.</p> <p>In Elasticsearch, there are two basic approaches to mapping; explicit and dynamic mapping.  </p> <p>explicit mapping, we define fields and their data types ourselves, typically when creating an index. </p> <p>dymanic mapping, field mapping will automatically be created when elasticsearch encounters a new field. It will inspect the supplied field value to figure out how the field should be mapped, if you supply a string value, for instance, elasticsearch will use the \"text\" data type for the mapping.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#datatypes","title":"datatypes","text":"<p>there are many, few are discussed and frequently used. </p> <p>Object: JSON documents are hierarchical in nature, the document may contain inner objects which, in turn, may contain inner objects themselves, they are mapped using properties parameter. </p> <pre><code>PUT my-index-000001/_doc/1\n{ \n  \"region\": \"US\",\n  \"manager\": { \n    \"age\":     30,\n    \"name\": { \n      \"first\": \"John\",\n      \"last\":  \"Smith\"\n    }\n  }\n}\n</code></pre> <p>Internally, this document is indexed as a simple, flat list of key-value pairs</p> <pre><code>{\n  \"region\":             \"US\",\n  \"manager.age\":        30,\n  \"manager.name.first\": \"John\",\n  \"manager.name.last\":  \"Smith\"\n}\n</code></pre> <p>In a case where you have documents of same tokens, then it would make a flat array of internal documents and query. which would perform \"OR\" operation and provides wrong results.. hence we use the word nested </p> <pre><code>{\n  \"region\":             \"US\",\n  \"manager.age\":        [30,50,60]\n  \"manager.name.first\": [\"John\",\"Mat\",\"Suj\"],\n  \"manager.name.last\":  [\"Smith\",\"Hew\",\"Samuel\"]\n}\n\n</code></pre> <p>Keyword: exact matching of values, typically used for filtering, aggregating, sorting  e.g search articles only \"published\"</p> <p>text: use for full text searches.  e.g entire body of article</p> <p>References:  https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-data-types.html</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#coercion","title":"coercion","text":"<p>Data types are inspected when indexing docs, invalid values are rejected. so that only text fields are indexed. </p> <p>Note:  - Enabled by default - Always try using correct data types</p> <pre><code>PUT /coercion_test/_doc/1 {\n    \"price\": 7.4\n}\n</code></pre> <p>Creates and indexde as float as a new document. </p> <pre><code>PUT /coercion_test/_doc/1 {\n    \"price\": \"7.4\"\n}\n</code></pre> <p>Creates and indexed as string as a new document. </p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#understand-arrays","title":"understand arrays","text":"<p>Array values should be of same data type Array may contain nested arrays Arrays are flattened during indexing</p> <p>Note: Remember to use the nested data type for arrays of objects if you need to query the objects independently</p> <pre><code>POST /products/_doc {\n    \"tags\": [\"item1\",\"item2\"]\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#adding-explict-mapping","title":"adding explict mapping","text":"<p>all field mapping defined in properties key, including nested objected and create index.</p> <pre><code>PUT /reviews {\n    \"mappings\": {\n        \"properties\" {\n            \"rating\": { \"type\": \"float\"},\n            \"content\": { \"type\": \"text\"},\n            \"product_id\": { \"type\": \"integer\"},\n            \"author\": {\n                \"properties\": {\n                    \"first_name\": {\"type\": \"text\"},\n                    \"last_name\": { \"type\", \"text\"},\n                    \"email\": {\"type\": \"keyword\"}\n\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>The above one is one method of creating an index, but there is second method of using i.e (dot-notation) which is easy to create index.</p> <pre><code>PUT /reviews {\n    \"mappings\": {\n        \"properties\" {\n            \"rating\": { \"type\": \"float\"},\n            \"content\": { \"type\": \"text\"},\n            \"product_id\": { \"type\": \"integer\"},\n            \"author.first_name\": {\"type\": \"text\"},\n            \"author.last_name\": { \"type\", \"text\"},\n            \"author.email\": {\"type\": \"keyword\"}\n\n\n\n        }\n    }\n}\n</code></pre> <p>create a new document</p> <pre><code>PUT /reviews/_doc/1 {\n    \"rating\": 4.3,\n    \"content\": \"Elastic search is good\",\n    \"product_id\": 123,\n    \"author\": {\n        \"first_name\": \"Sunil\",\n        \"last_name\": \"Kumar\",\n        \"email\": \"sun@gmail.com\"\n    }\n}\n</code></pre> <p>incase you don't provide email and submit, coercion while indexing would throw an error because its enabled by default.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#retrive-mapping","title":"retrive mapping","text":"<pre><code>GET /reviews/_mapping\n\nGET /reviews/_mapping/field/content\n\nGET /reviews/_mapping/field/author.email\n</code></pre> <p>lets say you need to map a new value to already created index..</p> <pre><code>PUT /reviews/_mapping {\n    \"properties\": {\n        \"created_at\":  {\n            \"type\": \"date\"\n            }\n    }\n}\n\nGET /reviews/_mapping\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#how-date-works-in-elasticsearch","title":"how date works in elasticsearch","text":"<p>dates are referred in three ways and elastic search would convert those date and time formts into a long string. </p> <ul> <li>specially formatted strings</li> <li>milliseconds since the EPOCH(long)</li> <li>seconds since the EPOCH(integrer)</li> </ul> <p>supported formats </p> <ul> <li>date without time</li> <li>date with time</li> <li>milliseconds since the EPOCH(long) defaults to UTC</li> </ul> <pre><code>PUT /reviews/_doc/1 {\n    \"rating\": 4.3,\n    \"content\": \"Elastic search is good\",\n    \"product_id\": 123,\n    \"author\": {\n        \"first_name\": \"Sunil\",\n        \"last_name\": \"Kumar\",\n        \"email\": \"sun@gmail.com\",\n        \"birth\": \"2015-04-15\"  # only date\n        \"birth\": \"2015-04-15T15:00:00Z\" T seperated by date and time, followed by Z(UTC)\n        \"birth\": \"123223023823772\" # UNIX EPOCH timestamp\n    }\n}\n\nGET /reviews/_search {\n    \"query\" {\n        \"match_all\": {}\n    }\n}\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#mapping-parameters","title":"mapping parameters","text":"<ul> <li>format: customize date format, however recommended <code>date</code></li> <li>properties: used for object and nested </li> <li>coerce: index validation (enabled by default). you can disable during start but can be overide  when creating</li> <li>doc_values: used mainly for apache lucene, opposite for interved index.</li> <li>norms: normalizing factors used for revelance scores, i.e ranking. </li> <li>index: </li> <li>null_value:  cannont be index or searched</li> <li>copy_to: used to copy multiple field values into a group field(not tokes/terms), when search this field you won't get results because its not indexed.</li> </ul>"},{"location":"monitoring/elk/kibana/overview/","title":"overview","text":""},{"location":"monitoring/elk/kibana/overview/#kibana","title":"Kibana","text":""},{"location":"monitoring/elk/logstash/overview/","title":"overview","text":""},{"location":"monitoring/elk/logstash/overview/#logstash","title":"Logstash","text":""},{"location":"nginx/access_control/","title":"access control","text":""},{"location":"nginx/access_control/#whitelisting","title":"whitelisting","text":"<p>you can allow traffic reahing to your web server by configuring in the nginx server, so that only those requests are sent to the webserver. </p> <pre><code>vim /etc/nginx/conf.d/whilelist.conf    \nallow 192.168.56.101\ndeny all\n\nvim /etc/nginx/conf.g/nginx.conf \nserver {\n    server_name localhost.com\n\n    location /var/www/html/webroot\n    index admin.html\n    include /etc/nginx/conf.d/whilelist.conf  \n}\n\nnginx -t\nsystemctl restart nginx\n\ncurl -I localhost.com/admin \n</code></pre>"},{"location":"nginx/access_control/#limit-connections","title":"limit connections","text":"<p>let's say you have a network speed of 100Mpbs, so if there are 10 users who need to download, the person who has the highest bandwidth speed would choke up the server and others won't be able to download the file. Hence we need to make sure we set the limits for downloaing.. this can be done using the module <code>limit_rate</code></p> <pre><code>vim /etc/nginx/conf.g/nginx.conf \nserver {\n    server_name localhost.com;\n\n    location /download;\n    limit_rate 50k;\n\nnginx -t \nsystemctl restart nginx\n}\n</code></pre> <p>Now, in the above usecase you have set the limits for the speed, but if we have same ip connecting to same server multiple times, we have resources being unnecesary choked up by web server.. so we need to restrict the connections by using the module </p> <pre><code>vim /etc/nginx/conf.g/nginx.conf\n\n#global config section\n\nlimit_conn_zone $binary_remote_addr zone=addr:10m;\n\nwhere, \nbinary_remote_addr - remote client\nspeed at which the remote client uses 10m\n\nserver {\n    server_name localhost.com;\n\n    location /download\n    limit_rate 50k;\n    limit_conn addr 1;\n\nwhere, \n\nfor the download directior, the `addr` module can have only 1 conection.\n\nnginx -t \nsystemctl restart nginx\n}\n</code></pre>"},{"location":"nginx/access_control/#basic-auth","title":"basic auth","text":"<p>There are 3 types of auth. <code>basic</code>, <code>digest</code> and <code>NTML</code>. Client would send an <code>GET /admin</code> to the webserver, who would then sees that the page is configured to have an <code>basic</code> auth sent and would respond to the client with <code>401</code>, meaning authorization required by setting header <code>WWW-Authenticate: Basic retain=Family</code> in which you see a dialog prompt asking for <code>username</code> and <code>password</code>. Once you have entered credentials, the browser would <code>encode</code> the packet with the header response to the server, where it <code>decodes</code> and then retrives the page and then sends back response to the client. </p> <p>Configuration</p> <p>Install <code>apache2-utils</code></p> <p>sudo htpasswd -c /etc/nginx/.htpasswd user1 sudo htpasswd -c /etc/nginx/.htpasswd user2</p> <pre><code>vim /etc/nginx/conf.g/nginx.conf\n\nhttp {\n    server {\n        listen 192.168.1.23:8080;\n        root   /usr/share/nginx/html;\n\n        location /api {\n            api;\n            satisfy all;\n\n            deny  192.168.1.2;\n            allow 192.168.1.1/24;\n            allow 127.0.0.1;\n            deny  all;\n\n            auth_basic           \"Administrator\u2019s Area\";\n            auth_basic_user_file /etc/apache2/.htpasswd;\n        }\n    }\n}\n</code></pre>"},{"location":"nginx/access_control/#hashing","title":"hashing","text":"<p>Hashing is a technique for converting data into a fixed-size value, called a hash value or hash code. The hash value is typically used to index a data structure such as a hash table. Hashing functions are designed to be fast and efficient, and to produce a uniform distribution of hash values for a given set of input data.</p> <ul> <li>Data storage</li> <li>Data security</li> <li>Cryptography</li> </ul> <p>different hashing algorithms</p> <ul> <li>MD5 128-bit hash value</li> <li>SHA-1  160-bit hash value.</li> <li>SHA-2</li> </ul>"},{"location":"nginx/access_control/#digest-authentication","title":"digest authentication","text":"<p>Digest authentication is a method used for authenticating users in network communication protocols, particularly in the context of web servers and clients. It's an improvement over the more basic HTTP Basic Authentication, providing stronger security by avoiding sending passwords in plaintext over the network.</p> <p>how it works</p> <p>When a client (typically a web browser) sends a request to a server that requires authentication, the server responds with a special HTTP 401 Unauthorized status code, along with a challenge in the form of a nonce (a random string of characters), and possibly other parameters. </p> <p>Upon receiving the 401 response, the client knows it needs to provide authentication. It prompts the user for their username and password, just like with HTTP Basic Authentication.</p> <p>Instead of sending the password directly, the client computes a cryptographic hash function (often MD5 or SHA-1) of various pieces of information, including the username, password, and nonce received from the server. This hash is called a digest.</p> <p>The client sends this digest along with the username and other required information to the server in another request.</p> <p>The server, upon receiving the authentication information, recalculates the digest using the same algorithm and compares it with the one sent by the client. If they match, the server can authenticate the user.</p> <p>Digest authentication offers several advantages over Basic Authentication:</p> <p>Security: Passwords are not sent in plaintext over the network, making it less susceptible to eavesdropping attacks.</p> <p>Nonce: The use of a nonce makes replay attacks difficult, as each nonce is valid only for a single authentication attempt.</p> <p>Flexibility: Digest authentication can be integrated with existing authentication mechanisms and is compatible with proxies and caching.</p> <p>Note: nginx doesn't support dynamic authentic</p>"},{"location":"nginx/caching/","title":"caching","text":""},{"location":"nginx/caching/#overview","title":"Overview","text":""},{"location":"nginx/caching/#benefits","title":"Benefits","text":"<ul> <li>it reduces the overhead of server's resource.</li> <li>decreases the network bankwidth.</li> <li>pages are loaded much more faster.</li> </ul>"},{"location":"nginx/caching/#caching-subsystems","title":"caching subsystems","text":""},{"location":"nginx/caching/#caching-controlheaders","title":"caching control(headers)","text":"<p>these are used to speificy directives for caching mechanism, they are used to defined caching policies with various directives provided by the header. you would specify whether you need your web page to be cached or not to be cached by using these headers..</p> <p>use cases..</p> <ul> <li>do not store any kind of cache at all </li> <li>store the cache, but verify with webserver wheather file is modified</li> <li>store the cache for 24 hours</li> </ul> <p>varipus cache control headers</p> <ul> <li>Cache-Control: no-store</li> <li>Cache-Control: no-cache</li> <li>Cache-Control: no-store, must-revalidate</li> <li>Cache-Control: public</li> <li>Cache-Control: private</li> </ul> <p>Configure all your .png files don't need your browser to store cache.</p> <pre><code>server {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n\n    location ~ \\.(.png) {\n       root   /usr/share/nginx/html;\n       add_header Cache-Control no-store;    \n    }\n\n}\n\nsystemctl nginx restart\n</code></pre> <p>From your client cli, reqeust the .png, when you check the response, you would see the cache header as response.</p> <pre><code>curl -I http://your-web-server/myseed.png\n</code></pre>"},{"location":"nginx/caching/#if-modified-header","title":"if modified header","text":"<p>when you request for the webpage by client, the nginx would first give an GET request to the server and check's for the timestamp header (Last-Modified) and if there's no response it would be 302 message to the client saying there is no change in the webpage and hence it can serve from the cache server. </p> <p>incase if ther's change, it would serve a new request to cache and then to the client.</p>"},{"location":"nginx/caching/#cache-control-header","title":"cache control header","text":"<p>The amount of time the webpage needs to be in the cache.</p> <pre><code>location ~ \\.(.png) {\n    root   /usr/share/nginx/html;\n    expires 1h;    \n}\n</code></pre> <pre><code>curl -I http://your-web-server/myseed.png\n</code></pre>"},{"location":"nginx/caching/#cache-no-store-must-revalidate","title":"cache: no-store, must-revalidate","text":""},{"location":"nginx/caching/#cache-max-age-s-max-age","title":"cache: max-age, s-max-age","text":""},{"location":"nginx/caching/#cache-time","title":"cache time","text":""},{"location":"nginx/caching/#expires-headers","title":"expires headers","text":""},{"location":"nginx/caching/#keep-alive-connections","title":"keep-alive connections","text":""},{"location":"nginx/caching/#date-time-expires","title":"date-time expires","text":""},{"location":"nginx/cryptography/","title":"cryptography","text":""},{"location":"nginx/cryptography/#asymmetric-cryptography","title":"asymmetric cryptography","text":"<p>Asymmetric public-key encryption, often referred to as asymmetric cryptography or public-key cryptography, is a cryptographic system that uses pairs of keys: public keys and private keys. Unlike symmetric encryption, where the same key is used for both encryption and decryption, asymmetric encryption uses different keys for these operations.</p> <p>Each user generates a pair of keys: a public key and a private key, The public key is made available to anyone who wishes to send an encrypted message to the user. The private key is kept secret and known only to the user.</p> <p>Encryption</p> <ul> <li>When User A wants to send a secure message to User B, User A uses User B's public key to encrypt the message.</li> <li>The encryption process uses complex mathematical algorithms that are easy to perform in one direction (using the public key) but computationally difficult to reverse without the corresponding private key.</li> </ul> <p>Decryption</p> <ul> <li>User B receives the encrypted message and uses their private key to decrypt it.</li> <li>Since the private key is kept secret, only User B can decrypt the message.</li> </ul> <p>Because of its advantages that it offers, it can be used in variery of protocols like PGP, SSH, Bitcoin, TLS, S/MIME</p>"},{"location":"nginx/cryptography/#https","title":"https","text":""},{"location":"nginx/cryptography/#handshake","title":"handshake","text":"<ul> <li>When a user's browser initiates a connection to a website over HTTPS, a process called the SSL/TLS handshake begins.</li> <li>The browser requests a secure connection to the server, and the server responds by sending its SSL/TLS certificate to the browser.</li> <li>The certificate contains the server's public key and information about the website, including the domain name and the certificate authority (CA) that issued the certificate.</li> <li>The browser verifies the certificate to ensure it's valid and trusted. This verification involves checking the certificate's digital signature against a list of trusted CAs stored in the browser, ensuring the certificate hasn't expired, and confirming that the domain name matches the one the user is trying to connect to.</li> </ul>"},{"location":"nginx/cryptography/#key-exchange","title":"key exchange","text":"<ul> <li>After verifying the certificate, the browser generates a session key, which is a randomly generated symmetric encryption key.</li> <li>The browser encrypts the session key with the server's public key from the certificate and sends it to the server.</li> <li>The server decrypts the session key using its private key, establishing a secure connection.</li> </ul>"},{"location":"nginx/cryptography/#data-transfer","title":"data transfer","text":"<ul> <li>Once the secure connection is established, data transferred between the browser and the server is encrypted using symmetric encryption with the session key.</li> <li>This encryption ensures that even if an attacker intercepts the data, they won't be able to decipher it without the session key.</li> </ul>"},{"location":"nginx/cryptography/#install-certs","title":"install certs","text":"<pre><code>yum install certbot-nginx\ncertbot --nginx -d yourdomain.com \n\n# configure your configurations\n# will add ssl_certs in the nginx.conf..\n\nnginx -t \nsystemctl restart nginx\n</code></pre>"},{"location":"nginx/cryptography/#certs-revokcations","title":"certs revokcations","text":""},{"location":"nginx/cryptography/#crl","title":"CRL","text":"<p>Certificate Revocation List(CRL) is a method used by CAs to maintain a list of revoked digital certificates.</p> <p>working</p> <ul> <li>The CA periodically publishes a CRL, which contains the serial numbers of all certificates that have been - revoked before their expiration date.</li> <li>When a user wants to verify the validity of a certificate, they can check the CRL to see if the certificate's serial number is listed as revoked.</li> <li>CRLs are typically distributed and accessed via HTTP, LDAP, or other protocols.</li> </ul> <p>drawbacks</p> <ul> <li>CRLs can become large and cumbersome to manage, especially for CAs with a large number of certificates.</li> <li>There can be delays between when a certificate is revoked and when it appears on the CRL, leaving a window of vulnerability.</li> <li>Frequent downloads of large CRLs can lead to network congestion and performance issues.</li> </ul>"},{"location":"nginx/cryptography/#ocsp","title":"OCSP","text":"<p>Online Certificate Status Protocol(OCSP) provides a real-time method for checking the status of a digital certificate.</p> <p>working</p> <ul> <li>When a user wants to verify a certificate, their system sends a request to the CA's OCSP responder, providing  the certificate's serial number.</li> <li>The OCSP responder checks its records to see if the certificate is still valid or has been revoked.</li> <li>The responder then sends a response back to the user's system indicating the current status of the certificate (e.g., valid, revoked, unknown).</li> <li>OCSP provides real-time validation, reducing the window of vulnerability compared to CRLs.</li> <li>It can be more efficient than downloading and parsing large CRLs, especially for individual certificate checks.</li> </ul> <p>drawbacks</p> <ul> <li>OCSP requests can introduce additional latency into the certificate validation process, especially if the OCSP responder is slow to respond.</li> <li>OCSP relies on the availability and reliability of the CA's OCSP responder. If the responder is unavailable, validation may fail.</li> <li>OCSP requests can also leak information about which certificates a user is accessing, potentially compromising privacy.</li> </ul>"},{"location":"nginx/http_compression/","title":"http compression","text":""},{"location":"nginx/http_compression/#overview","title":"Overview","text":"<p>When the data is transferred from the server to client, if there is more data, then we would have more packets transferred after calculating the MTU size. </p> <p>So if there is compression from the server, then we would have less packats to reach to the client.  This is called as compressing.</p> <p>Client</p> <p>After the handshake, the client requests(GET /) from the server, in which the client would have an header set to the server i.e <code>Accept-Encoding: gzip, deflate, compress</code> </p> <p>Server</p> <p>Server already knows about the data header, so it knows that the client can decode so while it sends the data back to the client it would use a header <code>Content-Encoding: gzip, defalte, compress</code> . </p> <p>Once the data is received to client, it would use its <code>gzip, defalte, compress</code> decoder to decode the data.  There by, you would have less number of packets between server-client</p> <p>Note:</p> <ul> <li>When client has not set any header, which means it would accept all types of decoding mechanism</li> <li>although, the server sends data back to the client in compressed way, the <code>Context-Type: text/plain</code> always remains plain text.</li> </ul>"},{"location":"nginx/http_compression/#configure","title":"configure","text":"<pre><code>vim /etc/nginx/conf.d/nginx.conf\nhttp { \n    &lt;snip&gt;\n    gzip on;\n    gzip_types text/plain text/css test/xml text/javascript;\n    gzip_disable \"MSIE [1-6]\\.*;\n    gzip_comp_level 9;\n}\n\nservice nginx restart\n</code></pre> <p>testing</p> <pre><code>curl http://&lt;your-web-server&gt;/&lt;somefile&gt; &gt; c1.txt\ncurl -H \"Accept-Encoding:gzip\" http://&lt;your-web-server&gt;/&lt;somefile&gt; &gt; c2.txt\n\nls -l *.txt\n</code></pre> <p>Check their sizes, you would have found differences.</p>"},{"location":"nginx/http_compression/#referer","title":"referer","text":"<p>When there is the need for your context images not be copied due to copyrights or etc, you can configure the referer field so that imags don't get loaded. </p> <p>Usecase</p> <p>When you like someone's website or blog, you tend to copy and write on your own, so in that case you can restrict your nginx server as to not to load any websites/url's accessing from it..</p> <pre><code>vim /etc/nginx/conf.d/nginx\n\nserver {\n    location ~ \\.(jpe?g|png|gif)$ {\n        valid_referes none blocked servera.com *.serverb.com;\n        if ($invalid_referer) {\n            return 403;\n        }\n    }\n}\n\nsystem restart nginx\n</code></pre>"},{"location":"nginx/http_compression/#accept-language","title":"accept-language","text":"<p>Client can send the request in the preferred lanuguare by setting the header, so that we could get the resopinse in they way browser is set to.</p> <pre><code>curl -H \"Accept-Language: en\" &lt;your-webserver.com/index.html&gt;\ncurl -H \"Accept-Language: jp\" &lt;your-webserver.com/index.html&gt;\n</code></pre>"},{"location":"nginx/http_protocol/","title":"http protocol","text":""},{"location":"nginx/http_protocol/#protocols-introduction","title":"protocols introduction","text":"<p>system of rules that allow two or more entities of a communications system to transmit information.</p> <ul> <li>File Transfer Protocol (FTP)</li> <li>Domain Name System Protocol (DNS)</li> <li>Transmission Control Protocol (TCP)</li> <li>Secure File Transfer Protocol (SFTP)</li> <li>Hyper Text Transfer Protocol  (HTTP)</li> <li>Internet Protocol (IP)</li> </ul>"},{"location":"nginx/http_protocol/#http-protocol","title":"http protocol","text":"<p>HTTP is a TCP/IP-based communication protocol, that is used to deliver data (HTML files, image files, query results, etc.) on the World Wide Web. The default port used for HTTP is TCP 80, but other ports can be used as well based on the requirements.</p> <p></p>"},{"location":"nginx/http_protocol/#http-get","title":"HTTP GET","text":"<p>try to connect to the telnet port and check if the server is responding to the request</p> <pre><code>[root@centos ~]# telnet 192.168.56.10 80\nTrying 192.168.56.10...\nConnected to 192.168.56.10.\nEscape character is '^]'.\nGET /sample.html HTTP/1.1  -&gt; you have to type here\nHost: 192.168.56.10 -&gt; you have to type here [ press enter two times to get the response]\n\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:07:45 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n\n&lt;h1&gt; nginx tutorial &lt;/h1&gt;\n&lt;p&gt; line1 &lt;/p&gt;\n&lt;p&gt; line2 &lt;/p&gt;\n</code></pre>"},{"location":"nginx/http_protocol/#partial-get","title":"partial GET","text":"<p>The partial GET method is used to retrieve only specific content instead of everything</p> <pre><code># returns 20 bytes from the webserver\n\n[root@centos ~]# curl --header \"Range: bytes=0-20\" http://192.168.56.10/sample.html\n&lt;h1&gt; nginx tutorial &lt;[root@centos ~]#\n\n# returns complete data from the webserver as response\n\n[root@centos ~]# curl -I http://192.168.56.10/sample.html\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:12:52 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"nginx/http_protocol/#conditional-get","title":"conditional GET","text":"<p>The conditional GET method is used to fetch the information with a condition.</p> <pre><code>[root@centos ~]# curl --header \"If-Modified-Since: Tue, 26 Mar 2024 12:57:00 GMT\" http://192.168.56.10/sample.html\n[root@centos ~]#\n\n[root@centos ~]# curl -I --header \"If-Modified-Since: Tue, 26 Mar 2024 12:57:00 GMT\" http://192.168.56.10/sample.html\nHTTP/1.1 304 Not Modified\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:23:19 GMT\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\n[root@centos ~]#\n\n[root@centos ~]# curl --header \"If-Modified-Since: Tue, 26 Mar 2024 12:50:00 GMT\" http://192.168.56.10/sample.html\n&lt;h1&gt; nginx tutorial &lt;/h1&gt;\n&lt;p&gt; line1 &lt;/p&gt;\n&lt;p&gt; line2 &lt;/p&gt;\n[root@centos ~]#\n</code></pre>"},{"location":"nginx/http_protocol/#http-post","title":"HTTP POST","text":"<p>POST method is used to send some information which will be processed by the web-server in some way.</p> <pre><code>POST /login.php HTTP/1.1  \n   user=admin password=test123\n</code></pre>"},{"location":"nginx/http_protocol/#http-head","title":"HTTP HEAD","text":"<p>HEAD method is used to fetch only the HTTP headers as part of the response.</p> <p>HEAD method is identical to GET method, except that the server MUST NOT return a message-body in the response</p> <pre><code>[root@centos ~]# curl -I http://192.168.56.10/sample.html\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:12:52 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"nginx/http_protocol/#http-trace","title":"HTTP TRACE","text":"<p>'TRACE' is a HTTP request method used for debugging which echo's back input back to the user.</p>"},{"location":"nginx/http_protocol/#http-options","title":"HTTP OPTIONS","text":"<p>OPTION  method is used to describe the communication option for the target resource. nginx doesn't support as security concern.</p> <pre><code>[root@centos ~]# curl -X \"OPTIONS\" http://192.168.56.10 -i\nHTTP/1.1 405 Not Allowed\nServer: nginx/1.20.1\nDate: Wed, 27 Mar 2024 02:31:35 GMT\nContent-Type: text/html\nContent-Length: 157\nConnection: keep-alive\n\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;405 Not Allowed&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;405 Not Allowed&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx/1.20.1&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n[root@centos ~]#\n</code></pre>"},{"location":"nginx/http_protocol/#conclusion","title":"conclusion","text":"<p>HTTP defines a set of request methods to indicate the desired action to be performed for a given resource</p> HTTP Method Description GET To retrieve data from the server. POST Send input data to the server. HEAD Exactly like GET, but server only responds with Headers. PUT Write documents to the server. DELETE Deletes resource from the server. OPTIONS Asks server on which methods it supports. TRACE ECHOS the Receive Request from the Web Server <p>HTTP Response Status Code</p> Status Code Description 100 Continue 101 Switching Protocols 200 OK 201 Created 202 Accepted 204 No Content 300 Multiple Choices 301 Moved Permanently 302 Found 304 Not Modified 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 500 Internal Server Error 501 Not Implemented 502 Bad Gateway 503 Service Unavailable"},{"location":"nginx/load_balancer/","title":"load balancer","text":""},{"location":"nginx/load_balancer/#overview","title":"Overview","text":"<p>LB: create set of multiple servers to distribute the traffic betweeen servers</p> <p>advantages of LB</p> <ul> <li>traffic distribution using multiple algorithms to backend servers</li> <li>health check of backend application</li> <li>supprts SSL/TLS terminations</li> </ul>"},{"location":"nginx/load_balancer/#implementation","title":"implementation","text":""},{"location":"nginx/load_balancer/#load-balancing","title":"load balancing","text":"<p>upstream</p> <p>upstream block can be used to specfiy the group of serves for which you want to load balanced traffic </p> <p>nginx server</p> <pre><code>[root@centos conf.d]# cat proxy.conf.backend\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://192.168.56.11;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host-Header $host;\n    }\n\n    location /admin {\n        proxy_pass http://192.168.56.12;\n        proxy_set_header X-Real-IP $remote_addr;\n      }\n}\n[root@centos conf.d]#\n</code></pre> <p>just hitting the nginx load balancer, it would be redirected to two of the backend servers.</p> <pre><code>[root@centos conf.d]# curl 192.168.56.11\nThis is application server backend\n[root@centos conf.d]# curl 192.168.56.12\nthis is backend server\n[root@centos conf.d]#\n[root@centos conf.d]# for i in `seq 1 7`\n&gt; do\n&gt; curl http://192.168.56.10\n&gt; done\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/load_balancer/#health-checks","title":"health checks","text":"<p>health checks are used to monitor the health of HTTP servers in upstream group i.e <code>backend</code>  if any server is not responding then nginx will stop sending the request to it. </p> <p>let's say one of the backend server has stopped so nginx would route to the healthy node. </p> <pre><code>[root@centos html]# systemctl stop nginx\n[root@centos html]# systemctl status nginx\n\u25cf nginx.service - nginx - high performance web server\n   Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled)\n   Active: inactive (dead) since Fri 2024-03-29 07:49:31 UTC; 5s ago\n     Docs: http://nginx.org/en/docs/\n  Process: 20660 ExecStop=/bin/sh -c /bin/kill -s TERM $(/bin/cat /var/run/nginx.pid) (code=exited, status=0/SUCCESS)\n  Process: 607 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS)\n Main PID: 614 (code=exited, status=0/SUCCESS)\n\nMar 27 12:36:31 centos systemd[1]: Starting nginx - hig...\nMar 27 12:36:32 centos systemd[1]: Can't open PID file ...\nMar 27 12:36:32 centos systemd[1]: Started nginx - high...\nMar 29 07:49:31 centos systemd[1]: Stopping nginx - hig...\nMar 29 07:49:31 centos systemd[1]: Stopped nginx - high...\nHint: Some lines were ellipsized, use -l to show in full.\n[root@centos html]#\n\n</code></pre> <p>load balancer server</p> <pre><code>[root@centos conf.d]# curl http://192.168.56.12\ncurl: (7) Failed connect to 192.168.56.12:80; Connection refused\n[root@centos conf.d]#\n</code></pre> <p>since one of the server is down, nginx load balancer would route the entry to healthy node itself.  this is as part of passive health checks</p> <pre><code>[root@centos conf.d]# for i in `seq 1 7`; do curl http://192.168.56.10; done\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\n[root@centos conf.d]#\n</code></pre> <p>One the node is up, traffic is routed across.</p> <pre><code>[root@centos html]# systemctl start nginx\n[root@centos html]# systemctl status nginx\n\u25cf nginx.service - nginx - high performance web server\n   Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled)\n   Active: active (running) since Fri 2024-03-29 07:52:10 UTC; 13s ago\n     Docs: http://nginx.org/en/docs/\n  Process: 20660 ExecStop=/bin/sh -c /bin/kill -s TERM $(/bin/cat /var/run/nginx.pid) (code=exited, status=0/SUCCESS)\n  Process: 20673 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS)\n Main PID: 20674 (nginx)\n   CGroup: /system.slice/nginx.service\n           \u251c\u250020674 nginx: master process /usr/sbin/ngin...\n           \u2514\u250020675 nginx: worker process\n\nMar 29 07:52:10 centos systemd[1]: Starting nginx - hig...\nMar 29 07:52:10 centos systemd[1]: Can't open PID file ...\nMar 29 07:52:10 centos systemd[1]: Started nginx - high...\nHint: Some lines were ellipsized, use -l to show in full.\n[root@centos html]#\n</code></pre> <pre><code>[root@centos conf.d]# for i in `seq 1 7`; do curl http://192.168.56.10; done\nThis is application server backend\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/load_balancer/#active-and-passive","title":"active and passive","text":"<p>active - only available in nginx plus, which is paid. In this check, the health check of an upstream server by sending special health check requests(GET /) to each server and check for the response. </p> <p>if there are any response of 2xx or 3xx then its considered and unhealthy and marked accordingly.</p> <p>passive - nginx monitors the communication between client and the upstream server.  if upstream server is not responding or rejecting connections, the passive health check with consider the serevr unhealthy and marked accordingly. </p>"},{"location":"nginx/load_balancer/#passive-healthcheck-parameters","title":"passive healthcheck parameters","text":"<p>you can configure passive checks in the nginx configs.. </p> <p>max_fails - set the number of failed attempts that must occur during the fail_timeout period for the server to be marked unavailable.</p> <p>fail_timeout - set the time during which number of failed attempts must happen for the server to be marked unavailable and also the time for which the server is marked unavailable.</p> <pre><code>[root@centos conf.d]# cat load-balancer.conf\nupstream backend {\n  server 192.168.56.11 max_fails=2 fail_timeout=30s;\n  server 192.168.56.12 max_fails=2 fail_timeout=30s;\n}\n\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://backend;\n }\n}\n[root@centos conf.d]#systemctl restart nginx\n</code></pre> <p>until the 30s for 2 attemps, nginx won't send any requests to that server.. you can configure it accordingly.  you can use the tool called 'ab' which is <code>httpd-tools</code> to send multiple requests to nginx. </p> <pre><code>ab -n 1000 localhost/ # send 1000 requests to the nginx proxy.\n</code></pre>"},{"location":"nginx/load_balancer/#server-weights","title":"server weights","text":"<p>allows us to customizd the request flow betwen nginx acting as LB to the upstream backends... you can send 80% of traffic to one server and 20% to another server. </p> <pre><code>upstream backend {\n  server 192.168.56.11;\n  server 192.168.56.12 weight=2;\n}\n</code></pre>"},{"location":"nginx/logging/","title":"logging","text":""},{"location":"nginx/logging/#access-logs","title":"access logs","text":"<p>what we could determine from the nginx access logs ..</p> <ul> <li>ip address of the requester</li> <li>date and time</li> <li>type of request (GET, POST, PUT ..etc)</li> <li>path to which request was asked</li> <li>response of the request </li> <li>browser name from which request was sent.</li> </ul> <p>logger file from the config goes below </p> <pre><code>log_format main \"\" ... \n</code></pre>"},{"location":"nginx/logging/#configure-custom-logs","title":"configure custom logs","text":"<p>vim /etc/nginx/conf.d/virualhost.conf</p> <pre><code>access_log /var/log/nginx/example.log main;\n</code></pre> <p>ls /var/log/nginx/example.log would have your log in the defined format.</p>"},{"location":"nginx/logging/#logging","title":"logging","text":"<p>logging levels</p> <p>no custom error logs can be configured</p> <p>emerg, alert, crit, error, warn, notice, info, debug </p> <pre><code>vim /etc/nginx/conf.d/nginx.conf\n\nerror_log /var/log/nginx.log emerg\nerror_log /var/log/example.log crit\nerror_log /var/log/domain_info.log info\n</code></pre>"},{"location":"nginx/nginx_overview/","title":"nginx overview","text":""},{"location":"nginx/nginx_overview/#nginx-architecture","title":"Nginx architecture","text":"<p>Configuration file: /etc/nginx/nginx.conf</p> <p>User: nginx</p> <p>Log: /var/log/nginx</p> <p>rpm: nginx-1.20.1-1.el7.ngx.x86_64.rpm</p> <p>OS: CentOS7</p> <pre><code>[root@centos ~]# ps -ef | grep nginx | grep -v grep\nroot       623     1  0 Mar26 ?        00:00:00 nginx: master process /usr/sbin/nginx\nnginx      627   623  0 Mar26 ?        00:00:00 nginx: worker process\n[root@centos ~]#\n</code></pre> <p>master process read and evaluate config file and maintain worker process. worker process can be 1 or more.</p> <p>worker process they do the actual processing.</p> <p><code>worker_processes</code> in the nginx config file, it would auto detect the number of CPU and then will launch the worker process accordingly. you have <code>error_log</code> and <code>pid</code> in the config file which is self explanatory. </p> <p>we can also include other modules instead of any modification in the main file.  <code>include /usr/share/nginx/modules/*.conf;</code>. To read more, please check the configuration file.</p> <p><code>worker_connections</code> sets max number of simultaneous connections that can be opened by a worker process. </p>"},{"location":"nginx/nginx_overview/#contexts","title":"contexts","text":"<p>nginx config file is divided across rage of contexts(sections) each context contains its own set of directives to control specific aspect of nginx ., i.e </p> <p>Any directive that exists entirely outside of the context is said to inhabit  the \"main\" context Main context is used to configure details that effect the entire application on a basic level - main - events - http - mail</p> <pre><code>cat /etc/nginx/nginx.conf\n&lt;snip&gt;\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main\n}\n&lt;snip&gt;\n</code></pre>"},{"location":"nginx/nginx_overview/#http","title":"http","text":"<p>contails all of the directives and other contexts necessary to define how to handle HTTP/HTTPS connections and associated parameters.</p> <pre><code>http {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n</code></pre>"},{"location":"nginx/nginx_overview/#custom-config","title":"custom config","text":"<p>You can create your custom config file using below file. you can change the <code>location</code> to your desired one for youe pplciation. </p> <pre><code>[root@centos conf.d]# pwd\n/etc/nginx/conf.d\n[root@centos conf.d]# cat default.conf  | grep -v '#'\nserver {\n    listen       80;\n    server_name  localhost;\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm; \n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n}\n</code></pre>"},{"location":"nginx/nginx_overview/#configure-multiple-websites-or-domains","title":"Configure multiple websites or domains","text":"<pre><code>[root@centos conf.d]# cat /etc/nginx/conf.d/dexter.conf\nserver {\n    listen       8080;\n    server_name  localhost;\n\n    access_log  /var/log/nginx/dexter.access.log  main;\n\n    location / {\n        root   /usr/share/nginx/html/dexter;\n        index  index.html;\n    }\n}\n[root@centos conf.d]# systemctl restart nginx\n[root@centos conf.d]# curl http://192.168.56.10:8080/index.html\nthis is an multiple site configured at nginx for domain dexter\n[root@centos conf.d]#\n\n\n[root@centos conf.d]# ls /var/log/nginx/dexter.access.log\n/var/log/nginx/dexter.access.log\n[root@centos conf.d]# cat /var/log/nginx/dexter.access.log\n192.168.56.1 - - [27/Mar/2024:10:45:26 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\" \"-\"\n192.168.56.10 - - [27/Mar/2024:10:45:40 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"curl/7.29.0\" \"-\"\n192.168.56.10 - - [27/Mar/2024:10:46:27 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"curl/7.29.0\" \"-\"\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/nginx_overview/#nginx-cli","title":"nginx cli","text":"<pre><code>nginx -v # version\nnginx -V # more detail\nnginx -t # check syntax\nnginx -c new_nginx.conf # testing purpose\nnginx -h # help menu\n</code></pre>"},{"location":"nginx/nginx_overview/#modular-architecture","title":"modular architecture","text":"<p>referes to any system composed of seperate componets that can be connected together i.e its a collection of modules, we can also extend the functionality by adding 3rd party modules.</p>"},{"location":"nginx/nginx_overview/#static-modules","title":"static modules","text":"<ul> <li>Modules that are compiled into nginx server binary at compile time</li> <li>Single package, portable and works out of box</li> <li>Creates issue when one of the module has bug, hence difficult to troubleshoot</li> </ul>"},{"location":"nginx/nginx_overview/#dynamic-modules","title":"dynamic modules","text":"<ul> <li>Create/download dynamic module files.</li> <li>Reference the path of the module with the <code>load_module</code> directive.</li> </ul>"},{"location":"nginx/nginx_overview/#install-using-source-module","title":"install using source module","text":"<pre><code>http://nginx.org/en/download.html\n\nwget http://nginx.org/download/nginx-1.16.0.tar.gz\ntar -xzvf nginx-1.16.0.tar.gz\nyum -y install gcc make zlib-devel pcre-devel openssl-devel wget nano\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --pid-path=/var/run/nginx.pid --lock-path=/var/lock/subsys/nginx --user=nginx --group=nginx --with-http_mp4_module --add-module=../nginx-hello-world-module\n\nuseradd Nginx\nmkdir -p /var/lib/nginx/tmp/\nchown -R nginx.nginx /var/lib/nginx/tmp/\n\nSystemD file\n\nhttps://www.nginx.com/resources/wiki/start/topics/examples/systemd/\n</code></pre>"},{"location":"nginx/nginx_overview/#build-dynamic-module","title":"build dynamic module","text":"<pre><code>yum -y install git\ngit clone https://github.com/perusio/nginx-hello-world-module\n./configure --add-dynamic-module=../nginx-hello-world-module\n\nvim /etc/nginx/conf.d/nginx.conf\nload_module /etc/nginx/modules/something.so # it should be in global section\n\nserver {\n    listen 8080;\n\n    location = /test {\n        hello_world;\n    }\n}\n\ncurl -i http://example.com/test #this will be loaded from the dynamic module\n</code></pre> <p>Reference: https://github.com/perusio/nginx-hello-world-module</p>"},{"location":"nginx/nginx_overview/#build-static-module","title":"build static module","text":"<p>You don't have to use <code>load_module</code> directive, its same as the src compilation but you would add the static compile method <code>--add-module</code> as referenced here https://github.com/perusio/nginx-hello-world-module</p> <pre><code>./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --pid-path=/var/run/nginx.pid --lock-path=/var/lock/subsys/nginx --user=nginx --group=nginx --with-http_mp4_module --add-module=../nginx-hello-world-module\n\nmake\nmake install\n\n\nserver {\nlisten 8080;\n\nlocation / {\n     hello_world;\n  }\n}\n\n\nsystemctl restart nginx\ncurl localhost:8080\n</code></pre>"},{"location":"nginx/nginx_overview/#web-application-firewallwaf","title":"web application firewall(waf)","text":"<p>A Web Application Firewall (WAF) is a security solution designed to protect web applications by monitoring, filtering, and potentially blocking HTTP traffic between a web application and the Internet. WAFs are deployed to provide an additional layer of defense, thereby protecting against data breaches, unauthorized access, and service disruption</p> <ul> <li>SQL Injection (SQLi)</li> <li>Cross-Site Scripting (XSS)</li> <li>Cross-Site Request Forgery (CSRF)</li> <li>File Inclusion</li> <li>Directory Traversal</li> <li>Brute Force Attacks</li> <li>Denial-of-Service (DoS) </li> <li>Distributed Denial-of-Service(DDoS)</li> </ul> <p>Signature-based Detection: WAFs use predefined signatures or patterns to identify known attacks and malicious traffic.</p> <p>Behavioral Analysis: Some advanced WAFs employ machine learning or heuristic algorithms to analyze traffic patterns and detect anomalies indicative of attack attempts.</p> <p>Request Inspection: WAFs inspect HTTP requests and responses, analyzing parameters, headers, payloads, and other attributes to identify suspicious activity.</p> <p>Traffic Filtering: WAFs can filter and block traffic based on predefined rules, such as IP addresses, user agents, request methods, or payloads.</p> <p>Protocol Validation: WAFs validate incoming requests against known HTTP standards and application-specific protocols to detect and block malformed or malicious requests.</p> <p>Logging and Reporting: WAFs provide logs and reports detailing detected threats, blocked requests, and other security events for analysis and investigation.</p>"},{"location":"nginx/reverse_proxy/","title":"reverse proxy","text":"<p>reverse proxy is type of proxy server which retrives resources onbehalf of the coient from one or more servers</p> <p></p> <p>use cases:</p> <ul> <li>it hides existance of the original backend servers</li> <li>can protect tthe backend server from webbased attacks, DOS etc </li> <li>can provide great caching functionality</li> <li>can optimize the content by compressing it </li> <li>can act as SSL terminating proxy</li> <li>request routing.. etc </li> </ul>"},{"location":"nginx/reverse_proxy/#reverse-proxy-setup","title":"Reverse Proxy Setup","text":"<p>proxy_pass</p> <p>directive forwards the request to the proxied servers specified along with the directive. </p> <p>1st - Nginx Reverse Proxy(192.168.56.10) 2nd - Application Server(192.168.56.11) 3rd - Authentication Server(192.168.56.12)</p> <p>BaseConfigurations for all 3 servers</p> <p>yum install -y wget net-tools wget https://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.20.1-1.el7.ngx.x86_64.rpm yum -y install nginx-1.20.1-1.el7.ngx.x86_64.rpm systemctl start nginx systemctl enable nginx setenforce 0 reboot</p> <p>application server: cd /usr/share/nginx/html echo \"This is application server backend\" &gt; index.html</p> <p>auth server: mkdir /usr/share/nginx/html/admin echo \"This is auth server file under admin\" &gt; /usr/share/nginx/html/admin/index.html</p> <p>Proxy server:</p> <p>cd /etc/nginx/conf.d nano proxy.conf server {     listen       80;     server_name  localhost;</p> <pre><code>location / {\n    proxy_pass http://192.168.56.11;\n}\n\nlocation /admin {\n    proxy_pass http://192.168.56.12;\n  }\n</code></pre> <p>} nginx -t systemctl restart nginx</p> <p>You can now see if the request is made from the client, it would first get to the nginx server and then would reverse proxy the response from the application or the auth server to the client. These are being logged into the application or auth nginx logs..</p> <p></p>"},{"location":"nginx/reverse_proxy/#x-real-ip","title":"X-Real-IP","text":"<p>Problem statement</p> <p>original webservers would require an originating ip address to send response</p> <p>When the client sends a request to the web server through the reverse proxy, its always as usual that the backend web application would be only knowing the reverse proxy instead of original client server. sometime in the production environment the application servers would require to bind the originating ip address to send the respose. so in that case we need to use x-real-ip so that the reverse proxy would send the ip address of the client to the application server. </p> <p>Configuration</p> <p>Reverse Proxy Side</p> <pre><code>vim /etc/nginx/conf.d/proxy.conf\nproxy_set_header X-Real-IP $remote_addr;\n</code></pre> <p>Backend Server Side</p> <pre><code>vim /etc/nginx/nginx.conf\n\n# append $http_x_real_ip at the end of the line.\nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\" **\"$http_x_real_ip\"**';\n\nnginx -t\nsystemctl restart nginx                    \n</code></pre> <p>so now, it would log the remote(original) ip address to the application server along with the proxy.</p>"},{"location":"nginx/reverse_proxy/#proxy-host-header","title":"proxy host header","text":"<p>problem statement</p> <p>Host header that is received at the reverse proxy level is not forwarded to backend server. </p> <p>if there are multiple websites hosted on the application server and your client sends the GET request along with the headers, the nginx proxy won't send the headers to the multiple hosted web application and hence you won't be able to get the response. </p> <p>Reverse Proxy Level</p> <pre><code>[root@centos ~]# cat /etc/nginx/conf.d/proxy.conf\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://192.168.56.11;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host-Header $host;\n    }\n\n    location /admin {\n        proxy_pass http://192.168.56.12;\n        proxy_set_header X-Real-IP $remote_addr;\n      }\n}\n[root@centos ~]#systemctl restart nginx\n\n[root@centos ~]# curl localhost\nThis is application server backend\n[root@centos ~]#\n</code></pre> <p>Backend Server Level</p> <pre><code>yum -y install tcpdump\ntcpdump -A -vvvv -s 9999 -i eth1 port 80 &gt; /tmp/headers\n\ncat /tmp/headers\n\nGET / HTTP/1.0\nX-Real-IP: 127.0.0.1  \nHost-Header: localhost -&gt; headers are passed from reverse proxy to backend\nHost: 192.168.56.11\nConnection: close\nUser-Agent: curl/7.29.0\nAccept: */*\n</code></pre>"},{"location":"nginx/static_assets/","title":"static assets","text":"<p>when we make a request from client, it would query the nginx server and then contact the upstream server to get the response to the client. so when we want to load the index.html, all the static content has to be served from the upstream server(.png, .js, .css..etc) to get the response. </p> <p>Incase if we have configured static content to be served from the nginx reverse proxy, then only index.html would be loaded from the upstream server, there by providing better response time, performance on the upstream would be better.</p> <p>try to load the application and check for the nginx access logs, you can find many of the GET requests to upstream server to server the static content</p>"},{"location":"nginx/static_assets/#configure","title":"configure","text":"<pre><code>vim /etc/nginx/conf.d/proxy.conf\n\nserver {\n    server_name yourweb.in\n\n    location / {\n        proxy_pass http://192.168.56.10;\n        proxy_set_header Host $host;\n    }\n\n    # configue your static assets to load from the nginx reverse proxy\n\n    location ~* \\.(css|js|jp?g|JPG|png|PNG) {\n        root /var/www/assets;\n        try_files $uri $uri/;\n    }\n\n}\nngix -t\nsystemctl restart nginx\n</code></pre> <p>on your application server, </p> <pre><code>scp -r /var/www/html/js &lt;nginxsverer&gt;@:/var/www/assets/\nscp -r /var/www/html/css &lt;nginxsverer&gt;@:/var/www/assets/\nscp -r /var/www/html/master &lt;nginxsverer&gt;@:/var/www/assets/\n</code></pre> <p>open your application /var/log/nginx/access.log and try to load the webpage , you could see the minimization of the GET requests from the upstream server.</p>"},{"location":"nginx/webserver_overview/","title":"web server","text":""},{"location":"nginx/webserver_overview/#introduction","title":"introduction","text":"<p>webserver is a program that uses HTTP to serve the web pages to users in response to their request.  e.g Apache, nginx, IIS</p> <p>How web server works?</p> <p></p> <p>Client Request: When a user wants to access a web page or resource, they send a request from their web browser to the web server. This request includes the URL (Uniform Resource Locator) of the desired resource.</p> <p>Routing and Processing: The web server receives the request and determines which resource the client is asking for based on the URL provided. It then processes the request, which may involve executing scripts, querying databases, or accessing files.</p> <p>Resource Retrieval: If the requested resource is a static file (such as an HTML, CSS, or image file), the web server retrieves it directly from its file system and sends it back to the client. If the resource is dynamic (generated by a script or retrieved from a database), the web server executes the necessary code to generate the content.</p> <p>Response Generation: Once the requested resource has been processed or retrieved, the web server generates an HTTP (Hypertext Transfer Protocol) response. This response includes headers with metadata about the resource and a body containing the actual content.</p> <p>Sending Response: The web server sends the HTTP response back to the client over the internet.</p> <p>Client Rendering: The client's web browser receives the response and renders the content to display it to the user. This may involve parsing HTML, applying CSS styles, executing JavaScript, and rendering images.</p> <p>nginx is more than a webserver and it can accomplish like</p> <ul> <li>reverse proxy</li> <li>load balancing</li> <li>HTTP caching</li> </ul>"},{"location":"nginx/webserver_overview/#installation","title":"installation","text":"<p>CentOS 7/8</p> <pre><code>yum install epel-release -y\nyum install nginx -y\nsystemctl start nginx\nsystemctl enable nginx\nnginx -V\n</code></pre>"},{"location":"programming/clean_code/control_structure/","title":"control structures","text":"<p>Keep your control structures clean</p> <ul> <li>avoid deep nesting.</li> <li>use factory functions and polymorphism</li> <li>prefer positive checks(e.g IsValid, IsEmpty..etc)</li> <li>Utilize errors</li> </ul>"},{"location":"programming/clean_code/control_structure/#guards","title":"Guards","text":"<p>Use guards and fail fast</p> <pre><code>if !(email.includes('@')):\n    return  # fail fast instead of deep nesting \n\n# if there remaining code, then you would execute from here\n</code></pre>"},{"location":"programming/clean_code/control_structure/#embrace-errors","title":"Embrace errors","text":"<p>Throwing + handling errors can replace if statements and lead to more focussed functions. If something is an error, make it error insterad of using if-else statements to fix it.. you can use try-catch to log error.</p>"},{"location":"programming/clean_code/methods/","title":"function","text":""},{"location":"programming/clean_code/methods/#minimize-paramters","title":"minimize paramters","text":"<p>when you are writing the function, you need to minimize the number of parameters.</p> <p>None parameter -&gt; best possible option </p> <p>1 parameters -&gt; very good possible option i.e log(message), sqr(x) ..etc </p> <p>2 parameters -&gt; use with caution e.g point(2,3), login(email, password) ..etc</p> <p>3 parameters -&gt; avoid if possible</p> <p>greater than 3 paramters -&gt; always avoid. </p> <p>let's say if you don't want to use any of the paramters, then you would create an blueprint of the class object and write methods into it. then you can simply call the function, without any object.. BTW, clean code means ..</p> <ol> <li>should be readable and meaningful</li> <li>should reduce cognitive load</li> <li>shoud be concise to the point</li> <li>should avoid unintuitive names, complex nesting and big blocks.</li> <li>should follow common best practices and patterns. </li> <li>should be fun to write and maintain code. </li> </ol> <p>if you have too many values, then try to add key:value pair in the functions and then use it. </p> <p>example</p> <pre><code>@dataclass\nclass Compare\n    x:int\n    y:int\n\n    def small(self):\n        if self.x &lt; self.y: return self.x\n        else: return self.y\n\n    def big(self):\n        if self.x &gt; self.y: return self.x\n        else: return self.y\n\noperators=Compare(x=10,y=12)\n\nprint(\"small\", operators.small())\nprint(\"big\", operators.big())\n</code></pre> <p>Functions should be small and should do exactly one thing. </p>"},{"location":"programming/clean_code/methods/#abstractions","title":"abstractions","text":"<p>There are two levels i.e </p> <p>high level  - functions which we write and there is no room for interpretation.  e.g isEmail(email), userExists(username) etc</p> <p>low level - these are low level API or built-in but the intrepation must be added..  e.g username.split(\"@\") list_of_user.append(username) .. etc</p> <p>Do not mix levels of abstrations, we must split the the functions if they are doing multiple works. </p> <p>so when do we need to split the functions ?</p> <ul> <li>Extract the code that works on the same functionality (do-not-repeat)</li> <li>Extract the code that requires more interpretation than the surrounding code</li> <li>Split functions reasonable</li> </ul> <p>How would you make decision and don't split  - you are jusy renaming the function  - finding the new function will take longer than reading the extracted code  - can't produce a reasonable name for the extracted function. </p> <p>Pure function</p> <p>the same input always yeilds same output. it means they are predictable.  no side effects.</p> <pre><code>def genId(username):\n    return f\"id-{username}\"\n</code></pre> <p>A function that just not only changes input/output but changes the overall system/program state. </p> <pre><code>def create_user(username):\n    newuser = CreateUser(username)\n\n    # inpure function\n    startsession(newuser) # side effect is not bad, but can be avoided\n    return newuser\n</code></pre>"},{"location":"programming/clean_code/naming/","title":"naming","text":""},{"location":"programming/clean_code/naming/#overview","title":"Overview","text":"<p>Naming (variables, properties, functions, methods, classes) correctly and in an understandable way if an extremely important part of writing clean code.</p> <p>Names have one simple purpose: They should describe what's stored in a variable or property or what a function or method does. Or what kind of object will be created when instantiating a class.</p> <p>Variables and properties hold data - numbers, text (strings), boolean values, objects, lists, arrays, maps etc. Hence the name should imply which kind of data is being stored. Therefore, variables and properties should typically receive a noun as a name. e.g: user, product, customer, database, transaction etc. </p> <p>Alternatively, you could also use a short phrase with an adjective - typically for storing boolean values. e.g: isValid, didAuthenticate, isLoggedIn, emailExists</p> <p>Functions &amp; Methods  - Functions and methods can be called to then execute some code. That means that they perform tasks and operations. Therefore, functions and methods should typically receive a verb as a name. e.g: login(), createUser(), database.insert(), log() etc.</p> <p>Alternatively, functions and methods can also be used to primarily produce values - then, especially when producing booleans, you could also go for short phrases with adjectives. e.g: isValid(...), isEmail(...), isEmpty(...) etc.</p> <p>Classes - Classes are used to create objects (unless it's a static class). Hence the class name should describe the kind of object it will create. Even if it's a static class (i.e. it won't be instantiated), you will still use it as some kind of container for various pieces of data and/ or functionality - so you should then describe that container. </p> <p>Good class names - just like good variable and property names - are therefore nouns. e.g: User, Product,RootAdministrator, Transaction, Payment etc</p>"},{"location":"programming/clean_code/naming/#name-casing","title":"Name Casing","text":"<p>snake_case - everything is lowercase and vars seperated by underscore, that includes functions and vars e.g python</p> <p>cameCase - There is no space and everystart of new char starts with capital letter, that includes functions, methods, and vars e.g Java, JavaScript</p> <p>PascalCase - Used many prg, used mainly in Classes e.g Python, Java, Javascript</p> <p>Kebab-case - mainly used in HTML prog lang</p>"},{"location":"programming/clean_code/ooo/","title":"oop principles","text":""},{"location":"programming/clean_code/ooo/#diff-bw-obj-ds","title":"Diff b/w obj &amp; ds","text":"<p>Object</p> <ul> <li>private internals/properties, public API</li> <li>Contain your business logic </li> <li>Abstractions over conceretions</li> </ul> <p>data structures</p> <ul> <li>public internals/properties, no API</li> <li>stores and transports data</li> <li>concertions only</li> </ul>"},{"location":"programming/clean_code/ooo/#object-and-polymorphism","title":"Object and polymorphism","text":"<p>The ability of an object to take many forms.</p> <p>Classes should be small, and it should have Single Responsibility.</p>"},{"location":"programming/clean_code/ooo/#cohestion","title":"Cohestion","text":"<p>how much are your class methods using the class properties. Goal: not all the methods uses the propeties, but you can make sure that most of the properties will be used by the methods. i.e they are highly cohesive.</p>"},{"location":"programming/clean_code/ooo/#law-of-demeter","title":"law of demeter","text":"<p>principles of least knowledge: Don't depend on the internals of \"strangers\"(other objects that you don't know directly.)</p> <p>Code in a methods may only access direct internals(properris and methods) of: - the object it belongs to - objects that are stored in properties of that object - objects which are received as methods parameters - objects which are created in the method</p> <p>always tell what to do, instead of asking.</p>"},{"location":"programming/clean_code/ooo/#solid","title":"SOLID","text":"<p>work on the solid principles desing for clean code.</p>"},{"location":"programming/clean_code/overview/","title":"overview","text":""},{"location":"programming/clean_code/overview/#clean-code","title":"clean code","text":"<p>It's code which is readable and understandable. Clean code should be concise and to the point therefore, and you'll, for example, wanna avoid unintutive names, complex nestings or big code blocks.</p> <p>You, can follow common best practices and patterns and also a bunch of concepts and rules, it should be fun to write and to maintain code and with clean code, you ensure that maintaining can be fun because your code can be understood by others.</p> <p>As a developer, you are the author of your code. And you wanna write it such that it's fun and easy to read and understand your code. That should be your goal</p>"},{"location":"programming/clean_code/overview/#key-pain-points","title":"Key pain points","text":"<ul> <li>names - vars, functions, classes</li> <li>structure &amp; comments - code formating, good and bad comments </li> <li>fucntions - length, parameters</li> <li>conditions and error handling - dee nesting, missing error handling</li> <li>class objects and data structures - missing distinction, bloated classes. </li> </ul> <p>solutions:</p> <ul> <li>Rules and concepts</li> <li>pattern &amp; principles</li> <li>TTD</li> </ul> <p>References: https://github.com/academind/clean-code-course-code/tree/general-resources</p>"},{"location":"programming/clean_code/structure/","title":"structure","text":"<p>We would discuss more about the code structure, formating and comments in this section. </p>"},{"location":"programming/clean_code/structure/#comments","title":"comments","text":""},{"location":"programming/clean_code/structure/#bad-comments","title":"Bad comments","text":"<ul> <li>avoid redunant comments. i.e your vars name and code would almost mean same thing</li> <li>avoid dividers or blockers i.e naming globals etc in \"****\" for better reading etc, if this is big then create a new file and import it.</li> <li>misleading comments i.e method is being done something but comment is something</li> <li>comented code i.e delete code incase not required</li> </ul>"},{"location":"programming/clean_code/structure/#good-comments","title":"Good comments","text":"<ul> <li>legal information i.e disclaimer etc </li> <li>explanation which can't be replaced by good namming i.e regular expression </li> <li>warining i.e works only in dev env or etc </li> <li>todo nodes i.e add unfinished code to be written </li> <li>docstring i.e writing for the API would make sense. </li> </ul>"},{"location":"programming/clean_code/structure/#code-formatting","title":"code formatting","text":"<p>code formating improves readibilty and transports meaning. this would be lang specific. always use specific conventions and guide lines while writing code. </p>"},{"location":"programming/clean_code/structure/#vertical-formating","title":"vertical formating","text":"<ul> <li>veritical space between lines</li> </ul> <p>code should be readable like an essay - top to bottom without too many jumps.  If the code is too big in the file, then try considering breaking code into different multiple files and classes. this makes code short and readable. </p> <p>Different concepts(\"areas\") should be seperated by spacing. i.e imports and function should be having a one blank line.  </p> <ul> <li>grouping of code i.e all the related concepts should be kept the same. if we are writing to save code, all the code related to save should be one below the block.</li> </ul>"},{"location":"programming/clean_code/structure/#horizontal-formating","title":"horizontal formating","text":"<p>lines of code should be readable without scrollng. avoid very long \"sentences\"</p> <ul> <li>indentation i.e use it even if its not required.</li> <li>line width  i.e break long line of code into shorter. </li> <li>space between code </li> <li>use clear variable name rather than long name</li> </ul>"},{"location":"programming/go/basics/","title":"Data Types in Golang","text":""},{"location":"programming/go/basics/#1-basic-types","title":"1. Basic Types","text":"<p>These are the fundamental data types in Go.</p> Type Description Example bool Boolean values (<code>true</code> or <code>false</code>) <code>var isActive bool = true</code> string Sequence of characters <code>var name string = \"Golang\"</code> int Signed integer (platform-dependent size) <code>var age int = 25</code> uint Unsigned integer (platform-dependent size) <code>var count uint = 10</code> float32 32-bit floating-point number <code>var pi float32 = 3.14</code> float64 64-bit floating-point number <code>var pi float64 = 3.14159265359</code> complex64 Complex number with float32 real/imag parts <code>var c complex64 = 1 + 2i</code> complex128 Complex number with float64 real/imag parts <code>var c complex128 = 2 + 3i</code>"},{"location":"programming/go/basics/#2-integer-types","title":"2. Integer Types","text":"<p>Go provides multiple integer types of different sizes.</p> Type Size Description Example int8 8 bits Signed integer (-128 to 127) <code>var a int8 = 127</code> int16 16 bits Signed integer (-32,768 to 32,767) <code>var b int16 = 32000</code> int32 32 bits Signed integer (-2^31 to 2^31-1) <code>var c int32 = 2147483647</code> int64 64 bits Signed integer (-2^63 to 2^63-1) <code>var d int64 = 9223372036854775807</code> uint8 8 bits Unsigned integer (0 to 255) <code>var e uint8 = 255</code> uint16 16 bits Unsigned integer (0 to 65,535) <code>var f uint16 = 65535</code> uint32 32 bits Unsigned integer (0 to 2^32-1) <code>var g uint32 = 4294967295</code> uint64 64 bits Unsigned integer (0 to 2^64-1) <code>var h uint64 = 18446744073709551615</code>"},{"location":"programming/go/basics/#3-derived-types","title":"3. Derived Types","text":"<p>Derived or composite types are built using basic types.</p>"},{"location":"programming/go/basics/#a-array","title":"a. Array","text":"<ul> <li>Fixed-size collection of elements of the same type.</li> <li>Example: <code>var arr [3]int = [3]int{1, 2, 3}</code></li> </ul>"},{"location":"programming/go/basics/#b-slice","title":"b. Slice","text":"<ul> <li>Dynamic-sized, more flexible version of an array.</li> <li>Example: <code>var slice []int = []int{1, 2, 3}</code></li> </ul>"},{"location":"programming/go/basics/#c-map","title":"c. Map","text":"<ul> <li>Key-value pairs (similar to dictionaries in Python).</li> <li>Example: <code>var m map[string]int = map[string]int{\"one\": 1, \"two\": 2}</code></li> </ul>"},{"location":"programming/go/basics/#d-struct","title":"d. Struct","text":"<ul> <li>Collection of fields, used to define custom data types.</li> <li>Example:</li> </ul> <pre><code>type Person struct {\n    Name string\n    Age  int\n}\nvar p Person = Person{Name: \"John\", Age: 30}\n</code></pre>"},{"location":"programming/go/basics/#e-pointer","title":"e. Pointer","text":"<p>Stores the memory address of a variable. Example: var p *int = &amp;a</p>"},{"location":"programming/go/basics/#f-interface","title":"f. Interface","text":"<p>Defines a set of method signatures, allowing polymorphism.</p> <pre><code>type Shape interface {\n    Area() float64\n}\n</code></pre>"},{"location":"programming/go/basics/#operators","title":"Operators","text":"<p>Arthematic operators:  <code>+, -, *, / , %, ++, --</code></p> <p>Assignment operators: <code>=, +=, -=, *=, /=, %=, &amp;=, !=, ^=, &gt;&gt;=, &lt;&lt;=</code></p> <p>Comparision operators : <code>&lt;, &gt;, ==, !=, &lt;=, &gt;=</code></p> <p>Logical operators: <code>&amp;&amp;, || !</code></p> <p>Bitwise operators: <code>&amp;, |, ^, &lt;&lt;, &gt;&gt;</code></p>"},{"location":"programming/go/basics/#verb-formats","title":"verb formats","text":"Verb Description Example <code>%v</code> Default format (value representation) <code>fmt.Printf(\"%v\", 42)</code> <code>%+v</code> Adds field names for structs <code>fmt.Printf(\"%+v\", struct{Name string}{\"Go\"})</code> <code>%#v</code> Go syntax representation <code>fmt.Printf(\"%#v\", struct{Name string}{\"Go\"})</code> <code>%T</code> Type of the value <code>fmt.Printf(\"%T\", 42)</code> <code>%%</code> Literal percent sign <code>fmt.Printf(\"%%\")</code>"},{"location":"programming/go/basics/#boolean","title":"Boolean","text":"Verb Description Example <code>%t</code> Boolean (<code>true</code> or <code>false</code>) <code>fmt.Printf(\"%t\", true)</code>"},{"location":"programming/go/basics/#integer","title":"Integer","text":"Verb Description Example <code>%b</code> Binary representation <code>fmt.Printf(\"%b\", 42)</code> <code>%c</code> Character corresponding to the Unicode code point <code>fmt.Printf(\"%c\", 65)</code> <code>%d</code> Decimal representation <code>fmt.Printf(\"%d\", 42)</code> <code>%o</code> Octal representation <code>fmt.Printf(\"%o\", 42)</code> <code>%O</code> Octal with leading <code>0o</code> <code>fmt.Printf(\"%O\", 42)</code> <code>%q</code> Single-quoted character literal, escapes if necessary <code>fmt.Printf(\"%q\", 65)</code> <code>%x</code> Hexadecimal (lowercase) <code>fmt.Printf(\"%x\", 255)</code> <code>%X</code> Hexadecimal (uppercase) <code>fmt.Printf(\"%X\", 255)</code> <code>%U</code> Unicode format (e.g., <code>U+1234</code>) <code>fmt.Printf(\"%U\", '\u2713')</code>"},{"location":"programming/go/basics/#floating-point-and-complex","title":"Floating-Point and Complex","text":"Verb Description Example <code>%b</code> Exponent as a power of two <code>fmt.Printf(\"%b\", 3.14)</code> <code>%e</code> Scientific notation (lowercase <code>e</code>) <code>fmt.Printf(\"%e\", 3.14)</code> <code>%E</code> Scientific notation (uppercase <code>E</code>) <code>fmt.Printf(\"%E\", 3.14)</code> <code>%f</code> Decimal point, no exponent <code>fmt.Printf(\"%f\", 3.14)</code> <code>%F</code> Same as <code>%f</code> <code>fmt.Printf(\"%F\", 3.14)</code> <code>%g</code> Compact representation (<code>%e</code> or <code>%f</code>) <code>fmt.Printf(\"%g\", 3.14)</code> <code>%G</code> Compact representation (<code>%E</code> or <code>%F</code>) <code>fmt.Printf(\"%G\", 3.14)</code>"},{"location":"programming/go/basics/#string-and-slice","title":"String and Slice","text":"Verb Description Example <code>%s</code> String (plain text) <code>fmt.Printf(\"%s\", \"Go\")</code> <code>%q</code> Double-quoted string, escapes if necessary <code>fmt.Printf(\"%q\", \"Go\")</code> <code>%x</code> Hex dump of string (lowercase) <code>fmt.Printf(\"%x\", \"Go\")</code> <code>%X</code> Hex dump of string (uppercase) <code>fmt.Printf(\"%X\", \"Go\")</code>"},{"location":"programming/go/basics/#pointer","title":"Pointer","text":"Verb Description Example <code>%p</code> Pointer (base 16, with leading <code>0x</code>) <code>fmt.Printf(\"%p\", &amp;a)</code>"},{"location":"programming/go/basics/#width-and-precision","title":"Width and Precision","text":"Option Description Example <code>%5d</code> Minimum width of 5 (right-aligned) <code>fmt.Printf(\"%5d\", 42)</code> <code>%-5d</code> Minimum width of 5 (left-aligned) <code>fmt.Printf(\"%-5d\", 42)</code> <code>%.2f</code> Precision (2 decimal places) <code>fmt.Printf(\"%.2f\", 3.14159)</code> <code>%5.2f</code> Width 5, precision 2 <code>fmt.Printf(\"%5.2f\", 3.14159)</code>"},{"location":"programming/go/basics/#control-flow","title":"Control flow","text":""},{"location":"programming/go/basics/#if-and-elif","title":"**if and elif **","text":"<pre><code>func main() {\na := 10\nb := 10\n\nif a &gt; b {\n    fmt.Println(\"a is greater than b\")\n    } else if b &gt; a {\n        fmt.Println(\"b is greater than a\")\n        } else if a == b {\n            fmt.Println(\"a is equal to b\")\n        }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#for","title":"for","text":"<pre><code>\nfunc main() {\n    for i:=0 ; i&lt;=10; i++ {\n        fmt.Println(i)\n    }\n\n    // for loop with condition\n    j := 0\n    for j &lt;= 10 {\n        fmt.Println(j)\n        j++\n    }\n\n    // for loop with range\n    k := []int{1,2,3,4,5}\n    for index, value := range k {\n        fmt.Println(index, value)\n    }   \n\n\n    // for loop with map\n    l := map[string]string{\"a\": \"apple\", \"b\": \"banana\"}\n    for key, value := range l {\n        fmt.Println(key, value)\n    }\n\n    // for loop with string\n    m := \"Hello World\"\n    for index, value := range m {\n        fmt.Println(index, string(value))\n    }\n\n    // for loop with break\n    for i:=0; i&lt;=10; i++ {\n        if i == 5 {\n            break\n        }\n        fmt.Println(i)\n    }\n\n    // for loop with continue\n    for i:=0; i&lt;=10; i++ {\n        if i == 5 {\n            continue\n        }\n        fmt.Println(i)\n    }\n\n    // for loop with goto\n    i := 0\n    Loop:\n        fmt.Println(i)\n        i++\n        if i &lt;= 10 {\n            goto Loop\n        }\n\n    // for loop with nested loop\n    for i:=0; i&lt;=5; i++ {\n        for j:=0; j&lt;=5; j++ {\n            fmt.Println(i, j)\n        }\n    }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#switch","title":"switch","text":"<pre><code>func main() {\n    var num int\n    fmt.Printf(\"%T %v\\n\", num, num)\n    fmt.Printf(\"Enter a number from 1 to 3: \")\n    fmt.Scan(&amp;num)\n\n    switch num {\n        case 1:\n            fmt.Printf(\"You entered 1\")\n        case 2:\n            fmt.Printf(\"You entered 2\")\n        case 3:\n            fmt.Printf(\"You entered 3\")\n        case 4,5:\n            fmt.Printf(\"You entered either 4 or 5\")\n        default:\n            fmt.Printf(\"You entered an invalid number\")\n        }\n}\n</code></pre>"},{"location":"programming/go/basics/#arrays","title":"Arrays","text":"<pre><code>func main() {\n  var arr1 = [3]int{1,2,3}\n  arr2 := [5]int{4,5,6,7,8}\n\n  fmt.Println(arr1)\n  fmt.Println(arr2)\n}\n</code></pre>"},{"location":"programming/go/basics/#slices","title":"Slices","text":"<p>Like arrays, slices are also used to store multiple values of the same type in a single variable.</p> <p>However, unlike arrays, the length of a slice can grow and shrink as you see fit.</p> <pre><code>func main() {\n  myslice1 := []int{}\n  fmt.Println(len(myslice1)) // 0\n  fmt.Println(cap(myslice1)) // 0\n  fmt.Println(myslice1) // []\n\n  myslice2 := []string{\"Go\", \"Slices\", \"Are\", \"Powerful\"}\n  fmt.Println(len(myslice2)) // 4\n  fmt.Println(cap(myslice2)) // 4\n  fmt.Println(myslice2) // [Go Slices Are Powerful]\n  fmt.Println(myslice2[0]) // Go\n\n  myslice2 = append(myslice2, \"New\", \"append\")\n  fmt.Println(myslice2)\n\n  fmt.Println(len(myslice2)) // 6\n  fmt.Println(cap(myslice2)) // 8\n\n  // append one slice to another\n\n  myslice1 := []int{1,2,3}\n  myslice2 := []int{4,5,6}\n  myslice3 := append(myslice1, myslice2...)\n}\n</code></pre>"},{"location":"programming/go/basics/#maps","title":"Maps","text":"<p>Maps are used to store data values in key:value pairs.</p> <p>A map is an unordered and changeable collection that does not allow duplicates.</p> <p>Maps hold references to an underlying hash table.</p> <pre><code>func main() {\n    var a = map[string]string {\"name\":\"John\", \"age\":\"25\", \"job\":\"Engineer\", \"salary\":\"50000\"}\n\n\n\n    b:=make(map[string]string) // empty map\n\n    b[\"name\"]=\"John\"\n    b[\"age\"]=\"25\"\n    b[\"job\"]=\"Engineer\"\n    b[\"salary\"]=\"50000\"\n\n\n    b[\"name\"]=\"Doe\" // update value\n    b[\"color\"]=\"Red\" // add new key value pair\n\n    delete(b, \"color\") // delete key value pair\n\n    key, value := a[\"name\"] // check if key exists\n    fmt.Println(key, value)\n\n    _, value = a[\"name1\"] // check if value exists\n    fmt.Println(value)  \n    fmt.Println(\"a: \",a)\n    fmt.Println(\"b: \",b)\n\n    for k, v := range a {\n        fmt.Printf(\"%v : %v, \", k, v) //loop with no order\n      }\n\n    fmt.Println()\n    fmt.Println()\n    for _, element := range a {\n        fmt.Printf(\"%v : %v, \\n\", element, a[element]) // loop with the defined order\n    }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#functions","title":"Functions","text":"<p>Information can be passed to functions as a parameter. Parameters act as variables inside the function.</p> <pre><code>func Add(a int, b int) int {\n    return a+b \n}\n\nfunc main() {\n    result := Add(1, 2)\n    fmt.Println(\"Sum of two numbers:\", result)\n}\n</code></pre> <p>If you need to omit the return result, you need to use '_'</p> <pre><code>\nfunc myFunction(x int, y string) (result int, txt1 string) {\n    result = x + x\n    txt1 = y + \" World!\"\n    return\n  }\n\nfunc main() {\n    _, b := myFunction(5, \"Hello\")\n  fmt.Println(b) // Hello World\n}\n</code></pre> <pre><code>func sumOfFib(n int ) int {\n    if n &lt; 0 {\n        return 0\n    }\n\n    a,b:=0,1\n    sum := a +b \n\n    for i:=3; i&lt;=n; i++ {\n        next:=a+b \n        sum+=next\n        a=b\n        b=next\n    }\n\n    return sum \n\n}\n\nfunc printFib(n int) {\n    if n &lt; 0 {\n        fmt.Println(\"invalid input\")\n        return\n    }\n\n    a,b := 0,1\n    for i:=1;i&lt;=n; i++ {\n        fmt.Printf(\"%d \", a)\n        a,b=b, a+b\n\n    }\n\n    fmt.Println()\n}\n\nfunc main() {\n    println(fact(5))\n    println(sumOfFib(6))\n    printFib(6)\n}\n</code></pre>"},{"location":"programming/go/basics/#pointers","title":"Pointers","text":""},{"location":"programming/go/basics/#struct-methods-and-interfaces","title":"Struct, Methods and Interfaces","text":"<p>A struct (short for structure) is used to create a collection of members of different data types, into a single variable.</p> <pre><code>\ntype Person struct {\n    name string\n    age int\n    job string\n    salary int\n  }\n\nfunc main() {\n    var pers1 Person\n\n\n  pers1.name = \"Hege\"\n  pers1.age = 45\n  pers1.job = \"Teacher\"\n  pers1.salary = 6000\n\n  // Access and print Pers1 info\n  fmt.Println(\"Name: \", pers1.name)\n  fmt.Println(\"Age: \", pers1.age)\n  fmt.Println(\"Job: \", pers1.job)\n  fmt.Println(\"Salary: \", pers1.salary)\n}\n</code></pre> <p>Pass struct as function</p> <pre><code>type Person struct {\n    name string\n    age int\n    job string\n    salary int\n  }\n\nfunc printPerson(pers1 Person) {\n      // Access and print Pers1 info\n      fmt.Println(\"Name: \", pers1.name)\n      fmt.Println(\"Age: \", pers1.age)\n      fmt.Println(\"Job: \", pers1.job)\n      fmt.Println(\"Salary: \", pers1.salary)\n}\n\n\nfunc main() {\n    var pers1 Person\n\n\n  pers1.name = \"Hege\"\n  pers1.age = 45\n  pers1.job = \"Teacher\"\n  pers1.salary = 6000\n\n  printPerson(pers1)\n\n}\n</code></pre>"},{"location":"programming/go/interviews/","title":"interviews faq","text":""},{"location":"programming/go/interviews/#coding-questions-1","title":"Coding Questions 1","text":""},{"location":"programming/go/overview/","title":"overview","text":"<p>Go (Golang) is a statically typed, compiled programming language developed by Google in 2007 and publicly released in 2009. It was designed to address challenges in software development, particularly for large-scale, distributed systems. It emphasizes simplicity, performance, and efficient concurrency.</p> <p>Go that make it different from other programming languages</p> <p>Simplicity and minimalism. Built-in support for concurrency (goroutines and channels). Statically typed with automatic garbage collection. Fast compilation and execution. Strong standard library for networking and web applications. Cross-platform support with built-in tools for building binaries.</p> <p>Key Features:</p> <p>Simplicity: Go has a minimalistic design with a straightforward syntax, making it easy to learn and use.</p> <p>Concurrency: Built-in support for concurrency using goroutines and channels.</p> <p>Performance: Compiled directly to machine code, Go offers performance comparable to C and C++.</p> <p>Garbage Collection: Automatic memory management for ease of development.</p> <p>Standard Library: A rich and robust standard library for handling networking, file I/O, and more.</p> <p>Cross-Platform: Easily compiles binaries for multiple platforms without external dependencies.</p> Aspect Golang Python Typing Statically typed Dynamically typed Compilation Compiled Interpreted Performance High (closer to C/C++) Moderate (slower than Go) Concurrency Native support via goroutines and channels Limited; requires libraries like <code>asyncio</code> or <code>threading</code> Ease of Learning Moderate (focus on simplicity) Easy (readable and beginner-friendly) Use Cases System-level programming, microservices, cloud applications Web development, data science, scripting, AI/ML Community Growing but smaller than Python Mature and vast Ecosystem Focused on performance and concurrency Broad, with extensive libraries Error Handling Explicit error handling (<code>if err != nil</code>) Exceptions and try-except blocks"},{"location":"programming/go/overview/#advantages-of-golang","title":"Advantages of Golang","text":"<ol> <li>Performance: Comparable to C/C++ due to compilation to native code.</li> <li>Concurrency: Efficient and lightweight goroutines enable high-performance concurrent programming.</li> <li>Simplicity: Minimalist syntax and design philosophy.</li> <li>Cross-Compilation: Build binaries for different platforms effortlessly.</li> <li>Standard Library: Comprehensive and robust, reducing the need for third-party dependencies.</li> <li>Scalability: Ideal for distributed systems and microservices.</li> </ol>"},{"location":"programming/go/overview/#disadvantages-of-golang","title":"Disadvantages of Golang","text":"<ol> <li>Verbose Error Handling: Requires explicit error checks, leading to repetitive code.</li> <li>Lack of Generics: (Resolved in Go 1.18, but earlier versions lacked this feature, leading to workarounds.)</li> <li>Limited Libraries: Smaller ecosystem compared to Python.</li> <li>No GUI Support: Not suitable for desktop application development.</li> <li>Minimalistic Design: Some developers find the lack of features (like inheritance or macros) restrictive.</li> </ol>"},{"location":"programming/go/overview/#advantages-of-python","title":"Advantages of Python","text":"<ol> <li>Ease of Use: Highly readable and beginner-friendly syntax.</li> <li>Rich Ecosystem: Extensive libraries for web development, data science, AI/ML, etc.</li> <li>Flexibility: Suitable for various domains, from scripting to AI.</li> <li>Community Support: Massive community and resources.</li> <li>Rapid Prototyping: Ideal for quickly building and testing applications.</li> </ol>"},{"location":"programming/go/overview/#disadvantages-of-python","title":"Disadvantages of Python","text":"<ol> <li>Performance: Slower than compiled languages like Go.</li> <li>Concurrency: Limited by the Global Interpreter Lock (GIL).</li> <li>Dynamic Typing: Can lead to runtime errors if not carefully managed.</li> <li>Deployment: Requires dependencies and runtime, unlike Go's single binary.</li> </ol>"},{"location":"programming/go/overview/#conclusion","title":"Conclusion","text":"<ul> <li>Choose Go: For performance-critical, scalable, and concurrent applications like microservices, system tools, and cloud-native apps.</li> <li>Choose Python: For rapid development, data science, AI/ML, web development, or scripting.</li> </ul>"},{"location":"programming/pandas/overview/","title":"Pandas Oneliners","text":"<p>A complete, example-driven quick reference using your attached dataset (<code>bios</code>, <code>results</code>). All examples are one-liners unless noted.</p>"},{"location":"programming/pandas/overview/#setup","title":"Setup","text":"<pre><code>import pandas as pd, numpy as np\nbios = pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"bios\")\nresults = pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"results\")\n</code></pre> <p>Columns:</p> <ul> <li>bios: athlete_id, name, born_date, born_city, born_region, born_country, NOC, height_cm, weight_kg, died_date</li> <li>results: year, type, discipline, event, as, athlete_id, noc, team, place, tied, medal</li> </ul>"},{"location":"programming/pandas/overview/#create-initialize-dataframes","title":"Create &amp; Initialize DataFrames","text":"<pre><code>pd.DataFrame({\"A\":[1,2], \"B\":[\"x\",\"y\"]})                           # from dict\npd.DataFrame([{\"A\":1,\"B\":\"x\"},{\"A\":2,\"B\":\"y\"}])                     # list of dicts\npd.DataFrame(np.arange(6).reshape(3,2), columns=[\"A\",\"B\"])          # from numpy\npd.date_range(\"2020-01-01\", periods=5, freq=\"D\").to_frame(\"date\")   # date range\npd.DataFrame(columns=[\"A\",\"B\"])                                     # empty schema\n</code></pre>"},{"location":"programming/pandas/overview/#io-read-write","title":"I/O (Read &amp; Write)","text":"<pre><code>pd.read_csv(\"file.csv\")                                            # read CSV\ndf.to_csv(\"out.csv\", index=False)                                  # write CSV\npd.read_excel(\"file.xlsx\", sheet_name=\"bios\")                      # read Excel\ndf.to_excel(\"out.xlsx\", index=False, sheet_name=\"S1\")              # write Excel\npd.read_parquet(\"file.parquet\")                                    # read Parquet\ndf.to_parquet(\"out.parquet\", index=False)                          # write Parquet\npd.read_json(\"file.json\")                                          # read JSON\ndf.to_json(\"out.json\", orient=\"records\", lines=False)              # write JSON\n</code></pre>"},{"location":"programming/pandas/overview/#inspect-explore","title":"Inspect &amp; Explore","text":"<pre><code>bios.head(3); bios.info(); bios.describe(include=\"all\")             # quick look\nbios.shape; bios.columns.tolist(); bios.dtypes                      # schema\nbios.nunique(); bios.isna().sum()                                   # counts\nbios[\"NOC\"].value_counts()                                          # freq\nresults.sample(5, random_state=0)                                   # random rows\n</code></pre>"},{"location":"programming/pandas/overview/#selecting-filtering-assigning","title":"Selecting, Filtering, Assigning","text":"<pre><code>bios[[\"name\",\"NOC\"]]                                               # select cols\nbios.loc[bios[\"born_country\"].eq(\"FRA\"), [\"name\",\"born_city\"]]     # filter eq\nbios.query(\"height_cm &gt; 190 and NOC == 'France'\")                  # query str\nbios.iloc[:5, :3]                                                  # by position\nbios.assign(bmi=lambda d: d[\"weight_kg\"] / (d[\"height_cm\"]/100)**2)# add col\nbios.drop(columns=[\"died_date\"])                                   # drop col\nresults.loc[results[\"medal\"].notna(), [\"year\",\"athlete_id\",\"medal\"]]# non-null\n</code></pre>"},{"location":"programming/pandas/overview/#missing-data","title":"Missing Data","text":"<pre><code>bios.fillna({\"height_cm\": bios[\"height_cm\"].median()})             # fill by stat\nbios[\"height_cm\"].fillna(0)                                        # fill scalar\nbios.dropna(subset=[\"height_cm\",\"weight_kg\"])                      # drop subset\nbios[\"born_date\"] = pd.to_datetime(bios[\"born_date\"], errors=\"coerce\")  # coerce\nbios.interpolate(numeric_only=True)                                # interpolate\n</code></pre>"},{"location":"programming/pandas/overview/#type-handling","title":"Type Handling","text":"<pre><code>bios[\"height_cm\"] = pd.to_numeric(bios[\"height_cm\"], errors=\"coerce\") # numeric\nbios[\"NOC\"] = bios[\"NOC\"].astype(\"category\")                           # category\nresults[\"year\"] = results[\"year\"].astype(\"int64\")                       # cast int\nresults[\"tied\"] = results[\"tied\"].astype(\"bool\")                        # cast bool\n</code></pre>"},{"location":"programming/pandas/overview/#datetime-features","title":"Datetime Features","text":"<pre><code>bios[\"born_date\"] = pd.to_datetime(bios[\"born_date\"], errors=\"coerce\")  # parse\nbios[\"born_year\"] = bios[\"born_date\"].dt.year                           # year\nbios[\"born_month\"] = bios[\"born_date\"].dt.month                         # month\nbios.set_index(\"born_date\").sort_index().last(\"5Y\")                     # recent 5y\n</code></pre>"},{"location":"programming/pandas/overview/#groupby-aggregations-olympics-examples","title":"GroupBy &amp; Aggregations (Olympics Examples)","text":"<pre><code>results.groupby(\"noc\")[\"medal\"].count().sort_values(ascending=False)     # medals by NOC (non-null)\nresults.groupby([\"year\",\"noc\"], as_index=False)[\"athlete_id\"].nunique()  # unique athletes per year/NOC\nbios.groupby(\"NOC\")[\"height_cm\"].agg([\"mean\",\"median\",\"count\"])          # height stats by NOC\nresults.groupby(\"discipline\").agg(events=(\"event\",\"nunique\"))            # events per discipline\n</code></pre>"},{"location":"programming/pandas/overview/#transform-window-ops","title":"Transform &amp; Window Ops","text":"<pre><code>bios[\"z_height\"] = (bios[\"height_cm\"] - bios[\"height_cm\"].mean())/bios[\"height_cm\"].std()    # z-score\nresults[\"rank_in_year\"] = results.groupby(\"year\")[\"place\"].rank(method=\"dense\", ascending=True) # rank\nresults.sort_values([\"year\",\"place\"]).groupby(\"year\")[\"place\"].rolling(3).mean().reset_index(level=0, drop=True) # rolling mean\n</code></pre>"},{"location":"programming/pandas/overview/#pivot-crosstab-reshape","title":"Pivot, Crosstab, Reshape","text":"<pre><code>pd.pivot_table(results, index=\"year\", columns=\"noc\", values=\"medal\", aggfunc=\"count\", fill_value=0) # medals heatmap\npd.crosstab(results[\"discipline\"], results[\"medal\"]).fillna(0)                                     # discipline x medal\nresults.melt(id_vars=[\"year\",\"noc\"], value_vars=[\"medal\",\"place\"], var_name=\"metric\", value_name=\"val\") # wide-&gt;long\n</code></pre>"},{"location":"programming/pandas/overview/#merge-join-bios-results","title":"Merge / Join (bios \u2194 results)","text":"<pre><code>res_bios = results.merge(bios[[\"athlete_id\",\"name\",\"NOC\",\"height_cm\",\"weight_kg\"]], on=\"athlete_id\", how=\"left\") # enrich\nbios_only = bios[~bios[\"athlete_id\"].isin(results[\"athlete_id\"])]                                                # anti-join style\n</code></pre>"},{"location":"programming/pandas/overview/#concatenate-append","title":"Concatenate &amp; Append","text":"<pre><code>pd.concat([results.query(\"year &lt; 2000\"), results.query(\"year &gt;= 2000\")], ignore_index=True) # stack rows\npd.concat([bios.set_index(\"athlete_id\"), bios.set_index(\"athlete_id\")], axis=1)             # concat cols\n</code></pre>"},{"location":"programming/pandas/overview/#sorting-ranking","title":"Sorting &amp; Ranking","text":"<pre><code>results.sort_values([\"year\",\"noc\",\"place\"], ascending=[True, True, True])                   # sort\nresults[\"noc_rank\"] = results.groupby(\"year\")[\"noc\"].rank(method=\"dense\")                   # rank within year\n</code></pre>"},{"location":"programming/pandas/overview/#string-ops","title":"String Ops","text":"<pre><code>bios[\"born_city\"].str.title()                                                                # title case\nresults[\"event\"].str.contains(\"Doubles\", na=False)                                           # contains\nresults[\"event_clean\"] = results[\"event\"].str.replace(r\"\\s+\\(Olympic\\)\", \"\", regex=True)  # regex replace\nresults[\"pair\"] = results[\"team\"].fillna(\"\").str.split(\",\").str[:2].str.join(\",\")            # first two teammates\n</code></pre>"},{"location":"programming/pandas/overview/#categorical-optimization","title":"Categorical Optimization","text":"<pre><code>results[\"discipline\"] = pd.Categorical(results[\"discipline\"])                                # category\nresults[\"medal\"] = pd.Categorical(results[\"medal\"], ordered=True, categories=[\"Bronze\",\"Silver\",\"Gold\"]) # ordered\n</code></pre>"},{"location":"programming/pandas/overview/#index-tricks-multiindex","title":"Index Tricks &amp; MultiIndex","text":"<pre><code>mi = results.set_index([\"year\",\"noc\"]).sort_index()                                          # multiindex\nmi.loc[(slice(2000,2012), \"USA\"), :]                                                         # slice by MI\nresults.reset_index(drop=True)                                                               # reset\n</code></pre>"},{"location":"programming/pandas/overview/#window-functions-rollingexpandingewm","title":"Window Functions (Rolling/Expanding/EWM)","text":"<pre><code>results.sort_values(\"year\").groupby(\"noc\")[\"place\"].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True) # rolling mean\nresults.groupby(\"noc\")[\"place\"].expanding().mean().reset_index(level=0, drop=True)                                   # expanding mean\nresults.sort_values(\"year\").groupby(\"noc\")[\"place\"].apply(lambda s: s.ewm(alpha=0.3).mean())                          # EWM\n</code></pre>"},{"location":"programming/pandas/overview/#conditional-logic-one-hot","title":"Conditional Logic &amp; One-Hot","text":"<pre><code>results[\"medal_flag\"] = np.where(results[\"medal\"].notna(), 1, 0)                           # binary flag\npd.get_dummies(results[\"medal\"], prefix=\"medal\")                                            # one-hot encode\n</code></pre>"},{"location":"programming/pandas/overview/#time-series-resampling-by-year","title":"Time-Series Resampling (by year)","text":"<pre><code>(results.assign(date=pd.to_datetime(results[\"year\"].astype(str) + \"-01-01\"))\n        .set_index(\"date\")\n        .resample(\"10Y\")[\"athlete_id\"].count())                                             # decadal counts\n</code></pre>"},{"location":"programming/pandas/overview/#deduplication-qc","title":"Deduplication &amp; QC","text":"<pre><code>results.drop_duplicates(subset=[\"year\",\"athlete_id\",\"event\"])                               # unique participations\nassert bios[\"athlete_id\"].is_unique                                                         # enforce unique key\n</code></pre>"},{"location":"programming/pandas/overview/#performance-tips","title":"Performance Tips","text":"<pre><code>pd.read_csv(\"big.csv\", chunksize=1_000_000)                                                 # chunked ingest\nresults[\"discipline\"] = results[\"discipline\"].astype(\"category\")                            # categorical\nresults.to_parquet(\"fast.parquet\", index=False)                                             # columnar storage\n</code></pre>"},{"location":"programming/pandas/overview/#handy-patterns-olympics-use-cases","title":"Handy Patterns (Olympics Use-Cases)","text":"<pre><code># Top 3 medal-heavy NOCs per discipline\n(results.dropna(subset=[\"medal\"])\n        .groupby([\"discipline\",\"noc\"]).size().reset_index(name=\"medals\")\n        .sort_values([\"discipline\",\"medals\"], ascending=[True,False])\n        .groupby(\"discipline\").head(3))\n\n# BMI distribution (needs height &amp; weight) for living athletes\n(bios.query(\"height_cm.notna() and weight_kg.notna() and died_date.isna()\")\n     .assign(bmi=lambda d: d[\"weight_kg\"]/((d[\"height_cm\"]/100)**2))\n     .groupby(\"NOC\")[\"bmi\"].describe())\n</code></pre>"},{"location":"programming/pandas/overview/#export-results","title":"Export Results","text":"<pre><code>summary = results.groupby([\"year\",\"noc\"], as_index=False)[\"athlete_id\"].nunique()\nsummary.to_excel(\"athletes_by_year_noc.xlsx\", index=False)                                  # Excel\nsummary.to_markdown(\"summary.md\", index=False)                                              # Markdown table\n</code></pre>"},{"location":"programming/pandas/overview/#end-to-end-one-liner-example","title":"End-to-End One-Liner Example","text":"<pre><code>(pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"results\")\n   .merge(pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"bios\")[[\"athlete_id\",\"NOC\"]], on=\"athlete_id\", how=\"left\")\n   .dropna(subset=[\"medal\"])\n   .groupby([\"year\",\"NOC\"], as_index=False).size()\n   .sort_values([\"year\",\"size\"], ascending=[True,False])\n   .to_csv(\"medals_per_year_noc.csv\", index=False))\n</code></pre> <p>complete-pandas-tutorial</p>"},{"location":"programming/python/concepts/","title":"concepts","text":""},{"location":"programming/python/concepts/#property-decorator","title":"property decorator","text":"<p>The @property decorator in Python is used to define methods that are accessed like attributes, providing a way to implement computed properties or control access to class attributes. It allows you to define a method that can be accessed as if it were an attribute, without the need to explicitly call it as a method.</p> <pre><code>class Circle:\n    def __init__(self,radius):\n        self.radius = radius\n\n    @property\n    def diameter(self):\n        return self.radius\n\n    @diameter.setter\n    def diameter(self,value):\n        if value &gt; 0:\n             self.radius=value\n\n    @diameter.deleter\n    def diameter(self):\n        del self.radius\n\n    @property\n    def area(self):\n        return 3.14 * self.radius**2\n\nmycircle=Circle(2)\nmycircle.diameter=10  # you can use method as an attribute.\nprint(mycircle.diameter)\n\nprint(\"setter\")\n\nmycircle.diameter=0  \nprint(mycircle.diameter) # Output: 10, since the value is set to 0, it would output the previous value associated in the object.\n\nprint(\"deleter\")\ndel mycircle.diameter\nprint(mycircle.diameter)  # raise an exception as the circle object has deleted the 'radius' attribute\n</code></pre>"},{"location":"programming/python/concepts/#abstract-classes","title":"abstract classes","text":"<p>Abstract classes are those classes that can't be instantiated directly, it would only serve as a blue print. They are always designed to be <code>subclassed</code> and they would contain <code>abstractmethod</code> that must be implemented by their subclass. </p> <pre><code>from abc import ABC, abstractmethod\n\nclass Animal(ABC):\n    @abstractmethod\n    def make_sound(self):\n        pass\n</code></pre> <p>When you are trying to instantiated <code>animal=Animal()</code>, it would throw an error.</p> <p><code>Error: Can't instantiate abstract class Animal with abstract methods make_sound</code></p> <p>Now, you would have an subclass of <code>Animal</code> that uses an <code>abstractmethod</code> </p> <pre><code>class Dog(Animal):\n    def make_sound(self):\n        print(\"Woof\")\n\nclass Cat(Animal):\n    def make_sound(self):\n        print(\"Meow\")\n\ndog=Dog()\ndog.make_sound() # Output: Woof\n\ncat=Cat()\ncat.make_sound() # Output: Meow\n</code></pre>"},{"location":"programming/python/concepts/#first-class-functions","title":"first class functions","text":"<p>If a function can be assigned to a variable or passed as object/variable to other function, that function is called as <code>first class function</code> </p> <p>example-1</p> <pre><code>def square(x):\n    return x*x \n\ndef cube(x):\n    return x*x*x\n\n# Receives a function, executes and returns result. You will provide as an list here\ndef mymap(func,args):\n    result = []\n    for each_item in args:\n        result.append(func(each_item))\n\n    return result \n\nsqr=mymap(square,[1,2,3]) # Output: [1,4,9]\ncub=mymap(cube,[1,2,3]) # Output: [1,8,27]\n\n# provide a single value to get the result.\ndef print_result(x, func):\n    \"\"\"recieve a function and execute it and return result\"\"\"\n    return func(x)\n\ndo_square = square     # assigning square function to a variable\ndo_cube = cube         # assigning cube function to a variable\n\nres = print_result(5, do_square)   # passing function to another function\n</code></pre> <p>example-2</p> <p>We would use first-class function to create an html tag</p> <pre><code>def create_html(tag):\n    def wrap_text(text):\n        print(f\"&lt;{tag}&gt;{msg}/&lt;{tag}&gt;\")\n    return wrap_text\n</code></pre> <p><code>create_html</code> would return the fucntion(<code>wrap_text</code>), we would use that function to call our msg wrapped in the html msg. </p> <pre><code>print_h1=create_html(\"h1\")\nprint_h1(\"Heading1\") # Output: &lt;h1&gt;Heading1&lt;/h1&gt; \n</code></pre>"},{"location":"programming/python/concepts/#decorator","title":"decorator","text":"<p>A decorator is a function that takes another function as an argument and returns a modified version of the function. Decorators are often used to add functionality to functions, such as logging, timing, or error handling.</p> <pre><code>def log_function(func):\n  def wrapper(*args, **kwargs):\n    print(\"Calling function:\", func.__name__)\n    result = func(*args, **kwargs)\n    print(\"Function returned:\", result)\n    return result\n  return wrapper\n\n@log_function\ndef my_function(x, y):\n  return x + y\n\nprint(my_function(1, 2))\n</code></pre> <pre><code>def my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n</code></pre> <p>A generator is a function that produces a sequence of values, one at a time. Generators are created using the yield keyword. Generators are useful for a variety of tasks, such as filtering a sequence of values, transforming a sequence of values, or iterating over a large sequence of values without storing the entire sequence in memory.</p> <pre><code>def read_file(filename):\n  with open(filename, 'r') as f:\n    for line in f:\n      yield line\n\nfor line in read_file('my_file.txt'):\n  print(line)\n</code></pre> <p></p> <pre><code>def timeit(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(\"Time taken to execute function:\", end - start)\n        return result\n    return wrapper\n\n@timeit\ndef factorial(n):\n  print(\"Without using cache decorator\")\n  return n*factorial(n-1) if n else 1 \n\nprint(factorial(5))\n</code></pre>"},{"location":"programming/python/concepts/#closures","title":"closures","text":"<p>example-1</p> <p>Python closure is a nested function that allows us to access variables of the outer function even after the outer function is closed.</p> <pre><code>def outer_function():\nmessge=\"Hi\"\n\ndef inner_function():\n    print(messge) # you would be able to access the variable of outer function.\n\nreturn inner_function()\n\nhi=outer_function() # Output: Hi\n</code></pre> <p>example-2</p> <p>let's modify the code and we could store the <code>inner_function</code> as variable and then call return value</p> <pre><code>def outer_function(message):\n    messge=message\n\n    def inner_function():\n        print(messge) # you would be able to access the variable of outer function.\n\n    return inner_function\n\nprint_hi=outer_function(\"hi\")\nprint_hello=outer_function(\"hello\")\n\nprint(print_hi.__name__) # Output: inner_function\nprint(print_hello.__name__) # Output: inner_function\n\nprint_hi() # Output: hi\nprint_hello()  # Output: hello\n</code></pre> <p>example-3</p> <p>let's use closure to add and subtract example, additionally while doing so, we could log it in the file <code>example.log</code></p> <pre><code>import logging\nlogging.basicConfig(filename=\"example.log\",level=logging.DEBUG)\n\ndef logger(func):\n    def log_func(*args):\n        logging.info(f\"logging {func.__name__} with arguments {args}\")\n        print(func(*args))\n    return log_func\n\ndef add(x,y):\n    return x+y\n\ndef sub(x,y):\n    return x-y\n\nadd_logger=logger(add)\nsub_logger=logger(sub)\n\nprint(add_logger.__name__) # Output: log_func  \nprint(sub_logger.__name__) # Output: log_func\n\nadd_logger(4,5) # Output: 9\nsub_logger(20,10) # Output: 10\n</code></pre>"},{"location":"programming/python/concepts/#shallow-deep-copy","title":"shallow &amp; deep copy","text":"<p>A shallow copy creates a new object but references the same elements as the original object. In other words, it creates a new container object and fills it with references to the same elements as the original. The references point to the same memory addresses as the original elements. If any of the referenced elements are mutable, changes made to them will be reflected in both the original and the shallow copy.</p> <p>You can think of \"hardlink\" in linux prespective. </p> <pre><code>import copy\n\nlist1 = [1, 2, [3, 4]]\nlist2 = copy.copy(list1)\n\nlist2[0] = 5\nlist2[2].append(5)\n\nprint(list1)  # Output: [1, 2, [3, 4, 5]]\nprint(list2)  # Output: [5, 2, [3, 4, 5]]\n</code></pre>"},{"location":"programming/python/concepts/#usecase-of-shallow-copy","title":"Usecase of shallow copy","text":"<p>Cloning mutable objects: Shallow copy is often used when you want to create a new object that shares the internal state with the original object. This can be useful when working with mutable objects like lists or dictionaries, where you want to create a modified version while preserving the original.</p> <p>Performance optimization: Shallow copy can be more efficient in terms of time and memory when dealing with large objects or data structures. Instead of duplicating the entire object, a shallow copy simply creates references to the existing elements.</p> <p>Nested data structures: Shallow copy is appropriate when you have nested data structures, such as a list of lists or a dictionary of dictionaries. It allows you to create a new structure that maintains references to the original nested objects.</p> <p>Deep Copy:</p> <p>A deep copy creates a new object and recursively copies all the elements from the original object to the new object. It creates a completely independent copy with its own set of elements. Any changes made to the copied elements will not affect the original object.</p> <p>softlink from linux prespective. </p> <pre><code>import copy\n\nlist1 = [1, 2, [3, 4]]\nlist2 = copy.deepcopy(list1)\n\nlist2[0] = 5\nlist2[2].append(5)\n\nprint(list1)  # Output: [1, 2, [3, 4]]\nprint(list2)  # Output: [5, 2, [3, 4, 5]]\n</code></pre>"},{"location":"programming/python/concepts/#usecase-of-deep-copy","title":"usecase of deep copy","text":"<p>Creating independent copies: Deep copy is used when you want to create a completely independent copy of an object and its internal state. This is useful when you need to modify the copied object without affecting the original object.</p> <p>Avoiding unintended side effects: Deep copy ensures that any modifications made to the copied object or its nested elements do not impact the original object. This can be important when dealing with complex data structures or when multiple objects need to maintain their own separate state.</p> <p>Immutable objects: Deep copy is suitable for creating copies of immutable objects, such as strings or tuples, where modifying the object is not possible. Deep copy allows you to create a new object with the same value.</p>"},{"location":"programming/python/concepts/#iter-and-generators","title":"iter and generators","text":"<p>Iterator: An iterator is an object that allows sequential access to elements in a collection (e.g., lists, tuples, sets) without exposing the underlying structure.</p> <p>It operates on the principle of \"lazy evaluation,\" meaning it fetches the next element only when requested. This saves memory and improves performance when dealing with large collections.</p> <p>Iterators use two primary methods: iter: Returns the iterator object itself. next: Retrieves the next element from the collection. If there are no more elements, it raises the StopIteration exception.</p> <p>Iterators are typically used with a for loop or the built-in next() function.</p> <pre><code>my_list = [1, 2, 3, 4]\nmy_iterator = iter(my_list)\n\nprint(next(my_iterator))  # Output: 1\nprint(next(my_iterator))  # Output: 2\nprint(next(my_iterator))  # Output: 3\nprint(next(my_iterator))  # Output: 4\n# print(next(my_iterator)) # Raises StopIteration because no more elements.\n\n</code></pre> <p>Generator:</p> <p>A generator is a special type of iterator that is created using a function with one or more yield statements. It allows you to define an iterative algorithm by suspending the execution state and yielding values one at a time, instead of returning the entire collection at once.</p> <p>Generators are memory-efficient because they produce elements on-the-fly and don't store the entire collection in memory.</p> <p>They are usually implemented using a for loop or by calling the generator function directly.</p> <pre><code>def my_generator():\n    yield 1\n    yield 2\n    yield 3\n    yield 4\n\ngen = my_generator()\n\nprint(next(gen))  # Output: 1\nprint(next(gen))  # Output: 2\nprint(next(gen))  # Output: 3\nprint(next(gen))  # Output: 4\n# print(next(gen)) # Raises StopIteration because there are no more elements.\n</code></pre> <p>iterators are objects that provide sequential access to elements in a collection, whereas generators are a type of iterator that allows you to define a sequence using a function with yield statements, offering memory-efficient and lazy evaluation behavior.</p>"},{"location":"programming/python/concepts/#args-and-kwargs","title":"args and *kwargs","text":"<p>*args allows you to pass a variable number of non-keyword arguments to a function. It is used to handle scenarios where the exact number of arguments is not known in advance.</p> <p>**kwargs allows you to pass a variable number of keyword arguments. It is used when you want to handle named arguments in a function.</p> <pre><code>def my_function(*args, **kwargs):\n    print(\"Arguments:\", args)\n    print(\"Keyword arguments:\", kwargs)\n\nmy_function(1, 2, 3, name=\"John\", age=30)\n</code></pre>"},{"location":"programming/python/concepts/#python-manage-memory","title":"Python manage memory?","text":"<p>Python uses a private heap that stores all objects and data structures. The memory management is handled by Python's memory manager, which ensures that memory is allocated efficiently and that the interpreter doesn't run out of memory. Python also has an in-built garbage collector, which reclaims memory by deallocating objects that are no longer in use. The primary mechanism for garbage collection is reference counting, but Python also uses a cycle detector to deal with reference cycles.</p>"},{"location":"programming/python/concepts/#staticmethod-and-classmethod","title":"@staticmethod and @classmethod","text":"<p>@staticmethod: Defines a method that doesn\u2019t require access to the instance (self) or class (cls). It behaves like a plain function but belongs to the class\u2019s namespace.</p> <p>@classmethod: Defines a method that receives the class (cls) as its first argument instead of the instance (self). It can modify the class state that applies across all instances of the class.</p> <pre><code>class MyClass:\n    @staticmethod\n    def static_method():\n        print(\"This is a static method.\")\n\n    @classmethod\n    def class_method(cls):\n        print(f\"This is a class method of {cls.__name__}\")\n\nMyClass.static_method()\nMyClass.class_method()\n</code></pre>"},{"location":"programming/python/concepts/#is-and","title":"is and ==","text":"<p><code>is</code>: Checks whether two references point to the same object (identity) <code>==</code>: Checks whether the values of two objects are equal (equality).</p>"},{"location":"programming/python/oops/","title":"oops","text":""},{"location":"programming/python/oops/#oops","title":"OOPS","text":""},{"location":"programming/python/oops/#encapsulation-abstraction","title":"Encapsulation &amp; Abstraction","text":"<p>Bundling data and methods within a single unit. when you create a class, it means you are <code>implementing encapsulation</code></p> <p>encapsulation allows us to restrict accessing variables and methods directly and prevent accidental data modification by creating private data members and methods within a class. </p> <p>public : accessable anywhere form outside the class. </p> <p>private : Accessible within the class</p> <p>Protected : within the class and its sub-classes(inheritance)</p>"},{"location":"programming/python/oops/#getters-and-setters","title":"Getters and Setters","text":"<p>getter method to access data members and the setter methods to modify the data members</p> <p>private variables are not hidden fields like in other programming languages. The getters and setters methods are often used when:</p> <p>When we want to avoid direct access to private variables To add validation logic for setting a value</p> <pre><code>class Employee:\n\n    def __init__(self,name,salary,age):\n        # public members\n        self.name = name\n\n        # private members\n        self.__salary=salary\n        self.__age=age\n\n        # protected members\n        self._project=\"Oracle pvt ltd\"\n\n    # getters\n    def get_age(self):\n        print(self.__age)\n\n    # setters\n    def set_age(self,age):\n        # you can use the logic before changing \n        # the object\n        if age &gt; 130:\n            print(\"not sure anyone is alive.. please submit proof of living!\")\n        self.__age = age\n\n    def show(self):\n        print(f\"{self.name}'s age is {self.__age} drawing salry of {self.__salary}\")\n\n\nclass EmployeeDepartment(Employee):\n    def __init__(self,name):\n        Employee().__init__(self)\n        self.name = name\n\n    def show(self):\n        print(f\"{self.name} employee working in project {self._project}\")\n\n\nemp=Employee(\"employee\",100000,33)\nemp.show()\n\nemp.set_age(29) #Private members can be accessed by using public methods.\nemp.get_age()\nprint(emp._Employee__age) #Name mangling.\n\nprint(f\"Employee project details: {emp._project}\")\n\n\nsunil=Employee(\"Sunil\",100000,34)\nsunil.set_age(32)\n</code></pre>"},{"location":"programming/python/oops/#constructors","title":"Constructors","text":"<p>Special method used to create and initialize an object of a class. The primary use of a constructor is to declare and initialize data member/instance variables of a class.</p>"},{"location":"programming/python/oops/#constructor-types","title":"Constructor Types","text":"<ul> <li> <p>Default Constructor - It does not perform any task but initializes the objects</p> </li> <li> <p>Non-parametrized constructor - A constructor without any arguments is called a non-parameterized constructor. </p> </li> <li> <p>parametrized constructor - A constructor with defined parameters or arguments is called a parameterized constructor.</p> </li> </ul> <pre><code>class Employee:\n    # class variable \n    count=0\n    # Parameterized Constructor with default values\n    def __init__(self,name=\"Raghu\",age=12):\n        self.name = name\n        self.age=age\n        Employee.count+=1\n\n    # Non Parametrized Constructor.\n    # def __init__(self):\n    #    self.name=\"Raju\"\n    #    self.age=18\"\n\n    # when there is no __init__ method, its default constructor\n\n    def display(self):\n        print(f\"employee {self.name} is around {self.age} years old\")\n\n\nemp1=Employee(\"Ram\",18)\nemp1.display() # Output: employee Ram is around 18 years old\n\n# Object created with default values\nemp2=Employee()\nemp2.display() # Output: employee Raghu is around 12 years old\n\nprint(f\"Total number of employees: {Employee.count})\n</code></pre>"},{"location":"programming/python/oops/#constructor-chaining","title":"Constructor Chaining","text":"<p>Constructor chaining is the process of calling one constructor from another constructor. Constructor chaining is useful when you want to invoke multiple constructors, one after another, by initializing only one instance. constructor chaining is convenient when we are dealing with inheritance.</p> <pre><code>class Vehicle:\n    def __init__(self,engine):\n        self.engine = engine \n\nclass Car(Vehicle): \n    def __init__(self,engine, max_speed):\n        super().__init__(engine)\n        self.max_speed=max_speed\n\nclass ElectricCar(Car):\n    def __init__(self,engine,max_speed,km):\n        super().__init__(engine,max_speed)\n        self.km=km\n\nev = ElectricCar('1500cc', 240, 750)\nprint(f\"{ev.engine} is having max speed {ev.max_speed} travelling distance of {ev.km}\")\n</code></pre>"},{"location":"programming/python/oops/#polymorphism","title":"Polymorphism","text":"<p>Polymorphism in Python is the ability of an object to take many forms. Polymorphism is mainly used with inheritance. </p>"},{"location":"programming/python/oops/#polymorphism-with-inheritance","title":"Polymorphism With Inheritance","text":"<p>Using method overriding <code>polymorphism</code> allows us to defines methods in the child class that have the same name as the methods in the parent class. This <code>process of re-implementing the inherited method in the child class</code> is known as Method Overriding.</p> <pre><code>class Vehicle:\n    def __init__(self,name,color):\n        self.name = name\n        self.color = color\n\n    def show(self):\n        print(f\"{self.name} is having color of {self.color}\")\n\n\n    def max_speed(self):\n        print(\"Vehicle max speed is 150\")\n\n    def change_gear(self):\n        print('Vehicle change 6 gear')\n\nclass Car(Vehicle):\n\n    # due to polymorphism, max_speed() and change_gear() \n    # methods are overridden for the car object.\n\n    def max_speed(self):\n        print(\"car max speed is 250\")\n\n    def change_gear(self):\n        print('car change 7 gear')\n\n\n\ncar = Car('Benz', 'Red')\ncar.show() # this method isin't overridden.\ncar.max_speed()\n\nvehicle = Vehicle('Volvo', 'Yello')\nvehicle.show()\nvehicle.max_speed()\n</code></pre>"},{"location":"programming/python/oops/#polymorphism-class-methods","title":"Polymorphism class methods","text":"<pre><code>class StudentASection:\n    def students(self):\n        print(\"A section students\")\n\n    def class_teacher(self):\n        print(\"Mohan\")\n\nclass StudentBSection:\n    def students(self):\n        print(\"B section students\")\n\n    def class_teacher(self):\n        print(\"Mohana\")\n\na_section_student=StudentASection()\nb_section_student=StudentBSection()\n\n# you can create function and pass as objects\n#def some_function(obj):\n#    obj.call_methods\n\nfor students in (a_section_student,b_section_student):\n    students.students()\n    students.class_teacher()\n</code></pre> <p>You could also pass as a single object to the above <code>objects into the function</code></p> <pre><code>def get_student_details(obj):\n    obj.students()\n    obj.class_teacher()\n\nget_student_details(a_section_student)\nget_student_details(b_section_student)\n</code></pre>"},{"location":"programming/python/oops/#class-static-methods","title":"Class &amp; static methods","text":"<p><code>class and static methods</code> are special types of methods that have different behaviours and use cases when compared to <code>regular instances methods</code></p>"},{"location":"programming/python/oops/#regularinstance-method","title":"regular/instance method","text":"<p>these are bound to regular object instance. they can access and modify the object's state and can be called on an object instance. </p> <pre><code>class Person:\n    def __init__(self, name, age):\n        # instance variables\n        self.name = name\n        self.age=age \n\n    # instance methods\n    def get_name(self):\n        return self.name\n\n    # instance methods\n    def get_age(self):\n        return self.age\n\n    # instance methods\n    def set_name(self,newname):\n        self.name = newname\n\n    # instance methods\n    def set_age(self, newage):\n        self.age = newage\n\n\nperson1 = Person(\"Sunil\", 30)   \nprint(person1.get_name())\nprint(person1.get_age())\nperson1.set_name(\"Kumar\")\nprint(person1.get_name())\n</code></pre>"},{"location":"programming/python/oops/#class-methods","title":"class methods","text":"<p>class methods are bound to class itself and they can access only class variables. It can only allow to change the class variable state across all the class objects.</p> <p>Class methods are used when we are dealing with factory methods. factory methods are those which returns the class object for different purposes.</p> <p>They are always called using <code>ClassName.method_name()</code></p> <p></p> <pre><code>from datetime import date \n\nclass Student:\n    collage_name = \"ABC Collage\"\n    def __init__(self,name,age):\n        self.name = name\n        self.age = age \n\n    @classmethod\n    def calculate_age(cls,name,birthyear):\n        # calulate age and set it as age, then \n        # return a new object\n        return cls(name,date.today().year-birthyear)\n\n\n    def show(self):\n        print(f\"{self.name} age is: {self.age} studying in colleage: {Student.collage_name}\")\n\n\nsunil = Student(\"Sunil\",39)\nsunil.show()\n\nshiva=Student.calculate_age(\"Shiva\",1983) # invoke a new clas\nshiva.show()\n</code></pre> <p>Explanation of above code. </p> <ul> <li> <p>we created two objects, one using the constructor and the second using the <code>calculate_age()</code></p> </li> <li> <p>The constructor takes two arguments name and age. On the other hand, class method takes cls, name, and birth_year and <code>returns a class instance which nothing but a new object</code></p> </li> <li> <p>The <code>@classmethod</code> decorator is used for converting <code>calculate_age() method to a class method.</code></p> </li> <li> <p>The <code>calculate_age()</code> method takes Student class (cls) as a first parameter and <code>returns constructor by calling Student(name, date.today().year - birthYear), which is equivalent to Student(name, age).</code></p> </li> </ul>"},{"location":"programming/python/oops/#static-method","title":"static method","text":"<p>Any method we create in a class will automatically be created as an instance method. We must explicitly tell Python that it is a static method using the <code>@staticmethod</code></p> <pre><code>class Student:\n    @staticmethod\n    def student_greeting(greeting_msg):\n        print(f\"Hello {greeting_msg}\")\n\n\nStudent.student_greeting(\"Welcome Students !\")\n\nsunil=Student()\nsunil.student_greeting('Welcome Sunil !')\n</code></pre> <pre><code>class Employee:\n    def __init__(self,name,project_name):\n        self.name = name \n        self.project_name=project_name\n\n    # instance method\n    def work(self):\n        requirements = self.gather_requirements(self.project_name)\n        for task in requirements:\n            print(f\"Completed: {task}\")\n\n\n    @staticmethod\n    def gather_requirements(project_name):\n        if project_name == \"ABC\":\n            requirements = [\"task1\", \"task2\"]\n        else:\n            requirements = [\"task1\"]\n\n        return requirements\n\nemp = Employee(\"Sunil\",\"ABC\")\nemp.work()\n</code></pre> <p>output:</p> <pre><code>Completed: task1\nCompleted: task2\n</code></pre>"},{"location":"programming/python/oops/#access-class-variables-in-class-methods","title":"Access Class Variables in Class Methods","text":"<pre><code>@classmethod\ndef change_collage(cls, newcollname):\n    # change class variable\n    cls.collage_name=newcollname\n\nStudent.change_collage(\"New coll\")\nsunil.show()\n</code></pre>"},{"location":"programming/python/oops/#another-example-of-static-and-class-methods","title":"another example of static and class methods","text":""},{"location":"programming/python/oops/#classmethod","title":"classmethod","text":"<p>A class method is a method that takes the class itself as the first argument, rather than an instance of the class. It is useful when you want to perform operations that involve the class itself (for example, modifying class variables or calling other class methods). Class methods are defined using the @classmethod decorator, and the first argument is conventionally named cls.</p> <p>When to use @classmethod: When you need to work with the class as a whole (like creating factory methods, manipulating class-level data, or altering class attributes). When you want to create alternative constructors. When the logic is related to the class, not specific instances.</p> <pre><code>class Server:\n    server_count = 0  # Class variable to track how many servers exist\n\n    def __init__(self, name, ip):\n        self.name = name\n        self.ip = ip\n        Server.server_count += 1\n\n    @classmethod\n    def create_default_server(cls):\n        return cls(\"DefaultServer\", \"192.168.0.1\")\n\n# Example usage\nserver1 = Server(\"MainServer\", \"192.168.1.1\")\nserver2 = Server.create_default_server()  # Using class method to create an instance\n\nprint(server1.name)  # Output: MainServer\nprint(server2.name)  # Output: DefaultServer\nprint(Server.server_count)  # Output: 2\n\n</code></pre>"},{"location":"programming/python/oops/#staticmethod","title":"staticmethod","text":"<p>A static method does not receive any special first argument (neither self nor cls). It behaves like a regular function but belongs to the class's namespace. You use a static method when the method's logic neither depends on the instance nor the class, but you want to group it inside the class for logical reasons.</p> <p>When to use @staticmethod:</p> <p>When you need utility functions that are logically related to the class but don't need access to instance (self) or class (cls) data. When the method doesn't modify class or instance state.</p> <pre><code>class Server:\n\n    def __init__(self, name, ip):\n        self.name = name\n        self.ip = ip\n\n    @staticmethod\n    def is_valid_ip(ip):\n        parts = ip.split(\".\")\n        return len(parts) == 4 and all(0 &lt;= int(part) &lt; 256 for part in parts)\n\n# Example usage\nprint(Server.is_valid_ip(\"192.168.1.1\"))  # Output: True\nprint(Server.is_valid_ip(\"999.999.999.999\"))  # Output: False\n\n</code></pre> <p>Summary:</p> <p>Class Method (@classmethod): Use when you need to operate on the class itself. The method has access to the class via cls.</p> <p>Static Method (@staticmethod): Use when the logic doesn't need to access the instance (self) or class (cls). It is more like a utility function within the class.</p>"},{"location":"programming/python/qa/","title":"Q&A","text":""},{"location":"programming/python/qa/#strings","title":"Strings","text":"<p>my_string = \"In 2010, someone paid 10k Bitcoin for two pizzas.\"</p> <pre><code>print(my_string[-1]) # get the last character in the string.\nprint(my_string[7]) # return the comma character from the string.\nprint(my_string.index(\"B\")) # index of the B character in the string.\nprint(my_string.count('o')) # number of occurrences of the letter o in the string.\nprint(my_string.upper()) # convert all letters in the string to uppercase.\nprint(my_string.find('Bitcoin')) # index at which the substring Bitcoin starts.\nprint(my_string.index('Bitcoin')) # index at which the substring Bitcoin starts.\nprint(my_string.startswith('X')) # check of the string starts with the letter X\nprint(my_string.swapcase()) # convert all uppercase letters to lowercase and viceversa\nprint(my_string.replace(\" \", \"\")) # remove all spaces from the string\nprint(\"&amp;\".join(my_string)) # join the characters of the string using the &amp; symbol as a delimiter.\nprint(my_string.title()) # convert the first letter of each word in the string to uppercase.\nprint(my_string[::7]) # return every 7th character of the string, starting with the first character.\nprint(my_string[10::]) # return the string except the first 10 characters\nprint(my_string[:-4]) # return the string except the last 4 characters\nprint(my_string[-9::]) # return the last 9 characters of the string\nprint(my_string[:12]) # return the first 12 characters in the string\nmy_other_string = \"Poor guy!\"\nprint(my_string+my_other_string) # concatenate strings\n</code></pre>"},{"location":"programming/python/qa/#lists","title":"lists","text":"<pre><code># array[start:end:step_count]\n\na = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n#i= [ 0 ,  1 ,  2 ,  3 ,  4 ,  5 ,  6 ,  7 ]\n#r= [-8 , -7 , -6 , -5 , -4 , -3 , -2 , -1 ]\nprint('Middle two:  ', a[3:5])\nprint('All but ends:', a[1:7])\na[:]      # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\na[:5]     # ['a', 'b', 'c', 'd', 'e']\na[:-1]    # ['a', 'b', 'c', 'd', 'e', 'f', 'g']\na[4:]     #                     ['e', 'f', 'g', 'h']\na[-3:]    #                          ['f', 'g', 'h']\na[2:5]    # ['c', 'd', 'e']\na[2:-1]   # ['c', 'd', 'e', 'f', 'g']\na[-3:-1]  # ['f', 'g']\na[::2]    # ['a','c', 'e', 'g'] # even numbers ::2 means \u201cSelect every second item \n          # starting at the beginning.\na[1::2]   # ['b','d', 'f', 'h'] # odd numbers \na[::-2]   # ['h', 'f', 'd', 'b'] # ::-2 means \u201cSelect every second item starting at the \n          # end and moving backward.\na[2::2]     # ['c', 'e', 'g']\na[-2::-2]   # ['g', 'e', 'c', 'a']\na[-2:2:-2]  # ['g', 'e']\na[2:2:-2]   # []\n</code></pre> <p>Given the code below, use the correct function on line 3 in order to find out the largest number in my_list.</p> <pre><code>my_list = [10, 10.5, 20, 30, 25.6, 19.25, 11.01, 29.99]\n\nprint(sorted(my_list)) # ascending sort\nprint(sorted(my_list), reverse=True) # descending sort\nprint(sorted(my_list)[-1]) # largest element\nprint(sorted(my_list)[0]) # smallest element\nprint(sum(my_list)) # sum of elements \nprint(set(my_list)) # remove duplicates\nprint(my_list.clear()) # delete all elemenents in list\n</code></pre> <ul> <li>add the elements of [30.01, 30.02, 30.03] to my_list and multiply the resulting list by 2.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 25.6, 19.25, 11.01, 29.99]\nadd = (my_list+[30.01, 30.02, 30.03])*2\nprint(add)\n</code></pre> <ul> <li>return the element 20 from my_list based on its index.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 'Python', 'Java', 'Ruby']\n\nelement = my_list[my_list.index(20)]\nor\nmy_list[2]\n\nprint(element)\n</code></pre> <ul> <li>return a slice made of [30, 'Python', 'Java'] from my_list based on negative indexes.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 'Python', 'Java', 'Ruby']\n\nmy_slice = my_list[-4:-2] # [30, 'Python', 'Java']\nmy_slice=my_list[-4:] # [30, 'Python', 'Java', 'Ruby']\nmy_slice=my_list[3:] # [30, 'Python', 'Java', 'Ruby']\nmy_slice=my_list[3:] # [10, 10.5, 20]\nmy_slice=my_list[:-4] # [10, 10.5, 20]\nmy_slice = my_list[0:5] # [10, 10.5, 20, 30, 'Python']\nmy_slice = my_list[0::3] # every 3rd element [10, 30, 'Ruby']\nmy_slice = my_list[-1::-4] # every 4th element from last element #['Ruby', 20]\nmy_slice = my_list[2:5] # consecutive element [20, 30, 'Python']\n</code></pre> <ul> <li>merge two lists</li> </ul> <pre><code>a = [3, 4, 6, 10, 11, 18]\nb = [1, 5, 7, 12, 13, 19, 21]\na.extend(b)\n\n# remove duplicate and merge two lists\nsorted(list(set(a+b)))\n\n# print common elemenents\nprint(set(a)&amp;set(b))\n</code></pre> <ul> <li>sort list by lengths</li> </ul> <pre><code>newlist=[\"sunil\",\"kua\",\"kumar\",\"ku\",\"kumaraswamy\",\"ramaswamy\",\"ramaswamykumaraswamy\"]\n\ndef sort_list_by_length(list):\n    list.sort(key=len)\n    return list\n\n# revese sort by length\ndef sort_list_by_length(list):\n    list.sort(key=len, reverse=True)\n    return list\n</code></pre>"},{"location":"programming/python/qa/#sets","title":"sets","text":"<p>my_set = {1, 4, 6, 5, 9, 0, 8, 3, 2, 7, 11}</p> <pre><code>my_set.add(19) # add element\nmy_set.remove(19) # delete element\n</code></pre> <pre><code>my_set1 = {1, 4, 6, 5, 9, 0, 8, 3, 2, 7, 11}\nmy_set2 = {12, 9, 4, 2, 0, 6}\ncommon = my_set1.intersection(my_set2)  # Common elements \njoin = my_set1.union(my_set2) # joining\n</code></pre> <p>find out the elements of my_set2 that are not members of my_set1.</p> <pre><code>diff = my_set2.difference(my_set1)\n</code></pre>"},{"location":"programming/python/qa/#tuples","title":"tuples","text":"<pre><code>my_tup = (\"Romania\", \"Poland\", \"Estonia\", \"Bulgaria\", \"Slovakia\", \"Slovenia\", \"Hungary\")\n\nnumber = my_tup.len(my_tup) # count of tuples\nindex = my_tup.index('Slovakia') # find out the index of Slovakia in my_tup.\nlast = max(my_tup) # find out the last element of my_tup in alphabetical order.\nnumber = my_tup.count('Estonia') # find out the number of occurrences of Estonia in my_tup.\nmy_slice = my_tup[2:] # return all the elements of my_tup, except the first two of them\nmy_slice = my_tup[-5::] # from negative index, return all the elements of my_tup, except the first two of them\n\nprint(number)\n</code></pre>"},{"location":"programming/python/qa/#ranges","title":"ranges","text":"<pre><code>print(list(range(10))) #[0,1,2..10]\nprint(list(range(0,10)))  #[0,1,2..10]\nprint(list(range(10,22,3))) # [10, 13, 16, 19] step function increase by 3\nmy_range = range(115, 125, 5) # return [115, 120] when converted to a list.\nmy_range = range(-75, -25,15 ) # return [-75, -60, -45, -30] \nmy_range = range(-25, 139, 30) # return [-25, 5, 35, 65, 95, 125]\nmy_range = range(-10,-9) # return [-10]\n</code></pre>"},{"location":"programming/python/qa/#dictionaries","title":"dictionaries","text":"<pre><code>crypto = {1: \"Bitcoin\", 2: \"Ethereum\", 3: \"Litecoin\", 4: \"Stellar\", 5: \"XRP\"}\n\ncrypto.pop(3) #  delete the key-value pair associated with key 3\ndel crypto[3] #  delete the key-value pair associated with key 3\nadd = sum(crypto) # sum of all the keys in the dictionary.\nval = crypto.values() # get a list of all the values in the dictionary.\nkey = min(crypto) #smallest key in the dictionary.\ncrypto.popitem() # arbitrary key-value pair from the dictionary.\n</code></pre>"},{"location":"programming/python/qa/#data-types","title":"data types","text":"<pre><code>value = 10\nconv = bin(value) # convert value to a binary representation\nconv = hex(value) # hexadecimal\nconv = int(value,2) # decimal\n</code></pre>"},{"location":"programming/python/qa/#conditions","title":"conditions","text":"<p>write code that prints out True! if x is a string and the first character in the string is T</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\nif type(x)==str and x[0] == 'T':\n    print(\"True!\")\n</code></pre> <p>write code that prints out True! if at least one of the following conditions occurs: - the string contains the character z - the string contains the character y at least twice</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif \"z\" in x or x.count(\"y\") &gt;= 2:\nprint(\"True!\")\n</code></pre> <p>write code that prints out True! if the index of the first occurrence of letter f is less than 10 and prints out False!</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif x.index('f') &lt; 10:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the last 3 characters of the string are all digits and prints out False!</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif x[-3:].isdigit():\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if x has at least 8 elements and the element positioned at index 6 is a floating-point number and prints out False! </p> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif len(x) &gt;= 8 and type(x[6]) is float:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the second string of the first list in x ends with the letter h and the first string of the second list in x also ends with the letter h, and prints out False! </p> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x[3][1].endswith(\"h\") and x[7][0].endswith(\"h\"):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if one of the following two conditions are satisfied and prints out False! otherwise.</p> <ul> <li>the third string of the first list in x ends with the letter h</li> <li>the second string of the second list in x also ends with the letter h</li> </ul> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x[3][2].endswith('h') or x[7][1].endswith('h'):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the largest value among the first 3 elements of the list is less than or equal to the smallest value among the next 3 elements of the list. Otherwise, print out False!</p> <pre><code>x = [115, 115.9, 116.01, 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif max(x[:3]) &lt;= min(x[3:6]):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if 115 appears at least once inside the list or if it is the first element in the list. Otherwise, print out False!</p> <pre><code>x = [115, 115.9, 116.01, 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x.count(115) &gt;= 1 or x.index(115) == 0:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the value associated with key number 5 is Perl or the number of key-value pairs in the dictionary divided by 5 returns a remainder less than 2. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[5] == \"Perl\" or len(x) % 5 &lt; 2:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if 3 is a key in the dictionary and the smallest value (alphabetically) in the dictionary is C#. Otherwise, print out False!</p> <p>```python  x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}</p> <p>if 3 in x and sorted(x.values())[0] == \"C#\":     print(\"True!\") else:     print(\"False!\")  ```</p> <p>write code that prints out True! if the last character of the largest (alphabetically) value in the dictionary is n. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sorted(x.values())[-1][-1] == \"n\":\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the largest key in the dictionary divided by the second largest key in the dictionary returns a remainder equal to the smallest key in the dictionary. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sorted(x.keys())[-1] % sorted(x.keys())[-2] == sorted(x.keys())[0]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the sum of all the keys in the dictionary is less than the number of characters of the string obtained by concatenating the values associated with the first 5 keys in the dictionary. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sum(x) &lt; len(x[1] + x[2] + x[3] + x[4] + x[5]):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the 3rd element of the first range is less than 2, prints out False! if the 5th element of the first range is 5, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[0][2] &lt; 2:\n    print(\"True!\")\nelif x[0][4] == 5:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the 3rd element of the 3rd range is less than 6, prints out False! if the 1st element of the second range is 5, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[2][2] &lt; 6:\n    print(\"True!\")\nelif x[1][0] == 5:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the last element of the first range is greater than 3, prints out False! if the last element of the second range is less than 9, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[0][-1] &gt; 3:\n    print(\"True!\")\nelif x[1][-1] &lt; 9:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the length of the first range is greater than or equal to 5, prints out False! if the length of the second range is 4, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif len(x[0]) &gt;= 5:\n    print(\"True!\")\nelif len(x[1]) == 4:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the sum of all the elements of the first range is greater than the sum of all the elements of the third range, prints out False! if the largest element of the second range is greater than the largest element of the third range, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif sum(x[0]) &gt; sum(x[2]):\n    print(\"True!\")\nelif max(x[1]) &gt; max(x[2]):\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the largest element of the first range minus the second element of the 3rd range is equal to the first element of the first range, prints out False! if the length of the first range minus the length of the 2nd range is equal to the first element of the 3rd range, prints out Maybe! if the sum of all the elements of the 3rd range divided by 2 returns a remainder of 0, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif max(x[0]) - x[2][1] == x[0][0]:\n    print(\"True!\")\nelif len(x[0]) - len(x[1]) == x[2][0]:\n    print(\"False!\")\nelif sum(x[2]) % 2 == 0:\n    print(\"Maybe!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the sum of the last 3 elements of the first range plus the sum of the last 3 elements of the 3rd range is equal to the sum of the last 3 elements of the 2nd range, and prints out False! if the length of the first range times 2 is less than the sum of all the elements of the 3rd range.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif sum(x[0][-3:]) + sum(x[2][-3:]) == sum(x[1][-3:]):\n    print(\"True!\")\nelif len(x[0]) * 2 &lt; sum(x[2]):\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the 2nd character of the value at key 1 is also present in the value at key 4, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[1][1] in x[4]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the second to last character of the value at key 3 is the first character of the value at key 5, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[3][-2] == x[5][0]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the number of characters of the smallest value in the dictionary is equal to the number of occurrences of letter a in the value at key 3, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif len(min(x.values())) == x[3].count(\"a\"):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre>"},{"location":"programming/python/qa/#loops","title":"loops","text":"<p>Write a for loop that iterates over the x list and prints out all the elements of the list in reversed order and multiplied by 10.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\nfor i in sorted(x,reverse=True):\n    print(i*10)\n</code></pre> <p>Write a for loop that iterates over the x list and prints out all the elements of the list divided by 2 and the string Great job! after the list is exhausted.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\nfor i in x:\n    print(i / 2)\nelse:\n    print(\"Great job!\")\n</code></pre> <p>Write a for loop that iterates over the x list and prints out the index of each element.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor index,item in enumerate(x):\n    print(index)\n\nx = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor i in x:\n    print(x.index(i))\n</code></pre> <p>Write a while loop that prints out the value of x squared while x is less than or equal to 5. Be careful not to end up with an infinite loop!</p> <pre><code>x = 0\n\nwhile x &lt;= 5:\n    print(x ** 2)\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the value of x times 10 while x is less than or equal to 4 and then prints out Done! when x becomes larger than 4. Be careful not to end up with an infinite loop!</p> <pre><code>x = 0\n\nwhile x &lt;= 4:\n    print(x * 10)\n    x = x + 1\nelse:\n    print(\"Done!\")\n</code></pre> <p>Write a while loop that prints out the value of x plus 10 while x is less than or equal to 15 and the remainder of x divided by 5 is 0. Be careful not to end up with an infinite loop!</p> <pre><code>x = 10\n\nwhile x &lt;= 15 and x % 5 == 0:\n    print(x + 10)\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the absolute value of x while x is negative. Be careful not to end up with an infinite loop!</p> <pre><code>x = -7\n\nwhile x &lt; 0:\n    print(abs(x))\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the value of x times y while x is greater than or equal to 5 and less than 10, and prints out the result of x divided by y when x becomes 10. Be careful not to end up with an infinite loop!</p> <pre><code>x = 5\ny = 2\n\nwhile x &gt;= 5 and x &lt; 10:\n    print(x * y)\n    x = x + 1\nelse:\n    print(x / y)\n</code></pre> <p>Write code that will iterate over the x list and multiply by 10 only the elements that are greater than 20 and print them out to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor i in x:\n    if i &gt; 20:\n        print(i * 10)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6]\ny = [5, 10]\n\nfor i in x:\n    for j in y:\n        print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y that is less than 12, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if j &lt; 12:\n            print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x that is greater than 5 with each element of y that is less than 12, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if i&gt; 5 and j &lt; 12:\n            print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y that is less than or equal to 10, also printing the results to the screen. For y's elements that are greater than 10, multiply each element of x with y squared.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if j &lt;= 10:\n            print(i * j)\n        else:\n            print(i * j ** 2)\n</code></pre> <p>Write code that will print out each character in x doubled if that character is also inside y.</p> <p>Hint: use nesting!</p> <pre><code>x = \"cryptocurrency\"\ny = \"blockchain\"\n\nfor i in x:\n    if i in y:\n        print(i * 2)\n</code></pre> <p>Write code that will iterate over the range generated by range(9) and for each element that is between 3 and 7 inclusively print out the result of multiplying that element by the second element in the same range.</p> <p>Hint: use nesting!</p> <pre><code>my_range = range(9)\n\nprint(newrange)\nfor i in my_range:\n    if 3 &lt;= i &lt;= 7:\n        print(i * my_range[1])\n</code></pre> <p>Write code that will iterate over the range starting at 1, up to but not including 11, with a step of 2, and for each element that is between 3 and 8 inclusively print out the result of multiplying that element by the last element in the same range. For any other element of the range (outside [3-8]) print Outside!</p> <p>Hint: use nesting!</p> <pre><code>for i in range(1,11,2):\n    if 3 &lt;= i &lt;= 8:\n        print(i * range(1,11,2)[-1])\n    else:\n        print(\"Outside!\")\n</code></pre> <p>Write code that will iterate over the range starting at 5, up to but not including 25, with a step of 5, and for each element that is between 10 and 21 inclusively print out the result of multiplying that element by the second to last element of the same range. For any other element of the range (outside [10-21]) print Outside! Finally, after the entire range is exhausted print out The end!</p> <p>Hint: use nesting!</p> <pre><code>for i in range(5,25,5):\n    if 10 &lt;= i &lt;= 21:\n        print(i * range(5,25,5)[-2])\n    else:\n        print(\"Outside!\")\nelse:\n    print(\"The end!\")\n</code></pre> <p>Write a while loop that prints out the value of x times 11 while x is less than or equal to 11.  When x becomes equal to 10, print out x is 10! Be careful not to end up with an infinite loop!</p> <pre><code>x = 5\n\nwhile x &lt;= 11:\n    if x == 10:\n        print(\"x is 10!\")\n        x = x + 1\n    else:\n        print(x * 11)\n        x = x + 1\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 1 100 20 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n            break\n        print(i)\n    print(j)\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 10 20 2 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n        print(i)\n        break\n    print(j)\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 1 100 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            break\n            print(i * j)\n        print(i)\n    print(j)\n</code></pre> <p>Insert a continue statement where necessary in order to obtain the following result:</p> <p>1 1 100 20 200 100</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n            continue\n        print(i)\n    print(j)\n</code></pre> <p>Insert a continue statement where necessary in order to obtain the following result:</p> <p>1 1 100 100</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            continue\n            print(i * j)\n        print(i)\n    print(j)\n</code></pre>"},{"location":"programming/python/qa/#exceptions","title":"exceptions","text":"<p>Add the necessary clause(s) to the code below so that in case the code under try raises no exceptions then the program prints out the result of the math operation and the string Clean! to the screen.</p> <pre><code>try:\n    print(25 % 5 ** 5 + 5)\nexcept:\n    print(\"Bug!\")\nelse:\n    print(\"Clean!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that no matter if the code under try raises any exceptions or not, then the program prints out the string Result! to the screen.</p> <pre><code>try:\n    print(25 % 0 ** 5 + 5)\nexcept:\n    print(\"Bug!\")\nfinally:\n    print(\"Result!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that in case the code under try raises the ZeroDivisionError exception then the program prints out the string Zero! to the screen; additionally, if the code under try raises the IndexError exception then the program prints out the string Index! to the screen.</p> <pre><code>x = [1, 9, 17, 32]\n\ntry:\n    print(x[3] % 3 ** 5 + x[4])\nexcept ZeroDivisionError:\n    print(\"Zero!\")\nexcept IndexError:\n    print(\"Index!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that in case the code under try raises no exceptions then the program prints out the result of the math operation and the string Clean! to the screen. If the code under try raises the ZeroDivisionError exception then the program prints Zero! to the screen. Ultimately, regardless of the result generated by the code under try, the program should print out Finish! to the screen.</p> <pre><code>try:\n    print(25 % 5 ** 5 + 5)\nexcept ZeroDivisionError:\n    print(\"Zero!\")\nelse:\n    print(\"Clean!\")\nfinally:\n</code></pre>"},{"location":"programming/python/qa/#functions","title":"functions","text":"<p>Implement a function called my_func() that takes a single parameter x and multiplies it with each element of range(5), also adding each multiplication result to a new (initially empty) list called my_new_list. Finally, the list should be printed out to the screen after the function is called.</p> <pre><code>def my_func(x):\n    my_new_list = []\n    for i in range(5):\n        my_new_list.append(i * x)\n    return my_new_list\n\nresult = my_func(2)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single parameter x (a tuple) and for each element of the tuple that is greater than 4 it raises that element to the power of 2, also adding it to a new (initially empty) list called my_new_list. Finally, the code returns the result when the function is called.</p> <pre><code>def my_func(x):\n    my_new_list = []\n    for i in x:\n        if i &gt; 4:\n            my_new_list.append(i ** 2)\n    return my_new_list\n\nresult = my_func((2, 3, 5, 6, 4, 8, 9))\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single parameter x (a dictionary) and multiplies the number of elements in the dictionary with the largest key in the dictionary, also returning the result when the function is called.</p> <pre><code>def my_func(x):\n    return len(x) * sorted(x.keys())[-1]\n\nresult = my_func({1: 3, 2: 3, 4: 5, 5: 9, 6: 8, 3: 7, 7: 0})\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single positional parameter x and a default parameter y which is equal to 10 and multiplies the two, also returning the result when the function is called.</p> <pre><code>def my_func(x, y = 10):\n    return x * y\n\nresult = my_func(5)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single positional parameter x and two default parameters y and z which are equal to 100 and 200 respectively, and adds them together, also returning the result when the function is called</p> <pre><code>def my_func(x, y = 100, z = 200):\n    return x + y + z\n\nresult = my_func(50)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes two default parameters x (a list) and y (an integer), and returns the element in x positioned at index y, also printing the result to the screen when called.</p> <pre><code>def my_func(x: list, y:int):\n    return x[y]\n\nresult = my_func(list(range(2,25,2)), 4)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a positional parameter x and a variable-length tuple of parameters and returns the result of multiplying x with the second element in the tuple, also returning the result when the function is called.</p> <pre><code>def my_func(x, *args):\n    return x * args[1]\n\nresult = my_func(5, 10, 20, 30, 50)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a positional parameter x and a variable-length dictionary of (keyword) parameters and returns the result of multiplying x with the largest value in the dictionary, also returning the result when the function is called.</p> <pre><code>def my_func(x, **kwargs):\n    return x * sorted(kwargs.values())[-1]\n\nresult = my_func(10, val1 = 10, val2 = 15, val3 = 20, val4 = 25, val5 = 30)\nprint(result)\n</code></pre> <p>Write code that will import only the pi variable from the math module and then it will format it in order to have only 4 digits after the floating point. Of course, print out the result to the screen using the print() function.</p> <pre><code>from math import pi\n\nprint(\"%.4f\" % pi)\n</code></pre>"},{"location":"programming/python/qa/#files","title":"files","text":""},{"location":"programming/python/qa/#regular-expressions","title":"regular expressions","text":"<p>s = \"Bitcoin was born on Jan 3rd 2009 as an alternative to the failure of the current financial system. In 2017, the price of 1 BTC reached $20000, with a market cap of over $300B.\"</p> <pre><code>import re\n\nresult = re.match(\"Bitcoin\", s) # match 'Bitcoin' at start\nresult = re.match(r\"B.{6} .{3}\", s) # match 'Bitcoin' using dot syntax\nresult = re.match(\"Bitcoin\", s, re.I) # match 'Bitcoin' at start of str ignore case\nresult = re.search(r\"(\\d{4})\\s\", s) # match the year `2009`\nresult = re.search(r\"(\\d{4}),\", s) # search 2017 \nresult = re.search(r\"(.{3}\\s\\d\\w\\w\\s\\d{4})\\s\", s) # match the date Jan 3rd 2009\nresult = re.search(r\"([A-Z]{3})\", s) # match BTC in the string\nresult = re.search(r\"([0-9]\\s[A-Z]{3})\", s) # match 1 BTC in the string\nresult = re.search(r\"(\\$\\d{5}),\", s) # match $20000\nresult = re.search(r\"(\\$\\d{3}[A-Z])\\.\", s) # match $300B\nresult = re.search(r\"\\s(.{6} .{3} .{2})\\s\", s) # match market cap\nprint(result.group())\n</code></pre> <p>s = \"Bitcoin, Market Cap: $184,073,529,068, Price: $10,259.02, Volume 24h: $15,670,986,269, Circulating Supply: 17,942,600 BTC, Change 24h: 0.10%\"</p> <pre><code>import re\n\nresult = re.search(r\"\\$(\\d{3},[0-9]{3},\\d{3},[0-9]{3}),\", s) #  match 184,073,529,068\nresult = re.search(r\"\\$(\\d{1,3},\\d{1,3}\\.\\d{1,3}),\", s) # match 10,259.02\nresult = re.search(r\"\\s([0-9]{2},[0-9]{3},[0-9]{3}\\s.{3}),\", s) # match 17,942,600 BTC\nresult = re.search(r\"\\s(.{4}\\s\\d\\.\\d\\d%)\", s) # match 24h: 0.10%\n\n# match Volume 24h: $15,670,986,269\nresult = re.search(r\"\\.\\d\\d, (.{1,}:\\s\\$\\d{2,},\\d{2,},\\d{2,},\\d{2,}), \", s)\n\n# match Circulating Supply: 17,942,600 BTC \nresult = re.search(r\"(\\w+ \\w+: \\d{2}.+? [A-Z]{3}), \", s) \n\nresult = re.search(r\",([0-9]{3}\\.[0-9]{2},\\s.)\", s) # match 259.02, V \nresult = re.findall(r\"\\s(\\d{4})\", s) # match all the years\nresult = re.findall(r\"\\d{1,}\", s) # match all the numbers (3, 2009\nresult = re.findall(r\"\\s(\\w{3})\\s\", s) # match all the three-letter words\nresult = re.findall(r\"([A-Z]{1}.+?)\\s\", s) # match all the words starting with an uppercase letter\nresult = re.findall(r\"\\s(o.{1})\\s\", s) # match all the two-letter words starting with the letter o\nresult = re.findall(r\"\\w{8,}\", s) # match all the words that have at least 8 characters\n\n# match all the words starting with a or c and that have at least 3 letters\nresult = re.findall(r\"\\s([ac]\\w{2,})\\s\", s) \n\nresult = re.sub(r\"\\s\\d{4}\", \" XXXX\", s) # replace all the years in the string with XXXX\nprint(result.group(1))\n</code></pre> <p>s = \"Bitcoin was born on Jan 3rd 2009 as an alternative to the failure of the current financial system. In 2017, the price of 1 BTC reached $20000, with a market cap of over $300B. Bitcoin, Market Cap: $184,073,529,068, Price: $10,259.02, Volume 24h: $15,670,986,269, Circulating Supply: 17,942,600 BTC, Change 24h: 0.10%\"</p> <pre><code>import re\n\n# replace each floating-point number in the string (10,259.02 and 0.10) with a dot (.) \nresult = re.sub(r\"\\d{1,},*\\d*\\.\\d{1,}\", \".\", s)\n\n# replace all occurrences of BTC in the string with Bitcoin\nresult = re.sub(r\"[A-Z]{3}\", \"Bitcoin\", s)\n\n# replace all the digits less than or equal to 5 in the string with 8\nresult = re.sub(r\"[0-5]\", \"8\", s)\n\n# replace all the words starting with an uppercase letter or digits greater than or equal to 6 in the string with W\nresult = re.sub(r\"[A-Z]\\w{1,}|[6-9]\", \"W\", s)\n\nprint(result)\n</code></pre>"},{"location":"programming/python/qa/#classes","title":"classes","text":"<p>Write a class called ClassOne starting on line 1 containing:</p> <p>The init method with two parameters p1 and p2. Define the corresponding attributes inside the init method.</p> <p>A method called square that takes one parameter p3 and prints out the value of p3 squared.</p> <pre><code>class ClassOne:\n    def __init__(self,p1:int, p2:int):\n        self.p1=p1\n        self.p2=p2\n\n    def square(p3:int):\n        print(p3**2)\n\np = ClassOne(1, 2)\nprint(type(p))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to access the p1 attribute for the current instance of the class and print its value to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(p.p1)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to call the square() method for the current instance of the class using 10 as an argument and print the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\np.square(10)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to set the value of the p2 attribute to 5 for the current instance of the class, without using a function.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\np.p2 = 5\n\nprint(p.p2)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on lines 11 and 12 in order to set the value of the p2 attribute to 50 for the current instance of the class using a function, and then get the new value of p2, again using a function, and print it out to the screen as well.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nsetattr(p, 'p2', 50)\nprint(getattr(p, 'p2'))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to check if p2 is an attribute of p, using a function, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(hasattr(p, 'p2'))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 to check if p is indeed an instance of the ClassOne class, using a function, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(isinstance(p, ClassOne))\n</code></pre> <p>Considering the ClassOne class, write code starting on line 9 to create a child class called ClassTwo that inherits from ClassOne and also has its own method called times10() that takes a single parameter x and prints out the result of multiplying x by 10.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        print(x * 10)\n\ny = ClassTwo(10, 20)\nprint(y.p1)\n</code></pre> <p>Considering the ClassOne and ClassTwo classes, where the latter is a child of the former, write code on line 15 in order to call the times10() method from the child class having x equal to 45, also printing the result to the screen</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        return x * 10\n\nobj = ClassTwo(15, 25)\n\nprint(obj.times10(45))\n</code></pre> <p>Considering the ClassOne and ClassTwo classes, write code on line 13 to verify that ClassTwo is indeed a child of ClassOne, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        return x * 10\n\nprint(issubclass(ClassTwo, ClassOne))\n</code></pre>"},{"location":"programming/python/qa/#other-concepts","title":"other concepts","text":"<p>Write a list comprehension on line 1 that will iterate over range(1, 5) and return a list of its elements.</p> <pre><code>cph = [i for i in range(1, 5)]\nprint(cph)\n</code></pre> <p>Write a list comprehension on line 1 that will iterate over range(1, 15, 5) and return a list of its elements squared.</p> <pre><code>cph = [i**2 for i in range(1,15,5)]\nprint(cph)\n</code></pre> <p>Write a list comprehension on line 1 that will iterate over range(5, 25, 3) and return a list of its elements squared only for the elements that are less than or equal to 16.</p> <pre><code>cph = [i ** 2 for i in range(5, 25, 3) if i &lt;= 16]\n\nprint(cph)\n</code></pre> <p>Write a dictionary comprehension on line 1 that will iterate over range(9) and return a dictionary of key-value pairs where the value is equal to the key times 3.</p> <pre><code>cph = {x: x * 3 for x in range(9)}\n\nprint(cph)\n</code></pre> <p>Write a set comprehension on line 1 that will iterate over range(10, 19) and return a set of its elements divided by 2.5.</p> <pre><code>cph = {x / 2.5 for x in range(10, 19)}\n\nprint(cph)\n</code></pre> <p>Write a lambda function on line 1 that takes two parameters x and y and multiplies x with y.</p> <pre><code>lam = lambda x, y: x * y\n\nprint(lam(2, 5))\n</code></pre> <p>Write a lambda function on line 1 that takes a list list1 as a parameter, and multiplies each element of range(1, 5) with each element of list1 using a list comprehension.</p> <pre><code>lam = lambda list1: [x * y for x in range(1, 5) for y in list1]\n\nprint(lam([1, 2]))\n</code></pre> <p>Use the correct function from the itertools module on line 6, in between the parentheses of list(), in order to concatenate list1 and list2.</p> <pre><code>import itertools\n\nlist1 = [1, 2, 3]\nlist2 = [4, 5]\n\nresult = list(itertools.chain(list1, list2))\n\nprint(result)\n</code></pre> <p>Use the correct function from the itertools module with a for loop and a nested if/else block in order to return all the numbers starting at 20 and up to 31 with a step of 2. Be careful not to end up with an infinite loop!</p> <pre><code>import itertools\n\nfor i in itertools.count(20, 2):\n    if i &lt; 31:\n        print(i)\n    else:\n        break\n</code></pre> <p>Use the correct function from the itertools module on line 5, in between the parentheses of list(), in order to return the elements for which the lambda function given as an argument returns False.</p> <pre><code>import itertools\n\nlam = lambda x: x &lt; 5\n\nresult = list(itertools.filterfalse(lam, range(10)))\n\nprint(result)\n</code></pre>"},{"location":"programming/python/qa/#interview-essentials","title":"interview essentials","text":"<p>factorial</p> <pre><code>def fact(n):\n    if n==1: return 1 \n\n    result=n*fact(n-1)\n    return result\n\nprint(fact(5)) # 120\n</code></pre> <p>fibonacci series</p> <pre><code>def fib(n):\n    sum=0\n    a,b=0,1 \n    while a&lt;n:\n        sum+=a\n        print(a, end=\" \")\n        a,b=b,a+b\n    print()\n    print(\"sum of fibonacci numbers:\", sum)\n\nfib(90) # 0 1 1 2 3 5 8 13 21 34 55 89 \n\n# Print fibonacci numbers using recurrsion\n\ndef fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return (fib(n-1) + fib(n-2))\n\nprint(fib(6)) # 8\n</code></pre> <p>sum of fibonacci numbers</p> <pre><code>def fib(n):\n    if n &lt;= 1:\n        return n\n    else:\n        return fib(n-1) + fib(n-2)\n\nprint(fib(10))\n</code></pre> <p>string reverse</p> <pre><code>string1=\"Sunil\"\nprint(string1[::-1])\n\n# traditional method\n\nrevlist=[]\nn=len(string1)-1\ni=n\nwhile i&gt;=0 : \n    revlist.append(string1[i])\n    i-=1\nprint(\"\".join(revlist))\n</code></pre> <p>reverse sentence</p> <pre><code>ss=\"This is sunil\"\nprint(\" \".join(ss.split()[::-1]))\n</code></pre> <p>Palindrome</p> <pre><code>string=\"mom\"\nprint(\"True\") if string == string[::-1] else print(\"False\")\n</code></pre> <p>word frequency</p> <pre><code>ss = \"\"\"Nory was a Catholic because her mother was a Catholic, \nand Nory's mother was a Catholic because her father was a Catholic, \nand her father was a Catholic because his mother was a Catholic, \nor had been.\"\"\"\n\nd={}\n\nfor eachword in ss.split():\n    d[eachword]=d.get(eachword,0)+1 \n\nprint(d)\n</code></pre> <p>digit frequency</p> <pre><code>L = [1,2,4,8,16,32,64,128,256,512,1024,32768,65536,4294967296]\n\n# {1: [1, 2, 4, 8], 2: [16, 32, 64], 3: [128, 256, 512], 4: [1024], 5: [32768, 65536], 10: [4294967296]}\n\nfrom collections import defaultdict \nd1=defaultdict(list)\n\nfor i in L:\n    d1[len(str(i))].append(i)\nprint(dict(d1))\n</code></pre> <p>list element frequency</p> <pre><code>l = [ 10, 20, 30, 40, 50, 50, 60,20,40, 40, 20,20]\n\nd={}\n\nfor eachitem in l:\n    d[eachitem]=d.get(eachitem,0)+1\n\nprint(d) # {10: 1, 20: 4, 30: 1, 40: 3, 50: 2, 60: 1}\n</code></pre> <p>anagams</p> <pre><code>def is_anagram(str1, str2):\n    \"\"\"a word, phrase, or name formed by rearranging the letters of another, such as cinema, formed from iceman.\"\"\"\n    if len(str1) != len(str2):\n        return False\n    else:\n        return sorted(str1) == sorted(str2)\n\nprint(is_anagram(\"sunil\",\"linus\")) # True\n</code></pre> <p>prime numbers</p> <pre><code>def is_prime(num):\n    if num &gt; 1:\n        for i in range(2, num):\n            if (num % i) == 0:\n                print(num, \"is not a prime number\")\n                print(i, \"times\", num // i, \"is\", num)\n                break\n        else:\n            print(num, \"is a prime number\")\n\n</code></pre> <p>max product in array</p> <pre><code>def max_product(arr):\n    max_product = 0\n    for i in range(len(arr)):\n        for j in range(i + 1, len(arr)):\n            if arr[i] * arr[j] &gt; max_product:\n                max_product = arr[i] * arr[j]\n    return max_product\n\nprint(max_product([5, 20, 2, 6])) # 120\n</code></pre> <p>revese digit</p> <pre><code>def reverse(x):\n    rev = 0\n    while x &gt; 0:\n        rev = rev * 10 + x % 10\n        x //= 10\n    return rev\nprint(reverse(1234))\n</code></pre> <p>sum digit</p> <pre><code>num=123\nsum=0\n\nfor i in str(num):\n    sum+=int(i)\nprint(sum)\n</code></pre> <p>sum of even/odd numbers</p> <pre><code>print(\"even numbers sum:\",sum(list(range(0,50,2))))\nprint(\"odd numbers sum:\"sum(list(range(1,50,1))))\n\neven_nums=0\nfor i in list(range(0,50,2)):\n    even_nums+=i\nprint(even_nums)\n</code></pre> <p>Count vovels</p> <pre><code>from collections import Counter\n\ndef count_each_vowel():\n    s= \"I am going outside for lunch. will be back by another thirty minutes\"\n    vowels = \"aeiouAEIOU\"\n    vowel_counts = Counter(char for char in s if char in vowels)\n    print(vowel_counts)\n\ncount_each_vowel()\n</code></pre> <p>Reverse a String Without Using Slicing</p> <pre><code>def reverse_string(s):\n    result = ''\n    for char in s:\n        result = char + result\n    return result\n\nprint(reverse_string(\"DevOpsEngineer\"))  # reenignEsPOveD\n</code></pre> <p>Sort a Dictionary by Value</p> <pre><code>def sort_dict_by_value(d):\n    return dict(sorted(d.items(), key=lambda item: item[1]))\n\nprint(sort_dict_by_value({'a': 3, 'b': 1, 'c': 2}))  # {'b': 1, 'c': 2, 'a': 3}\n</code></pre> <p>Check for Anagrams</p> <pre><code>def are_anagrams(s1, s2):\n    return sorted(s1) == sorted(s2)\n\nprint(are_anagrams(\"listen\", \"silent\"))  # True\n</code></pre> <p>Find the Frequency of Each Element in a List</p> <pre><code>from collections import Counter\n\nlst = [1, 2, 2, 3, 4, 4, 4]\nprint(dict(Counter(lst)))  # {1:1, 2:2, 3:1, 4:3}\n</code></pre> <p>Read a File and Count Word Frequency</p> <pre><code>def count_words(filepath):\n    with open(filepath) as f:\n        words = f.read().split()\n    return dict(Counter(words))\n\n# count_words(\"test.txt\")\n</code></pre> <p>Find All Pairs in List That Sum to Target</p> <pre><code>def find_pairs(lst, target):\n    seen = set()\n    result = set()\n    for num in lst:\n        diff = target - num\n        if diff in seen:\n            result.add((min(num, diff), max(num, diff)))\n        seen.add(num)\n    return result\n\nprint(find_pairs([2, 4, 3, 5, 7], 7))  # {(3, 4), (2, 5)}\n</code></pre> <p>Rotate a List by k Elements</p> <pre><code>def rotate_list(lst, k):\n    k = k % len(lst)\n    return lst[-k:] + lst[:-k]\n\nprint(rotate_list([1,2,3,4,5], 2))  # [4,5,1,2,3]\n</code></pre> <p>Flatten a Nested List</p> <pre><code>def flatten(lst):\n    result = []\n    for item in lst:\n        if isinstance(item, list):\n            result.extend(flatten(item))\n        else:\n            result.append(item)\n    return result\n\nprint(flatten([1, [2, [3, 4], 5], 6]))  # [1,2,3,4,5,6]\n</code></pre> <p>Find Duplicate Characters in a String</p> <p>```python  def find_duplicates(s):     return [char for char, count in Counter(s).items() if count &gt; 1]</p> <p>print(find_duplicates(\"programming\"))  # ['r', 'g', 'm']  ```</p> <p>Check Armstrong Number</p> <pre><code>def is_armstrong(n):\n    power = len(str(n))\n    return sum(int(d) ** power for d in str(n)) == n\n\nprint(is_armstrong(153))  # True\n</code></pre> <p>List All Prime Numbers in a Range</p> <pre><code>def primes_in_range(start, end):\n    return [n for n in range(start, end+1) if is_prime(n)]\n\nprint(primes_in_range(10, 30))\n</code></pre> <p>Capitalize First Letter of Each Word in a String</p> <pre><code>def capitalize_words(s):\n    return ' '.join(word.capitalize() for word in s.split())\n\nprint(capitalize_words(\"hello devops team\"))  # Hello Devops Team\n</code></pre> <p>Validate IPv4 Address</p> <pre><code>import re\n\ndef is_valid_ip(ip):\n    pattern = re.compile(r\"^(25[0-5]|2[0-4]\\d|1?\\d{1,2})(\\.(25[0-5]|2[0-4]\\d|1?\\d{1,2})){3}$\")\n    return bool(pattern.match(ip))\n\nprint(is_valid_ip(\"192.168.1.1\"))  # True\n</code></pre> <p>Merge Two Dictionaries</p> <pre><code>def merge_dicts(d1, d2):\n    return {**d1, **d2}\n\nprint(merge_dicts({'a': 1}, {'b': 2}))  # {'a':1, 'b':2}\n</code></pre> <p>Implement a Decorator to Measure Function Execution Time</p> <pre><code>import time\n\ndef timer(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f\"Executed in {time.time() - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef slow_function():\n    time.sleep(1)\n    return \"done\"\n\nprint(slow_function())\n</code></pre> <p>JSON Parsing and Writing</p> <pre><code>import json\n\ndata = {\"name\": \"DevOps\", \"role\": \"SRE\"}\n\n# Convert to JSON string\njson_string = json.dumps(data)\n\n# Convert back to dict\nparsed_data = json.loads(json_string)\n\nprint(json_string)     # {\"name\": \"DevOps\", \"role\": \"SRE\"}\nprint(parsed_data)     # {'name': 'DevOps', 'role': 'SRE'}\n</code></pre> <p>Check if a List is Sorted  ```python  def is_sorted(lst):     return lst == sorted(lst)</p> <p>print(is_sorted([1, 2, 3, 4]))  # True print(is_sorted([1, 3, 2]))    # False</p> <pre><code>\n**Find the First Non-Repeating Character**\n\n```python\nfrom collections import Counter\n\ndef first_non_repeating(s):\n    counts = Counter(s)\n    for char in s:\n        if counts[char] == 1:\n            return char\n    return None\n\nprint(first_non_repeating(\"aabbccdef\"))  # d\n</code></pre> <p>Move All Zeros to the End of the List</p> <pre><code>def move_zeros(lst):\n    non_zeros = [x for x in lst if x != 0]\n    return non_zeros + [0] * (len(lst) - len(non_zeros))\n\nprint(move_zeros([0, 1, 0, 3, 12]))  # [1, 3, 12, 0, 0]\n</code></pre> <p>Count Occurrences of Each Word in a String</p> <pre><code>def word_frequency(s):\n    words = s.lower().split()\n    return dict(Counter(words))\n\nprint(word_frequency(\"This is a test. This test is simple.\"))\n</code></pre> <p>Check if Two Strings are Rotations of Each Other</p> <pre><code>def is_rotation(s1, s2):\n    return len(s1) == len(s2) and s2 in (s1 + s1)\n\nprint(is_rotation(\"abcd\", \"cdab\"))  # True\n</code></pre> <p>Find Second Largest Number in List</p> <pre><code>def second_largest(lst):\n    unique = list(set(lst))\n    unique.sort()\n    return unique[-2] if len(unique) &gt;= 2 else None\n\nprint(second_largest([4, 1, 3, 2, 5]))  # 4\n</code></pre> <p>Check Leap Year</p> <pre><code>def is_leap(year):\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\nprint(is_leap(2024))  # True\n</code></pre> <p>Extract Digits from a String</p> <pre><code>def extract_digits(s):\n    return [int(char) for char in s if char.isdigit()]\n\nprint(extract_digits(\"abc123xyz456\"))  # [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Find Common Elements in Two Lists</p> <pre><code>def common_elements(a, b):\n    return list(set(a) &amp; set(b))\n\nprint(common_elements([1, 2, 3], [2, 3, 4]))  # [2, 3]\n</code></pre> <p>Sum of All Digits in a String</p> <pre><code>def sum_of_digits(s):\n    return sum(int(char) for char in s if char.isdigit())\n\nprint(sum_of_digits(\"abc123xyz\"))  # 6\n</code></pre>"},{"location":"programming/python/qa/#builtins","title":"Builtins","text":""},{"location":"programming/python/qa/#string","title":"string","text":"<pre><code>capitalize()    Converts the first character to upper case\ncasefold()      Converts string into lower case\ncenter()        Returns a centered string\ncount()         Returns the number of times a specified value occurs in a string\nencode()        Returns an encoded version of the string\nendswith()      Returns true if the string ends with the specified value\nexpandtabs()    Sets the tab size of the string\nfind()          Searches the string for a specified value and returns the position of where it was found\nformat()        Formats specified values in a string\nformat_map()    Formats specified values in a string\nindex()         Searches the string for a specified value and returns the position of where it was found\nisalnum()       Returns True if all characters in the string are alphanumeric\nisalpha()       Returns True if all characters in the string are in the alphabet\nisascii()       Returns True if all characters in the string are ascii characters\nisdecimal()     Returns True if all characters in the string are decimals\nisdigit()       Returns True if all characters in the string are digits\nisidentifier()  Returns True if the string is an identifier\nislower()       Returns True if all characters in the string are lower case\nisnumeric()     Returns True if all characters in the string are numeric\nisprintable()   Returns True if all characters in the string are printable\nisspace()       Returns True if all characters in the string are whitespaces\nistitle()       Returns True if the string follows the rules of a title\nisupper()       Returns True if all characters in the string are upper case\njoin()          Converts the elements of an iterable into a string\nljust()         Returns a left justified version of the string\nlower()         Converts a string into lower case\nlstrip()        Returns a left trim version of the string\nmaketrans()     Returns a translation table to be used in translations\npartition()     Returns a tuple where the string is parted into three parts\nreplace()       Returns a string where a specified value is replaced with a specified value\nrfind()         Searches the string for a specified value and returns the last position of where it was found\nrindex()        Searches the string for a specified value and returns the last position of where it was found\nrjust()         Returns a right justified version of the string\nrpartition()    Returns a tuple where the string is parted into three parts\nrsplit()        Splits the string at the specified separator, and returns a list\nrstrip()        Returns a right trim version of the string\nsplit()         Splits the string at the specified separator, and returns a list\nsplitlines()    Splits the string at line breaks and returns a list\nstartswith()    Returns true if the string starts with the specified value\nstrip()         Returns a trimmed version of the string\nswapcase()      Swaps cases, lower case becomes upper case and vice versa\ntitle()         Converts the first character of each word to upper case\ntranslate()     Returns a translated string\nupper()         Converts a string into upper case\nzfill()         Fills the string with a specified number of 0 values at the beginning\n</code></pre>"},{"location":"programming/python/qa/#list","title":"list","text":"<pre><code>append()    Adds an element at the end of the list\nclear()     Removes all the elements from the list\ncopy()      Returns a copy of the list\ncount()     Returns the number of elements with the specified value\nextend()    Add the elements of a list (or any iterable), to the end of the current list\nindex()     Returns the index of the first element with the specified value\ninsert()    Adds an element at the specified position\npop()       Removes the element at the specified position\nremove()    Removes the first item with the specified value\nreverse()   Reverses the order of the list\nsort()      Sorts the list\n</code></pre>"},{"location":"programming/python/qa/#dictionary","title":"dictionary","text":"<pre><code>clear()     Removes all the elements from the dictionary\ncopy()      Returns a copy of the dictionary\nfromkeys()  Returns a dictionary with the specified keys and value\nget()       Returns the value of the specified key\nitems()     Returns a list containing a tuple for each key value pair\nkeys()      Returns a list containing the dictionary's keys\npop()       Removes the element with the specified key\npopitem()   Removes the last inserted key-value pair\nsetdefault() Returns the value of the specified key.If the key does not exist: insert the key, with the specified value\nupdate()    Updates the dictionary with the specified key-value pairs\nvalues()    Returns a list of all the values in the dictionary\n</code></pre>"},{"location":"programming/python/qa/#tuple","title":"tuple","text":"<pre><code>count()   Returns the number of times a specified value occurs in a tuple\nindex()   Searches the tuple for a specified value and returns the position of where it was found \n</code></pre>"},{"location":"programming/python/qa/#set","title":"set","text":"<pre><code>add()           Adds an element to the set\nclear()         Removes all the elements from the set\ncopy()          Returns a copy of the set\ndifference()    Returns a set containing the difference between two or more sets\ndifference_update() Removes the items in this set that are also included in another, specified set\ndiscard()       Remove the specified item\nintersection()  Returns a set, that is the intersection of two or more sets\nintersection_update()   Removes the items in this set that are not present in other, specified set(s)\nisdisjoint()    Returns whether two sets have a intersection or not\nissubset()      Returns whether another set contains this set or not\nissuperset()    Returns whether this set contains another set or not\npop()           Removes an element from the set\nremove()        Removes the specified element\nsymmetric_difference()  Returns a set with the symmetric differences of two sets\nsymmetric_difference_update()   inserts the symmetric differences from this set and another\nunion()         Return a set containing the union of sets\nupdate()        Update the set with another set, or any other iterable\n</code></pre>"},{"location":"programming/python/qa/#files_1","title":"files","text":"<pre><code>close()         Closes the file\ndetach()        Returns the separated raw stream from the buffer\nfileno()        Returns a number that represents the stream, from the operating system's perspective\nflush()         Flushes the internal buffer\nisatty()        Returns whether the file stream is interactive or not\nread()          Returns the file content\nreadable()      Returns whether the file stream can be read or not\nreadline()      Returns one line from the file\nreadlines()     Returns a list of lines from the file\nseek()          Change the file position\nseekable()      Returns whether the file allows us to change the file position\ntell()          Returns the current file position\ntruncate()      Resizes the file to a specified size\nwritable()      Returns whether the file can be written to or not\nwrite()         Writes the specified string to the file\nwritelines()    Writes a list of strings to the file\n</code></pre>"},{"location":"programming/python/qa/#other","title":"other","text":"<pre><code>abs()       Returns the absolute value of a number\nall()       Returns True if all items in an iterable object are true\nany()       Returns True if any item in an iterable object is true\nascii()     Returns a readable version of an object. Replaces none-ascii characters with escape character\nbin()       Returns the binary version of a number\nbool()      Returns the boolean value of the specified object\nbytearray() Returns an array of bytes\nbytes()     Returns a bytes object\ncallable()  Returns True if the specified object is callable, otherwise False\nchr()       Returns a character from the specified Unicode code.\nclassmethod()   Converts a method into a class method\ncompile()   Returns the specified source as an object, ready to be executed\ncomplex()   Returns a complex number\ndelattr()   Deletes the specified attribute (property or method) from the specified object\ndict()      Returns a dictionary (Array)\ndir()       Returns a list of the specified object's properties and methods\ndivmod()    Returns the quotient and the remainder when argument1 is divided by argument2\nenumerate() Takes a collection (e.g. a tuple) and returns it as an enumerate object\neval()      Evaluates and executes an expression\nexec()      Executes the specified code (or object)\nfilter()    Use a filter function to exclude items in an iterable object\nfloat()     Returns a floating point number\nformat()    Formats a specified value\nfrozenset() Returns a frozenset object\ngetattr()   Returns the value of the specified attribute (property or method)\nglobals()   Returns the current global symbol table as a dictionary\nhasattr()   Returns True if the specified object has the specified attribute (property/method)\nhash()      Returns the hash value of a specified object\nhelp()      Executes the built-in help system\nhex()       Converts a number into a hexadecimal value\nid()        Returns the id of an object\ninput()     Allowing user input\nint()       Returns an integer number\nisinstance()    Returns True if a specified object is an instance of a specified object\nissubclass()    Returns True if a specified class is a subclass of a specified object\niter()      Returns an iterator object\nlen()       Returns the length of an object\nlist()      Returns a list\nlocals()    Returns an updated dictionary of the current local symbol table\nmap()       Returns the specified iterator with the specified function applied to each item\nmax()       Returns the largest item in an iterable\nmemoryview()    Returns a memory view object\nmin()       Returns the smallest item in an iterable\nnext()      Returns the next item in an iterable\nobject()    Returns a new object\noct()       Converts a number into an octal\nopen()      Opens a file and returns a file object\nord()       Convert an integer representing the Unicode of the specified character\npow()       Returns the value of x to the power of y\nprint()     Prints to the standard output device\nproperty()  Gets, sets, deletes a property\nrange()     Returns a sequence of numbers, starting from 0 and increments by 1 (by default)\nrepr()      Returns a readable version of an object\nreversed()  Returns a reversed iterator\nround()     Rounds a numbers\nset()       Returns a new set object\nsetattr()   Sets an attribute (property/method) of an object\nslice()     Returns a slice object\nsorted()    Returns a sorted list\nstaticmethod()  Converts a method into a static method\nstr()       Returns a string object\nsum()       Sums the items of an iterator\nsuper()     Returns an object that represents the parent class\ntuple()     Returns a tuple\ntype()      Returns the type of an object\nvars()      Returns the __dict__ property of an object\nzip()       Returns an iterator, from two or more iterators\n</code></pre>"},{"location":"programming/python/qa/#counter","title":"counter","text":"<pre><code>Counter(elements)   Creates a Counter object from an iterable or a dictionary.\n.most_common([n])   Returns the n most common elements as a list of tuples. If n is omitted, returns all elements.\n.elements() Returns an iterator over elements, repeating each as per its count.\n.subtract(iterable or mapping)  Subtracts counts from another iterable or mapping.\n.update(iterable or mapping)    Adds counts from another iterable or mapping.\n.clear()    Removes all elements from the Counter.\n.copy() Returns a shallow copy of the Counter.\n.keys() Returns a list of unique elements (like a dictionary).\n.values()   Returns a list of counts corresponding to the elements.\n.items()    Returns a list of (element, count) pairs.\n.total()    Returns the sum of all counts (Python 3.10+).\n+Counter    Adds two Counters, summing up counts for common elements.\n-Counter    Subtracts counts, keeping only positive counts.\n&amp;Counter    Returns the intersection (min counts) of two Counters.\n</code></pre>"},{"location":"programming/python/qa/#todo","title":"TODO","text":""},{"location":"programming/python/qa/#practical-questions","title":"Practical Questions","text":"<ul> <li>what is the use of yield and why should we use them ?</li> <li>What is the use of map, filter and reduce ? can you provide some examples ?</li> <li>Why do we need to use lambda and how can we use them ?</li> <li>what is dictionary comprehension, provide me with an example ?</li> <li>Given a string, how would you remove the list of whitespaces from it and create a new string ?</li> <li>what's the difference between append, extend and concatenate ?</li> <li>find the first non-repeated character in a string.</li> <li>count the number of words in a string.</li> <li>find the maximum subarray sum in a given list of integers.</li> <li>check if a given string contains only digits.</li> <li>check if a given string is a valid email address.</li> <li>generate all possible permutations of a given string.</li> <li>convert a decimal number to binary.</li> <li>convert a binary number to decimal.</li> <li>check if a given number is a perfect square.</li> <li>check if a given number is an Armstrong number.</li> <li>find the longest common prefix among a list of strings.</li> <li>find the longest common suffix among a list of strings.</li> <li>find the first non-repeating character in a list.</li> <li>find the first repeating character in a list.</li> <li>remove all whitespace characters from a string.</li> <li>implement a Caesar cipher.</li> <li>find the number of occurrences of a substring in a given string.</li> <li>find the index of the first occurrence of a substring in a given string.</li> <li>count the number of lines in a file.</li> <li>count the number of words in a file.</li> <li>count the number of characters in a file.</li> <li>check if a given string is a valid IP address.</li> <li>check if a given string is a valid URL.</li> <li>find the most common element in a list.</li> </ul>"},{"location":"programming/python/qa/#data-structures","title":"Data Structures","text":""},{"location":"programming/python/qa/#single-linked-list","title":"Single linked list","text":""},{"location":"programming/python/qa/#double-linked-list","title":"Double linked list","text":""},{"location":"programming/python/qa/#stacks-queues","title":"Stacks &amp; Queues","text":""},{"location":"programming/python/qa/#trees","title":"Trees","text":""},{"location":"programming/python/qa/#hash-tables","title":"Hash tables","text":""},{"location":"programming/python/qa/#graphs","title":"Graphs","text":""},{"location":"programming/python/qa/#heaps","title":"Heaps","text":""},{"location":"programming/python/qa/#recursions","title":"Recursions","text":""},{"location":"programming/python/qa/#sorting","title":"Sorting","text":""},{"location":"programming/python/qa/#other-coding-exercises","title":"Other coding exercises","text":""},{"location":"programming/python/data_structures/hashing/","title":"hashing","text":""},{"location":"programming/python/data_structures/hashing/#overview","title":"Overview","text":"<p>Hashing is a technique used in computer science to map data of arbitrary size to fixed-size values, usually integers, which are used as keys to access or store data in a data structure called a hash table. Hashing allows for efficient data retrieval and storage by reducing the search space and providing constant-time average complexity for common operations like insertion, deletion, and retrieval.</p> <p>Deterministic hashing refers to the property of a hash function where the same input will always produce the same hash value. In other words, given a specific input, the hash function will consistently generate the exact same output hash code. This property is crucial for the reliability and predictability of hash-based data structures and algorithms.</p> <p>Python's dictionaries handle the details of hashing and collision resolution internally, making it easy to work with hashed data without needing to implement these aspects yourself.</p> <pre><code>\nstudent_scores = {\n    \"Alice\": 95,\n    \"Bob\": 88,\n    \"Charlie\": 92,\n    \"David\": 78\n}\n\nfor name, score in student_scores.items():\n    print(f\"{name}: {score}\")\n\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#collisions","title":"Collisions","text":"<p>Separate chaining is a collision resolution strategy used in hash-based data structures, such as hash tables, to handle situations where multiple keys hash to the same index. It involves creating a separate data structure, often a linked list, for each index in the hash table where collisions occur.</p>"},{"location":"programming/python/data_structures/hashing/#hash-table-initialization","title":"Hash Table Initialization","text":"<p>The hash table is divided into a fixed number of buckets (or slots), each with a unique index. Each bucket can hold multiple key-value pairs.</p>"},{"location":"programming/python/data_structures/hashing/#hashing","title":"Hashing","text":"<p>When a new key needs to be inserted into the hash table or when you want to retrieve a value associated with a key, a hash function is applied to the key to determine the index (bucket) where it should be stored or looked up.</p>"},{"location":"programming/python/data_structures/hashing/#collision-handling","title":"Collision Handling","text":"<ul> <li>If two or more keys hash to the same index (collision), separate chaining is used to handle the collision.</li> <li>At each index where a collision occurs, a separate data structure (typically a linked list) is maintained.</li> </ul>"},{"location":"programming/python/data_structures/hashing/#insertion","title":"Insertion","text":"<p>To insert a new key-value pair: - Hash the key to find the appropriate index (bucket). - Insert the key-value pair at the end of the linked list (or another chosen data structure) associated with that index.</p>"},{"location":"programming/python/data_structures/hashing/#retrieval","title":"Retrieval","text":"<p>To retrieve the value associated with a key: - Hash the key to find the appropriate index. - Traverse the linked list at that index to find the desired key and retrieve its corresponding value.</p>"},{"location":"programming/python/data_structures/hashing/#deletion","title":"Deletion","text":"<p>To delete a key-value pair: - Hash the key to find the appropriate index. - Search the linked list for the key and remove the corresponding pair if found.</p>"},{"location":"programming/python/data_structures/hashing/#constructor","title":"Constructor","text":"<pre><code>class HashTable:\n  def __init__(self, size=7):\n    self.data_map = [None]*size\n\n  def __hash(self,key):\n    my_hash=0\n    for letter in key:\n      my_hash=(my_hash+ord(letter)*23) % len(self.data_map)\n    return my_hash\n\nhash_table = HashTable()\nhash_table.print_hash_table()\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#print-table","title":"print table","text":"<pre><code>def print_hash_table(self):\n    for k,v in enumerate(self.data_map):\n        print(k,\": \", v)\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#set-item","title":"set item","text":"<pre><code>def set_item(self,key,value):\n    index = self.__hash(key)\n    if self.data_map[index] == None:\n        self.data_map[index]=[]\n    self.data_map[index].append([key,value])\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#get-item","title":"get item","text":"<pre><code>def get_item(self,key):\n    index = self.__hash(key)\n    if self.data_map[index] is not None:\n        for i in range(len(self.data_map[index])):\n        if self.data_map[index][i][0] == key:\n            return self.data_map[index][i][1]\n    return None\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#get-all-keys","title":"get all keys","text":"<pre><code>def keys(self):\n    all_keys=[]\n    for i in range(len(self.data_map)):\n        if self.data_map[i] is not None:\n        for j in range(len(self.data_map[i])):\n            all_keys.append(self.data_map[i][j][0])\n    return all_keys\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#big-o","title":"Big-O","text":"<p>In the best scenerio, the item we are searching in the hash table is found at the index, it wouldbe O(1).  In wrost case even if the item is entirely hashed in the list, the max would be O(n), but however since we are hashing the key value pair are evenly distributed and have very less collisions, we would say its always the hash is  O(1)</p>"},{"location":"programming/python/data_structures/hashing/#interview-q","title":"Interview Q","text":"<p>provided the two lists, get the item in common. provide me the best solution in terms of bigO</p> <p>list1 = [2,3,4] list2 = [1,6,4]</p> <p>There are two approches for this. </p> <ol> <li>You would be iterating first item of list1 against all the elements in list2 and so on. Once you find you would be returning True or else False. Since there are multiple nested loops O(n^2)</li> </ol> <pre><code># code goes here\n</code></pre> <ol> <li>Create a dict for list1 and check if the key of dict in list2. Once you find you would return True or else False.  This would have O(2n) i.e removing constants, it would be O(n)</li> </ol> <pre><code># code goes here\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/","title":"linked list","text":""},{"location":"programming/python/data_structures/linked_list/#olinked-list-operations","title":"O(linked list operations)","text":"<p>There are few of the operations that which we do in the linked list, below snapshot describes you about the Big-O cases. </p> <p></p>"},{"location":"programming/python/data_structures/linked_list/#linked-list-under-the-hood","title":"linked list under the hood.","text":"<p>linked list appears to be as below, however in the memory they are scarattered in the different locations, but they are always connected with the pointer. There is head which is the start of the node and the tail which is the end of the node in the list. the connection elements between the nodes are the pointers which are connected</p> <p></p> <p>Under the hood, its nothing but a dictionary, which can be used as a variable to set and iterate the values.  </p> <p></p>"},{"location":"programming/python/data_structures/linked_list/#singly-linked-list-sll","title":"Singly linked list (sll)","text":"<p>Methods that are used in the linked list class. </p>"},{"location":"programming/python/data_structures/linked_list/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value\n        self.next = None\n\nclass LinkedList:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.head = new_node\n        self.tail = new_node\n        self.length = 1\n\nmy_linked_list = LinkedList(1)\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#print-sll","title":"print sll","text":"<pre><code>def print_list(self):\n    temp = self.head \n    while temp is not None:\n        print(temp.value,end=' ')\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#append-sll","title":"append sll","text":"<pre><code>def append(self,value):\n    new_node=Node(value)\n    if self.length==0:\n        self.head = new_node.head\n        self.tail = new_node.tail \n    else:\n        self.tail.next = new_node\n        self.tail = new_node\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#prepend-node-list","title":"prepend node list","text":"<pre><code>def prepend(self,value):\n    new_node=Node(value)\n\n    if self.length == 0:\n        self.head = new_node\n        self.tail = new_node\n    else:\n        new_node.next = self.head\n        self.head = new_node\n\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#get-node-at-particular-index","title":"get node at particular index","text":"<pre><code>def get(self,index):\n    if index &lt; 0 or index &gt;= self.length:\n        return None \n    temp = self.head \n    for _ in range(index):\n        temp = temp.next \n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#insert-node-at-particular-index","title":"insert node at particular index","text":"<pre><code>def insert(self,index,value):\n    if index&lt; 0 or index&gt;self.length:\n        return False \n    if index == 0:\n        return self.prepend(value)\n    if index==self.length:\n        return self.append(value)\n    new_node=Node(value)\n    temp = self.get(index-1) # get index from above function\n    new_node.next= temp.next \n    temp.next = new_node\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#set-value-to-node-using-particular-index","title":"set value to node using particular index","text":"<pre><code>def set_value(self,index,value):\n    temp = self.get(index)\n    if temp:\n        temp.value=value\n        return True \n    return False\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-at-particule-index","title":"remove node at particule index","text":"<pre><code>def remove(self,index):\n    if index &lt; 0  or index&gt;=self.length:\n        return None \n    if index == 0:\n        return self.pop_first()\n    if index == self.length:\n        return self.pop()\n    pre = self.get(index-1)\n    temp = pre.next \n    pre.next = temp.next \n    temp.next = None\n    self.length-=1\n    return temp   \n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-list","title":"remove node list","text":"<pre><code>def pop(self):\n    # if node is empty\n    if self.length == 0:\n        return None\n    else:\n        # more than two nodes \n        temp=self.head \n        pre=self.head\n        while(temp.next):\n            pre=temp\n            temp=temp.next\n        self.tail = pre\n        self.tail.next=None\n        self.length-=1\n        # if node is 0 after decrementing\n        if self.length==0:\n            self.head = None \n            self.tail = None\n    return temp.value\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-first-node-in-list","title":"remove first node in list","text":"<pre><code>def pop_first(self):\n    if self.length == 0:\n        return None \n    else:\n        temp = self.head \n        self.head = self.head.next \n        temp.next = None \n        self.length-=1\n        if self.length==0:\n            self.tail = None\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#reverse-node-in-list","title":"reverse node in list","text":"<pre><code>def reverse(self):\n    # swap head and tail of the list\n    temp=self.head\n    self.head = self.tail\n    self.tail = temp \n\n    after = temp.next \n    before = None \n\n    for _ in range(self.length):\n        after = temp.next \n        temp.next = before \n        before = temp\n        temp = after\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#doubly-linked-listdll","title":"Doubly linked list(dll)","text":""},{"location":"programming/python/data_structures/linked_list/#constructor_1","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None\n        self.prev=None\n\nclass LinkedList:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.head = new_node\n        self.tail = new_node\n        self.length = 1\n\ndoubly_linked_list = DoublyLinkedList(\"10\")\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#print-dll","title":"print dll","text":"<pre><code>def print_list(self):\n    temp = self.head \n    while temp:\n        print(f\"{temp.value}\")\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#append-dll","title":"append dll","text":"<pre><code>def append(self,value):\n    new_node = Node(value)\n\n    if self.head is None:\n        self.head = new_node\n        self.tail = new_node\n    else:\n        self.tail.next = new_node\n        new_node.prev=self.tail\n        self.tail = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#pop-dll","title":"pop dll","text":"<pre><code>def pop(self):\n    if self.length == 0:\n        return None\n\n    temp = self.tail \n    if self.length==1:\n        self.head = None \n        self.tail = None\n    else:\n        self.tail = self.tail.prev\n        self.tail.next = None \n        temp.prev = None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#prepend-dll","title":"prepend dll","text":"<pre><code>def prepend(self,value):\n    new_node = Node(value)\n\n    if self.length == 0:\n        return None\n    else:\n        new_node.next = self.head \n        self.head.prev = new_node\n        self.head = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#pop-first-dll","title":"pop first dll","text":"<pre><code>def pop_first(self):\n    if self.length==0:\n        return None\n\n    temp = self.head \n    if self.length==1:\n        self.head = None\n        self.tail = None\n    else:\n        self.head = self.head.next\n        self.head.prev= None\n        temp.next = None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#get-node-using-index","title":"get node using index","text":"<pre><code>def get(self,index):\n    if index &lt; 0 or index &gt;=self.length:\n        return None\n    temp = self.head \n    if index &lt; self.length/2:\n        for _ in range(index):\n            temp = temp.next\n    else:\n        temp = self.tail\n        for _ in range(self.length-1, index,-1):\n            temp = temp.prev\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#set-value-using-index","title":"set value using index","text":"<pre><code>def set_value(self,index,value):\n    temp = self.get(index)\n    if temp:\n        temp.value = value\n        return True \n\n    return False\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#insert-node-at-particular-index_1","title":"insert node at particular index","text":"<pre><code>def insert(self,index,value):\n    if index &lt; 0 or index &gt;=self.length:\n        return False\n\n    if index==0:\n        return self.prepend(value)\n\n    if index == self.length:\n        return self.append(value)\n\n    new_node=Node(value)\n    before = self.get(index-1)\n    after = before.next\n    new_node.prev = before\n    new_node.next = after \n    before.next = new_node\n    after.prev = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-at-particular-index","title":"remove node at particular index","text":"<pre><code>def remove(self,index):\n    if index &lt; 0 or index &gt;=self.length:\n        return None\n\n    if index==0:\n        return self.pop_first()\n\n    if index == self.length-1:\n        return self.pop()\n\n    temp = self.get(index)\n    temp.next.prev=temp.prev\n    temp.prev.next = temp.next\n    temp.next = None \n    temp.prev=None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/overview/","title":"overview","text":"<p>A data structure is a storage that is used to store and organize data. It is a way of arranging data on a computer so that it can be accessed and updated efficiently.</p>"},{"location":"programming/python/data_structures/overview/#classification-of-ds","title":"Classification of DS","text":"<p>Linear data structure data elements are arranged sequentially or linearly, where each element is attached to its previous and next adjacent elements</p> <p>Static: it has a fixed memory size. It is easier to access the elements in a static data structure. e.g arrary</p> <p>Dynamic: the size is not fixed. It can be randomly updated during the runtime which may be considered efficient concerning the memory (space) complexity of the code. e.g queue, stack </p> <p>Non-linear data structure: Data structures where data elements are not placed sequentially or linearly are called non-linear data structures. In a non-linear data structure, we can\u2019t traverse all the elements in a single run only. e.g trees and graphs</p>"},{"location":"programming/python/data_structures/overview/#big-o-complexities","title":"Big-O Complexities","text":"<p>Big-O notation is a way to describe the time and space complexity of a given algorithm. Big-O notation tells you the number of operations an algorithm will make. Big-O establishes a worst-case run time. </p>"},{"location":"programming/python/data_structures/overview/#time-complexity","title":"Time complexity","text":"<p>The amount of time take to execute the operations per second.</p>"},{"location":"programming/python/data_structures/overview/#space-complexity","title":"Space complexity","text":"<p>The amount of RAM or memory taken to execute. however, this would depend on the performance on the system. the higher the capacity of the machine, space complexity would be differing. </p> <p></p>"},{"location":"programming/python/data_structures/overview/#big-o-notations","title":"Big-O notations","text":""},{"location":"programming/python/data_structures/overview/#o1","title":"O(1)","text":"<p>The O(1) is also called constant time, it will always execute at the same time regardless of the input size.</p> <pre><code>x=10\nprint(x)\n\n# The input array could be 1 item or 1,000 items, \n# but this function would still just require one step.\n\nfor i in range(10):\n    print(i[0])\n</code></pre>"},{"location":"programming/python/data_structures/overview/#on","title":"O(n)","text":"<p>This function runs in O(n) time (or \"linear time\"), where n is the number of items in the array. If the array has 10 items, we have to print 10 times. If it has 1000 items, we have to print 1000 times.</p> <pre><code>for i in range(10):\n    print(i)\n</code></pre>"},{"location":"programming/python/data_structures/overview/#on2","title":"O(n^2)","text":"<pre><code>for i in range(10):\n    for j in range(10):\n        print(i,j)\n</code></pre> <p>Here we're nesting two loops. If our array has n items, our outer loop runs n times and our inner loop runs n times for each iteration of the outer loop, giving us n*n total prints. Thus this function runs in O(n2) time (or \"quadratic time\"). If the array has 10 items, we have to print 100 times. If it has 1000 items, we have to print 1000000 times.</p>"},{"location":"programming/python/data_structures/overview/#o2n","title":"O(2n)","text":"<pre><code>def fib(n):\n    if n&lt;=1: return n;\n    return fib(n-1)+fib(n-2)\n</code></pre> <p>An example of an O(2n) function is the recursive calculation of Fibonacci numbers. O(2n) denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2n) function is exponential - starting off very shallow, then rising meteorically.</p>"},{"location":"programming/python/data_structures/overview/#drop-less-significant-terms","title":"Drop less significant terms","text":"<pre><code>for i in range(10):\n    for j in range(10):\n        print(i,j)\n\nfor k in range(10):\n    print(k)\n</code></pre> <p>we would re-iterate i and j i.e n^2 along with 'k' would be 'n' times.  hence it would be O(n^2+n), in which we can easily drop the constant(n) as its insignificient.</p>"},{"location":"programming/python/data_structures/overview/#drop-constants","title":"Drop constants","text":"<pre><code>for i in range(10):\n    print(i)\n\nfor j in range(10):\n    print(j)\n</code></pre> <p>We would be looping 'i' n-times and 'j' n-times i.e O(n+n)=O(2n).</p> <p>Remember, for big O notation we're looking at what happens as n gets arbitrarily large. As n gets really big, adding 100 or dividing by 2 has a decreasingly significant effect, hence we would drop any constants. </p>"},{"location":"programming/python/data_structures/overview/#common-ds-alogs","title":"Common ds alogs","text":""},{"location":"programming/python/data_structures/overview/#references","title":"References","text":"<p>bigocheatsheet</p>"},{"location":"programming/python/data_structures/queues/","title":"queues","text":"<p>queue is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first.</p> <p></p>"},{"location":"programming/python/data_structures/queues/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None \n\nclass Queue:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.first = new_node\n        self.last = new_node\n        self.length=1\n\nmy_queue=Queue(4)\n</code></pre>"},{"location":"programming/python/data_structures/queues/#print-queue","title":"print queue","text":"<pre><code>\ndef print_queue(self):\n    temp =self.first \n    while temp is not None:\n        print(temp.value)\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/queues/#enqueue","title":"enqueue","text":"<pre><code>def enqueue(self,value):\n    new_node=Node(value)\n    if self.first is None:\n        self.first=new_node\n        self.last=new_node\n    else:\n        self.last.next = new_node\n        self.last = new_node\n    self.length+=1\n</code></pre>"},{"location":"programming/python/data_structures/queues/#dequeue","title":"dequeue","text":"<pre><code>def dequeue(self):\n    if self.length==0:\n        return None \n\n    temp = self.first\n    if self.length==1:\n        self.first = None \n        self.last = None \n    else:\n        self.first = self.first.next \n        temp.next = None \n    self.length-=1\n    return temp \n</code></pre>"},{"location":"programming/python/data_structures/recursions/","title":"recursions","text":"<p>Recursion is a programming technique in which a function calls itself to solve a problem. Recursive functions break down complex problems into smaller, more manageable subproblems, and these subproblems are solved by invoking the same function recursively. </p> <p>examples using recursion</p> <pre><code># sum of factorial\ndef fact(n):\n    if n == 0: \n        return 1 \n    else:\n        return n*fact(n-1)\n</code></pre> <pre><code># sum of fibonacci numbers \ndef fib(n):\n    if n&lt;=1: \n        return n\n    else:\n        return fib(n-1)+fib(n-2)\n</code></pre> <pre><code># list of fib numbers\ndef fibonacci_list(n):\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n    else:\n        fib_list = fibonacci_list(n - 1)\n        fib_list.append(fib_list[-1] + fib_list[-2])\n        return fib_list\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#binary-search-treebst","title":"Binary Search Tree(BST)","text":""},{"location":"programming/python/data_structures/recursions/#contains","title":"contains","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\n    def __r_contains(self,current_node,value):\n      if current_node == None:\n        return False \n\n      if value == current_node.value:\n        return True\n\n      if value &lt; current_node.value:\n        return self.__r_contains(current_node.left,value)\n\n      if value&gt;current_node.value:\n        return self.__r_contains(current_node.right, value)\n\n    def r_contains(self,value):\n      return self.__r_contains(self.root,value)\n\n\n    def insert(self,value):\n        new_node = Node(value)\n        if self.root is None:\n            self.root=new_node\n            return True \n        temp = self.root\n        while(True):\n            # if the node of same value already exists\n            if new_node.value==temp.value: \n                return False\n\n            # if the node value is less then root value, add to left\n            if new_node.value &lt; temp.value:\n                if temp.left is None:\n                    temp.left = new_node\n                    return True\n                temp = temp.left\n            else:\n                # if the node value is greater then root value, add to right\n                if temp.right is None:\n                    temp.right = new_node\n                    return True\n                temp=temp.right\n\nmy_tree=BinarySearchTree()\nmy_tree.insert(2)\nmy_tree.insert(1)\nmy_tree.insert(3)\nprint(my_tree.r_contains(13))\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#insert","title":"insert","text":"<pre><code>\nclass Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\n    def __r_insert(self,current_node,value):\n\n        if current_node == None:\n            return Node(value)\n\n        if value &lt; current_node.value:\n            current_node.left = self.__r_insert(current_node.left,value)\n\n        if value &gt; current_node.value:\n            current_node.right = self.__r_insert(current_node.right,value)\n\n        return current_node\n\n    def r_insert(self,value):\n        if self.root==None:\n            self.root=Node(value)\n\n        self.__r_insert(self.root,value)\n\nmy_tree=BinarySearchTree()\nmy_tree.r_insert(2)\nmy_tree.r_insert(1)\nmy_tree.r_insert(3)\n\nprint(\"Root\", my_tree.root.value)\nprint(\"left:\", my_tree.root.left.value)\nprint(\"right:\", my_tree.root.right.value)\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#delete","title":"delete","text":""},{"location":"programming/python/data_structures/sorting/","title":"sorting","text":"<p>Sorting refers to the process of arranging elements in a specific order, typically in ascending or descending order based on their values.</p>"},{"location":"programming/python/data_structures/sorting/#bubble","title":"bubble","text":"<p>Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.</p> <pre><code>def bubble_sort(my_list):\n    n = len(my_list)\n    for i in range(n):\n        # Last i elements are already in place, no need to compare them\n        for j in range(0, n-i-1):\n            if my_list[j] &gt; my_list[j+1]:\n                # Swap the elements if they are in the wrong order\n                my_list[j], my_list[j+1] = my_list[j+1], my_list[j]\n\n    return my_list\n\nprint(bubble_sort([3,5,1,4,2]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#selection","title":"selection","text":"<p>Selection Sort is a simple sorting algorithm that repeatedly selects the minimum (or maximum) element from the unsorted part of the array and places it at the beginning (or end) of the sorted part.</p> <pre><code>def selection_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        min_index = i\n        for j in range(i + 1, n):\n            if arr[j] &lt; arr[min_index]:\n                min_index = j\n\n        # Swap the minimum element with the first element in the unsorted part\n        arr[i], arr[min_index] = arr[min_index], arr[i]\n\nprint(selection_sort([64, 34, 25, 12, 22, 11, 90]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#insertion","title":"insertion","text":"<p>Insertion Sort is a simple sorting algorithm that builds the final sorted array one item at a time. It works by iteratively considering each element and inserting it into its correct position within the already sorted part of the array.</p> <pre><code>def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]  # Current element to be inserted\n        j = i - 1\n\n        # Move elements of arr[0..i-1], that are greater than key, to one position ahead of their current position\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\nprint(insertion_sort([64, 34, 25, 12, 22, 11, 90]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#merge","title":"merge","text":""},{"location":"programming/python/data_structures/sorting/#sorted-merge-list","title":"Sorted merge list","text":"<p>two merge list in the sorted order to make a single list</p> <pre><code>def merge_helper(list1,list2):\n    combined=[]\n    i=0\n    j=0\n    while i&lt;len(list1) and j&lt;len(list2):\n        if list1[i] &lt; list2[j]:\n            combined.append(list1[i])\n            i+=1\n        else:\n            combined.append(list2[j])\n            j+=1\n\n    while i &lt; len(list1):\n        combined.append(list1[i])\n        i+=1\n\n    while j &lt; len(list2):\n        combined.append(list2[j])\n        j+=1\n\n    return combined\n\ndef merge_sort(mylist):\n  if len(mylist) ==1:\n    return mylist\n  mid_index = int(len(mylist)/2) # find the mid of index\n  left = merge_sort(mylist[:mid_index]) # split list to left until 1 item in sorted list\n  right = merge_sort(mylist[mid_index:]) # split list to right until 1 item in sorted list\n\n  return merge_helper(left,right) # combine two sorted list to make one\n\nprint(merge_sort([3,1,4,2])) # [1, 2, 3, 4]\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#big-o","title":"Big O","text":"<p>Space complexity: O(n) Time complexity: O(n log(n))</p>"},{"location":"programming/python/data_structures/sorting/#quick","title":"quick","text":"<p>Quick Sort is a widely used efficient sorting algorithm that follows the divide-and-conquer approach to sort an array or list of elements. It works by selecting a 'pivot' element from the array and partitioning the other elements into two subarrays: one containing elements less than the pivot and another containing elements greater than the pivot. The subarrays are then recursively sorted.</p> <pre><code>def swap(mylist,index1,index2):\n  mylist[index1],mylist[index2]=mylist[index2],mylist[index1]\n\n# return the index of the list\ndef pivot(mylist,pivot_index,end_index):\n  swap_index = pivot_index\n  for i in range(pivot_index+1,end_index+1):\n    if mylist[i]&lt;mylist[pivot_index]:\n      swap_index+=1\n      swap(mylist,swap_index,i)\n\n  swap(mylist,pivot_index,swap_index)\n  return swap_index\n\ndef quick_sort(mylist,left,right):\n  if left&lt;right:\n    pivot_index = pivot(mylist,left,right)\n    quick_sort(mylist,left,pivot_index-1)\n    quick_sort(mylist,pivot_index+1,right)\n  return mylist\n\nmylist = [4,6,1,7,3,2,5]\nprint(pivot(mylist,0,6))\n</code></pre>"},{"location":"programming/python/data_structures/stacks/","title":"stacks","text":"<p>A stack is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner. In stack, a new element is added at one end(push) and an element is removed from that end only(pop). </p> <p></p>"},{"location":"programming/python/data_structures/stacks/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None \n\nclass Stack:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.top = new_node \n        self.height=1\n\nmy_stack = Stack(4)\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#print-stack","title":"print stack","text":"<pre><code>\ndef print_stack(self):\n    temp = self.top \n    while temp is not None: \n        print(temp.value)\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#push","title":"push","text":"<pre><code>def push(self,value):\n    new_node = Node(value)\n    if self.height == 0:\n        self.top = new_node\n    else:\n        new_node.next =self.top \n        self.top = new_node\n    self.height+=1\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#pop","title":"pop","text":"<pre><code>def pop_stack(self):\n    if self.height==0:\n        return None \n\n    temp = self.top \n    self.top = self.top.next \n    temp.next = None\n    self.height-=1\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/trees/","title":"trees","text":"<p>A tree consists of a root node, leaf nodes, and internal nodes. Each node is connected to its child via a reference, which is called an edge.</p> <p></p> <p>Root Node: The root node is the topmost node of a tree. It is always the first node created while creating the tree and we can access each element of the tree starting from the root node. e.g the node containing element 50 is the root node</p> <p>Parent Node: The parent of any node is the node that references the current node. e.g 50 is the parent of 20 and 45, and 20 is the parent of 11, 46, and 15. Similarly, 45 is the parent of 30 and 78.</p> <p>Child Node: Child nodes of a parent node are the nodes at which the parent node is pointing using the references. In the example above, 20 and 45 are children of 50. The nodes 11, 46, and 15 are children of 20 and 30 and 78 are children of 45.</p> <p>Edge: The reference through which a parent node is connected to a child node is called an edge. In the above example, each arrow that connects any two nodes is an edge.</p> <p>Leaf Node: These are those nodes in the tree that have no children. In the above example, 11, 46, 15, 30, and 78 are leaf nodes.</p> <p>Internal Nodes: Internal Nodes are the nodes that have at least one child. In the above example, 50, 20, and 45 are internal nodes.</p>"},{"location":"programming/python/data_structures/trees/#binary-search-treebst","title":"Binary Search Tree(BST)","text":"<p>If the value of the node is lesser than the root, it would be alligned to left or else right to the node of the tree. </p>"},{"location":"programming/python/data_structures/trees/#bst-big-o","title":"BST Big-O","text":"<p>When you want to access or search for particular node in the tree, you start from root if the value is lesser than the root, you would search from left of it, which means you would get rid of the right side of the tree. Similary vide versa for the value greater than the root node. </p> <p>The input number of nodes directs the output time resulting in an average time complexity of O(log(n)).</p> <p>Let's take the wrost case scenerio, the value of the node you are searching is greater, then you need to go to the end of the right of the tree, which is similar to the list. In such case, it would be O(n)</p> <p>Summary, </p> Operations stack BST Big-O lookup Not preferred Preferred O(log(n)) insert Preferred Not preferred O(n) or O(1) remove Not preferred Preferred O(log(n))"},{"location":"programming/python/data_structures/trees/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\nmy_tree=BinarySearchTree()\nprint(my_tree.root)\n</code></pre>"},{"location":"programming/python/data_structures/trees/#insert","title":"insert","text":"<pre><code>def insert(self,value):\n    new_node = Node(value)\n    if self.root is None:\n        self.root=new_node\n        return True \n    temp = self.root\n    while(True):\n        # if the node of same value already exists\n        if new_node.value==temp.value: \n            return False\n\n        # if the node value is less then root value, add to left\n        if new_node.value &lt; temp.value:\n            if temp.left is None:\n                temp.left = new_node\n                return True\n            temp = temp.left\n        else:\n            # if the node value is greater then root value, add to right\n            if temp.right is None:\n                temp.right = new_node\n                return True\n            temp=temp.right\n\nmy_tree=BinarySearchTree()\nmy_tree.insert(2)\nmy_tree.insert(1)\nmy_tree.insert(3)\nprint(my_tree.root.value) # root \nprint(my_tree.root.left.value) # left of root\nprint(my_tree.root.right.value) # right of root\n</code></pre>"},{"location":"programming/python/data_structures/trees/#search","title":"search","text":"<pre><code>def search(self,value):\n    if self.root == None:\n        return None\n    temp = self.root \n    while temp is not None:\n        if value &lt; temp.value:\n            temp = temp.left \n        elif value &gt; temp.value:\n            temp = temp.right\n        else:\n            return True \n    return False\n</code></pre>"},{"location":"programming/python/data_structures/trees/#tree-traversals","title":"Tree Traversals","text":""},{"location":"programming/python/data_structures/trees/#breadth-first-searchbfs","title":"Breadth-First Search(BFS)","text":"<p>Breadth-First Search (BFS) is a graph traversal algorithm used to explore all the nodes in a graph, starting from a specific source node and visiting its neighbors before moving on to their neighbors, and so on</p> <pre><code>def BFS(self):\n    current_node = self.root\n    queue=[]\n    results = []\n    queue.append(current_node)\n    while len(queue) &gt; 0:\n        current_node=queue.pop(0)\n        results.append(current_node.value)\n        if current_node.left is not None:\n            queue.append(current_node.left)\n        if current_node.right is not None:\n            queue.append(current_node.right)\n    return results\n</code></pre>"},{"location":"programming/python/data_structures/trees/#depth-first-searchdfs","title":"Depth-First Search(DFS)","text":"<p>DFS explores nodes in depth-first fashion, meaning it goes as deep as possible along a branch before backtracking to explore other branches.</p>"},{"location":"programming/python/data_structures/trees/#pre-order","title":"pre-order","text":"<p>First, append the root node and traverse all the way to the left and add the node value, and then back track until the root node and then move from right</p> <pre><code>def dfs_pre_order(self):\n    results = []\n\n    def traverse(current_node):\n        results.append(current_node.value)\n\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n    traverse(self.root)\n\n    return results\n</code></pre>"},{"location":"programming/python/data_structures/trees/#post-order","title":"post-order","text":"<p>First, move to the left of the root node and append the value. </p> <pre><code>\ndef dfs_post_order(self):\n    results = []\n\n    def traverse(current_node):\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n        results.append(current_node.value)\n\n    traverse(self.root)\n\n    return results \n</code></pre>"},{"location":"programming/python/data_structures/trees/#in-order","title":"in-order","text":"<pre><code>def dfs_in_order(self):\n    results = []\n\n    def traverse(current_node):\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        results.append(current_node.value)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n    traverse(self.root)\n\n    return results \n</code></pre>"},{"location":"programming/python/data_structures/trees/#output","title":"output","text":"<pre><code>\nmy_tree=BinarySearchTree()\nmy_tree.insert(47)\nmy_tree.insert(21)\nmy_tree.insert(76)\nmy_tree.insert(18)\nmy_tree.insert(27)\nmy_tree.insert(52)\n\nprint(\"root node = \", my_tree.root.value)\nprint(\"pre order = \",my_tree.dfs_pre_order())\nprint(\"post order = \",my_tree.dfs_post_order())\nprint(\"in order = \",my_tree.dfs_in_order())\n\nroot node =  47\npre order =  [47, 21, 18, 27, 76, 52]\npost order =  [18, 27, 21, 52, 76, 47]\nin order =  [18, 21, 27, 47, 52, 76]\n</code></pre>"},{"location":"programming/python/design_patterns/adapter/","title":"Adapter","text":"<p>The Adapter pattern is a structural design pattern that allows objects with incompatible interfaces to work together. It acts as a bridge between two incompatible interfaces, converting the interface of one class into another interface that the client expects</p> <pre><code>class PaymentGateway:\n    def process_payment(self, amount):\n        pass\n\nclass BankPaymentGateway:\n    def make_payment(self, amount):\n        print(f\"Making payment of ${amount} via Bank Payment Gateway\")\n\nclass PayPalPaymentGateway:\n    def send_payment(self, amount):\n        print(f\"Sending payment of ${amount} via PayPal Payment Gateway\")\n\nclass PaymentGatewayAdapter(PaymentGateway):\n    def __init__(self, payment_gateway):\n        self.payment_gateway = payment_gateway\n\n    def process_payment(self, amount):\n        if isinstance(self.payment_gateway, BankPaymentGateway):\n            self.payment_gateway.make_payment(amount)\n        elif isinstance(self.payment_gateway, PayPalPaymentGateway):\n            self.payment_gateway.send_payment(amount)\n\n# Usage\nbank_gateway = BankPaymentGateway()\npaypal_gateway = PayPalPaymentGateway()\n\nadapter1 = PaymentGatewayAdapter(bank_gateway)\nadapter2 = PaymentGatewayAdapter(paypal_gateway)\n\nadapter1.process_payment(100)  # Output: \"Making payment of $100 via Bank Payment Gateway\"\nadapter2.process_payment(200)  # Output: \"Sending payment of $200 via PayPal Payment Gateway\"\n\n</code></pre> <p>In this example, we have a common PaymentGateway interface that represents the desired interface for processing payments. The PaymentGateway class defines the process_payment() method.</p> <p>We also have two existing payment gateway classes: BankPaymentGateway and PayPalPaymentGateway. These classes have their own specific methods for making payments (make_payment() for the bank gateway and send_payment() for the PayPal gateway).</p> <p>The PaymentGatewayAdapter class acts as an adapter that implements the PaymentGateway interface and internally holds an instance of the specific payment gateway. It adapts the specific payment gateway's method calls to the process_payment() method of the PaymentGateway interface.</p> <p>In the usage section, we create instances of the specific payment gateways: bank_gateway and paypal_gateway. We then create adapter instances (adapter1 and adapter2) and pass the corresponding payment gateway instances to their constructors.</p> <p>When calling the process_payment() method on the adapter objects, they internally invoke the specific methods (make_payment() or send_payment()) of the respective payment gateway objects. The adapters bridge the gap between the common PaymentGateway interface and the specific payment gateway classes.</p>"},{"location":"programming/python/design_patterns/bridge/","title":"Bridge","text":"<p>The Bridge pattern is a structural design pattern that decouples an abstraction from its implementation, allowing them to vary independently. It provides a way to separate the interface and implementation of a class hierarchy, enabling them to evolve independently.</p> <pre><code>class Device:\n    def __init__(self):\n        self.state = False\n\n    def is_enabled(self):\n        return self.state\n\n    def enable(self):\n        self.state = True\n\n    def disable(self):\n        self.state = False\n\n\nclass RemoteControl:\n    def __init__(self, device):\n        self.device = device\n\n    def toggle_power(self):\n        if self.device.is_enabled():\n            self.device.disable()\n        else:\n            self.device.enable()\n\n    def volume_up(self):\n        pass\n\n    def volume_down(self):\n        pass\n\n\nclass TV(Device):\n    def __init__(self):\n        super().__init__()\n        self.volume = 50\n\n    def volume_up(self):\n        if self.volume &lt; 100:\n            self.volume += 10\n\n    def volume_down(self):\n        if self.volume &gt; 0:\n            self.volume -= 10\n\n    def get_volume(self):\n        return self.volume\n\n\nclass Radio(Device):\n    def __init__(self):\n        super().__init__()\n        self.volume = 30\n\n    def volume_up(self):\n        if self.volume &lt; 100:\n            self.volume += 5\n\n    def volume_down(self):\n        if self.volume &gt; 0:\n            self.volume -= 5\n\n    def get_volume(self):\n        return self.volume\n\n\n# Usage\ntv = TV()\nremote_control = RemoteControl(tv)\n\nremote_control.toggle_power()\nremote_control.volume_up()\nremote_control.volume_up()\nprint(tv.is_enabled())  # Output: True\nprint(tv.get_volume())  # Output: 70\n\nradio = Radio()\nremote_control = RemoteControl(radio)\n\nremote_control.toggle_power()\nremote_control.volume_down()\nprint(radio.is_enabled())  # Output: True\nprint(radio.get_volume())  # Output: 25\n</code></pre> <p>In this example, we have the Device class hierarchy, which represents different entertainment devices such as TVs and radios. Each device has its own implementation of enabling/disabling and adjusting volume.</p> <p>The RemoteControl class acts as the abstraction and holds a reference to a Device object. It provides methods for toggling power, increasing volume, and decreasing volume. These methods delegate the operations to the respective methods of the assigned device.</p> <p>The TV and Radio classes are concrete implementations of the Device class. They provide specific implementations for enabling/disabling and adjusting volume. In this example, they have additional methods get_volume() to retrieve the current volume.</p> <p>In the usage section, we create instances of the TV and Radio classes. Then, we create instances of the RemoteControl class, passing the respective device objects. We can then use the remote control to toggle power and adjust the volume, and we can retrieve the current state and volume of the devices.</p> <p>By using the Bridge pattern, we separate the abstraction (remote control) from its implementation (device). This allows us to independently extend and modify both the remote controls and the entertainment devices, and easily switch between different combinations of remote controls and devices.</p>"},{"location":"programming/python/design_patterns/builder-facet/","title":"Builder facet","text":"<p>Sometimes it would be difficuilt to build multiple attributes in the object and hence we would need to create a step-by-step process for builder creation object</p>"},{"location":"programming/python/design_patterns/builder-facet/#person-example","title":"Person Example","text":"<p>let's assume, you have a person class in which the object person needs to have two builders i.e  <code>personal</code> and <code>work</code></p> <pre><code>class Person:\n    def __init__(self):\n        print(f\"Creating person class interface\")\n\n        # personal\n\n        self.address = None\n        self.code = None\n        self.city = None\n\n        # professional\n        self.company = None\n        self.position = None\n        self.salary = None\n\n    def __str__(self):\n        return f\"address: {self.address} location in code {self.code} in city {self.city}\\n\" + \\\n                   f\"Employed at {self.company} as {self.position} with salary {self.salary}\"\n</code></pre> <p>We will create a two seperate builders one for the <code>personal</code> and another for <code>professional</code></p> <p>Seperate Builder Class</p> <pre><code>class PersonBuilder:\n    def __init__(self, person=None):\n        self.person = Person() if person is None else person\n\n    # function for personal builder\n    @property\n    def personal(self):\n        return PersonPersonalBuilder(self.person)\n\n    # function for work builder\n    @property\n    def work(self):\n        return PersonWorkBuilder(self.person)\n\n    def build(self):\n        return self.person\n</code></pre> <p>PersonalBuilder</p> <pre><code>class PersonPersonalBuilder(PersonBuilder):\n    def __init__(self, person):\n        super().__init__(person)\n\n    def location_at(self, address):\n        self.person.address = address\n        return self\n\n    def location_code(self, code):\n        self.person.code = code\n        return self\n\n    def location_city(self, city):\n        self.person.city = city\n        return self\n</code></pre> <p>Professional Work Builder</p> <pre><code>class PersonWorkBuilder(PersonBuilder):\n    def __init__(self, person):\n        super().__init__(person)\n\n    def work_company(self, company):\n        self.person.company = company\n        return self\n\n    def work_position(self, position):\n        self.person.position = position\n        return self\n\n    def work_salary(self, salary):\n        self.person.salary = salary\n        return self\n</code></pre> <p>Finally, start to build the person. </p> <pre><code>pb = PersonBuilder()\nperson1 = pb.personal.location_at(\"Bang\").location_city(\"Bang\").location_code(560026)\\\n            .work.work_company(\"ino\").work_position(\"engineer\").work_salary(\"232323\").build()\n\nprint(person1)\nperson2=PersonBuilder().build() # None object\nprint(person2)\n</code></pre> <p>Output</p> <pre><code>address: Bang location in code 560026 in city Bang\nEmployed at ino as engineer with salary 232323\naddress: None location in code None in city None\nEmployed at None as None with salary None\n</code></pre>"},{"location":"programming/python/design_patterns/builder/","title":"Builder","text":"<p>The Builder pattern is a well-known pattern in Python world. It\u2019s especially useful when you need to create an object with lots of possible configuration options.</p>"},{"location":"programming/python/design_patterns/builder/#example-1-violation","title":"Example - 1 [ Violation ]","text":"<pre><code>class Product:\n    def __init__(self):\n        self.name = None\n        self.price = None\n        self.quantity = None\n\n    def set_name(self, name):\n        self.name = name\n\n    def set_price(self, price):\n        self.price = price\n\n    def set_quantity(self, quantity):\n        self.quantity = quantity\n\n    def display(self):\n        print(f\"Product: {self.name}, Price: {self.price}, Quantity: {self.quantity}\")\n\n\n\nclass ProductBuilder:\n    def __init__(self):\n        self.product = Product()\n\n    def set_name(self, name):\n        self.product.set_name(name)\n\n    def set_price(self, price):\n        self.product.set_price(price)\n\n    def set_quantity(self, quantity):\n        self.product.set_quantity(quantity)\n\n    def build(self):\n        return self.product\n\n# the Product class exposes its setters publicly, \n# allowing the client to directly set the attributes \n# instead of going through the builder.\n\n# Usage\nbuilder = ProductBuilder()\nbuilder.set_name(\"Widget\")\nbuilder.set_price(9.99)\nbuilder.set_quantity(10)\nproduct = builder.build()\nproduct.display()  # Output: \"Product: Widget, Price: 9.99, Quantity: 10\"\n</code></pre> <p>To fix this violation and adhere to the Builder Pattern, we need to encapsulate the construction process within the builder class and make the attributes private.</p> <pre><code>class Product:\n    def __init__(self,name, price, quantity):\n        self.name=name\n        self.price=price\n        self.quantity=quantity\n\n    def display(self):\n        print(f\"Product: {self.name}, Price: {self.price}, Quantity: {self.quantity}\")\n\nclass ProductBuilder:\n    def __init__(self):\n        self.name=None\n        self.price=None \n        self.quantity=None\n\n    def set_name(self,name):\n        self.name=name\n        return self \n\n    def set_price(self,price):\n        self.price=price\n        return self \n\n    def set_quantity(self,quantity):\n        self.quantity=quantity\n        return self\n\n    def build(self):\n        return Product(self.name,self.price,self.quantity)\n\n\n#Usage \nbuilder=ProductBuilder()\nproduct=builder.set_name(\"Widget\").set_price(9.99).set_quantity(10).build()\nproduct.display() #Output: \"Product: Widget, Price: 9.99, Quantity: 10\"\n\n</code></pre>"},{"location":"programming/python/design_patterns/builder/#example-2","title":"Example - 2","text":"<p>https://refactoring.guru/design-patterns/builder/python/example</p>"},{"location":"programming/python/design_patterns/composite/","title":"Composite","text":"<p>The Composite pattern is a structural design pattern that allows you to treat individual objects and groups of objects uniformly. It composes objects into tree-like structures to represent part-whole hierarchies. The pattern enables clients to work with individual objects and groups of objects in a consistent manner.</p> <pre><code>from abc import ABC, abstractmethod\n\n# Component\nclass FileComponent(ABC):\n    @abstractmethod\n    def get_size(self):\n        pass\n\n# Leaf\nclass File(FileComponent):\n    def __init__(self, name, size):\n        self.name = name\n        self.size = size\n\n    def get_size(self):\n        return self.size\n\n# Composite\nclass Directory(FileComponent):\n    def __init__(self, name):\n        self.name = name\n        self.children = []\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def remove_child(self, child):\n        self.children.remove(child)\n\n    def get_size(self):\n        total_size = 0\n        for child in self.children:\n            total_size += child.get_size()\n        return total_size\n\n# Usage\nroot = Directory(\"Root\")\n\nfile1 = File(\"File1.txt\", 10)\nfile2 = File(\"File2.txt\", 15)\nfile3 = File(\"File3.txt\", 20)\n\nsubdirectory = Directory(\"Subdirectory\")\nfile4 = File(\"File4.txt\", 5)\n\nsubdirectory.add_child(file4)\n\nroot.add_child(file1)\nroot.add_child(file2)\nroot.add_child(file3)\nroot.add_child(subdirectory)\n\nprint(root.get_size())  # Output: 50\n</code></pre> <p>In this example, we have the FileComponent interface that defines the common operations for both individual files and directories. The File class represents a leaf component, which is an individual file with a specific size. The Directory class represents a composite component, which is a directory containing other files and subdirectories.</p> <p>The File class implements the get_size() method to return its own size.</p> <p>The Directory class implements the get_size() method, which recursively calculates the total size of all its children. It maintains a list of children and provides methods to add and remove them.</p> <p>In the usage section, we create instances of files and directories and organize them into a hierarchy. We add individual files and a subdirectory to the root directory. When we call the get_size() method on the root directory, it recursively calculates the total size of all its children, including the files in the subdirectory.</p> <p>The resulting output is the total size of all the files and subdirectories within the root directory: 50.</p> <p>By using the Composite pattern, we can treat individual files and directories uniformly as FileComponent objects. This allows us to work with complex tree-like structures in a unified manner, enabling easy traversal and manipulation of the hierarchy.</p>"},{"location":"programming/python/design_patterns/decorators/","title":"Decorator","text":""},{"location":"programming/python/design_patterns/decorators/#functional-decorators","title":"Functional Decorators","text":"<p>Function decorators in Python are a way to modify the behavior of a function without changing its source code. Decorators are implemented using the concept of higher-order functions, where a function takes another function as an argument and returns a modified version of it.</p> <pre><code>def uppercase_decorator(func):\n    def wrapper():\n        result = func()\n        return result.upper()\n    return wrapper\n\n@uppercase_decorator\ndef say_hello():\n    return \"Hello, World!\"\n\nprint(say_hello())  # Output: \"HELLO, WORLD!\"\n\n</code></pre> <p>In this example, we define a decorator function called <code>uppercase_decorator</code>. It takes a function (func) as an argument and returns a new function called wrapper. The wrapper function modifies the behavior of the original function by calling it and returning the result in uppercase.</p> <p>Function decorators are commonly used in Python for various purposes, such as logging, timing, authentication, and data validation. They provide a clean and reusable way to modify the functionality of functions without modifying their original code.</p>"},{"location":"programming/python/design_patterns/decorators/#classic-decorators","title":"Classic Decorators","text":"<pre><code>class TextComponent:\n    def render(self):\n        pass\n\nclass BaseTextComponent(TextComponent):\n    def __init__(self,text):\n        self.text = text \n\n    def render(self):\n        return self.text\n\n    def __str__(self):\n        return f'rendering the text {self.text}'\n\n\nclass BoldDecorator(TextComponent):\n    def __init__(self,text_component):\n        self.text_component=text_component\n\n    def render(self):\n        base_text=self.text_component.render()\n        return f\"&lt;b&gt;{base_text}&lt;/b&gt;\"\n\nclass ItalicDecorator(TextComponent):\n    def __init__(self,text_component):\n        self.text_component=text_component\n\n    def render(self):\n        base_text=self.text_component.render()\n        return f\"&lt;i&gt;{base_text}&lt;/i&gt;\"\n\ntext=\"Hello world\"\ncomponent = BaseTextComponent(text)\nbold_decorator=BoldDecorator(component)\nitalic_decorator=ItalicDecorator(component)\n\nprint(bold_decorator.render()) # Output: &lt;b&gt;Hello world&lt;/b&gt;\nprint(italic_decorator.render()) # Output: &lt;i&gt;Hello world&lt;/i&gt;\n</code></pre> <p>In this example, we have the TextComponent abstract class, which defines the common interface for rendering text components.</p> <p>The BaseTextComponent class is a concrete implementation of the TextComponent class. It represents the base text that can be rendered.</p> <p>The BoldDecorator and ItalicDecorator classes are concrete decorators that inherit from the TextComponent class. They wrap around existing text components and provide additional functionality. Each decorator adds its own formatting tags ( for bold,  for italic) around the base text. <p>In the usage section, we create an instance of the BaseTextComponent with the original text. We then wrap it with a BoldDecorator, followed by an ItalicDecorator. The decorators add the respective formatting tags around the base text.</p> <p>When calling the render() method on the italic_decorator, it invokes the render() method of the wrapped bold_decorator, which in turn invokes the render() method of the wrapped component. This way, the decorators stack up, adding the desired formatting to the text.</p> <p>The resulting output is the decorated text with both bold and italic formatting: \"Hello, World!\"</p> <p>By using the Decorator pattern, you can dynamically add or modify the behavior of objects at runtime by wrapping them with different decorators. This allows for flexible and extensible composition of objects with additional features or behaviors.</p>"},{"location":"programming/python/design_patterns/decorators/#dynamic-decorators","title":"Dynamic decorators","text":"<p>the flexibility of dynamic decorators, where the behavior of the decorator can be determined at runtime based on the provided options.</p> <pre><code>def dynamic_decorator(option):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if option == 'uppercase':\n                result = func(*args, **kwargs)\n                return result.upper()\n            elif option == 'reverse':\n                result = func(*args, **kwargs)\n                return result[::-1]\n            else:\n                return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@dynamic_decorator('uppercase')\ndef say_hello():\n    return \"Hello, World!\"\n\nprint(say_hello())  # Output: \"HELLO, WORLD!\"\n\n@dynamic_decorator('reverse')\ndef say_hi():\n    return \"Hi, there!\"\n\nprint(say_hi())  # Output: \"!ereht ,iH\"\n\ndef do_nothing():\n    return \"Doing nothing.\"\n\nprint(do_nothing())  # Output: \"Doing nothing.\"\n</code></pre>"},{"location":"programming/python/design_patterns/factory-method/","title":"Factory Methods","text":"<p>Design pattern that provides an interface for creating objects, but allows <code>subclasess to decide which class to instantiate</code>. It provides loose coupling and flexibility in object creation. Factory Method pattern can be implemented using a base class or an abstract class and allowing subclasses to override a factory method</p> <pre><code>from abc import ABC, abstractmethod \n\n# Abstract class representing the product\nclass Animal(ABC):\n    @abstractmethod\n    def speak(self):\n        pass\n\n# Concrete class implementing the product\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof !\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow !\"\n\nclass Fox(Animal):\n    def speak(self):\n        return \"foxxx!\"\n\n# Creator class with factory method. \nclass AnimalFactory(ABC):\n    @abstractmethod\n    def create_animal(self)-&gt;Animal:\n        pass \n\nclass DogFactory(AnimalFactory):\n    def create_animal(self):\n        return Dog()\n\nclass CatFactory(AnimalFactory):\n    def create_animal(self):\n        return Cat()\n\nclass FoxFactory(AnimalFactory):\n    def create_animal(self):\n        return Fox()\n\n# Usage\n\ndog_factory=DogFactory()\ndog=dog_factory.create_animal()\nprint(dog.speak()) # Output: Woof !\n\nfox_factory=FoxFactory()\nfox=fox_factory.create_animal()\nprint(fox.speak()) # Output: Foxx !\n</code></pre>"},{"location":"programming/python/design_patterns/observer/","title":"Observer","text":"<p>The Observer Pattern is a behavioral design pattern used in software development to establish a one-to-many dependency between objects. In this pattern, one object (called the subject) maintains a list of dependent objects (observers) that are notified when the subject's state changes. This allows the observers to react and update themselves when the subject changes without the subject having to know about the observers specifically.</p> <pre><code># Define the Observer interface\nclass Observer:\n    def update(self, stock_symbol, stock_price):\n        pass\n\n# Define the Subject interface\nclass Subject:\n    def register_observer(self, observer):\n        pass\n\n    def remove_observer(self, observer):\n        pass\n\n    def notify_observers(self):\n        pass\n\n# Concrete implementation of the Subject\nclass StockMarket(Subject):\n    def __init__(self):\n        self.observers = []\n        self.stock_data = {}\n\n    def register_observer(self, observer):\n        self.observers.append(observer)\n\n    def remove_observer(self, observer):\n        self.observers.remove(observer)\n\n    def notify_observers(self):\n        for observer in self.observers:\n            observer.update(self.stock_data)\n\n    def set_stock_price(self, stock_symbol, stock_price):\n        self.stock_data[stock_symbol] = stock_price\n        self.notify_observers()\n\n# Concrete implementation of an Observer\nclass StockPriceDisplay(Observer):\n    def update(self, stock_data):\n        print(\"Stock Price Display:\")\n        for symbol, price in stock_data.items():\n            print(f\"{symbol}: {price}\")\n\n# Concrete implementation of another Observer\nclass StockAlert(Observer):\n    def update(self, stock_data):\n        for symbol, price in stock_data.items():\n            if price &gt; 200:\n                print(f\"Alert: {symbol} has crossed $100!\")\n\n# Main program\nif __name__ == \"__main__\":\n    stock_market = StockMarket()\n\n    price_display = StockPriceDisplay()\n    stock_alert = StockAlert()\n\n    stock_market.register_observer(price_display)\n    stock_market.register_observer(stock_alert)\n\n    # Simulate stock price changes\n    stock_market.set_stock_price(\"AAPL\", 150)\n    stock_market.set_stock_price(\"GOOGL\", 2500)\n    stock_market.set_stock_price(\"TSLA\", 800)\n</code></pre>"},{"location":"programming/python/design_patterns/prototype/","title":"Prototype","text":"<p>The Prototype design pattern is a creational design pattern that allows you to create new objects by cloning existing ones, rather than creating them from scratch.It promotes object reuse and reduces the need for subclassing</p> <pre><code>import copy\n\nclass Prototype:\n    def clone(self):\n        return copy.copy(self)\n\nclass Sheep(Prototype):\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def display(self):\n        print(f\"Sheep: {self.name}, Age: {self.age}\")\n\n# Usage\noriginal_sheep = Sheep(\"Shawn\", 2)\noriginal_sheep.display()  # Output: \"Sheep: Shawn, Age: 2\"\n\ncloned_sheep = original_sheep.clone()\ncloned_sheep.display()  # Output: \"Sheep: Shawn, Age: 2\"\n\ncloned_sheep.name = \"Dolly\"\ncloned_sheep.age = 3\n\ncloned_sheep.display()  # Output: \"Sheep: Dolly, Age: 3\"\n</code></pre> <p>In this example, we have a Prototype base class that defines the clone() method. The clone() method makes use of the copy.copy() function from the copy module to create a shallow copy of the object. This creates a new instance of the same class and copies the attributes of the original object to the clone.</p> <p>We can modify the cloned sheep's attributes independently. In the example, we change the cloned sheep's name to \"Dolly\" and age to 3. Calling the display() method on the cloned sheep shows the updated attribute values.</p>"},{"location":"programming/python/design_patterns/prototype/#prototype-using-factory-method","title":"Prototype using factory method","text":"<pre><code>import copy\n\nclass AnimalPrototype:\n    def clone(self):\n        return copy.copy(self)\n\n    def speak(self):\n        pass\n\nclass Dog(AnimalPrototype):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(AnimalPrototype):\n    def speak(self):\n        return \"Meow!\"\n\nclass AnimalFactory:\n    def __init__(self):\n        self.prototypes = {}\n\n    def register_prototype(self, animal_type, prototype):\n        self.prototypes[animal_type] = prototype\n\n    def create_animal(self, animal_type):\n        if animal_type in self.prototypes:\n            return self.prototypes[animal_type].clone()\n        else:\n            raise ValueError(\"Invalid animal type\")\n\n# Usage\nanimal_factory = AnimalFactory()\n\ndog_prototype = Dog()\nanimal_factory.register_prototype(\"Dog\", dog_prototype)\n\ncat_prototype = Cat()\nanimal_factory.register_prototype(\"Cat\", cat_prototype)\n\ndog = animal_factory.create_animal(\"Dog\")\nprint(dog.speak())  # Output: \"Woof!\"\n\ncat = animal_factory.create_animal(\"Cat\")\nprint(cat.speak())  # Output: \"Meow!\"\n\n</code></pre> <p>In this example, we have the AnimalPrototype base class that serves as the prototype for creating animal objects. It defines the clone() method to create a shallow copy of the object using copy.copy().</p> <p>The Dog and Cat classes inherit from AnimalPrototype and provide their own implementation of the speak() method.</p> <p>The AnimalFactory class acts as a factory that registers and creates animal objects based on their types. It maintains a dictionary of registered prototypes. The register_prototype() method allows new prototypes to be registered with their respective animal types, and the create_animal() method creates a new animal object based on the requested type by cloning the corresponding prototype.</p> <p>In the usage section, we create an instance of the AnimalFactory class. We then create instances of the Dog and Cat classes and register them as prototypes with their respective animal types using the register_prototype() method.</p> <p>Finally, we use the create_animal() method of the factory to create instances of animals by specifying their types. The factory retrieves the corresponding prototype, clones it, and returns the cloned object.</p> <p>By combining the Prototype and Factory Method patterns, we can create new animal objects by cloning existing prototypes, which promotes object reuse and avoids the need to create objects from scratch.</p>"},{"location":"programming/python/design_patterns/singleton/","title":"Singleton","text":"<p>The Singleton pattern is a creational design pattern that ensures that a class has only one instance and provides a global point of access to that instance.</p> <pre><code>class Singleton:\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n# Usage\nsingleton1 = Singleton()\nsingleton2 = Singleton()\n\nprint(singleton1 is singleton2)  # Output: True\n\n</code></pre>"},{"location":"programming/python/design_patterns/singleton/#singleton-using-decorator","title":"Singleton using decorator","text":"<pre><code>def singleton(cls):\n    instances = {}\n\n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n\n    return wrapper\n\n@singleton\nclass Singleton:\n    def __init__(self, value):\n        self.value = value\n\n# Usage\nsingleton1 = Singleton(42)\nsingleton2 = Singleton(24)\n\nprint(singleton1 is singleton2)  # Output: True\nprint(singleton1.value)  # Output: 42\nprint(singleton2.value)  # Output: 42\n\n</code></pre> <p>In this example, we define a singleton decorator function. The decorator function takes a class as an argument and returns a wrapper function that manages the creation and storage of instances.</p> <p>The wrapper function checks if an instance of the class exists in the instances dictionary. If not, it creates a new instance of the class and stores it in the dictionary. Subsequent calls to the wrapper function return the existing instance.</p> <p>The @singleton decorator is then applied to the Singleton class. This means that whenever we create an instance of the Singleton class, it goes through the singleton decorator and returns the existing instance if it has already been created.</p>"},{"location":"programming/python/design_patterns/singleton/#another-example-for-decorator","title":"Another example for decorator","text":"<pre><code>def singleton(cls):\n    instances={}\n\n    def get_instance(*args,**kwargs):\n        if cls not in instances:\n            instances[cls]=cls(*args,**kwargs)\n        return instances[cls]\n\n    return get_instance\n\n\n@singleton\nclass Database:\n    def __init__(self):\n        print(\"loading from the database\")\n\nd1=Database()\nd2=Database()\n\nprint(d1 is d2) # Output: true\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/","title":"Solid principles","text":"<p>SOLID is an acronym that stands for five key design principles: </p> <ul> <li>Single responsibility principle</li> <li>Open-closed principle</li> <li>Liskov substitution principle</li> <li>Interface segregation principle</li> <li>Dependency inversion principle</li> </ul>"},{"location":"programming/python/design_patterns/solid_principles/#single-responsibility-principle-srp","title":"Single Responsibility Principle (SRP)","text":"<p>The Single Responsibility Principle (SRP) states that a class or module should have only one reason to change. In other words, each class or module should have a single responsibility or purpose.</p> <pre><code>class Student:\n    def __init__(self, name, id_number):\n        self.name = name\n        self.id_number = id_number\n\n    def get_name(self):\n        return self.name\n\n    def get_id_number(self):\n        return self.id_number\n\n    def __str__(self):\n        return f\"name: {self.name} \\nid: {self.id_number}\\n\"\n\n    # Breaking SRP because, its saving to database and not associated with \n    Student class, so it should not be overloaded.\n    def register(self, filename):\n        with open(filename, \"w\") as fh:\n            fh.write(f\"name: {self.name} \\nid: {self.id_number}\\n\")\n</code></pre> <p>In order not to use the above rule, you would be modifying the rule as below and re-writing the function. </p> <pre><code>class StudentRegistration:\n    def save_to_file(self,student, filename):\n        with open(filename, \"w\") as fh:\n            fh.write(f\"name: {student.name} \\nid: {student.id_number}\\n\")\n</code></pre> <p>Create couple of students</p> <pre><code>student1=Student(\"Sunil\", \"1\")\nstudent2=Student(\"Shiva\", \"2\")\n</code></pre> <p>Save the student registration by using seperate class and save them.</p> <pre><code>register_student=StudentRegistration()\nfilename=r\"/Users/sunilamperayani/sandbox/design-patterns/students.db\"\nprint(\"saving the student to the database.\")\nregister_student.save_to_file(student1,filename)\n</code></pre> <p>Another example, </p> <pre><code>class FileReader:\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def read_file(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#open-closed-principle-ocp","title":"Open-Closed Principle (OCP)","text":"<p>The Open-Closed Principle (OCP) states that software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. In other words, the behavior of a software entity should be easily extended without modifying its source code.</p> <p>Let's explain by example...</p> <p>You have a products, and you need to filter the products.</p>"},{"location":"programming/python/design_patterns/solid_principles/#without-ocp","title":"Without OCP","text":"<pre><code>class Product:\n    def __init__(self, name,size,color):\n        self.name = name \n        self.size = size\n        self.color = color\n\nclass ProductFilter:\n    def filter_by_size(self,product,size):\n        for p in products:\n            if p.size == size: yield p \n</code></pre> <p>Let's say you need to <code>filter_by_color</code> / <code>filter_by_color_and_size</code> / <code>filter_by_color_or_size</code>. In future, you have more parameters like \"weight, height, thickness etc\" .. you can't scale the class as such. </p> <p>Hence once you create a class you would need to close the class for modifications and open for extension.</p> <pre><code>apple = Product(\"Apple\", COLOR.GREEN, SIZE.SMALL)\ntree = Product(\"Tree\", COLOR.GREEN, SIZE.LARGE)\nhouse = Product(\"House\", COLOR.BLUE, SIZE.MEDIUM)\n\nproducts = [apple,tree,house]\n\npf = ProductFilter()\nprint('Green products (old):')\nfor p in pf.filter_by_color(products,COLOR.GREEN):\n    print(f' - {p.name} is green')\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#with-ocp","title":"With OCP","text":"<p>You create a new functionality for <code>specifications</code> which satisfies the criteria for filtering. We would create a two base classes which can be used to override logic in future cases. </p> <pre><code># Base class\nclass Specification:\n    def is_satisfied(self, item):\n        pass\n\n# Overide the base class and use your logic for \n# future application.\n\nclass ColorSpecification(Specification):\n    def __init__(self, color):\n        self.color = color\n\n    def is_satisfied(self, item):\n        return item.color == self.color\n\nclass SizeSpecification(Specification):\n    def __init__(self, size):\n        self.size = size\n\n    def is_satisfied(self, item):\n        return item.size == self.size\n\nclass HeightSpecifcation(Specification):\n    pass \n\nclass WeightSpecification(Specification):\n    pass\n\n</code></pre> <p>We will create filter specification and override from this section. </p> <pre><code># Base class\nclass Filter:\n    def filter(self, items, spec):\n        pass\n\n\n# Overrise the baseclass and use for your application/business logic.\n\nclass BetterFilter(Filter):\n    def filter(self, items, spec):\n        for item in items:\n            if spec.is_satisfied(item):\n                yield item\n</code></pre> <pre><code>apple = Product(\"Apple\", COLOR.GREEN, SIZE.SMALL)\ntree = Product(\"Tree\", COLOR.GREEN, SIZE.LARGE)\nhouse = Product(\"House\", COLOR.BLUE, SIZE.MEDIUM)\n\nproducts = [apple,tree,house]\n\nbf = BetterFilter()\n\nprint('Green products (new):')\ngreen = ColorSpecification(COLOR.GREEN)\nfor p in bf.filter(products,green):\n    print(f' - {p.name} is green')\n\nprint('Blue products (new):')\nblue = ColorSpecification(COLOR.BLUE)\nfor p in bf.filter(products,blue):\n    print(f' - {p.name} is blue')\n\nprint('Large products:')\nlarge = SizeSpecification(SIZE.LARGE)\nfor p in bf.filter(products, large):\n    print(f' - {p.name} is large')\n</code></pre> <p>Another example, </p> <pre><code>class Shape:\n    def area(self):\n        pass\n\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius**2\n\n\ndef calculate_total_area(shapes):\n    total_area = 0\n    for shape in shapes:\n        total_area += shape.area()\n    return total_area\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#liskov-substitution-principle-lsp","title":"Liskov substitution principle (LSP)","text":"<p>In this principle, if the program is using the base class, it should be able to work correctly with any derived class of that base class without needing to know the specific subclass. </p> <p>let's read using demo</p>"},{"location":"programming/python/design_patterns/solid_principles/#without-lsp","title":"Without LSP","text":"<p>we would calculate the area of <code>rectangle</code> and <code>square</code>. A rectable would have a <code>height</code> and <code>width</code> where as square would have all the sides as same so we would equate height=width and then calculate the result of the area. </p> <pre><code># Base class\nclass Rectangle:\n    def __init__(self, width, height)\n        self.width=width\n        self.height=height\n\n    def set_width(self,width):\n        self.width=width \n\n    def set_height(self,height):\n        self.height=height\n\n    def area(self):\n        return self.height * self.width\n\n# Derived/subclass\nclass Square(Rectangle):\n    # Override from the base class\n\n    # This breaks the LSP principle, you can see the output\n    def __init__(self,length):\n        super().__init__(length,length)\n\n    def set_width(self,width):\n        self.width=width \n        self.height=height\n\n    def set_height(self,height):\n        self.height=height\n        self.width = height\n\ndef print_area(rectangle):\n    rectangle.set_width(5)\n    rectangle.set_height(4)\n    print(\"Area\", rectangle.area())\n\nrectangle=Rectangle(5,4) # Area: 20\nsquare=Square(5) # Area: 16 \n</code></pre> <p>You might have seen the above square method, since its being overridden from the rectangle class, the value has been changed completely in violation of LSP. </p> <p>We would rectify now and see how we can use that prunciple. </p>"},{"location":"programming/python/design_patterns/solid_principles/#with-lsp","title":"With LSP","text":"<p>we would define the base class <code>shape</code> which can be overridden and calculates the area of the shape. </p> <pre><code>class Shape:\n    def area(self):\n        pass\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\nclass Square(Shape):\n    def __init__(self, length):\n        self.side_length = length\n\n    def area(self):\n        return self.side_length**2\n\ndef print_area(shape):\n    print(\"Area:\", shape.area())\n\nrectangle=Rectangle(5,3)\nsquare=Square(5)\n\nprint_area(rectangle) # Output: Area: 15\nprint_area(square) # Output: Area: 25\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#interface-segregation-principle-isp","title":"Interface Segregation Principle (ISP)","text":"<p>The Interface Segregation Principle (ISP) states that clients should not be forced to depend on interfaces they do not use.</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-1-violation","title":"Example - 1 [ Violation ]","text":"<pre><code>class Animal:\n    def move(self):\n        pass\n\n    def eat(self):\n        pass\n\n    def swim(self):\n        pass\n\n\nclass Fish(Animal):\n    def move(self):\n        print(\"Swimming\")\n\n    def eat(self):\n        print(\"Eating underwater\")\n\n    def swim(self):\n        print(\"Swimming\")\n\n\nclass Bird(Animal):\n    def move(self):\n        print(\"Flying\")\n\n    def eat(self):\n        print(\"Eating insects\")\n\n    def swim(self):\n        raise NotImplementedError()\n\n\nclass Client:\n    def __init__(self, animal):\n        self.animal = animal\n\n    def move_animal(self):\n        self.animal.move()\n\n    def feed_animal(self):\n        self.animal.eat()\n\n    def swim_animal(self):\n        self.animal.swim()\n\n\n# Usage\nfish = Fish()\nbird = Bird()\n\nfish_client = Client(fish)\nbird_client = Client(bird)\n\nfish_client.swim_animal() # Expected output: \"Swimming\"\nbird_client.swim_animal() # Raises NotImplementedError\n</code></pre> <p>In this example, we have an Animal interface that defines three methods: move, eat, and swim. The Fish and Bird classes implement the Animal interface. <code>The Fish class implements all three methods, while the Bird class implements only move and eat</code>.</p> <p>The Client class depends on the Animal interface, but it doesn't require all the methods defined in the interface. <code>The swim method is not relevant for birds, so it raises a NotImplementedError</code>.</p> <p>This violates the Interface Segregation Principle because the <code>Animal interface is too broad and forces clients to depend on methods they don't need</code>. In this case, the Bird class is forced to implement a method it doesn't need.</p> <p>In order to make this principle non-violating, we could create separate interfaces for <code>Swimmable, Flyable, and Eatable</code>, and let the clients depend on the specific interfaces they require.</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-2-non-violation","title":"Example - 2 [ Non-Violation ]","text":"<pre><code>from abc import ABC, abstractmethod\n\n\nclass Moveable(ABC):\n    @abstractmethod\n    def move(self):\n        pass\n\n\nclass Eatable(ABC):\n    @abstractmethod\n    def eat(self):\n        pass\n\n\nclass Swimmable(ABC):\n    @abstractmethod\n    def swim(self):\n        pass\n\n\nclass Fish(Moveable, Eatable, Swimmable):\n    def move(self):\n        print(\"Swimming\")\n\n    def eat(self):\n        print(\"Eating underwater\")\n\n    def swim(self):\n        print(\"Swimming\")\n\n\nclass Bird(Moveable, Eatable):\n    def move(self):\n        print(\"Flying\")\n\n    def eat(self):\n        print(\"Eating insects\")\n\n\nclass Client:\n    def __init__(self, moveable_animal, eatable_animal=None, swimmable_animal=None):\n        self.moveable_animal = moveable_animal\n        self.eatable_animal = eatable_animal\n        self.swimmable_animal = swimmable_animal\n\n    def move_animal(self):\n        self.moveable_animal.move()\n\n    def feed_animal(self):\n        if self.eatable_animal:\n            self.eatable_animal.eat()\n\n    def swim_animal(self):\n        if self.swimmable_animal:\n            self.swimmable_animal.swim()\n\n\n# Usage\nfish = Fish()\nbird = Bird()\n\nfish_client = Client(fish, fish, fish)\nbird_client = Client(bird, bird)\n\nfish_client.swim_animal()  # Expected output: \"Swimming\"\nbird_client.swim_animal()  # No output (bird cannot swim)\n\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#dependency-inversion-principle-dip","title":"Dependency Inversion Principle (DIP)","text":"<p>The Dependency Inversion Principle (DIP) is a principle in object-oriented design that states that high-level modules should not depend on low-level modules. Instead, both should depend on abstractions. It promotes decoupling and flexibility in the codebase</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-1-violation_1","title":"Example - 1 [ Violation ]","text":"<pre><code>class PaymentProcessor:\n    def process_payment(self, amount):\n        print(f\"Processing payment of ${amount}\")\n\nclass PaymentService:\n    def __init__(self):\n        self.payment_processor = PaymentProcessor()\n\n    def perform_payment(self, amount):\n        self.payment_processor.process_payment(amount)\n\n# Usage\npayment_service = PaymentService()\npayment_service.perform_payment(100)  # Output: \"Processing payment of $100\"\n\n</code></pre> <p><code>PaymentService</code> class directory depends on the <code>PaymentProcessor</code>, creating an instance insite its consyructor there by, establishing a strong coupling between the two classes, violating <code>Dependency Inversion Principle</code></p>"},{"location":"programming/python/design_patterns/solid_principles/#example-2-fixing","title":"Example - 2 [ Fixing ]","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass PaymentProcessor(ABC):\n    @abstractmethod\n    def process_payment(self, amount):\n        pass\n\nclass CreditCardPaymentProcessor(PaymentProcessor):\n    def process_payment(self, amount):\n        print(f\"processing credit card payment {amount}\")\n\nclass PaypalPaymentProcessor(PaymentProcessor):\n    def process_payment(self, amount):\n        print(f\"processing paypal payment {amount}\")\n\nclass PaymentServices:\n    def __init__(self, payment_processor):\n        self.payment_processor=payment_processor\n\n    def perform_payment(self, amount):\n        self.payment_processor.process_payment(amount)\n\ncreditcard_processor=CreditCardPaymentProcessor()\npaypal_processor=PaypalPaymentProcessor()\n\npayment_service=PaymentServices(creditcard_processor)\npayment_service.perform_payment(100)\n\npayment_service=PaymentServices(paypal_processor)\npayment_service.perform_payment(200)\n</code></pre> <p>the <code>PaymentProcessor</code> abstract class, which serves as the abstraction that both the <code>PaymentService</code> and the concrete payment processors (<code>CreditCardPaymentProcessor and PayPalPaymentProcessor</code>) depend on</p> <p>By introducing the abstraction and decoupling the high-level and low-level modules, we adhere to the Dependency Inversion Principle, promoting flexibility, maintainability, and easier integration of new payment processors in the future</p>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/","title":"fastapi","text":""},{"location":"programming/python/webapp_framework/fastapi/fastapi/#install-fastapi","title":"Install FastAPI","text":"<pre><code>pip install \"fastapi[all]\"\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#hello-world-from-fastapi","title":"Hello World from FastAPI","text":"<pre><code>from fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#path-parameters","title":"path parameters","text":"<p>You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings path parameter item_id will be passed to your function as the argument item_id</p> <pre><code>@app.get(\"/items/{item_id}\")\nasync def read_item(item_id): # data convertion happens here\n    return {\"item_id\": item_id}\n</code></pre> <p>you can declare it with the data types</p> <pre><code>async def read_item(item_id: int ): # item_id is declared to be an int \n</code></pre> <p>All the data validation is performed under the hood by Pydantic, so you get all the benefits from it. You can use the same type declarations with str, float, bool and many other complex data types.</p> <p>Always remember, the order in which path parameters execute matters.  let's say you have two api calls, the one which is first defined will be returned. the path for /users/{user_id} would match also for /users/me, \"thinking\" that it's receiving a parameter user_id with a value of \"me\". </p> <p>If there is same two rest api end points, the one which is defined first would always be executed. </p> <pre><code>@app.get(\"/users/me\")\n#some code\n\n@app.get(\"/users/{user_id}\")\n#some code\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#query-parameters","title":"query parameters","text":"<p>When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. The query is the set of key-value pairs that go after the ? in a URL, separated by &amp; characters.</p> <pre><code>fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n\n@app.get(\"/items/\")\nasync def read_item(skip: int = 0, limit: int = 10):\n    # Query params are: skip = 0, limit = 10\n    return fake_items_db[skip : skip + limit]\n</code></pre> <p>http://127.0.0.1:8000/items/?skip=0&amp;limit=10</p>"},{"location":"programming/python/webapp_framework/flask/flask/","title":"flask","text":""},{"location":"programming/python/webapp_framework/flask/flask/#flask-overview","title":"Flask Overview","text":""},{"location":"secretmgmt/vault/access_vault_token/","title":"access token","text":""},{"location":"secretmgmt/vault/access_vault_token/#vault-interfaces","title":"vault interfaces","text":"<p>when user is authenticated to vault in any form, vault would generate the token based on the policies that's being set and would return back the token for usage. We could use that token for any future authenticate again. </p> <p>tokens are the core method of authetication, most of the operations in vault require an existing token. </p> <ul> <li>The token auth method is responsible for creating and storing tokens</li> <li>The token auth method cannot be disabled</li> <li>Tokens can be used directly, or they can be used with another auth method</li> <li>Authenticating with an external identity (e.g. LDAP) dynamically generate tokens</li> <li>Tokens have one or more policies attached to control what the token is allowed to perform</li> </ul>"},{"location":"secretmgmt/vault/access_vault_token/#types-of-tokens","title":"types of tokens","text":"<p>service tokens are the default token type in Vault</p> <ul> <li>They are persisted to storage (heavy storage reads/writes)</li> <li>Can be renewed, revoked, and create child tokens</li> </ul> <p>batch tokens are encrypted binary large objects (blobs)</p> <ul> <li>Designed to be lightweight &amp; scalable</li> <li>They are NOT persisted to storage but they are not fully-featured</li> <li>Ideal for high-volume operations, such as encryption</li> <li>Can be used for DR Replication cluster promotion as well</li> </ul> <p></p> <p>Tokens carry information and metadata that determines how the token can be used, what type of token, when it expires, etc.</p> <ul> <li>Accessor</li> <li>Policies</li> <li>TTL</li> <li>Max TTL</li> <li>Number of Uses Left</li> <li>Orphaned Token</li> <li>Renewal Status</li> </ul> <pre><code>vault token lookup s.d1BCdhug8buTgAnSZhtPm8Hp\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#token-heirarchy","title":"token heirarchy","text":"<p>Each token has a time-to-live (TTL), except root token.</p> <p>Tokens are revoked once reached its TTL unless renewed</p> <ul> <li>Once a token reaches its max TTL, it gets revoked</li> <li>May be revoked early by manually revoking the token</li> <li>When a parent token is revoked, all of its children are revoked as well.</li> </ul>"},{"location":"secretmgmt/vault/access_vault_token/#controlling-token-life-cycle","title":"controlling token life cycle","text":""},{"location":"secretmgmt/vault/access_vault_token/#periodic-token-life-cycle","title":"Periodic token life cycle","text":"<ul> <li>Root or sudo users have the ability to generate periodic tokens</li> <li>Periodic tokens have a TTL, but no max TTL</li> <li>Periodic tokens may live for an infinite amount of time, so long as they are renewed within their TTL</li> </ul> <pre><code>vault token create -policy=training -period=24h\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#service-token-with-use-limit","title":"Service token with use limit","text":"<p>When you want to limit the number of requests coming to Vault from a particular token: - Limit the token's number of uses in addition to TTL and Max TTL - Use limit tokens expire at the end of their last use, regardless of their remaining TTLs - Use limit tokens expire at the end of their TTLs, regardless of remaining uses.</p> <pre><code>vault token create -policy=\"training\" -use-limit=2\nvault token lookup &lt;token&gt;\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#orphan-service-token","title":"Orphan service token","text":"<p>When the token hierarchy behavior is not desirable:</p> <ul> <li>Root or sudo users have the ability to generate orphan tokens</li> <li>Orphan tokens are not children of their parent; therefore, do not expire when their parent does</li> <li>Orphan tokens still expire when their own Max TTL is reached</li> </ul> <pre><code>vault token create -policy=\"training\" -orphan\nvault token lookup &lt;token&gt; \n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#set-token-types","title":"Set token types","text":"<pre><code>vault token create -policy=\"training\" -period=\"24h\"\n</code></pre> <p>configure the AppRole auth method to generate batch tokens</p> <pre><code>vault auth enable approle\nvault write auth/approle/role/training policies=\"training\" token_type=\"batch\" token_ttl=\"60s\"\n</code></pre> <p>configure the AppRole auth method to generate periodic tokens:</p> <pre><code>vault write auth/approle/role/jenkins policies=\"jenkins\" period=\"72h\"\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#managing-tokens-in-vault","title":"Managing Tokens in Vault","text":""},{"location":"secretmgmt/vault/access_vault_token/#cli","title":"CLI","text":"<pre><code>vault token create \u2013display_name=jenkins \u2013policy=training,certs \u2013ttl=24h \u2013explicit-max-ttl = 72h\n\nvault token create -ttl=5m -policy=training\nvault token lookup s.12VNpg4OA9tTdCd4V6ODuDRK\nvault token revoke s.12VNpg4OA9tTdCd4V6ODuDRK\nSuccess! Revoked token (if it existed)\n</code></pre> <p>Look up the capabilities of a token on a particular path</p> <pre><code>vault token capabilities s.dhtIk8VsE3Mj61PuGP3ZfFrg kv/data/apps/webapp\nvault token lookup s.dhtIk8VsE3Mj61PuGP3ZfFrg   # know ttl for this token\nvault token renew s.dhtIk8VsE3Mj61PuGP3ZfFrg\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#ui","title":"UI","text":"<p>skipping</p>"},{"location":"secretmgmt/vault/access_vault_token/#api","title":"API","text":"<p>parse the output for <code>.auth.client_token</code> thats the client token you need to read</p> <pre><code>curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/sunil | jq\n</code></pre> <p>store the token in a variable</p> <pre><code>token = $(curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/bryan |\njq -r \".auth.client_token\")\necho $token\n</code></pre> <p>Get the complete output and set into ENV variable</p> <pre><code>OUTPUT=$(curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/sunil)\nVAULT_TOKEN=$(echo $OUTPUT | jq '.auth.client_token' -j)\necho $VAULT_TOKEN\n</code></pre> <p>Client token must be sent in the X-Vault-Token HTTP header and put into KV store</p> <pre><code>curl --header \"X-Vault-Token: s.dhtIk8VsE3Mj61PuGP3ZfFrg\" --request POST --data '{ \"apikey\": \"3230sc$832d\" }'https://vault.example.com:8200/v1/secret/apikey/splunk\n\n\ncurl --header \"X-Vault-Token: s.dhtIk8VsE3Mj61PuGP3ZfFrg\" --request GET https://vault.example.com:8200/v1/secret/data/apikey/splunk \n\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#root-token","title":"Root token","text":"<p>Root token is a superuser that has unlimited access to Vault</p> <ul> <li>It does NOT have a TTL \u2013 meaning it does not expire</li> <li>Attached to the root policy</li> <li>Note: Root tokens can create other root tokens that DO have a TTL</li> </ul> <p>Root tokens should NOT be used on a day-to-day basis</p> <ul> <li>In fact, rarely should a root token even exist</li> <li>Once you have used the root token, it should be revoked.</li> </ul> <p>Where Do Root Tokens Come From?</p> <p>Initial root token comes from Vault initialization</p> <ul> <li>Only method of authentication when first deploying Vault</li> <li>Used for initial configuration \u2013 such as auth methods or audit devices</li> <li>Once your new auth method is configured and tested, the root token should be revoked</li> </ul> <pre><code>vault token revoke s.dhtIk8VsE3Mj61PuGP3ZfFrg\n</code></pre> <p>Create a root token from an existing root token</p> <p>You can authenticate with a root token and run a vault token create.</p> <p>Now you have 2 root tokens</p> <pre><code>vault login s.lmmOCfNH1HZvvBwxnLErWrhK\nvault token create\n</code></pre> <p>Create a root token using unseal/recovery keys</p> <ul> <li>Helpful if you need to generate a root token in an emergency or a root token is needed for a particular task</li> <li>A quorum of unseal key holders can generate a new root token</li> <li>Enforces the \"no single person has complete access to Vault\"</li> </ul> <p>There are few steps to generate recovery root keys or unseal keys</p> <pre><code>vault operator generate-root -init # Generates the OTP\nvault operator generate-root # keep repeating until threshold is met and copy **Encoded Token**\nvault operator generate-root -otp=\"hM9q24nNiZfnYIiNvhnGo4UFc3\" -decode=\"G2NeKUZgXTsYYxILAC9ZFBguPw9ZXBovFAs\"\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#token-accessors","title":"token accessors","text":"<p>Every token has a token accessor that is used as a reference to the token</p> <p>Token accessors can be used to perform limited actions - Look up token properties - Look up the capabilities of a token - Renew the token - Revoke the token</p> <pre><code>vault login s.cbC7GJ6U6WJaDuDSgkyVcKDv\nvault token create -policy=training -ttl=30m\nvault token lookup -accessor gFq2UwnJ0jo87kESKwUcl1Ub\nvault token create -policy=training -ttl=30m # you can use token_accessor\nvault token revoke 2ogWa36gDH5wsO8VbuxroByx\nvault token renew -accessor gFq2UwnJ0jo87kESKwUcl1Ub\n</code></pre> <p>Cannot Use an Accessor to Perform Traditional Vault Actions</p> <pre><code>set VAULT_TOKEN=gFq2UwnJ0jo87kESKwUcl1Ub\nvault kv get secret/apps/training\nError making API request.\nURL: GET\nhttp://127.0.0.1:8200/v1/sys/internal/ui/mounts/secret/apps/training\nCode: 403. Errors:\n* permission denied\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#ttl-explanation","title":"TTL explanation","text":"<p>Every non-root token has a TTL, which is the period of validity (how long it's good for)</p> <p>TTL is based on the creation (or renewal) time: - Example: New token was created - valid for 30 minutes from now - Example: token was just renewed for 30 min = has a new 30m TTL</p> <p>When a token's TTL expires, the token is revoked and is no longer valid and cannot be used for authentication. - Renewal must take place before the TTL expires</p> <p>ttl examples</p> <p></p> <p></p> <p>Vault has a default TTL of 768 hours (which is 32 days), This can be changed in the Vault configuration file default_lease_ttl = 24h</p> <pre><code>vault token create \u2013policy=training \u2013ttl=60m\nvault write auth/approle/role/training-role token_ttl=1h token_max_ttl=24h\nvault token create \u2013policy=training\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/","title":"auth methods","text":""},{"location":"secretmgmt/vault/auth_methods/#introduction","title":"Introduction","text":"<ul> <li>vault components that perform authentication and manage identities.</li> <li>Responsible for assigning identity and policies to a user.</li> <li>Multiple authentication methods can be enabled depending on your use case.</li> <li>Auth methods can be differentiated by human vs. system methods.</li> <li>Once authenticated, vault will issue a client token used to make all subsequent vault requests (read/write).</li> <li>The fundamental goal of all auth methods is to obtain a token.</li> <li>Each token has an associated policy (or policies) and a TTL.</li> </ul> <p>fundamental goal of auth method is to get token, which are core method for auth within vault.</p> <p>token method is responsible for creating and storing token, which can't be disabled. Authenticating with external identity (LDAP, OIDC) will generate a token. if you are not supplying token for auth, you would get 403 error. </p> <p></p>"},{"location":"secretmgmt/vault/auth_methods/#working-with-auth-methods","title":"Working with auth methods","text":"<p>few auth methods which are valid and must be enabled before use..</p> <ul> <li>okta</li> <li>Github</li> <li>kubenetes</li> <li>kerberos</li> <li>username/password</li> <li>TLS certs</li> </ul> <p>The token auth method is enabled by default, and you cannot enable another nor disable the tokens auth method - New vault deployment will use a token for authentication - Only method of authentication for a new vault deployment is a root token. you can later change your auth once root token login. </p> <p>Auth methods can be enabled/disabled and configured using the UI, API, or CLI. You must provide a valid token to enable, disable, or modify auth methods in vault. The token must also have the proper privileges.</p> <p>Each auth method is enabled at a path. You can choose the path name when (and only when) you enable the auth method. If you do not provide a name, the auth method will be enabled at its default path</p> <p>auth methods</p>"},{"location":"secretmgmt/vault/auth_methods/#auth-using-cli","title":"auth using CLI","text":"<pre><code>vault auth enable approle or userpass\nvault auth disable approle or userpass\nvault auth list\n\nvault auth enable \u2013path=vault-course \u2013description=MyApps approle\n</code></pre> <p>vault auth -  Type of vault object you want to work with  enable  - Subcommand  \u2013path=vault-course - Customize the Path Name  \u2013description=MyApps - Add a description approle - Type of Auth Method</p> <p>After the auth method has been enabled, use the auth prefix to configure the auth method:</p> <pre><code>vault write auth/&lt;path name&gt;/&lt;option&gt; \n&lt;options&gt; = \"users\" for userpass or \"role\" for approle\n</code></pre> <p>auth methods using cli reference docs</p> <p>There are a few ways to authenticate to vault when using the CLI - Use the vault login command    - Authenticate using a token or another auth method    - Makes use of a token helper</p> <ul> <li>Use the vault_TOKEN Environment Variable</li> <li>Used if you already have a token</li> </ul> <pre><code>vault login  &lt;root token&gt; # uses token method\nvault login -method=userpass username=sunil # Once your username/password is correct, you would get the token. \n\n\u2013method=userpass - Type of Auth Method Used to Authenticate (not the enabled path)\n</code></pre> <p>Token Helper: Caches the token after authentication. Stores the token in a local file(.vault-token)so it can be referenced for subsequent requests.</p> <p></p> <p>HTTP API Response: </p> <p>Parsing the JSON Response to Obtain the vault Token</p> <pre><code>$ export vault_ADDR=\"https://vault.example.com:8200\"\n$ export vault_FORMAT=json\n$ OUTPUT=$(vault write auth/approle/login role_id=\"12345657\" secret_id=\"1nv84nd3821s\")\n$ vault_TOKEN=$(echo $OUTPUT | jq '.auth.client_token' -j)\n$ vault login $vault_TOKEN\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#auth-using-api","title":"auth using API","text":"<p>Create a new json object and save the file.</p> <pre><code>curl --header \"X-vault-Token: $vault_TOKEN\" --request POST --data '{\"type\": \"approle\"}' \\\nhttp://127.0.0.1:8200/v1/sys/auth/approle\n\n# The following command specifies that the tokens issued under the AppRole my-role should be associated with my-policy.\n\ncurl --header \"X-vault-Token: $vault_TOKEN\" --request POST --data '{\"policies\": [\"my-policy\"]}' \\\nhttp://127.0.0.1:8200/v1/auth/approle/role/my-role\n\n\n# fetches the RoleID of the role named my-role\n\ncurl --header \"X-vault-Token: $vault_TOKEN\" \\\nhttp://127.0.0.1:8200/v1/auth/approle/role/my-role/role-id | jq -r \".data\"\n\n</code></pre> <p>auth method using api reference docs</p> <p>Authentication requests to the vault HTTP API return a JSON response that include: - the token - the token accessor - information about attached policies</p> <p>It is up to the user to parse the response for the token and use that token for any subsequent requests  to vault.</p> <p>Autheticate to the vault server</p> <pre><code>curl --request POST --data @auth.json https://vault.example.com:8200/v1/auth/approle/login\n</code></pre> <p>If your role id and secret id are correct, then you would have below response where you have your token i.e  client_token. </p> <pre><code>{\n\"request_id\": \"0f874bea-16a6-c3da-8f20-1f2ef9cb5d22\",\n\"lease_id\": \"\",\n\"renewable\": false,\n\"lease_duration\": 0,\n\"data\": null,\n\"wrap_info\": null,\n\"warnings\": null,\n    \"auth\": {\n    \"client_token\": \"s.wjkffdrqM9QYTOYrUnUxXyX6\", -&gt; this is the user token\n    \"accessor\": \"Hbhmd3OfVTXnukBv7WxMrWld\",\n        \"policies\": [\n        \"admin\",\n        \"default\"\n        ],\n    }\n}\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#api-explorer","title":"API explorer","text":"<p>login to vault UI and to the right top corner, open an terminal and type \"API\". This will open a swagger which has all the API methods of the vault. You can try to send an request, if its authenticated correctly, it would response back.</p>"},{"location":"secretmgmt/vault/auth_methods/#vault-entities","title":"vault entities","text":"<ul> <li> <p>vault creates an entity and attaches an alias to it if a corresponding entity doesn't already exist.</p> </li> <li> <p>This is done using the Identity secrets engine, which manages internal identities that are recognized by vault</p> </li> <li> <p>An entity is a representation of a single person(userpass or LDAP) or system used(approle) to log into vault. Each has a unique value. Each entity is made up of zero or more aliases</p> </li> <li> <p>Alias is a combination of the auth method plus some identification. It is a mapping between an entity and auth method(s)</p> </li> </ul> <p>e.g let's say I am Sunil, I have to login to vault using to validate my creds to get the token. so I would be associated with different policies to get the different tokens, so I would always need to logout when requited to get secrets from different roles, which is cumbersome process... what if we have all policies grouped with single entity and provide entiry_id etc, so when I get creds it would use the entityy id to get the tokens for all the roles(Token inherits capabilities granted by both policies), so that I don't have to logout each and every time.</p> <p></p>"},{"location":"secretmgmt/vault/auth_methods/#vault-identity-groups","title":"vault identity groups","text":"<p>\u2022 A group can contain multiple entities as its members. \u2022 A group can also have subgroups. \u2022 Policies can be set on the group and the permissions will be granted to all members of the group.</p>"},{"location":"secretmgmt/vault/auth_methods/#vault-groups","title":"vault groups","text":"<p>Internal:</p> <p>Groups created in vault to group entities to propagate identical permissions, manually created</p> <p>Internal groups can be used to easily manage permissions for entities</p> <ul> <li>Frequently used when using vault Namespaces to propagate permissions down to child namespaces</li> <li>Helpful when you don't want to configure an identical auth method on every single namespace</li> </ul> <p>External:</p> <p>Groups which vault infers and creates based on group associations coming from auth methods, created manually or automatically.</p> <p>External groups are used to set permissions based on group membership from an external identity provider, such as LDAP, Okta, or OIDC provider. - Allows you to set up once in vault and continue manage permissions in the identity provider. - Note that the group name must match the group name in your identity provider</p>"},{"location":"secretmgmt/vault/auth_methods/#choosing-auth-methods","title":"Choosing auth methods","text":"<p>Many auth methods may satisfy the requirements, but often there's one that works \"the best\" for a situation e.g incase you are using a certain platform does not mean you need to use the related auth method e.g: Azure virtual machines can authenticate using the Azure auth method, but AppRole, Userpass, TLS, OIDC, etc. would still be a possibility. It's usually easy to eliminate auth methods based on the way they operate or integrate with applications</p> <p>Key words when choosing an auth method:</p> <p>Frequently Rotated - generally means a dynamic credential. - Meets the requirements: AWS, LDAP, Azure, GCP, K8s. - Does not meet the requirements: Userpass, TLS, AppRole.</p> <p>Remove Secrets from Process or Build Pipeline - generally means a dynamic or integrated credential. - Meets the requirements: AWS, Azure, GCP, K8s. - Does not meet the requirements: Userpass, LDAP.</p> <p>Use Existing User Credentials - Generally means you should integrate with an existing Identity Provider. - Meets the Requirement: OIDC, LDAP, Okta, GitHub. - Does not meet the requirements: Userpass, AWS, Azure, GCP.</p>"},{"location":"secretmgmt/vault/auth_methods/#human-based-auth","title":"Human based Auth","text":"<ul> <li>Integrates with an Existing Identity Provider</li> <li>Requires a Hands-On Approach to Use(Okta, Userpas, GitHub, JWT/OIDC, radius)</li> <li>Logging in via Prompt or Pop-up</li> <li>Often configured with the Platforms Integrated MFA</li> </ul>"},{"location":"secretmgmt/vault/auth_methods/#system-based-auth-methods","title":"System-Based Auth Methods","text":"<ul> <li>Uses non-human friendly methologies (not easy to remember) i.e tokens</li> <li>Usually Integrates with an Existing Platform(AWS, Azure, Oracle Cloud, kerberos, kubernetes, TLS certs)</li> <li>Vault validates credentials with the platform</li> </ul>"},{"location":"secretmgmt/vault/auth_methods/#lab","title":"lab","text":""},{"location":"secretmgmt/vault/auth_methods/#create-auth-method-of-userpass-and-get-the-user-token-from-vault","title":"Create auth method of userpass and get the user token from vault","text":"<pre><code>vault auth list\nvault auth enable userpass\nvault write auth/userpass/users/sunil password=sunil policies=sunil\nvault write auth/userpass/users/shiva password=shiva policies=shiva\nvault list auth/userpass/users\nvault read auth/userpass/users/sunil\nvault login -method=userpass username=sunil [Enter]\nPassword: sunil\n\n&lt;lists the token&gt;\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#create-auth-method-of-approle-and-to-get-from-vault","title":"Create auth method of approle and to get from vault","text":"<pre><code>vault auth list\nvault auth enable approle\nvault write auth/approle/role/sunil policies=sunil token_ttl=20m\nvault list auth/approle/role/\nvault read auth/approle/role/sunil/role_id \nvault write -f auth/approle/role/sunil/secret-id \nvault write auth/approle/login role_id=\"\" secret_id=\"\"\n</code></pre>"},{"location":"secretmgmt/vault/policies/","title":"policies","text":""},{"location":"secretmgmt/vault/policies/#overview","title":"Overview","text":"<p>Vault policies provide operators a way to permit or deny access to certain paths or actions within Vault (RBAC).</p> <ul> <li>Gives us the ability to provide granular control over who gets access to secrets.</li> <li>Policies are written in declarative statements and can be written using JSON or HCL.</li> <li>When writing policies, always follow the principal of least privilege, in other words, give users/applications only the permissions they need.</li> <li>Policies are Deny by Default (implicit deny) - therefore you must explicitly grant to paths and related capabilities to Vault clients(No policy = no authorization).</li> <li>Policies support an explicit DENY that takes precedence over any other permission.</li> <li>Policies are attached to a token. A token can have multiple policies</li> </ul>"},{"location":"secretmgmt/vault/policies/#policy-types","title":"policy types","text":"<p>root policy is created by default \u2013 superuser with all permissions. - You cannot change nor delete this policy - Attached to all root tokens</p> <p>Note: The root policy does not contain any rules but can do anything within Vault. It should be used with extreme care.</p> <p>default policy is created by default \u2013 provides common permissions. - You can change this policy but it cannot be deleted - Attached to all non-root tokens by default (can be removed if needed)</p> <pre><code>vault policy list\nvalut policy read root\nvault policy read default\nvault policy write admin-policy /tmp/admin.hcl where, \n\npolicy - type of object you want to work with \nwrite - sub command\nadmin-policy - define the name of the policy you want to create\n/tmp/admin.hcl - location of the file containing pre-written policy.\n</code></pre>"},{"location":"secretmgmt/vault/policies/#manage-vault-policies","title":"Manage vault policies","text":""},{"location":"secretmgmt/vault/policies/#cli","title":"cli","text":"<p>Use the vault policy command has the below methods to perform management</p> <ul> <li>delete</li> <li>fmt</li> <li>list</li> <li>read</li> <li>write</li> </ul> <pre><code>vault policy list\nvault policy write webapp /tmp/webapp.hcl\nvault policy write packer /tmp/packer.hcl\n</code></pre>"},{"location":"secretmgmt/vault/policies/#http-api","title":"http API","text":"<pre><code>vim payload.json\n{\"policy\": \"path \\\"kv/apps/webapp\\\" { capabilities\u2026 \" }}\n\n# curl --header \"X-Vault-Token: s.bCEo8HFNIIR8wRGAzwXwkqUk\" --request PUT --data @payload.json \\\nhttp://127.0.0.1:8200/v1/sys/policy/webapp\n</code></pre>"},{"location":"secretmgmt/vault/policies/#permission","title":"permission","text":"<p>You know everything in vault is path based, hence you need to provide access or forbid access to these paths and operations.</p> <pre><code>path \"&lt;path&gt;\" {\n    capabilities = [ \"&lt;list of permissions&gt;\"]\n}\n\ne.g: \n\npath \"kv/data/apps/jenkns\" {\n    capabilities = [ \"read\", \"update\", \"delete\"]\n}\n</code></pre>"},{"location":"secretmgmt/vault/policies/#paths","title":"Paths","text":"<p>These are some of the standard paths that are defined in the vault.</p> <pre><code>sys/policy/vault-admin\nkv/apps/app01/web\nauth/ldap/group/developers\ndatabase/creds/prod-db\nsecrets/data/platform/aws/tools/ansible/app01\nsys/rekey\n</code></pre>"},{"location":"secretmgmt/vault/policies/#root-protected","title":"root-protected","text":"<ul> <li>Many paths in Vault require a root token or sudo capability to use</li> <li>These paths focus on important/critical paths for Vault or plugins</li> </ul> <p>e.g of root-protected paths:</p> <ul> <li>auth/token/create-orphan (create an orphan token)</li> <li>pki/root/sign-self-issued (sign a self-issued certificate)</li> <li>sys/rotate (rotate the encryption key)</li> <li>sys/seal (manually seal Vault)</li> <li>sys/step-down (force the leader to give up active status)</li> <li>sys/rotate (rotate the encryption key)</li> <li>sys/seal (manually seal Vault)</li> <li>sys/step-down (force the leader to give up active status)</li> </ul>"},{"location":"secretmgmt/vault/policies/#capabilities","title":"capabilities","text":"<p>Capabilities define what can we do? Capabilities are specified as a list of strings (yes, even if there's just one)</p> Capability HTTP Verb create POST/PUT read GET update POST/PUT delete DELETE list LIST Capability Description sudo Allows access to paths that are root-protected deny Disallows access regardless of any other defined capabilities <ul> <li>Create \u2013 create a new entry</li> <li>Read \u2013 read credentials, configurations, etc</li> <li>Update \u2013 overwrite the existing value of a secret or configuration</li> <li>Delete \u2013 delete something</li> <li>List \u2013 view what's there (doesn't allow you to read)</li> <li>Sudo \u2013 used for root-protected paths</li> <li>Deny \u2013 deny access \u2013 always takes presedence over any other capability</li> </ul> <p>example of deny polict in vault</p> <p></p>"},{"location":"secretmgmt/vault/policies/#customization","title":"customization","text":"<p>The glob (*) is a wildcard and can only be used at the end of a path, can be used to signify anything \"after\" a path or as part of a pattern.</p> <p>e.g</p> <p>secret/apps/application1/ - allows any path after application1 kv/platform/db- - would match kv/platform/db-2 but not kv/platform/db2</p> <p>i.e  secret - Path where the secrets engine is mounted apps/application1 - Path created on the secrets engine called secret * - Apply capabilities on anything AFTER application1</p> <pre><code>path \"secret/apps/application1/*\" {\ncapabilities = [\"read\"]\n} \n</code></pre> <p>Can I read from the following path? No, because the policy only permits read access for anything AFTER application1, not the path secret/apps/application1 itself.</p> <p>if you wanted to read, then change the policy...</p> <pre><code>path \"secret/apps/application1/*\" {\ncapabilities = [\"read\"]\n}\n\npath \"secret/apps/application1\" {\ncapabilities = [\"read\"]\n}\n</code></pre> <p>The plus (+) supports wildcard matching for a single directory in the path, can be used in multiple path segments (i.e., secret/+/+/db)</p> <p>e.g - secret/+/db - matches secret/db2/db or secret/app/db - kv/data/apps/+/webapp \u2013 matches the following: - kv/data/apps/dev/webapp - kv/data/apps/qa/webapp - kv/data/apps/prod/webapp</p> <p>secret/data/+/apps/webapp</p> <p>secret - Path where the secrets engineis mounted\\ data - Used for KV V2 Secrets Engine  + - Can be ANY Remaining path value  apps/webapp - Remaining path</p> <p>e.g combining + and * in policy. </p> <pre><code>path \"secret/+/+/webapp\" {\ncapabilities = [\"read\", \"list\"]\n}\n\npath \"secret/apps/+/team-*\" {\ncapabilities = [\"create\", \"read\"]\n}\n</code></pre>"},{"location":"secretmgmt/vault/policies/#acl","title":"acl","text":"<p>Use variable replacement in some policy strings with values available to the token, define policy paths containing double curly braces: {{}} <p>Creates a section of the key/value v2 secret engine to a specific user </p> <pre><code>path \"secret/data/{{identity.entity.id}}/*\" {\ncapabilities = [\"create\", \"update\", \"read\", \"delete\"]\n}\n\npath \"secret/metadata/{{identity.entity.id}}/*\" {\ncapabilities = [\"list\"]\n}\n</code></pre> <p></p> <p>Reference: https://developer.hashicorp.com/vault/tutorials/policies/policy-templating?in=vault%2Fpolicies</p> <p>How to identify the policies that are attached.</p> <pre><code>$ vault token create -policy=\"web-app\"\n\n# Authenticate with the newly generated token\n$ vault login &lt;token&gt;\n\n# Make sure that the token can read\n$ vault read secret/apikey/Google\n\n# This should fail\n$ vault write secret/apikey/Google key=\"ABCDE12345\"\n\n# Request a new AWS credentials\n$ vault read aws/creds/s3-readonly \n\n</code></pre>"},{"location":"secretmgmt/vault/policies/#admin-policies","title":"admin policies","text":"<ul> <li>Permissions for Vault backend functions live at the sys/ path</li> <li>Users/admins will need policies that define what they can do within Vault to administer Vault itself<ul> <li>Unsealing</li> <li>Changing policies</li> <li>Adding secret backends</li> <li>Configuring database configurations</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/secret_engine/","title":"secret engine","text":""},{"location":"secretmgmt/vault/secret_engine/#secrets","title":"secrets","text":""},{"location":"secretmgmt/vault/secret_engine/#static-secrets","title":"static secrets","text":"<p>what issues you get while using static secrets</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#dynamic-secrets","title":"dynamic secrets","text":""},{"location":"secretmgmt/vault/secret_engine/#example-for-application-using-vault","title":"example for application using vault","text":""},{"location":"secretmgmt/vault/secret_engine/#secret-engine","title":"secret engine","text":"<p>Secrets engines are components that can store, generate, or encrypt data - Many secrets engines can be enabled in Vault - You can even enable multiple instances of the same secrets engine - Secrets engines are plugins that extend the functionality of Vault</p> <p>Secrets engines are enabled and isolated at a path - All interactions with the secrets engine are done using the path - Path must be unique</p>"},{"location":"secretmgmt/vault/secret_engine/#secrets-as-a-service","title":"secrets-as-a-service","text":"<p>Use Vault to generate and manage the lifecycle of credentials on-demand - No more sharing credentials - Credentials get revoked automatically at the end of its lease - Audit trail can identify points of compromise - Use policies to control the access based on the client's role</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#enable-secret-engine","title":"enable secret engine","text":"<ul> <li>Cubbyhole and Identity are enabled by default (can\u2019t disable)</li> <li>Any other secrets engine must be enabled, enable using the CLI, API, or UI (most)</li> </ul> <p>Secrets engines are enabled and isolated at a path - All interactions with the secrets engine are done using the path - Path must be unique - Paths do not need to match the secrets engines name or type - Make them meaningful for you and your organization</p>"},{"location":"secretmgmt/vault/secret_engine/#responsibilities","title":"responsibilities","text":"<p>vault admin/securtiy team 1. Enable the Secrets Engine 2. Configure the connection to the backend platform (AWS, Database, etc.) 3. Create roles that define permissions to the backend platform 4. Create policies that grant permission to read from the secrets engine</p> <p>vault client(app/sercvices/users/machines)</p> <ol> <li>Read a set of credentials using token and associated policy</li> <li>Renew the lease before its expiration if needed (or permitted)</li> <li>Renew the token if needed (or permitted)</li> </ol> <p>How to enable vault secrets</p> <pre><code>vault secrets enable aws\nvault secrets tune -default-lease-ttl=72h pki/\n\nvault secrets list\nvault secrets list \u2013detailed\nvault secrets enable \u2013path=developers kv\nvault secrets enable \u2013description=\"my first kv\" kv\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#configure-secret-engine","title":"configure secret engine","text":"<p>configuring a secrets engine that will generate dynamic credentials. vault client must be authenticated before it can be requested for dynamic credentials.</p> <p>Step 1: Configure Vault with access to the platform</p> <p>Example 1: Vault to AWS </p> <p>The path is the default path for aws to configure <code>aws/config/root</code></p> <p>Provide credentials to a secrets engine that gives Vault permission to create, list, and delete credentials on the platform:</p> <pre><code>vault write aws/config/root access_key=AKIAIOSFODNN7EXAMPLE secret_key= wJalrXUtnFEMI/K7MDENGbPxRfiCYEXAMPLEKEY region=us-east-1\n</code></pre> <p>Example 2: vault to db</p> <p>default path for vault to configure database <code>database/config/prod-database</code></p> <pre><code>vault write database/config/prod-database \\\nplugin_name=mysql-aurora-database-plugin \\\nconnection_url=\"{{username}}:{{password}}@tcp(prod.cluster.us-east-1.rds.amazonaws.com:3306)/\" \\\nallowed_roles=\"app-integration, app-lambda\" \\\nusername=\"vault-admin\" \\\npassword=\"vneJ4908fkd3084Bmrk39fmslslf#e&amp;349\"\n</code></pre> <p>For other services, you need to refer to the documentation.</p> <p>Step 2: Configure Roles based on permissions needed</p> <p>Vault does not know what permissions, groups, and policies you want to attach to generated credentials. Each role maps to a set of permissions on the targeted platform.</p> <p></p> <pre><code>vault read aws/creds/data-consultant\n</code></pre> <p></p> <pre><code>vault read database/creds/oracle-reporting\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#kv-secret-engine","title":"kv secret engine","text":"<p>Key/Value secrets engine is used to store static secrets - There are two versions: v2 (kv-v2) is versioned but v1 (v1) is not. - Secrets are accessible via UI, CLI, and API \u2013 interactive or automated - Access to KV paths are enforced via policies (ACLs).</p> <p>like everything else in Vault, secrets written to the KV secrets engine are encrypted using 256-bit AES.</p> <p>Key/Value secrets engine can be enabled at different paths, each key/value secrets engine is isolated and unique - Secrets are stored as key-value pairs at a defined path \u2013 (e.g.,secret/applications/web01) - Writing a new secret will replace the old value (i.e v1 or v2). - Writing a new secret requires the create capability. - Updating/overwriting a secret to an existing path requires update capability.</p> <p>When you run Vault in \u2013dev server mode, Vault enables a KV v2 secrets engine at the secret/ path, by default</p>"},{"location":"secretmgmt/vault/secret_engine/#organize-kv-engine","title":"organize kv engine","text":"<p>Organize Data However It Makes Sense to Your Organization</p> <p></p> <p></p> <pre><code>vault secrets enable kv\nvault secrets enable \u2013path=training kv\nvault secrets list \u2013detailed\n\nvault secrets enable kv-v2\nvault secrets enable \u2013path=training \u2013version=2 kv\nvault secrets list \u2013detailed\n</code></pre> <p>how v2 is different from v1?</p> <p>Introduces two prefixes that must be accounted for when referencing secrets and/or metadata</p> <ul> <li>cloud/data \u2013 data is where the actual K/V data is stored</li> <li>cloud/metadata \u2013 the metadata prefix stores our metadata about a secret</li> </ul> <p>The data/ and metadata/ prefix is required for API and when writing Vault policies It does NOT change the way you interact with the KV store when using the CLI.</p>"},{"location":"secretmgmt/vault/secret_engine/#working-with-kv-engine","title":"working with kv engine","text":"<p>Use the vault kv command - put - write data to the KV - get - read data from the KV - delete - delete data from the KV - list - list data within the KV (paths)</p> <p>Only available for KV V2</p> <ul> <li>undelete - undelete version of secret</li> <li>destroy - permanently destroy data</li> <li>patch - add specific key in the KV</li> <li>rollback - recover old data in the KV</li> </ul> <pre><code>vault kv put kv/app/db pass=123 \nvault kv put kv/app/db pass=123 user=admin api=a8ee4b50cce124\nvault kv put kv/app/db @secrets.json\n\nvault kv get kv/app/db\nvault kv put kv/app/db api=39cms1204mfi2m\n\nvault kv rollback -version=1 kv/app/db\nvault kv patch kv/app/db user=bryan\nvault kv get kv/app/db\nvault kv get -format=json kv/app/db\n\nvault kv get \u2013version=3 kv/app/db\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#deleting-kv","title":"deleting kv","text":"<p>If the latest version of the secret has been deleted (KV V2), it will return the related metadata.</p> <p>You can read a previous version of a secret (if one exists) by adding the \u2013version=x flag to the request</p> <ul> <li>A delete on KV V1 is a delete \u2013 the data is destroyed</li> <li>You'd have to restore Vault/Consul to retrieve the old data</li> <li>A delete on KV V2 is a soft delete \u2013 data is not destroyed</li> <li>Data can be restored with a undelete/rollback action</li> <li>A destroy (only KV V2) is a permanent action \u2013 destroyed on disk</li> <li>Cannot be restored except for a Vault/Consul restore action</li> </ul> <pre><code>vault kv delete secret/app/database #latest version is deleted\nvault kv delete secret/app/database --version=2 # previous version deleted\nvault kv get secret/app/database\n</code></pre> <pre><code>vault kv destroy \u2013versions=3 secret/app/web\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#cubbyhole","title":"cubbyhole","text":"<p>cubbyhole secret engine is used to store the arbitary secrets enabled by default at cubbyhole/ path. Its lifeline is linked to the token used to write data. </p> <ul> <li>no concept of TTL or any refresh tokens</li> <li>even root token cannot be read if its not written by root. </li> </ul> <p>cubbyhole secrets engine cannot be disabled, moved, or enabled multiple times. </p> <ul> <li>each service will have its own cubbyhole. </li> <li>one token of cubbyhole cannot access another services cubbyhole. </li> <li>cubbyhole expires when token expires. </li> </ul> <pre><code>vault secrets list\nvault write cubbyhole/training certification=hcvop\nvault read cubbyhole/training\n\ncurl  --header \"X-Vault-Token: ...\" --request POST --data '{\"certification\":\"hcvop\"}' http://127.0.0.1:8200/v1/cubbyhole/training\n\ncurl --header \"X-Vault-Token: ...\" --request LIST http://127.0.0.1:8200/v1/cubbyhole/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#wrapping-response","title":"wrapping response","text":"<p>If one user has to send slack token to another, it can't be shared across any messager as it would be sent in the plain text, hence we use wrapping resonse where the token creates a cubbyhole and the other user can access using it. </p> <p>When requested, Vault can take the response it would have sent to an HTTP client and instead insert it into the cubbyhole of a single-use token, returning that single-use token instead. </p> <p>Logically speaking, the response is wrapped by the token, and retrieving it requires an unwrap operation against this token. Functionally speaking, the token provides authorization to use an encryption key from Vault's keyring to decrypt the data.</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#benefits","title":"benefits","text":"<ul> <li>privacy</li> <li>malfeasance detection</li> <li>limitation of the lifetime secret exposure</li> </ul> <p>Reference: https://developer.hashicorp.com/vault/docs/concepts/response-wrapping</p> <pre><code>vault kv get -wrap-ttl=5m secrets/certification/hcvop\nvault token lookup &lt;wrap_token&gt;\n\nvault unwrap &lt;wrap-token&gt;\nOR\nvault VAULT_TOKEN=&lt;wrap-token&gt; vault unwrap\nOR\nvault login &lt;wrap-token&gt;\nvault unwrap\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#vault-transit-secret-engine","title":"vault transit secret engine","text":"<p>Transit secrets engine provides functions for encrypting/decrypting data,  Enables organizations to outsource/centralize encryption to Vault.</p> <ul> <li>Applications can send cleartext data to Vault for encryption</li> <li>Vault encrypts using the specified key and returns ciphertext to the app</li> <li>The application NEVER has access to the encryption key (stored in Vault)</li> <li>Decouples storage from encryption and access control</li> </ul> <p></p> <p>Note: Transit secrets engine DOES NOT STORE the encrypted data. It would encrypt and returns cipher teext back to application.</p> <ul> <li>Encryption keys are created and stored in Vault to process data</li> <li>Each application can have its own encryption key (or more!)</li> <li> <p>Apps must have permission to use the key for encryption/decryption operations, which is bound by the policy attached to its token.</p> </li> <li> <p>Keys can be easily rotated as often as needed</p> </li> <li>Keys are stored on keyring</li> <li>Can limit what version(s) of keys can be used for decryption</li> <li>You can create, rotate, delete, and export a key (need permissions)</li> <li>Easily rewrap ciphertext with a newer version of a key</li> </ul> <p></p> <p></p> <ul> <li>Vault also supports convergent encryption mode</li> <li>Means that every time you encrypt the same data, you'll get the same ciphertext back</li> <li>This enables you to have searchable ciphertext</li> </ul>"},{"location":"secretmgmt/vault/secret_engine/#working-with-vault-transit-secret-engine","title":"working with vault transit secret engine","text":"<p>Before you can use the Transit secrets engine to encrypt data, it must first be enabled we can use the default path of transit or enable on another path.</p> <p>The next step is to create one or many encryption keys used to encrypt/decrypt data</p> <p>Encrypt</p> <pre><code>vault secrets enable transit\nvault write -f transit/keys/vault_training\nvault write -f transit/keys/training_rsa type=\"rsa-4096\"\n</code></pre> <p>Pass the cleartext data to Vault \u2013 specifying the action and desired encryption key to use</p> <pre><code>vault write transit/encrypt/training plaintext=$(base64 &lt;&lt;&lt; \"Getting Started with HashiCorp Vault\")\n</code></pre> <p></p> <p>Decrypt</p> <pre><code>vault write transit/decrypt/training ciphertext=\"vault:v1:Fpyph6C7r5MUILiEiFhCoJBxelQbsGeEahal5LhDPSoN6HkTO\u2026\u2026\u2026\"\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#rotating-encryption-keys","title":"Rotating encryption keys","text":"<ul> <li>Transit allows for a simplified key rotation process</li> <li> <p>keys can be rotated manually or by an  automated process <li> <p>Vault maintains a versioned keyring</p> </li> <li>All versions of the encryption key are stored</li> <li>Vault admins can limit the minimum key version allowed to be used for decryption operations (older keys won't work)</li> <li>You can rewrap encrypted data (ciphertext) to use a newer version of the encryption key</li> <pre><code>vault write \u2013f transit/keys/training/rotate\nvault read transit/keys/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#key-configuration","title":"key configuration","text":"<ul> <li>We can limit what version of the key can be used to decrypt data</li> <li>Maybe we have old data that we have converted and don't want anybody to be able to decrypt it</li> <li>This is configured using the minimum key version configuration</li> <li>It can be configured for each encryption key (not key version)</li> </ul> <pre><code>vault write transit/keys/training/config min_decryption_version=4\nvault read transit/keys/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#rewrapping-ciphertex","title":"Rewrapping Ciphertex","text":"<p>How can we upgrade our encrypted data to be encrypted by the latest version of the key?</p> <p>The data was never available in plaintext when rewrapping the data with the latest version of the key</p> <pre><code>vault write transit/rewrap/training ciphertext=\"vault:v1:Fpyph6C7r5MUILiEiFhCoJBxelQbsGeEahal5LhDPSoN6 HkTOhwn79DCwt0mct1ttLokqikAr0PAopzm2jQAKJg=2/QGPTMnzKPlw4cCPGTbkzE PlX5OyPkLIgX+erFWdUXKkKUIEbb6D2Gm5ZjTaola314LsVkbLF5G1RkBTACtskk=\"\n</code></pre>"},{"location":"secretmgmt/vault/vault_agent/","title":"agent","text":"<p>The Vault Agent is a client daemon that runs alongside an application to enable legacy applications to interact and consume secrets.</p> <p>Vault Agent provides several different features: - Automatic authentication including renewal - Secure delivery/storage of tokens (response wrapping) - Local caching of secrets - Templating</p>"},{"location":"secretmgmt/vault/vault_agent/#auto-auth","title":"auto-auth","text":"<ul> <li>The Vault Agent uses a pre-defined auth method to authenticate to Vault and obtain a token</li> <li>The token is stored in the \"sink\", which is essentially just a flat file on a local file system that contains  the Vault token</li> <li>The application can read this token and invoke the Vault API directly</li> <li>This strategy allows the Vault Agent to manage the token and guarantee a valid token is always available to the application</li> </ul> <p>Vault Agent supports many types of auth methods to authenticate and obtain a token. </p> <p>Auth methods are generally the methods you'd associate with \"machine-oriented\" auth methods - AliCloud - AppRole - AWS - Azure - Certificate - CloudFoundary - GCP - JWT - Kerberos - Kubernetes - LDAP</p> <p>Vault Agent Configuration File</p> <pre><code>auto_auth {\n    method \"approle\" {\n        mount_path = \"auth/approle\"\n        config = {\n        role_id_file_path = \"&lt;path-to-file&gt;\"\n        secret_id_file_path = \"&lt;path-to-file&gt;\"\n        }\n    }\n    sink \"file\" {\n        config = {\n        path = \"/etc/vault.d/token.txt\"\n        }\n    }\n}\n\nvault {\naddress = \"http://&lt;cluster_IP&gt;:8200\"\n}\n</code></pre> <p>file is the only supported method of storing the auto-auth token</p> <p>Common configuration parameters include: \u2022 type \u2013 what type of sink to use (again, only file is available) \u2022 path \u2013 location of the file \u2022 mode \u2013 change the file permissions of the sink file (default is 640) \u2022 wrap_ttl = retrieve the token using response wrapping</p>"},{"location":"secretmgmt/vault/vault_agent/#protecting-auto-auth","title":"protecting auto-auth","text":"<p>To help secure tokens when using Auth-Auth, you can have Vault response wrap the token when the Vault Agent authenticates \u2022 Response wrapped by the auth method \u2022 Response wrapped by the token sink</p> <p>The placement of the wrap_ttl in the Vault Agent configuration file determines where the response wrapping happens.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_agent/#vault-templating","title":"vault templating","text":""},{"location":"secretmgmt/vault/vault_agent/#consul-template","title":"Consul Template","text":"<p>A standalone application that renders data from Consul or Vault onto the target file system \u2022 https://github.com/hashicorp/consul-template \u2022 Despite its name, Consul Template does not require a Consul cluster to operate</p> <p>Consul Template retrieves secrets from Vault \u2022 Manages the acquisition and renewal lifecycle \u2022 Requires a valid Vault token to operate</p> <p>Consule template workflow</p> <p></p>"},{"location":"secretmgmt/vault/vault_agent/#vault-agent-templating","title":"vault agent templating","text":"<p>To further extend the functionality of the Vault Agent, a subset of the Consul-Template functionality is directly embedded into the Vault Agent. i.e No need to install the Consul-Template binary on the application server</p> <p>Vault secrets can be rendered to destination file(s) using the ConsulTemplate markup language, Uses the client token acquired by the auto-auth configuration.</p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_architecture/","title":"architecture","text":""},{"location":"secretmgmt/vault/vault_architecture/#vault-components","title":"vault components","text":""},{"location":"secretmgmt/vault/vault_architecture/#storage-backends","title":"storage backends","text":"<p>The storage stanza configures the storage backend, which represents the location for the durable storage of Vault's information. These are configured in the file and encryted in transit. </p> <p>There is only 1 storage backend/cluster</p> <p>https://developer.hashicorp.com/vault/docs/configuration/storage</p>"},{"location":"secretmgmt/vault/vault_architecture/#secrets-engine","title":"secrets engine","text":"<p>Secrets engines are components which store, generate, or encrypt data. Secrets engines are incredibly flexible, so it is easiest to think about them in terms of their function. Secrets engines are provided some set of data, they take some action on that data, and they return a result.</p> <p>Some secrets engines simply store and read data - like encrypted Redis/Memcached. Other secrets engines connect to other services and generate dynamic credentials on demand(GCP, AWS Secrets etc). Other secrets engines provide encryption as a service, totp generation, certificates, and much more.</p> <p>Secrets engines are enabled at a path in Vault. When a request comes to Vault, the router automatically routes anything with the route prefix to the secrets engine</p> <p>(secret engine life cycle)[https://developer.hashicorp.com/vault/docs/secrets#secrets-engines-lifecycle]</p> <p>https://developer.hashicorp.com/vault/docs/secrets</p>"},{"location":"secretmgmt/vault/vault_architecture/#auth-methods","title":"auth methods","text":"<p>Auth methods are the components in Vault that perform authentication and are responsible for assigning identity and a set of policies to a user. In all cases, Vault will enforce authentication as part of the request processing. In most cases, Vault will delegate the authentication administration and decision to the relevant configured external auth method (e.g., Amazon Web Services, GitHub, Google Cloud Platform, Kubernetes, Microsoft Azure, Okta ...).</p> <p>Once authenticated, vault will issue a client token used to make all subsequent vault requests.  - The main goal of all auth methods is to obtain a token. - each token has an associated policy(or policies) and a TTL(token validity)</p> <p>https://developer.hashicorp.com/vault/docs/auth</p>"},{"location":"secretmgmt/vault/vault_architecture/#audit-devices","title":"audit devices","text":"<p>Audit devices are the components in Vault that collectively keep a detailed log of all requests to Vault, and their responses. Because every operation with Vault is an API request/response, when using a single audit device, the audit log contains every interaction with the Vault API, including errors - except for a few paths which do not go via the audit system. </p> <p>Each line in the audit log is a JSON object and any sensitive information are hashed before logging.</p> <p>vault requires at least one audit device to write the log before completing the vault request. - if enabled.</p>"},{"location":"secretmgmt/vault/vault_architecture/#vault-architecture","title":"vault architecture","text":"<p>vault architecture document</p>"},{"location":"secretmgmt/vault/vault_architecture/#vault-path-structure","title":"vault path structure","text":"<ul> <li>Everything in vault is path-based</li> <li>path prefix tells vault which component a request should be routed</li> <li>secret engines, auth methods, audit devices are \"mounts\" at a specified path referred as mount</li> <li>system backend is default backend in vault which is mounted at the /sys endpoint</li> <li>vault components can be enabled at ANY path using --path flag or it does by default path</li> <li>vault has few system paths what are resrved and you cannot use it. <ul> <li>auth/  - endpoint for auth methods configs</li> <li>cubbyhole - endpoint for cubbyhole secrets engine</li> <li>identity/ - endpoint for config vault identity</li> <li>secret/  - endpoint used by key/vaulut v2 secrets engine if running in dev mode</li> <li>sys/ - system endpoint for config vault</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-data-protection","title":"vault data protection","text":"<p>How data is stored in vault...</p> <p>master key:  - used to decrypt the encryption key - created during vault init or rekey ops - never written to storage when using traditional unseal mechanism - written to core/master(storage backend) when using auto unseal</p> <p>encryption key: - used to encrypt/decrypt data written to storage backend. - encrypted by the master key. - stored alongside the data in a keyring on the storage backend. - can be easily rotated(manual ops).</p>"},{"location":"secretmgmt/vault/vault_architecture/#sealunseal","title":"seal/unseal","text":"<ul> <li>vault starts in a sealed state, meaning it knows where to access the data and how but can't decrypt it, which means there is no read or write etc.. i.e almost no ops are possible when vault in a sealed state.</li> <li>unsealing vault means that a node can reconstruct the master key in order to decrypt the encryption key and untilmately read the data, after unsealing the encryption key is stored in the memory, it can be unsealed using CLI/UI options</li> <li>incase you need to seal vault i.e \"throw away\" the encrytpion key, it requires another unseal to perform any further ops. When I need to seal vault ?</li> <li>key shars are exposed</li> <li>detecting of compromise or netwrom intrustions</li> <li>spyware/malware on the vault nodes</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#unseal-using-key-shards","title":"unseal using key shards","text":"<p>We know, by now that master key is required to encrypt \"encryption key\" to store the data in the backend.. so how do we protect the \"master key\". It is done by \"key shards\". In this key shards, vault expects alteast 3 keys to provide to unseal the master key. These key shards would be available to min of 5 members when we init the vault using diff encryption alogrithms. when you are unsealing you will provide equal number of employees to provide their key which is equal to threshold.</p> <p>As part of security, no single person should have all the key shards, and it should never be stored online.</p> <pre><code>vault status\nvault operator init\n&lt;vault status set to initialized&gt;\n&lt;displays 5 keys along with root token&gt;\n\nvault status\nvault operator unseal\n&lt;enter any of 3/5 keys from above until status of thershold is met&gt;\n\nvault status\n\nvault login &lt;root-token&gt; # you would authenticate the cluster\n\nvault secrets list\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#unsealing-using-auto-unseal","title":"unsealing using auto-unseal","text":"<p>When we have an vault mainteance, or restart of service, vault would seal itself. so we need a place so that it would unseal itself.. </p> <p>instead of key shards being with 5 ppl, you would have a key stored in the cloud services(KMS) to encrypt/decrypt the master key. encrypted master key is stored in the backend in core using KMS, so incase if vault restarts, it would read the encrypted master key using KMS then decrypt the master key which would then decrypt encryption key which will store in the memory to read/write secrets on the vault node.</p> <p>You can find in the config file <code>/etc/vault/vault.hcl</code></p> <p>Create a new KMS key and provide an endpoint for <code>kms_key_id</code>. That would be sufficient when vault restarts or so, it would use the KMS key to get it unsealed.. </p> <pre><code>seal \"awskms\" {\n    region = REGION\n    kms_key_id = \"KMSKEY\"\n}\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#unsealing-with-transit-auto-unseal","title":"unsealing with transit auto-unseal","text":"<p>We would be having an vaulut cluster running transit secret engine from which we would be unsealing the vault. In other words, we would use that one cluster dependent on another cluser for unsealing.</p> <p>This supports key rotation and also can be configured for HA.</p> <pre><code>seal \"transit\" {\n  address: \"https://vault.example.com:8200\",\n  token =\"x.Qft42..\", - acl token to use if enabled.\n  disable_renewal = \"false\" \n}\n\nkey_name = \"transit_key_name\", - transit key used for encryption/decryption\nmount_path = \"transit/\", - mount path to transit secret engine\nnamespace = \"ns1/\" - namespace path to transit secret engine.\n</code></pre> <p>Configure transite secret engine..</p> <p>host: 192.168.56.100</p> <pre><code>vault secrets enable transit\nvault write -f transit/keys/unseal-key\nvault list transit/keys\n\nvim policies.hcl\n\npath \"transit/encrypt/unseal-key\" {\n  capabilities = [\"update\"]\n}\n\npath \"transit/decrypt/unseal-key\" {\n  capabilities = [\"update\"]\n}\n\nvault policy write unseal policy.hcl\nvault policy list\nvault policy read unseal\nvault token create -policy=unseal - you get token from here\n</code></pre> <p>Now, go to your vault cluster which needs to be unsealed</p> <pre><code>vault status\nsudo vim /etc/vault/vault.hcl \n\nseal \"transit\" {\n  address: \"https://192.168.56.100:8200\",\n  token =\"use above token from transit engine\",\n  disable_renewal = \"true\",\n  key_name = \"unseal-key\",\n  mount_path = \"transit\"\n}\n\nsudo system restart vault\n\nvault status\nvault operator init\nvault status - vault would have unsealed\n\nvault login &lt;token&gt;\n\nvault secrets list\n\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#pros-and-cons-of-unsealing","title":"pros and cons of unsealing","text":"<p>pros:</p> <p>key shards: - simplest form of unsealing - works on any platform  - config options make it flexible</p> <p>auto unseal: - automatic unsealing - set and forget - integration benefits for running on same platform</p> <p>transit unseal: - automatic unsealing - set and forget - platform agnostic - useful when runing many vault clusters across clouds/data centres</p> <p>Cons:</p> <p>key shards:</p> <ul> <li>introduces risk of storing keys</li> <li>requires manual intervention</li> <li>require rotation manually</li> </ul> <p>auto unseal:</p> <ul> <li>regional requirements for cloud HSM</li> <li>cloud/vendor lockin</li> </ul> <p>transit unseal:</p> <ul> <li>requires centralized vault cluster</li> <li>since centralized vault cluster need highest level of uptime.</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-initialization","title":"vault initialization","text":"<ul> <li>initilization vault prepares the backend storage to receive data</li> <li>only need to initialize one time via single node</li> <li>vault init is when vault creates the master key and key shares</li> <li>options to defined threshold, key shares, receovery keys and encryption</li> <li>vault init is also where the intitial root token is generated and returned to user</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-configs","title":"vault configs","text":"<ul> <li>vault written in HCL or JSOn</li> <li>config file is specified(/etc/vault/vault.hcl) when starting vault using the<code>--config</code> flag</li> </ul> <pre><code>vault server -config &lt;localtion&gt;\n</code></pre> <p>vault config documentation</p> <p>example config file </p> <pre><code>storage \"consul\" {\n  address = \"127.0.0.1:8500\"\n  path    = \"vault/\"\n  token   = \"1a2b3c4d-1234-abdc-1234-1a2b3c4d5e6a\"\n}\nlistener \"tcp\" {\n address = \"0.0.0.0:8200\"\n cluster_address = \"0.0.0.0:8201\"\n tls_disable = 0\n tls_cert_file = \"/etc/vault.d/client.pem\"\n tls_key_file = \"/etc/vault.d/cert.key\"\n tls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n  region = \"us-east-1\"\n  kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n  endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\nreporting { #only for Vault 1.14 and up\n    license {\n        enabled = false\n   }\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = false\nlog_level = \"INFO\"\nlicense_path = \"/opt/vault/vault.hcl\"\ndisable_mlock=true\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#storage-backends_1","title":"storage backends","text":"<ul> <li>configures location for storage of vault data</li> <li>enterprice vault cluster use \"Hashicorp Consul\" or \"integrated storage\" </li> </ul> <p>There is only 1 storage backend / cluster</p>"},{"location":"secretmgmt/vault/vault_architecture/#audit-devices_1","title":"audit devices","text":"<ul> <li>keep detailed log of all autenticated req and resp to vault</li> <li>audit log is in JSON</li> <li>sensitive info is hashed </li> <li>log files shoud be protected as a user permission can still check the values of those secrets via /sts/audit-hash API and compare to the log file</li> </ul> <pre><code>vault audit enable file file_path=/var/log/vault_audit_log.log\n</code></pre> <ul> <li>Can and should have more than one audit device enabled</li> <li>if there are any sudit devices enabled, vault requires that it can write to the log before completing the client request</li> <li>if vault cannot write to a persistent log, it will stop responding to client requests - which mean its down !</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-interfaces","title":"vault interfaces","text":"<ul> <li>3 interfaces interact with vault: UI, CLI, HTTP</li> <li>CLI is just a wrapper and all CLI or UI underneath uses HTTP</li> <li>Auth is required to access any of the interfaces.</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/","title":"introduction","text":"<p>Manage Secrets and Protect Sensitive Data and provides a Single Source of Secrets for both Humans and Machines</p> <p>Secret</p> <ul> <li>Anything your organization deems sensitive:</li> <li>Usernames and passwords</li> <li>API keys</li> <li>Certificates</li> <li>Encryption Keys</li> </ul> <p>Lifecycle Management for Secrets</p> <ul> <li>Eliminates secret sprawl</li> <li>Securely store any secret</li> <li>Provide governance for access to secrets</li> </ul> <p>Vault has mainly 3 ways you can interact, CLI, UI and API</p> <p>As a human, you would authenticate to vault server with credentials(username/password | roleid | secretid| tls certs | cloud creds..etc), which gives you an generated token to carry out certain taks that you need for like (read/write/delete//list) to perform some actions(writing to path or reading from path) on the applications with certain time limit(TTL).</p> <p>When we want to retrive the data from path, we would proivde the token generated from auth and vault would validate the below tokens</p> <ol> <li>token provided is correct/valid</li> <li>token is not expired</li> <li>token has permission</li> </ol> <p>Once above are successful, you would be retrived the data from th path.</p>"},{"location":"secretmgmt/vault/vault_overview/#benefits","title":"Benefits","text":"<ul> <li>Store Long-Lived, Static Secrets</li> <li>Dynamically Generate Secrets, upon Request</li> <li>Fully-Featured API</li> <li>Identity-based Access Across different Clouds and Systems</li> <li>Provide Encryption as a Service(secret engine)</li> <li>Act as a Root or Intermediate Certificate Authority </li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#usecases","title":"Usecases","text":"<ul> <li> <p>Centralize The Storage Of Secrets  </p> <ul> <li>Chef </li> <li>Jenkins </li> <li>AWS secrets</li> <li>Azure key</li> </ul> </li> <li> <p>Migrate to Dynamically Generated Secrets </p> <ul> <li>short-lived</li> <li>follows principle of least priv </li> <li>auotmatically revoked</li> <li>each system can retrive unique creds</li> <li>prog retrived</li> <li>no human interactions</li> </ul> </li> <li> <p>Secure Data with a centralized workflow for Encryption Operations (secret engine)</p> <ul> <li>transit</li> <li>KMIP</li> <li>Key mgmt</li> <li>transform</li> </ul> </li> <li> <p>Automate the Generation of X.509 Certificates ( can work like PKI) you would provide an Certificate Request, in which valut would sign and provide the certtficate and key returned</p> </li> <li> <p>Migrate to IdentityBased Access</p> <ul> <li>Quickly Scale Up and Down</li> <li>Reduce/Eliminate Ticket-based Access</li> <li>Increase Time to Value</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#installations","title":"Installations","text":"<p>Choose your OS and download the vault server from the below link https://releases.hashicorp.com/vault/1.15.1/</p> <ul> <li>Install vault</li> <li>Create Configuration file </li> <li>Initialize vault </li> <li>Unseal vault</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#development-vault","title":"Development vault","text":"<p>features of development vault server</p> <ul> <li>Quickly run Vault without configuration</li> <li>Automatically initialized and unsealed</li> <li>Enables the UI \u2013 available at localhost</li> <li>Provides an Unseal Key</li> <li>AutomaBcally logs in as root</li> <li>Non-Persistent \u2013 Runs in memory</li> <li>Insecure \u2013 doesn\u2019t use TLS </li> <li>Sets the listener to 127.0.0.1:8200</li> <li>Mounts a K/V v2 Secret Engine</li> <li>Provides a root token</li> </ul> <p>benefits of development vault server</p> <ul> <li>POC</li> <li>New dev integrations</li> <li>testing new vault features</li> <li>experiment new features</li> </ul> <pre><code>vault server -dev\n</code></pre> <p>Open another terminal and set the env vars</p> <pre><code>\u279c  ~ export VAULT_ADDR='http://127.0.0.1:8200'\n\u279c  ~ vault status\nKey             Value\n---             -----\nSeal Type       shamir\nInitialized     true\nSealed          false\nTotal Shares    1\nThreshold       1\nVersion         1.15.1\nBuild Date      2023-10-20T19:16:11Z\nStorage Type    inmem\nCluster Name    vault-cluster-49bd9ee3\nCluster ID      e5a97669-5d1d-386f-5eb3-a72d5b72f744\nHA Enabled      false\n\n\u279c  ~ vault kv put secret/vault/sunil sunil=sunil\n</code></pre>"},{"location":"secretmgmt/vault/vault_overview/#production-vault","title":"Production vault","text":"<p>features of production vault server</p> <ul> <li>Deploy one or more persistent nodes via configuration file</li> <li>Use a storage backend that meets the requirements</li> <li>Multiple Vault nodes will be configured as a cluster</li> <li>Deploy close to your applications</li> <li>Most likely, you\u2019ll automate the provisioning of Vault</li> </ul> <p>To start Vault, run the vault <code>server \u2013config=&lt;file&gt;</code> command</p> <ul> <li> <p>In a production environment, you'll have a service manager executing and managing the Vault service (systemctl, Windows Service Manager, etc.)</p> </li> <li> <p>For Linux, you also need a systemd file to manage the service for Vault (and Consul if you're running Consul)</p> </li> </ul> <p>follow manual install process for vault incase you need to deploy locally or in cloud.</p> <ul> <li>Download Vault from HashiCorp</li> <li>Unpackage Vault to a Directory</li> <li>Set Path to Executable</li> <li>Add ConfiguraBon File &amp; Customize</li> <li>Create Systemd Service File</li> <li>Download Consul from HashiCorp</li> <li>Configure and Join Consul Cluster</li> <li>Launch Vault Service</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#consul-as-backend","title":"Consul as backend","text":"<p>You can use the two of the storage backends for using vault(storing key/value(KV) pairs) .. Consul is one of them and and then integrated storage..</p> <p>Consule storage uses spearte consul cluster to store key-value pairs for vault, which would store all its key-value pairs on that backend KV store.</p> <p>benefits of consul as backend</p> Provides Durable K/V Storage For Vault Supports High Availability Can Independently Scale Backend Distributed System Easy To Automate Built-in Snapshots For Data Retention Built-in Integration Between Consul/Vault HashiCorp Supported <p>They would mainly be 3-5 nodes for HA, but good news from consul is that you can independently scale the backend from vault. incase you want to increase the backend resouces(RAM, CPU ..etc) you never had to touch the vault nodes.</p> <ul> <li>Consul is deployed using multiple nodes(3-5) and configured as a cluster.</li> <li>Clusters are deployed in odd numbers (for voting members)</li> <li>All data is replicated among all nodes in the cluster, </li> <li>A leader election promotes a single Consul node as the leader</li> <li>The leader accepts new logs entries and replicates to all other nodes</li> <li>Consul cluster for Vault storage backend shouldn\u2019t be used for Consul functions in a production setting</li> </ul> <p>Note: Consul cluster for vault should not be used for other consul functions like (service discovery, service mesh and network automations etc because you would get resource constraints.. )</p>"},{"location":"secretmgmt/vault/vault_overview/#consul-deployment","title":"Consul deployment","text":"<p>Now, its always recommended that you have 3 vault nodes you would atleast have 5 consul nodes. Within the vault we would have consul agents listening on vault locally which join to consul clusters and it will communicate with consul backend clusters. </p> <p>example consul configs</p> <pre><code>storage \"consul\" {\n address = \"127.0.0.1:8500\"\n path = \"vault/\"\n token = \"1a2b3c4d-1234-abdc-1234-1a2b3c4d5e6a\"\n}\nlistener \"tcp\" {\naddress = \"0.0.0.0:8200\"\ncluster_address = \"0.0.0.0:8201\"\ntls_disable = 0\ntls_cert_file = \"/etc/vault.d/client.pem\"\ntls_key_file = \"/etc/vault.d/cert.key\"\ntls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n region = \"us-east-1\"\n kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = true\nlog_level = \"INFO\"\n</code></pre> <pre><code>{\n \"log_level\": \"INFO\",\n \"server\": true,\n \"key_file\": \"/etc/consul.d/cert.key\",\n \"cert_file\": \"/etc/consul.d/client.pem\",\n \"ca_file\": \"/etc/consul.d/chain.pem\",\n \"verify_incoming\": true,\n \"verify_outgoing\": true,\n \"verify_server_hostname\": true,\n \"ui\": true,\n \"encrypt\": \"xxxxxxxxxxxxxx\",\n \"leave_on_terminate\": true,\n \"data_dir\": \"/opt/consul/data\",\n \"datacenter\": \"us-east-1\",\n \"client_addr\": \"0.0.0.0\",\n \"bind_addr\": \"10.11.11.11\",\n \"advertise_addr\": \"10.11.11.11\",\n \"bootstrap_expect\": 5,\n \"retry_join\": [\"provider=aws tag_key=Environment-Name tag_value=consul-cluster region=us-east-1\"],\n \"enable_syslog\": true,\n \"acl\": {\n \"enabled\": true,\n \"default_policy\": \"deny\",\n \"down_policy\": \"extend-cache\",\n \"tokens\": {\n \"agent\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n }\n},\n \"performance\": {\n \"raft_multiplier\": 1\n }\n}\n</code></pre>"},{"location":"secretmgmt/vault/vault_overview/#local-storage-as-backend","title":"local storage as backend","text":"Vault Internal Storage Option Supports High Availability Leverages RaG Consensus Protocol Only need to troubleshoot Vault All Vault nodes have copy of Vault's Data Built-in Snapshots For Data Retention Eliminates Network Hop to Consul HashiCorp Supported"},{"location":"secretmgmt/vault/vault_overview/#local-deployment","title":"local deployment","text":"<ul> <li>Integrated Storage (aka Raft) allows Vault nodes to provide its own replicated storage across the Vault nodes within a cluster.</li> <li>Define a local path to store replicated data.</li> <li>All data is replicated among all nodes in the cluster.</li> <li>Eliminates the need to also run a Consul cluster and manage it.</li> </ul> <p>You would have 1 leader and rest other followers, but make sure to proivde the node_id uniquely so that you won't mess up the data. you would see the raft as storage</p> <pre><code>storage \"raft\" {\n path = \"/opt/vault/data\"\n node_id = \"node-a-us-east-1.example.com\"\n retry_join {\n auto_join = \"provider=aws region=us-east-1 tag_key=vault tag_value=us-east-1\"\n }\n}\nlistener \"tcp\" {\naddress = \"0.0.0.0:8200\"\ncluster_address = \"0.0.0.0:8201\"\ntls_disable = 0\ntls_cert_file = \"/etc/vault.d/client.pem\"\ntls_key_file = \"/etc/vault.d/cert.key\"\ntls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n region = \"us-east-1\"\n kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = true\nlog_level = \"INFO\"\n</code></pre> <p>Manually join the standby ndoes to the cluster</p> <pre><code>vault operator raft join https://active_node.example.com:8200\nvault operator raft list-peers\n</code></pre> <p>References:</p> <ul> <li>https://hashicorp.com/certification/vault-associate</li> <li>https://learn.hashicorp.com/tutorials/vault/associate-study</li> <li>https://vaultproject.io/docs</li> <li>https://vaultproject.io/api-docs</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/","title":"replication","text":""},{"location":"secretmgmt/vault/vault_replication/#replication","title":"replication","text":"<p>Organizations usually have infrastructure that spans multiple datacenters</p> <ul> <li>Vault needs to be highly-available for application access</li> <li>Needs to scale as organizations continue to add use cases and apps</li> <li>Common set of policies that are enforced globally</li> <li>Consistent set of secrets and configurations available to applications that need them regardless of data center</li> <li>Only available in Vault Enterprise</li> <li>Replication operates on a leader-follower model (primaries and secondaries)</li> <li>The primary cluster acts as the system of record and replicates most Vault data asynchronously</li> <li>All communication between primaries and secondaries is end-to-end encrypted with mutually-authenticated TLS sessions</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#performance-replication","title":"Performance Replication","text":""},{"location":"secretmgmt/vault/vault_replication/#dr-replication","title":"DR replication","text":"<ul> <li>Provides a warm-standby cluster where EVERYTHING is replicated to the DR secondary cluster(s)</li> <li>DR clusters DO NOT respond to clients unless they are promoted to a primary cluster</li> <li>Even as an admin or using a root token, most paths on a secondary cluster are disabled, meaning you can't do much of anything on a DR cluster</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#replication-comparisions","title":"replication comparisions","text":""},{"location":"secretmgmt/vault/vault_replication/#replication-architecture","title":"replication architecture","text":"<p>Real world examples</p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_replication/#replication-networking","title":"replication networking","text":"<ul> <li>Communication between clusters must be permitted to allow replication, RPC forwarding, and cluster bootstrapping to work as expected.</li> <li>If using DNS, each cluster must be able to resolve the name of the other cluster</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#setup","title":"setup","text":""},{"location":"secretmgmt/vault/vault_replication/#activating-dr-replication","title":"Activating DR Replication","text":"<ul> <li>Replication is NOT enabled by default, so you must enable it on each cluster that will participate in the replica set</li> <li>Enables an internal root CA on the primary Vault cluster - creates a root certificate and client cert</li> <li>Vault creates a mutual TLS connection between the nodes using self-signed certificates and keys from the internal CA \u2013 NOT the same TLS configured for the listener</li> <li>If Vault sits behind a load balancer that is terminating TLS, it will break the mutual TLS between the nodes if inter-cluster traffic is forced through the load balancer</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#secondary-token","title":"Secondary Token","text":"<ul> <li>A secondary token is required to permit a secondary cluster to replicate from the primary cluster</li> <li>Due to its sensitivity, the secondary token is protected with response wrapping</li> <li>Multiple people should \u201chave eyes\u201d on the secondary token once it\u2019s been issued until it is submitted to the secondary cluster</li> <li>Once the token is successfully used, it is useless (single-use token)</li> <li>The secondary token includes information such as:</li> <li>The redirect address of the primary cluster</li> <li>The client certificate and CA certificate</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#configure-replication","title":"Configure replication","text":"<pre><code>primary$ vault write -f sys/replication/dr/primary/enable\nprimary$ vault write sys/replication/dr/primary/secondary-token id=&lt;id&gt; # provide meaningful name\nsecondary$ vault write sys/replication/dr/secondary/enable token=&lt;token&gt; #Provide token from primary cluster (command above)\n</code></pre>"},{"location":"softskills/leadership/","title":"leadership","text":"<p>Leadership Overview</p>"},{"location":"softskills/overview/","title":"Overview","text":"<p>Soft Skills</p>"},{"location":"sysdesign/airbnb/","title":"Airbnb","text":""},{"location":"sysdesign/airbnb/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Hotel </li> <li>Onboarding</li> <li>Updates</li> <li>bookings</li> <li>User</li> <li>search</li> <li>book</li> <li>check bookings</li> <li>Analytics</li> </ul>"},{"location":"sysdesign/airbnb/#non-functional-requirements","title":"Non functional requirements","text":"<ul> <li>Low latency</li> <li>HA</li> <li>high consistency</li> <li>scale </li> <li>500k hotels</li> <li>10M rooms</li> <li>10000 Rooms/hotel ( max 7500)</li> </ul>"},{"location":"sysdesign/amazon/","title":"Amazon","text":""},{"location":"sysdesign/amazon/#amazon-design-goes-here","title":"Amazon design goes here","text":""},{"location":"sysdesign/arch_process/","title":"Architecture process","text":""},{"location":"sysdesign/arch_process/#functional-system-requirements","title":"Functional system requirements","text":"<p>The requirements are usually defined by the system analysts</p> <p>Functional requirements outline the specific functions, features, and capabilities that a system or software must provide to meet the needs of users and achieve its intended purpose. These requirements describe the interactions between the system and its users, as well as the system's behavior under various conditions.</p> <p>Examples of functional requirements:</p> <ul> <li>Users must be able to create an account and log in.</li> <li>The system shall allow users to add items to their shopping cart.</li> <li>The software must generate monthly sales reports.</li> <li>The website should display a list of recommended articles based on user preferences.</li> </ul>"},{"location":"sysdesign/arch_process/#non-functional-system-requirememts","title":"Non-Functional system requirememts","text":"<p>Non-functional requirements define the qualities, characteristics, and constraints that govern how the system performs rather than what it does. These requirements focus on aspects such as performance, security, reliability, usability, and other attributes that contribute to the overall quality and user experience of the system.</p> <p>Performance: The system must be able to handle a minimum of 1000 concurrent users without significant degradation in response time.</p> <p>Security: User passwords must be securely hashed and stored in the database.</p> <p>Reliability: The system should have an uptime of at least 99.9%.</p> <p>Usability: The user interface must be intuitive and accessible to users with disabilities.</p> <p>Scalability: The system architecture should support easy scaling to accommodate increased user load.</p> <p>Maintainability: Code must be well-documented and adhere to coding standards.</p> <p>Compatibility: The software must be compatible with major web browsers, including Chrome, Firefox, and Safari.</p>"},{"location":"sysdesign/arch_process/#component-mapping","title":"Component mapping","text":"<p>Effective communication with stakeholders, including users, clients, developers, and testers, is crucial throughout this process. Regularly review and refine the mapped components to ensure that all requirements are accurately captured and prioritized. This mapping process serves as a foundation for design, development, testing, and evaluation activities during the system's lifecycle.</p>"},{"location":"sysdesign/arch_process/#select-technology-stack","title":"Select technology stack","text":"<ul> <li>After mapping the components, you would be required to wisely select the technology stack, that includes the frontend, backend, data stores etc</li> </ul>"},{"location":"sysdesign/arch_process/#design-architecture","title":"Design architecture","text":"<p>The architectural design process aims to define how different software modules or components will interact, how data will flow, and how the system will achieve its intended functionality while adhering to quality attributes. Here's an overview of the steps involved in designing software architecture:</p> <ol> <li> <p>Requirements Analysis and Understanding:</p> <ul> <li>Gather and analyze the functional and non-functional requirements of the software system.</li> <li>Understand the user needs, business goals, and technical constraints that the architecture must address.</li> </ul> </li> <li> <p>System Decomposition and Module Identification:</p> <ul> <li>Break down the software system into smaller, manageable modules or components.</li> <li>Identify the major functional areas and define the responsibilities of each module.</li> </ul> </li> <li> <p>Architectural Styles and Patterns Selection:</p> <ul> <li>Choose appropriate architectural styles (e.g., client-server, microservices, layered, event-driven) that best suit the system's requirements.</li> <li>Apply well-established architectural patterns (e.g., MVC, MVVM, RESTful API) to address common design challenges.</li> </ul> </li> <li> <p>Component Interaction and Communication:</p> <ul> <li>Define how different modules will communicate and interact with each other.</li> <li>Determine communication protocols, data formats, and interfaces between components.</li> </ul> </li> <li> <p>Data Management and Storage:</p> <ul> <li>Design the data storage and management mechanisms, including databases, data models, and data access layers.</li> <li>Address data consistency, integrity, and security requirements.</li> </ul> </li> <li> <p>User Interface Design:</p> <ul> <li>Create a user interface design that aligns with user needs and provides an intuitive user experience.</li> <li>Determine the layout, navigation, and presentation of information.</li> </ul> </li> <li> <p>Security and Authentication:</p> <ul> <li>Plan for security measures, including user authentication, authorization, data encryption, and protection against common vulnerabilities.</li> </ul> </li> <li> <p>Scalability and Performance Considerations:</p> <ul> <li>Address scalability requirements by designing for horizontal or vertical scalability, load balancing, and caching strategies.</li> <li>Optimize the system's performance through efficient algorithms and proper resource management.</li> </ul> </li> <li> <p>Error Handling and Resilience:</p> <ul> <li>Design error handling mechanisms to gracefully handle exceptions and failures.</li> <li>Implement strategies for fault tolerance, redundancy, and system recovery.</li> </ul> </li> <li> <p>Integration with External Services:</p> <ul> <li>Define how the software system will integrate with external APIs, third-party services, and other systems.</li> </ul> </li> <li> <p>Documentation and Communication:</p> <ul> <li>Document the architectural decisions, design rationale, and key design patterns used.</li> <li>Communicate the architecture to the development team, stakeholders, and collaborators.</li> </ul> </li> <li> <p>Review and Validation:</p> <ul> <li>Conduct architecture reviews and design discussions to validate the proposed architecture against requirements.</li> <li>Address any feedback and make necessary adjustments.</li> </ul> </li> <li> <p>Implementation and Development:</p> <ul> <li>Translate the architectural design into code by developing individual modules and components.</li> <li>Ensure that the implementation adheres to the defined architecture and design principles.</li> </ul> </li> <li> <p>Continuous Improvement:</p> <ul> <li>Regularly review and refine the architecture as the project progresses and new insights are gained.</li> <li>Consider feedback from users, performance metrics, and evolving requirements.</li> </ul> </li> </ol>"},{"location":"sysdesign/arch_process/#write-architecture-document","title":"Write architecture document.","text":"<ul> <li>Use visial or document representation so that all the users including CTO, CFO, developers, testers would be able to check the document to understand the system.</li> <li>Any changes in the architectute or the document has to be updated with changes.</li> </ul>"},{"location":"sysdesign/arch_process/#support-team","title":"Support team","text":"<ul> <li>Once the system is in production, we would be required to maintain system for reliability and so on, hence we would be required to support the team for any issues or help. </li> </ul>"},{"location":"sysdesign/autonomouscars/","title":"Design Autonomous cars","text":"<p>In this case study, we are working on autonomous cars that are manufacturing vehicles, at present there are already 20K cars running, by end of year it would be arounf 200K. We are required to work on the telementry of the cars relability and display details about it.  i.e breakdown, malfunctions, accidents, and use the same data for better needs.</p>"},{"location":"sysdesign/autonomouscars/#requirements","title":"Requirements","text":"<p>what should system do ?</p> <ul> <li>Web based</li> <li>CURD Operations</li> <li>Manager alerts</li> <li>breakdowns</li> <li>malfunctions</li> <li>provide information to the admin team</li> <li>alert to near hospitals or mechanic shops</li> <li>road assistance</li> </ul>"},{"location":"sysdesign/autonomouscars/#functional","title":"Functional","text":"<p>What system should do ?</p> <ol> <li>Web based </li> <li>Receive telementry from cars(localtion, speed, breakdown)</li> <li>Store telementry in persistent store</li> <li>Display dashboard summarizing the data</li> <li>Perform analysis on the data.</li> </ol>"},{"location":"sysdesign/autonomouscars/#non-functional","title":"Non Functional","text":"<p>What the systen should deal with ?</p> <p>So when we are designing the non-functional, we must first start witj what we already know of..</p> <ol> <li>Data intensive system </li> <li>Not a lot of users</li> <li>A lot of data</li> <li>Performance is important</li> </ol> <p>Questions to ask customers ?</p> <ol> <li> <p>How many expected concurrent users ? Sol: 10</p> </li> <li> <p>How many telementry msg received per second (load avg) Sol: 7000/sec</p> </li> <li> <p>What is the avg size of each msg sol: 1KB ( quite small)</p> </li> <li> <p>Is the msg schema-less ( any predefined data structures, etc ) sol: yes ( not predefined)</p> </li> <li> <p>Can we tolerate some msg loss ? sol: sort of.. </p> </li> <li> <p>What is desired SLA ? sol: Highest possible..</p> </li> </ol> <p>Data volume:</p> <p>1msg =1KB i.e 7000 msg = 7MB/sec i.e ~25GB/hr i.e ~605GB/day i.e ~221TB/year </p> <p>Retention period</p> <p>How long you want your rectods to be kept in the DB ?</p> <p>What happens to them after the retention period ?</p> <ul> <li>Deleted</li> <li>Move to archived data store</li> </ul> <p>Motivation:</p> <ul> <li>keeps db from expoloding</li> <li>improve query performance</li> </ul> <p>Two types of data:</p> <ul> <li> <p>Operational, near realtime (location, speed).. etc Retention period: 1 week</p> </li> <li> <p>Aggregated and ready for analysis(Business Intelligence) Retention period: Forever</p> </li> </ul>"},{"location":"sysdesign/autonomouscars/#mapping-components","title":"Mapping components","text":"<p>Component: - Receive telementry - Validate telementry - Store telementry - Query &amp; analyze telementry</p> <p>First component: Cars - Source of the data, since no control for us</p> <p>Second component: Telementry gateway - receives telementry data from cars. Since the load is very high, no validation, storing or query etc will be done.</p> <p>Third component: Telementry pipeline - It will receive the msg from the gateway, and put into the pipeline. hence no load on the system. i.e It will queue the telementry msg for processing</p> <p>Fourth component telementry processor - Validate and process any msg and store in the data base.</p> <p>Fifth component telementry viewer - Queries the database and displays real time data.(dashboards)</p> <p>Six component Data warehouse - store aggregated msg from databases</p> <p>Seventh component BI application: It is used only to report and analysis.</p> <p>Eighth component: Archive DB - Make sure you have huge database to just store the information. we don't need to query anything for operational purpose. incase you need to work, you need to put that data in the operational DB and work upon.</p>"},{"location":"sysdesign/autonomouscars/#telementry-gateway","title":"Telementry Gateway","text":"<ul> <li>Receives data from cars using TCP</li> <li>Pushes the data to pipeline</li> </ul>"},{"location":"sysdesign/autonomouscars/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - No</li> <li>Console - Yes</li> <li>Service - Yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/autonomouscars/#technology-stack","title":"Technology stack","text":"<p>Considerations: - Load ( 7000 msg/sec)  - Persormance is very good - Team's current knowledge and skill set - Environment (OS..etc)</p>"},{"location":"sysdesign/autonomouscars/#telementry-gateway-ques-for-customer","title":"Telementry gateway ques for customer","text":"<p>ask customer about environment and skill set..  Custome reply : python and javascript with good linux skills</p> <p>But we can't use python for this as its interpreeted lang and very slow Hence we go for nodejs which is great performance runs on linux and leverage javascript</p>"},{"location":"sysdesign/autonomouscars/#architecture","title":"Architecture","text":"<ul> <li>User/interface: No </li> <li>Business logic : No</li> <li>Data Access: No</li> <li>Data store: No</li> </ul> <p>Since telementry data uses none of the above, we don't need anything from above  We need Service interface and qquickly pushes into pipeline</p> <p>Redunancy for telementry gateway place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/autonomouscars/#telementry-pipeline","title":"Telementry pipeline","text":"<ul> <li>Gets msg from the gateway</li> <li>Queus the telementry for further processing</li> <li>Basically, queue for streaming high volume of data</li> </ul>"},{"location":"sysdesign/autonomouscars/#telementry-pipeline-ques-for-customer","title":"Telementry pipeline ques for customer","text":"<ul> <li>is there any existing queue mechanism in company ?  No</li> <li>use 3rd party - we will go with Apache kafka.</li> </ul> <p>Apache Kafka </p> <p>Pros:</p> <ul> <li>Very popular</li> <li>Handle massive amount of data</li> <li>HA</li> </ul> <p>Cons:</p> <ul> <li>Complex to setup and configure</li> </ul>"},{"location":"sysdesign/autonomouscars/#telementry-processor","title":"Telementry processor","text":"<ul> <li>Receives from the pipeline</li> <li>Process the msg(mainly validation)</li> <li>Stores msg in the database</li> </ul>"},{"location":"sysdesign/autonomouscars/#application-type_1","title":"Application Type","text":"<p>Web App &amp; Web API - No Mobile App - No Console - Yes Service - Yes Desktop App - No</p>"},{"location":"sysdesign/autonomouscars/#technology-stack_1","title":"Technology stack","text":"<p>Considerations:</p> <ul> <li>Processor: Solution: nodejs - already used, fast and great support for kafka</li> </ul> <p>Operational DB: - Data store: schema-less msg support - quick retrivals - no complex queries</p> <p>Solutiom: MangoDB </p> <p>Archive Db:</p> <ul> <li>support for huge amt of data</li> <li>not accessed frequently</li> <li>no need for fast retrival</li> <li>save costs</li> </ul> <p>Solution: Cloud storage (Azure)</p>"},{"location":"sysdesign/autonomouscars/#architecture_1","title":"Architecture","text":"<p>User/interface: No  Business logic : No Data Access: No Data store: No</p> <p>Redunancy for telementry processor place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/autonomouscars/#telementry-viewer","title":"Telementry viewer","text":"<ul> <li>Allow end uses to query data</li> <li>Displays real time data</li> </ul> <p>What it doesn't do - Analyze the data..</p>"},{"location":"sysdesign/autonomouscars/#application-type_2","title":"Application Type","text":"<p>Web App &amp; Web API - yes Mobile App - No Console - No Service - No Desktop App - No</p>"},{"location":"sysdesign/autonomouscars/#technology-stack_2","title":"Technology stack","text":"<p>Backend: Solution: NodeJS</p> <p>Frontend:  Solution: ReactJS</p>"},{"location":"sysdesign/autonomouscars/#architecture_2","title":"Architecture","text":"<p>We need to use classic 3-layered arch for this component.</p> <p>User/interface: Yes  Business logic : Yes Data Access: Yes Data store: No</p>"},{"location":"sysdesign/autonomouscars/#design-api-component","title":"Design API component","text":"<ul> <li>Get latest errors for all cars</li> </ul> <p>```GET /api/v1/telementry/erorrs  200 Ok or Empty list or dict</p> <pre><code>\n- Get latest telementry for specified car\n\n</code></pre> <p>GET /api/v1/telementry/{carid} 200 OK / 404 Not Found</p> <pre><code>- Get latest errors for specific car\n</code></pre> <p>GET /api/v1/telementry/error/{carid} 200 OK/ 404 Not Found ```</p> <p>Redunancy for telementry viewer place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/autonomouscars/#bi-application","title":"BI Application","text":"<ul> <li>Analyze telementry data</li> <li>Display custom reports about data, trends, forcasts</li> </ul> <p>how many cars did break during the last month what is the total distance the cars drove  etc </p>"},{"location":"sysdesign/autonomouscars/#application-type_3","title":"Application Type","text":"<p>Doesn't matter  BI application is always based on the existing ones</p> <p>Figure our from the gartner </p> <p>Power BI - Microsoft  ableau </p> <p>Note: Designing BI is not part of arch job Inform the customer saying that we are not part of BI experts, he can bring anyone he wishes to and we would be working with him to assist him as we laid out the high level system specs.</p>"},{"location":"sysdesign/facebook/","title":"Facebook","text":""},{"location":"sysdesign/facebook/#facebook-or-instangram-system-design-goes-here","title":"facebook or instangram system design goes here","text":""},{"location":"sysdesign/gmaps/","title":"Google Maps","text":""},{"location":"sysdesign/gmaps/#google-maps-system-design-goes-here","title":"google maps system design goes here","text":""},{"location":"sysdesign/grosscollections/","title":"Design gross collections","text":"<p>In this case study, we are allowing customer to create shopping lists that get collected and delivered by grosscoll employees. It already available all oever world. Employees have dedicated tables and displaying the list</p> <p>Design the collection side of the system. Cutomer side is already developed,</p>"},{"location":"sysdesign/grosscollections/#system-requirements","title":"System Requirements","text":""},{"location":"sysdesign/grosscollections/#functional","title":"functional","text":"<p>What should system do ?</p> <ul> <li>Web based </li> <li>tablets receive the list to be called </li> <li>Employees can mark items as collected or unavailable.</li> <li>When collection is done, list must be transferred to payment engine</li> <li>offline support is a must</li> </ul>"},{"location":"sysdesign/grosscollections/#non-functional","title":"non-functional","text":"<p>What should system deal with ?</p> <ol> <li>is it data intrinsic ?</li> <li>Users ?</li> <li>How much of data ?</li> <li>Performance ?</li> </ol> <p>Questions to ask customers ?</p> <ol> <li> <p>How many concurrent users ? sol: 200</p> </li> <li> <p>How many list processed/day ? sol: 10000</p> </li> <li> <p>whats the avg size of the shopping list? sol: 500KB</p> </li> <li> <p>Do we need offline support ? sol: yes</p> </li> <li> <p>What's the SLA ? sol: highest possible</p> </li> <li> <p>Since we already know that customer shopping is already developed, then how do lists arrive to the system ? sol: Queue - which means we know that all the meg arrive in queue and we need to pull the msg and process the data. </p> </li> </ol> <p>Data volume</p> <p>1 list = 500KB 10000 list/per day =  ~5GB/per day monthly = ~150GB/month yearly = ~1800GB/year = ~2TB/year</p>"},{"location":"sysdesign/grosscollections/#map-the-components","title":"Map the components","text":"<p>Components</p> <ul> <li>Employee have tablets</li> <li>offline support</li> <li>retrive lists</li> <li>mark items</li> <li>export list to payment engine</li> </ul> <p></p> <p>we keep list receiver and service differently, because in future if there is any change in the database we don't have to work completely modifying the service..</p>"},{"location":"sysdesign/grosscollections/#list-receiver","title":"list receiver","text":"<ul> <li>receives the shopping list to handle queue</li> <li>stores the list in the db</li> </ul>"},{"location":"sysdesign/grosscollections/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - No</li> <li>Console - Yes</li> <li>Service - Yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/grosscollections/#technology-stack","title":"Technology stack","text":"<p>Should be able to connect to queue and nothing else</p> <p>we would ask for the customer about the skill expertise that they use as we only consume the queue.</p> <p>Solution from customer: Java. </p> <p>Java is a best solution for the queue solution and hence we go with it. </p> <p>Now, we need to use the database for storing the values as our data is relational we use relational DB also the expected volume of data is around 2TB/year which is a lot. We can use paritioning for this</p>"},{"location":"sysdesign/grosscollections/#architecture","title":"Architecture","text":"<ul> <li>Queue receiver: yes</li> <li>Business logic : yes</li> <li>Data Access: yes </li> <li>Data store: yes</li> </ul> <p>Redunancy</p> <p>We will need to use the kafka where we have consumer group and all the instances of list receiver would be grouped.</p>"},{"location":"sysdesign/grosscollections/#list-service","title":"list service","text":"<ul> <li>allow employees to query list</li> <li>marks items in list</li> <li>export payment type</li> </ul>"},{"location":"sysdesign/grosscollections/#application-type_1","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - yes</li> <li>Mobile App - no</li> <li>Console - no</li> <li>Service - no</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/grosscollections/#technology-stack_1","title":"Technology stack","text":"<p>You can use Java</p>"},{"location":"sysdesign/grosscollections/#architecture_1","title":"Architecture","text":"<ul> <li>User/interface: yes</li> <li>Business logic : yes</li> <li>Data Access: yes</li> <li>Data store: yes</li> </ul> <p>Any service that exposes, we need to use API</p>"},{"location":"sysdesign/grosscollections/#design-api-component","title":"Design API component","text":"<ul> <li>Get list to be processed(by location)</li> </ul> <pre><code>GET /api/v1/lists/next?location= ..\n\n200 OK / 400 Bad Request\n</code></pre> <ul> <li>mark item as collected/unavailable</li> </ul> <pre><code>PUT /api/v1/lists/{listId}/item/{itemId}\n\n200 OK / 404 Not Found\n</code></pre> <ul> <li>export payment list data</li> </ul> <pre><code>POST /api/v1/list/{listId}/export\n\n200 OK / 404 Not Found\n</code></pre> <p>Reduancy</p> <p>place the list service behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/grosscollections/#front-end","title":"front end","text":"<ul> <li>Displays shopping list</li> <li>marks items as unavailble / collected</li> <li>send list to payment system</li> <li>support offline mode</li> </ul>"},{"location":"sysdesign/grosscollections/#application-type_2","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - yes</li> <li>Console - no</li> <li>Service - no</li> <li>Desktop App - yes</li> </ul>"},{"location":"sysdesign/grosscollections/#technology-stack_2","title":"Technology stack","text":"<p>Desktop, window based</p> <ul> <li>support all OS applications</li> <li>utilizes other app on the machine(db)</li> <li>requires setup, windows</li> </ul> <p>Web based </p> <ul> <li>limited functionality</li> <li>cannot use other apps</li> <li>fully compactible with other form factors</li> <li>no setup required</li> <li>cheaper hardware</li> </ul> <p>We don't need any redenacy for front end as there is no load. </p>"},{"location":"sysdesign/grosscollections/#exporter-list","title":"exporter list","text":"<ul> <li>used to send the shopping data to payment system</li> <li>it should be queue </li> </ul> <p>since its queue, we must ask question to customer</p> <ul> <li>do we already have a queue in the company ? soln: yes</li> </ul> <p>since there is already a list queue, we use the same for data queue to store or export data..</p> <ul> <li>do we need to use any third party ? soln: Nil</li> </ul>"},{"location":"sysdesign/grosscollections/#technical-diagram","title":"Technical diagram","text":""},{"location":"sysdesign/grosscollections/#physical-diagram","title":"physical diagram","text":""},{"location":"sysdesign/notifications/","title":"Notification service","text":""},{"location":"sysdesign/notifications/#notification-system-design-at-scale-goes-here","title":"notification system design at scale goes here","text":""},{"location":"sysdesign/papersource/","title":"Design newsletter","text":"<p>In this we are designing case study for papersource, that sells paper supplies. i.e Printer paper, Envelopes.. etc. They also need a new HR system, which manages salaries, vacation, payments for the employees. </p>"},{"location":"sysdesign/papersource/#requirements","title":"Requirements","text":""},{"location":"sysdesign/papersource/#functional","title":"Functional","text":"<p>What system should do ?</p> <ul> <li>Web based</li> <li>CURD operations</li> <li>Manage salaries<ul> <li>Allow manager to ask for emplyess salary chage</li> <li>Allow HR manager to approve/reject request</li> </ul> </li> <li>Manage vacation days</li> <li>Use external payment systems</li> </ul>"},{"location":"sysdesign/papersource/#non-functional","title":"Non functional","text":"<p>what the system should deal with ?</p> <ol> <li>what type of system (classic, legacy, cloud)</li> <li>Not a lot of users</li> <li>Not a lot of data</li> <li>Interface to external system (i.e)</li> </ol> <p>Questions</p> <p>You must be getting these details from the customer/users</p> <ul> <li> <p>how many concurrent users - 10</p> </li> <li> <p>How many employees ? - 250</p> </li> <li> <p>What we know abt external payment system ?</p> <p>legacy hosted system in company farm received input files monthy once for processing payment informations.</p> <p>Data volume calculations</p> <p>1 Employess = 1MB </p> <p>Each employess has ~10scanned documents(contract, reviews, etc) i.e each employee has 51MB docs. As company expected to grow, 500 in 5 yrs, 51MB*500 = 25.5GB</p> </li> <li> <p>What about criticality of the system ?</p> <p>HR based system, not very critical</p> </li> </ul> <p>so finally, we have these below as non-functional requirments</p> <pre><code>10 Concurrent users\nManage 500 users\n25.5 GB of data volume forcast\nNot mission critical \nfile-based interface\n</code></pre>"},{"location":"sysdesign/papersource/#component-mapping","title":"Component mapping","text":"<p>Based on function requirements, it would be good practice to map single component to single service for managing and maintaining. </p> <pre><code>Entities: Employee, Vacation, Salary\nInterface to payment system\n</code></pre> <p></p>"},{"location":"sysdesign/papersource/#logging-services","title":"logging services","text":"<p>Questions to be asked for logging service.</p> <ul> <li>Are there any logging service used in the company at present ? Client: No</li> <li>Any 3rd party ? we can choose, ELK, but its quite complex and resources are needed to operate and maintain.     since, our application is not being used so much, we can develop our own logging mechanism</li> <li>Should we need to develop our own logging  ?  Architect: Yes</li> </ul> <p>How would you decide for developing own logging mechanism ?</p> <ul> <li> <p>Application type(what it does)</p> <ul> <li>read log records from queue</li> <li>validate the records</li> <li>store in the data store</li> </ul> </li> <li> <p>Alternatives</p> <ul> <li>Webapp app or webAPI - No</li> <li>Mobile app - No</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul> </li> </ul>"},{"location":"sysdesign/papersource/#technology-stack","title":"Technology stack","text":"<ul> <li>what should code do here ?<ul> <li>Access Queue API</li> <li>validate the data</li> <li>store the data</li> </ul> </li> </ul> <p>- Architecture</p> <ul> <li>3 layer architecture</li> <li>UI/Service interface: Not required for logging</li> <li>Business logic: validate the records (polling)</li> <li> <p>Data access: saves validated records into data store</p> </li> <li> <p>logging redunancy  if there are any crash for logging, you need to have another service that would detect. </p> </li> </ul> <p>always have 3 instances of logging service to replicate that helps you to log. (active-active)</p> <p>we must use method called is-alive which will check for the active instances and if it gets reponse it would not log, incase if that doesn't get any reposnse after waiting for some period of time, it would consider that the service is down and new leader would be elected. </p>"},{"location":"sysdesign/papersource/#view-service","title":"view service","text":"<p>Get request from end users browsers and return static content (html, css, js) which after that access service like employee, vacation, salary data.</p> <ul> <li>Applicatio type(what it does)</li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture </p> </li> </ul> <p>Since this is only service the static content between the user and the service, we would only have UI</p> <p>UI/Service interface: UI  Business/application logic: No  Data access: No</p> <ul> <li>view service redunancy we use load balancer to view service so it would have balance traffic and security as well.</li> </ul>"},{"location":"sysdesign/papersource/#employee-service","title":"Employee service","text":"<p>Allows end users to query employee's data and perform CURD ops, but it does't display as its <code>view</code> service.</p> <ul> <li>Applicatio type(what it does)</li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li>.NET Core </li> <li>Emloyee data(relational db) - Microsoft SQL - since the documents are around(~1MB) it would be good. since we also use .NET</li> <li> <p>Employee documents storage (relational DB, file system,object,cloud storage)</p> </li> <li> <p>Architecture</p> </li> <li> <p>Employee API : create an API for employee service</p> <ul> <li>get full emp details by ID <code>GET /api/v1/employee/{id}</code></li> <li>list of employees by parameters <code>GET /api/v1/employees?name=..&amp;birthdate=..</code></li> <li>add employee <code>POST /api/v1/employee</code></li> <li>update employee details <code>PUT /api/v1/employee/{id}</code></li> <li>remove employee(don't remove record, mark it as removed) <code>DELETE /api/v1/employee/{id}</code></li> </ul> </li> <li> <p>Document API: get list of docs for the employees</p> <ul> <li>Add document <code>POST /api/v1/employee/{id}/document</code></li> <li>Remove document <code>DELETE /api/v1/employee/{id}document/{docid}</code></li> <li>Get document <code>GET /api/v1/employee/{id}/document/{docid}</code></li> <li>Get documents by parameters <code>GET /api/v1/employees/{id}/documents</code></li> </ul> </li> <li> <p>employee service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/papersource/#salary-service","title":"salary service","text":"<ul> <li>Allow managers to ask for an employee's salary change</li> <li> <p>Allows HR to approve/reject the request</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     Salary API request</p> <ul> <li>Add salary request <code>POST /api/v1/salaryRequest/</code></li> <li>Remove salary request <code>DELETE /api/v1/salaryRequest{id}</code></li> <li>Get salary request <code>GET /api/v1/salaryRequests</code></li> <li>Approve salary request <code>POST /api/v1/salaryRequest/{id}/approval</code></li> <li>Reject salary request <code>POST /api/v1/salaryRequest/{id}/rejection</code></li> </ul> </li> <li> <p>salary service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/papersource/#vacation-service","title":"vacation service","text":"<ul> <li>Allows employees to manage their vacation days</li> <li> <p>Allows HR to set available vacation days for employees</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     Vacation API request</p> <ul> <li>set available vacation days(HR) <code>PUT /api/v1/vacation/{empid}</code></li> <li>Get available vacation days(Employee) <code>GET /api/v1/vacation/{empid}</code></li> <li>Reduce vacation days(Employee) <code>POST /api/v1/vacation/{empid}/reduction</code></li> </ul> </li> <li> <p>vacation service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/papersource/#payment-interface-service","title":"payment interface service","text":"<ul> <li>Queries the db once a month for salary data</li> <li> <p>passes payment data to the external payment system</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - No</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - Yes</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     you need to use the timer as the application logic for triggering the job. </p> </li> <li> <p>Payment interface redunancy     use is-alive parameter between instance services of payment interface.. </p> </li> </ul>"},{"location":"sysdesign/papersource/#queue-tech-stack","title":"Queue - Tech stack","text":"<p>Choose as to which Queue service you need to use for your application. </p> <ul> <li> <p>RabbitMQ: General purpose message-broker engine, which is easy to setup and use</p> </li> <li> <p>Apache Kafka: Stream processing platform, which is more used in extensive data intrensic scenerios. </p> </li> </ul> <p>In our this case, since nothing is involved in the streaming platform, we can choose <code>RabbitMQ</code></p>"},{"location":"sysdesign/paymentprocessing/","title":"Payment processing","text":"<p>In this case study, we are studying about the payroll. Basically, payroll designs and builds payment processing system and what the system does is it receives files from various sources, mainly companies that want to pay their employees. </p> <p>The payroll system validates and processes the files and then sends instruction files to the banks in order to execute the actual payment. Now, one very important aspect of the payroll system is that it must be fully automatic, reliable and very quick. </p> <p>Note: there shouldn't be any human interaction while processing the file</p>"},{"location":"sysdesign/paymentprocessing/#system-requirements","title":"System Requirements","text":""},{"location":"sysdesign/paymentprocessing/#functional","title":"functional","text":"<p>What should system do ?</p> <ul> <li>receives files from various sources</li> <li>validates and processes the files</li> <li>work with various file formats</li> <li>perform various calculations on the file</li> <li>create bank payment file </li> <li>put payment file in a designated folder</li> <li>keep the log file for 7 years</li> </ul>"},{"location":"sysdesign/paymentprocessing/#non-functional","title":"non-functional","text":"<p>Questions to ask customers ?</p> <ol> <li> <p>How many files are we going to get each day? sol: 500 </p> </li> <li> <p>Any limitation on the latency of the process? sol: 1 min</p> </li> <li> <p>what is avg size of file ? sol: 1MB</p> </li> <li> <p>can we tolerate data loss ? sol: No</p> </li> </ol> <p>Data volume</p> <p>1 file = MB  500 file = 500MB  ~182GB/year ~1.3TB/7 years</p> <p>the space for the log file for 1 min procssing time</p> <p>~ 500KB log data ~ 500 file/day = 250MB log data/day ~ 91GB log data/year ~ 638GB log data/ 7 years</p>"},{"location":"sysdesign/paymentprocessing/#map-the-components","title":"Map the components","text":"<ul> <li>Passes payloads from logic unit to another.</li> <li>Balances load.</li> <li>Persists messages. (Durability!)</li> </ul> <p>Queue provides async, since we don't have any UI also the components are not waiting for any response, we would always use queue. </p> <p>So there are 2 queues to select </p> <ol> <li> <p>RabbitMQ  - This is Genral purpose and very easy to setup. this is not suitable for streaming services. </p> </li> <li> <p>Kafka - Great for streaming services, high load systems, but very complex to setup. </p> </li> </ol> <p>In this scenerio, we would use RabbitMQ as this is not a streaming servies also easy setup..</p>"},{"location":"sysdesign/paymentprocessing/#file-handler","title":"File handler","text":"<ul> <li>Pulls payment files from folders</li> <li>Put the files in the queue</li> </ul>"},{"location":"sysdesign/paymentprocessing/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - no</li> <li>Mobile App - no</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/paymentprocessing/#technology-stack","title":"Technology stack","text":"<ul> <li>Should be able to pull files from folders</li> <li>Should be able to connect to queue</li> </ul> <p>Would you want to ask the customer regarding the technology they know or they use or team using etc ..</p> <p>incase we would want to suggest, here is what it is ..</p> <ul> <li>performance</li> <li>community</li> <li>cross platform</li> <li>easy to learn </li> <li>evolving</li> <li>great threading support</li> </ul> <p>from above, we could either use java or .net core</p>"},{"location":"sysdesign/paymentprocessing/#architecture","title":"Architecture","text":"<p>3 layered arch</p> <ul> <li>service interface: yes</li> <li>Business logic : yes</li> <li>Data Access: yes </li> <li>Data store: yes</li> </ul> <p>File watcher, topic is selected by the file location, monitor the zip folder, put into the folder. </p>"},{"location":"sysdesign/paymentprocessing/#redunancy","title":"Redunancy","text":"<p>two of the file handler keeps shaking to check for liveness ..etc</p>"},{"location":"sysdesign/paymentprocessing/#file-formatter","title":"File Formatter","text":"<ul> <li>received files from its specific topic</li> <li>validates and formats the file to unified format</li> <li>puts the new file in quwue</li> <li>new formaters will be developed for new file</li> </ul>"},{"location":"sysdesign/paymentprocessing/#application-type_1","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - no</li> <li>Mobile App - no</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/paymentprocessing/#technology-stack_1","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core. no need for any other specifics.</p>"},{"location":"sysdesign/paymentprocessing/#architecture_1","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/paymentprocessing/#redunancy_1","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic, hence no extra work needed to work.</p>"},{"location":"sysdesign/paymentprocessing/#file-calculations","title":"File calculations","text":"<ul> <li>Receives file from queue</li> <li>performs some calculations</li> <li>puts new file in queue</li> </ul>"},{"location":"sysdesign/paymentprocessing/#technology-stack_2","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core.</p>"},{"location":"sysdesign/paymentprocessing/#architecture_2","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/paymentprocessing/#redunancy_2","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic</p>"},{"location":"sysdesign/paymentprocessing/#file-exporter","title":"File exporter","text":"<ul> <li>receives file from queue</li> <li>put trhe file in bank's folder</li> </ul>"},{"location":"sysdesign/paymentprocessing/#technology-stack_3","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core. no need for any other specifics.</p>"},{"location":"sysdesign/paymentprocessing/#architecture_3","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/paymentprocessing/#redunancy_3","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic</p>"},{"location":"sysdesign/paymentprocessing/#logging-service","title":"logging service","text":"<ul> <li>write log of records</li> <li>allow easy visualizations and analytics</li> <li>Preferrably, based on existing platform. </li> </ul> <p>you can use elk component for log storage and visualizations. </p> <p></p> <p>You can use serilog to transport file to the elastic db. from queue, you can send the data using logstash. beats or .... would be used to send the data to the elastic. </p>"},{"location":"sysdesign/paymentprocessing/#arch-diagrams","title":"arch diagrams","text":""},{"location":"sysdesign/paymentprocessing/#logic","title":"logic","text":""},{"location":"sysdesign/paymentprocessing/#technical","title":"technical","text":""},{"location":"sysdesign/paymentprocessing/#physical","title":"physical","text":""},{"location":"sysdesign/selectingdb/","title":"Selecting a database","text":""},{"location":"sysdesign/selectingdb/#database-choice","title":"Database choice","text":"<p>In system design how good your database design is and how well it can scale depends very much on the choice of database that you've used. Now databases generally do not impact your functional requirements, but normally the non-functional requirements are impacted by the choice of database. </p> <p>We will look at some of the potential solutions or possible set of databases that you can use to handle non-functional requirements (i.e certain query patterns, or certain kind of a data structure or certain kind of a scale to handle.) </p> <p>Now normally the choice of database depends on a below three factors. </p> <ul> <li>Data structure(very structured data or a totally non structured data)</li> <li>Choice of database(query pattern that you have.)</li> <li>Scaling(kind of a scale to handle)</li> </ul> <p>Structured information: it would be an information that you can easily model in form of tables and tables would have rows and columns of information. e.g user information, payments, social network etc. let's say if we have a structured data the next question would be..do we need any atomic City or transactional guarantees from the db or not? </p> <p>Example: you have two bank accounts A and B. So when transactions happens i.e debited from A account, it should be either credited to B account and vice versa, your total bank account statements should also reflect those changes. then you need an relational db with atomicity, consistent transactions i.e ACID guarantee</p> <p>Relational DB's frequently used are MySQL, Oracle, SQL Server, Postgres. Even if there is no atomicity required, you can still use relational or non-relational db.</p>"},{"location":"sysdesign/selectingdb/#caching","title":"Caching","text":"<p>Use cases</p> <ul> <li>While querying a db and you do not want to query the db many times, so you cache the value. </li> <li>Alternativel, if you are making a remote call to a different service and that is having a high latency, you might want to cache the response of that system locally at your end in a caching solution.</li> </ul> <p>Caching always stored as key/value pairs. So key normally is whatever your where clause is in the query or whatever your query param or request params are when you're making an API call and value is basically the response that you are expecting from the other system. So all of these kind of values would be stored in normally  key value stores. </p> <p>Caching tools:</p> <ul> <li>Redis - Preferrable as easy to use/stable and very well tested</li> <li>Memcached, you could use \"</li> <li>etcd </li> <li>Hazelcast</li> </ul>"},{"location":"sysdesign/selectingdb/#storage","title":"Storage","text":"<p>File storage: - You can store images locally and can serve but not redunant to failures.</p> <p>Cloud storage: -  These are not dbs, but when ever you want to store images/videos, you can use this as a data store, which comes along with the CDN for faster serving across different geographical locations.</p>"},{"location":"sysdesign/selectingdb/#search","title":"Search","text":"<p>you might be designing something like an uber or Google Maps kind of a thing where you want to provide text searching capability with support for fuzzy search. So for all of these kind of use cases you would be using something called as a Text Search Engine. Now a very common implementation of a Text Search Engine is provided by Elastic Search and Solr, and both of them are built on top of something called an Apache Lucene now Lucene fundamentally provides these text searching capabilities and that is then being used by both of these products Elastic Search(ES) and Solr, Now one more thing that they support is something called as a Fuzzy Search. So what is that is.. </p> <p>Let's say if you are searching for the word Airport and let's say is the user typed in \"AIRPROT\" with the wrong spelling, okay. this O and R are interchanged right.  Now, If a user searches for this and you do not return back any result, then that's a bad user experience, right. So you need your database to be able to figure out that the user did not really meant this thing [AIRPROT], the user actually meant AIRPORT, right? how does that database identify, so this word can be converted into the correct spelling of airport by changing two characters, right. R needs to be converted into O and O needs to be converted into R, right? so this is at an edit distance of two. So you can provide a level of fuzziness that your search engine needs to support. This has a fuzziness factor of two, which is the Edit Distance. Normally there are a lot of other factors also that come in but this is roughly how fuzzy searching is implemented in most of the solutions. So wherever you have any search capabilities there you use either Elastic Search or Solr. </p> <p>Note: One important thing about both of these are, these are NOT databases. These are search engines. So the difference between a Search Engine and a Database is whenever you write something in a Database, Database gives you a guarantee that that data wouldn't be lost.</p>"},{"location":"sysdesign/selectingdb/#metrics","title":"Metrics","text":"<p>Let's say you are storing some metrics kind of a data. So let's say if you are building a system like Graphite, Grafana or Prometheus which is basically an application metrics tracking system. So let's say if the use case that you are given is a lot of applications are pushing metrics related to their throughput, their CPU utilizations, their latencies, and all of that. And you want to build a system to support that. Then is when comes something called as a Time Series Database. Now think of Time Series Database as an extension of relational databases but with not all the functionalities and certain additional functionalities. </p> <p>So regular relational databases that you have would have the ability to update a lot of records right or they would also give you the ability to very random records but whenever you are building a metrics monitoring kind of a system you would never do random updates. You would always do sequential update in append-only mode if you wish to write entries. Also, read queries that you do, they are kind of bulk read queries with the time range. You query for last few minutes of data or few hours of data or few days of data, now time series databases are optimized for this kind of a query pattern and input pattern. </p> <p>Few of the time-series databases</p> <ul> <li>InfluxDB</li> <li>openTSDB(Open Time Series Database)</li> </ul>"},{"location":"sysdesign/selectingdb/#analytics","title":"Analytics","text":"<p>When you have lot of information and you want to store all of that information of a company in a certain kind of a data store for various kind of analytics requirements, like Amazon or Uber or any system design where you want to provide analytics on all the transactions of how many orders i'm having or geographies are giving me what revenues, most purchaes item etc, you need a Data Warehouse</p> <p>Data Warehouse is basically a large database in which you can dump all of that data and provide various querying capabilities on top of the data to serve a lot of reports. Now these are generally not used for transactional systems, these are generally used for offline reporting. So if you have that kind of use case then you can use something like a Hadoop which can very well sit in for that purpose, where you put in a lot of data from various transactional systems and then build a lot of systems that can provide reporting on top of that data. </p>"},{"location":"sysdesign/selectingdb/#document-database","title":"Document database","text":"<p>Now let's say you do not have structured data, So maybe you are trying to build a catalog kind of a system for an e-commerce platform like Amazon which has information of all the items that are available on that platform. e.g catalog for Amazon i.e there is the item like shirt - A shirt would have an attribute like a size:(large, a color). Now normally when you are on an e-commerce platform you not only need to see these attributes if it is just about seeing them you could kind of dump it as a Json and store it in any databases. Now querying on a JSON or random attributes is a bit tricky on the relational databases, but there are certain kinds of databases that are optimized for those kind of queries. So these are the databases where you have a lot of data not just data in terms of volume but in terms of structure. So if you have lot of attributes that can come in and a wide variety of queries that can come in, then you fall into this category, and if that is the case then you need to use something called as a document DB. Now there are a lot of providers of document DBs, MongoDB, Couchbase are some of them. Earlier we looked at Elastic Search and Solr for text searching those are also special cases of Document Database.</p> <p>### Columnar database</p> <p>If you have an ever-increasing data.(i.e Uber), all the drivers of Uber are continuously sending location pings. so let's stay there are some number of drivers and they kind of translate into X number of location records per day. So there would be X number of records inputted per day. But this X would not be a constant, it would be a growing number why because the number of drivers of uber are increasing day by day right so this data would become probably X one day one. Additively 1.1X on day two, 1.2x on date three, so on and so forth. So it would not be increasing in a linear fashion it would be increasing but in a more than a linear fashion. So that is what I am calling an ever increasing data. Plus if you have finite number of type of queries. So let's if you want to track location pings of drivers the most important query that you will do is find all the locations of a driver whose driver ID is something right. So if you have less number of type of queries(maybe high volume) but a large amount of data then these kind of databases would be the best choice for you. This is something called as a columnar DB or Column oriented DB. Cassandra and HBase are the most used and most stable options out there for these kind of scenarios. most preffereeed option would be Cassandra which is not very heavy for deployment.</p>"},{"location":"sysdesign/selectingdb/#use-cases","title":"Use cases","text":""},{"location":"sysdesign/selectingdb/#use-case-1","title":"Use case - 1","text":"<p>Now let's take an example of us building an e-commerce platform something like an Amazon. Now when you are managing inventory on that side you want to make sure that you are not over selling items. So let's say there is just one quantity of a particular item left and there are 10 users who want to buy that you want to have this ACID properties there to make sure that only one of the users should be able to commit the transaction other users should not be able to commit the transaction, right. You can put constraints and all of that on there. It would make sense to use a RDBMS database for the inventory management system of a system like Amazon or maybe an order management system right But if you look at the data of Amazon it is an ever interesting data. Why? because the number of orders are additive each day new orders are coming in they cannot purge the data due to lot of legal reasons plus the number of orders are also increasing so it naturally fits into this model right with you where I am recommending together Cassandra. So what you could use is a combination of both of these databases. You could use MySQL or any other RDBMS alternate for storing data about the orders it has just placed and not yet deliver to the customer. Once the item is delivered to the customer you can then remove from this RDBMS, and put it into Cassandra as a permanent store.</p>"},{"location":"sysdesign/selectingdb/#use-case-2","title":"Use case - 2","text":"<p>Now let's look at another example again taking an example of Amazon, let's say if you want to build a reporting kind of a thing which lets you query something like get me all the users who have bought sugar in last five days now sugar is not just a product. There are a lot of sellers selling different sugar alternates of different companies maybe, of different qualities maybe, right. so sugar would then be like a lot of item IDs right and on top of those a lot of item IDs there must be a lot of orders. Now again I am saying that orders would either be in Cassandra or RDBMS, but if you are doing random queries where some you might want to query on who bought sugar, who bought TV, who bought TV of certain quality, who bought a fridge of certain quality, that kind of logically goes into a model where I was recommending to use a Document DB. now what you could do is you could store the querying part over here(Document DB). You could basically say that I'll store a subset of order information into a MongoDB which says that user ID so and so, had an order ID so-and-so, on some particular date, which has these ten item ID in the certain quantity, right On this database you could run a query which will return you a list of users and list of order right and then you could take those Order IDs and query on both of these systems(RDBMS+Cassandra) right so here we are using all the three systems in combination to provide various querying capabilities like with user bought what kind of use case, right. So in any real world scenario you would have to use a combination of such databases to fulfill the functional and non-functional requirements that you have.</p>"},{"location":"sysdesign/twitter/","title":"Twitter","text":""},{"location":"sysdesign/twitter/#twitter-system-design-goes-here","title":"twitter system design goes here","text":""},{"location":"sysdesign/uber/","title":"Uber","text":""},{"location":"sysdesign/uber/#uber-system-design-goes-here","title":"uber system design goes here","text":""},{"location":"sysdesign/url_shortner/","title":"URL Shortner","text":""},{"location":"sysdesign/url_shortner/#functional","title":"Functional","text":"<ul> <li>Get the long url and shortern it</li> <li>On clicking, it must be re-directed to the long url</li> </ul>"},{"location":"sysdesign/url_shortner/#non-functional","title":"Non functional","text":"<ul> <li>very low latency</li> <li>HA </li> </ul> <p>Questions</p> <ul> <li>What's the traffic generated for the long urls </li> <li>what kind of the length should I need to keep for shorturl</li> <li>what's the duration of the shorturl ?</li> </ul> <p>let's say you have 'x' requests for a year, then you would have traffic around </p> <p>x606024365 days = y  based on the value of 'y' you would then be required to choose the length of the short url which contains  A-z, a-z, 0-9 which are 62 characters.. </p> <p>i.e 62^5 or 62^6 would provide around 3.5billion combinations..</p> <p></p> <p>since you have LB, the request can be moved to any of the services.. which ideally means our short url can be duplicated with the same alpha numbers.. i.e ideally you could have 2 same short urls for the long ones which is incorrect..</p> <p>so we can use an <code>redis</code> so that it would assign a unique way of assigning the requests.. but what if the redis goes down, it would forget the tokens that are created and will start from afresh which defeats our purpose.  </p> <p>so we are using <code>service token</code> in sitting on the mysql.. we could be assigning some range for every url_shortner_node so that it would assign token starting from their defaults, solving our problems..</p>"},{"location":"sysdesign/url_shortner/#logging","title":"logging","text":"<p>Implement the logger service for analytics.. once we get response from the user, we would can add additional fields to the reponse like, country orgin, platform, ip address etc which can be used for tracking purpose.. </p> <p>we can send the log to write into kafka async where it would miss few of the requests, but since this is not payment related it should be okay gradually .. we still should avoid kafka as it is IO bound ops and we could lose the latency as we stated in the non-functional requirement. Instead we could use a local logger and aggregate all the logs and write at once to kafka, which improves IO but if the service is gone, we would lose all the logs at once. it would be better to check with the customer on this design requirement. </p>"},{"location":"sysdesign/whatsapp/","title":"Whatsapp","text":""},{"location":"sysdesign/whatsapp/#whatsapp-system-design-goes-here","title":"whatsapp system design goes here","text":""},{"location":"sysdesign/youtube/","title":"Youtube","text":""},{"location":"sysdesign/youtube/#youtube-or-netflix-system-design-goes-here","title":"youtube or netflix system design goes here","text":""},{"location":"sysdesign/zoom/","title":"Zoom","text":""},{"location":"sysdesign/zoom/#zoom-system-design-goes-here","title":"zoom system design goes here","text":""},{"location":"vcontroller/git/faq/","title":"Interview Questions","text":""},{"location":"vcontroller/git/overview/","title":"Git Arch","text":"<p>Working directory - current files that are stored, it's also called as untracked files</p> <p>Staging area - files that you wish to commit(to create snapshot of the files).</p> <p>Git directory - after commit is fired, files which are in staging area will move to git repository.</p>"},{"location":"vcontroller/git/overview/#checkout","title":"Checkout","text":"<p>you would use the commit id and move your HEAD to that particular commit and then you would branch from there. </p> <pre><code>git commit &lt;commit_id&gt;\ngit checkout -b &lt;new_branch&gt;\n</code></pre> <p>Incase you don't need to branch out, this method is not suitable</p> <p>second way,</p> <pre><code>git log --oneline\ngit checkout &lt;commit-id&gt; -- filename.txt\ngit status\ngit log --oneline\ngit commit -m 'new commit id'\n</code></pre>"},{"location":"vcontroller/git/overview/#branching","title":"branching","text":"<p>fast-forward </p> <p>Developers create a feature branch, work on it, and when it's ready to be integrated into the main development branch, they perform a fast-forward merge if the conditions are met. This keeps the commit history clean and straightforward.</p> <p>Default merging startergy would be <code>ff</code></p> <pre><code>git branch -b feature/feature1\ngit checkout main\ngit merge feature/feature1\n</code></pre>"},{"location":"vcontroller/git/overview/#diff","title":"diff","text":"<p>git diff command is used to display the differences between two sets of changes, such as comparing files between branches.</p> <pre><code># differences between the master branch and a feature\ngit diff master..feat1 \n\n#differences for file1.txt and file2.js between the master and feat1 branch\ngit diff master..feat1 file1.txt file2.js \n\n#Difference for a Single File\ngit diff master:app.js feat1:app.js\n\n#Differences for Staged Changes\ngit diff --staged\n</code></pre>"},{"location":"vcontroller/git/overview/#stash-unstash","title":"stash &amp; unstash","text":"<p>Let's assume you created file and commited in the main branch. Now, lets say you would checout a new branch and modify the files without committing. Once you switch back to main branch you would get the changes made in another branch to main which is not recommended. The solution would be to stash changes to the branch before switching.</p> <pre><code>git stash list\ngit stash / git stash save\n</code></pre> <p>Once you are back to your branch(or any branch you would want to apply changes on), you can unstash and start working from where you left off. </p> <p><code>pop</code> it would apply changes to the current branch you are in, and removes from the stash. <code>apply</code> applies the changes to the current directory and would not remove from the stash.</p> <pre><code># removes from the stash\ngit stash pop\n\nor \n\n# would not remove, but can be applied later to any branch.\n\ngit stash apply\n</code></pre> <p>you can clear or drop stashes</p> <pre><code>git stash drop stash@{1}\ngit stash clear\n</code></pre>"},{"location":"vcontroller/git/overview/#detached-head","title":"detached HEAD","text":"<p>When we commit any file in the branch, the HEAD always points to the branch. when we checkout and workon, we still have our HEAD being pointed at the branch we work upon.</p> <p>When we checkout a particular commit, HEAD points at that commit rather than the branch, then we call it as a <code>detached HEAD</code></p> <pre><code>git log --oneline\ngit commit &lt;commit-id&gt;\ngit status \n</code></pre> <p>It would be very essential sometimes that you need to take up a particular commit and then work upon. In that case, you can checkout particular commit, where youe HEAD would point to the branch you took from.</p> <pre><code>git checkout &lt;commit-id&gt;\n&lt;modify your files&gt;\ngit add .\ngit commit -m 'your new commits'\ngit status\n</code></pre> <p>You can select the previous commit using HEAD</p> <pre><code>git checkout HEAD~1\ngit checkout HEAD~2\ngit switch - # it would take you back where you were there\n</code></pre>"},{"location":"vcontroller/git/overview/#discard-changes","title":"discard changes","text":"<p>you have made changes to the file, but don't want to keep those. You can revert back the file to whatever it looked like when you last committed, <code>reverting back to HEAD</code></p> <pre><code>1. git checkout HEAD file1.txt\n2. git checkout -- file1.txt file2.txt\n3. git restore file1.txt file2.txt\n</code></pre> <p>Let's say you have commited the secrets file(i.e staged area), and now you want to restore it.</p> <pre><code>git status\ngit restore --staged secrets.txt\ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#git-reset","title":"git reset","text":"<p>soft</p> <p>let's say you have made a couple of bad commits on the master branch, but you actually made them on a seperate branch instead, then you use <code>git reset &lt;commitid&gt;</code>. here, in this case you would actually lose the commit id's but your changes in the file remains the same. Hence you would need to take a new branch and commit those files. Once commited then come back to <code>master/main</code> branch. you won't see those commits as well as the changes made to that file.</p> <p>simple words, won't effect working directory or staging area, only commitid would be changed.</p> <pre><code>git log --oneline\ngit reset commitid\ngit log --oneline - bad commits to the file not seen, head is relaed to your commitid\ngit status - it says modified, but your changes still exists\ngit checkout -b do_your_commits\ngit add . \ngit commit 'removed changed made to files from the branch'\ngit checkout master\ngit status\n</code></pre> <p>hard</p> <p>If you want your changes and your commitid to be reverted from <code>working directory and stage</code> then you would be performing the <code>hard</code> reset. </p> <pre><code>git log --oneline\ngit status \ngit reset --hard commitid - all your changes are lost along with commitid\ngit status\n</code></pre> <p>Note: Make sure always that you need to be careful in hard reset as you would lose changes made in working directory.</p>"},{"location":"vcontroller/git/overview/#git-revert","title":"git revert","text":"<p>Creates a new commit which reverses/undo the changes from a commit. </p> <pre><code>git add .\ngit commit -m 'bad commits'\ngit log --oneline\ngit revert commitid - this will prompt an editor for changes and on saving you would \"revert bad commit\", changes to the file are lost\ngit status\ngit log --oneline - your revert entry will be added so that collaborators would know that you have reverted changes on the code.\n</code></pre>"},{"location":"vcontroller/git/overview/#git-originmain","title":"git origin/main","text":"<p>remote tracking branch</p> <p>At the time you last committed with this remote repo. </p> <p>let's say you have cloned the repo, when you search for the branches you would see there are <code>remote/origin</code>  which is nothing but the pointer for the remote repo of main branch. when you add files and commit, then you would get message like your <code>origin/main</code> is ahead by 1 commit.</p> <p>origin/master - references the state of the master branch on the remote repo named origi</p> <pre><code>remote branch -r\ngit add newfile.txt\ngit commit 'your remote branch ahead'\n</code></pre> <p>In case, you wanted to know what exactly your changes were in the remote repo, then you would switch to <code>origin/master</code>. It would message as your <code>HEAD</code> has been detached, no need to panic. incase you want to make some more new changes, then take out a new branch from it and then work on. </p> <pre><code>git switch -c origin/master\n&lt;detached head&gt;....\n\ngit checkout -b &lt;new_branch&gt; - incase you need a new branch\n\ngit switch -c master\n</code></pre>"},{"location":"vcontroller/git/overview/#git-fetch-and-pull","title":"git fetch and pull","text":"<p>git fetch </p> <ul> <li>Allows changes from the remote repository to the local repository.</li> <li>Updates the remote-tracking branches with new changes</li> <li>Does not merge changes onto your current HEAD branch</li> <li>Safe to do anytime. </li> </ul> <p>git pull </p> <ul> <li>Allows changes from the remote repository to local repository to working directory.</li> <li>Updates the current branch with new changes, mergung them</li> <li>Can result in merge conflicts</li> <li>Not recommended if you have un-committed chanegs</li> </ul> <p>git pull = git fetch + git merge</p>"},{"location":"vcontroller/git/overview/#merge-pr-with-conflicts","title":"merge PR with conflicts","text":"<p>Switch to the branch in question. Merge in master/main and resolve conflicts</p> <pre><code>git fetch \ngit switch my-new-feature\ngit merge master\nfix conflicts\n</code></pre> <p>Switch to master, marge the feature branch(with no conflicts now), push changes to github. </p> <pre><code>git switch master\nget merge my-new-feature\ngit push origin master\n</code></pre> <p>Now, your PR would be without conflicts</p> <ul> <li>what is rebase and explain</li> <li>Explain about the branchinig startergy</li> </ul>"},{"location":"vcontroller/git/overview/#clone-and-fork","title":"clone and fork","text":"<p>Cloning is about creating a local copy for working on a project, while forking is about creating a separate copy, often used in the context of open-source collaboration, with the potential to contribute changes back to the original project. The choice between cloning and forking depends on your specific needs and the collaborative context of the project you're working on.</p> <p>Let's say you forked, which creates a copy from the original in your account, but when the changes happens to the original repo, your changes would be out of sync. Hence you would need to configure to allow remote repo to get changes incase the original repo changes. </p> <pre><code>git remote -v \ngit remote add upstream main # Configure to the original repo for incoming changes.\ngit remote -v \n</code></pre> <pre><code>git pull upstream main \ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#rebase","title":"rebase","text":"<ul> <li>It can be used as an alternative to merge</li> <li>It can be used as a clean up tool for your commits. </li> </ul> <p>Let's say when you are working on the feature branch, there are few of the bug fixes and they would have commited to master branch. So now you need to merge the chages from main branch to your feature branch, resulting in a merge commit. </p> <p>When the above keeps happening for a quite long time, your branch would have all the merge commits from main and your commits description on the feature would not be so much visible. Hence in this case, we would use rebase all the feature branch commits would be available at the tip of master branch, so no merge commits</p> <pre><code>mkdir music\ncd music; git init \nvim songs.txt\n\ngit add songs.txt\ngit commit -m 'added songs file' \n\nvim songs.txt\ngit commit -m 'added two film albums'\n\ngit switch newalbum \nvim songs.txt\ngit add songs.txt\n\ngit commit -am 'two more new albums added' \n</code></pre> <p>let's say couple of songs added by someone and merged into master </p> <pre><code>git switch master\nvim songs.txt\ngit commit -am 'couple more albums added' \n</code></pre> <p>however, you are still working on the feat branch and would like to bring changes from master. Hence you would start merging changes on the master. </p> <pre><code>git switch feat - your working branch\ngit merge master\n\n&lt; Now you have a merge commit from master branch &gt;\n\ngit log --oneline\n</code></pre> <p>When above process has been repeated, you would have more merge commits. In order to make the merge commit appear at the last of the feature branch, we would go for rebase</p> <pre><code>git switch feat\ngit rebase master\n</code></pre> <p>When, you have a conflict in the master branch, you would fix the conflict and add the files to the branch. </p> <pre><code>git switch feat\ngit merge master\n\n&lt;RESOLVE AUTO CONFLICTS&gt;\n\ngit commit -am 'fixed merge conflicts'\ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#git-commits","title":"git commits","text":"<p>In above section we told that we could use <code>rebase</code> as a clean up tool, here we will learn more about this. i.e rewrite, delete, rename, or even re-order commits(before sharing to others)</p> <pre><code>git log --oneline\ngit rebase -i HEAD~6\n\n# after modifying file, save and quit, you would be opened by another edit, save a new commit msg.\ngit status \ngit log --oneline\n</code></pre> <p>Some of them you must try out..</p> <pre><code># p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n</code></pre>"},{"location":"vcontroller/git/overview/#tag","title":"tag","text":"<p>Two types of tagging</p> <ol> <li>lightweight tag: they are just a name/label that points to a prticular commit</li> <li>annotated tags: store extra info including the authors anme, email, date, tag msg.. etc</li> </ol> <pre><code>git tag\ngit tag -l \"*beta*\"\ngit tag -l \"v17*\"\ngit diff v17.0.0..v17.0.1\n</code></pre> <p>lightweight</p> <pre><code>git commit -am 'added patch version'\ngit tag v18.0.1\n\ngit commit -am 'added readme.md'\ngit tag v10.0.2\n\ngit diff v18.0.1..v10.0.2\n</code></pre> <p>annotated</p> <pre><code>git tag -a v18.0.3 # it would open an editor to provide msg.\n</code></pre> <p>you can also use tag from taking previous commit id</p> <pre><code>git tag &lt;tagname&gt; &lt;commitid&gt;\ngit tag &lt;tagname&gt; &lt;commitid&gt; -f # force tag for commit incase it already exists\ngit tag -d &lt;tagname&gt;\n</code></pre> <p>Note: when you update the remote repo using tags, it won't push all the tags. instead you need to explicitly tell to push it</p> <pre><code>git push origin &lt;tagname&gt;\n\ngit push origin --tags # push all the tags\n</code></pre>"},{"location":"vcontroller/git/overview/#behind-the-git","title":"behind the git","text":"<p>when you do <code>git init</code> you would be getting an <code>.git</code> directory and it holds all the version files for the repo. we would look few of the main files that helps in understanding the git better. </p> <pre><code>ls .git/\nCOMMIT_EDITMSG config         hooks          info           objects        refs\nHEAD           description    index          logs           packed-refs\n\n\u279c  react git:(main) ls .git/refs\nheads   remotes tags\n\u279c  react git:(main) ls .git/refs/heads\nmain\n\u279c  react git:(main) ls .git/refs/remotes\norigin\n\u279c  react git:(main) ls .git/refs/remotes/origin\nHEAD\n\u279c  react git:(main) ls .git/objects\n0e   19   41   52   54   6e   8f   a8   dc   info pack\n</code></pre> <p>refs -  Contains one file per branch in a repository. Each file is named after a branch and contains the hash of the commit at the tip of the branch(last commit). </p> <p>HEAD - text file that keeps track of where HEAD points. during the DETACHED HEAD it contains a hash instead of branch. </p> <p>objects - contains all the repo files. This is where git stores the backups of files, commits in a repo etc  The files are all compressed and encrypted..</p> <p>There are 4 types of objects</p> <ol> <li>commit </li> <li>tree</li> <li>blob </li> <li>annotated tag</li> </ol> <p>When ever we write commits, its SHA-1 that encrypts and stores in a database that has key-value pairs.  you need anything to hash out and check, you can use the below.</p> <p>Encrypt</p> <pre><code>echo 'hello' | git hash-object --stdin \necho 'hello' | git hash-object --stdin -w\nce013625030ba8dba906f756967f9e9ca394464a\n\nls .git/objects # your hash object stored in this directory, which is encrypted.\n</code></pre> <p>Decrypt</p> <pre><code>\u279c git:(main) git cat-file -p ce013625030ba8dba906f756967f9e9ca394464a\nhello\n</code></pre> <p>git blobs</p> <p>git uses to store the contents of files in a given repository.Blobs don't even include the filenames of each file or any other data. It looks like a commit hash, but its only blob hash.</p> <p>trees</p> <p>Tress are git objects used to store the contents of a directory. each tree contains pointers that can refer to blobs and to other trees. each entry in a tree contains SHA-1 hash of a blob or tree, as well as the mode,type, and filename</p> <pre><code>git cat-file -p main^{tree}\n040000 tree e4cacd8e23c9de749e53b4d06e5cf76fd10bf22d    .circleci\n040000 tree d0b0e04eb6108e5cd4c4a2c87a7c68f80772bbb1    .codesandbox\n100644 blob 07552cfff88bafaf4d207e6255394bc6d6215302    .editorconfig\n100644 blob 7d79ef692311259a6986aaa9160b1f6e7e795180    .eslintignore\n100644 blob 9d88915811935871123b2b450e972de4251ae109    .eslintrc.js\n100644 blob 176a458f94e0ea5272ce67c36bf30b6be9caf623    .gitattributes\n040000 tree 855f8e70e7e3e1bc69f0f4771e4591dceee09e54    .github\n100644 blob 6ec345e172e5e034cf68ef9c4a9c34fd8043da95    .gitignore\n100644 blob e661c3707d5de330ad0b939af9623f894a0bc0d8    .mailmap\n100644 blob e329619ca22426dece9974cfc626442201c19afa    .nvmrc\n100644 blob 6f69f7f891d672bc7b6696c3c33aaa760956e715    .prettierignore\n100644 blob 4f7ef193130c9019539a08bfb5738ba7af968c83    .prettierrc.js\n100644 blob 0967ef424bce6791893e9a57bb952f80fd536e93    .watchmanconfig\n100644 blob 146796383fbeaa19371045f4559d8d32817bf939    AUTHORS\n100644 blob 6040a0b246cb3402a626d3624f1ae6fe939adbbb    CHANGELOG-canary.md\n100644 blob 8f6df415e32a3706fe120094a22f8253fcd900fa    CHANGELOG.md\n100644 blob 08b500a221857ec3f451338e80b4a9ab1173a1af    CODE_OF_CONDUCT.md\n100644 blob 589af800fdc36dd1658ea6e96ad88a60572ca523    CONTRIBUTING.md\n100644 blob b93be90515ccd0b9daedaa589e42bf5929693f1f    LICENSE\n100644 blob a8d33198d23c47de66eca1caccdeeea7d9e78661    README.md\n100644 blob ef97ee7e3afc3716226dfb5d33a34e951c142690    ReactVersions.js\n100644 blob 655dfeaec0e67a9c448bf08a5f32d1f73aaa9611    SECURITY.md\n100644 blob d4d1e3213c574c85ed774d85c73567c06d534129    babel.config.js\n100644 blob e29426afda7a956b0cebbc24f374cc2b3276044b    dangerfile.js\n040000 tree 16d7de144f85053e52b5df9c2fa2741113e03e0e    fixtures\n100644 blob 76443cdd50285039de8c4e1ff755722c402bc03c    netlify.toml\n100644 blob d45f2f57c4b5bfa9507f134f6eea6adf88782464    package.json\n040000 tree 11c2952c7a7bd2fb611231701ba90a7766e1c09d    packages\n040000 tree a4a83cf4096449c73f4d552f4e8c9bbf15be22ad    scripts\n100644 blob 30b017680e30f3699c43aec49bfcf31e6f1a4dab    yarn.lock\n</code></pre> <p>commits</p> <p>Commit objects combine a tree object along with information about the context that led to the current tree. commits store a reference to parent commit, authors, the committer and commit msg.</p>"},{"location":"vcontroller/git/overview/#reflogs","title":"reflogs","text":"<p>Git keeps a record of when the tips of branches and other references were updated in the repo. you can view and update these ref logs using the <code>git reflog</code>. It would be helpful incase you have messed up commits or while you rebase you need to know what changes in the past you made etc .. you can always use <code>reflogs</code> to go that commit and work on.</p> <p>Note: reflogs keep track only local activity. they are not shared with collaborators. older entries are removed after 90 days.</p> <pre><code>git reflog show HEAD\ngit reflog\n</code></pre> <p>differences between <code>log</code> and <code>reflog</code> is that, log gives the commits history where as reflog provides the switch between the different branches along with all commit ids. </p>"},{"location":"vcontroller/git/overview/#alias","title":"alias","text":"<p>you can find the config file in <code>~/.git/.gitconfig</code>, you can define your alias as below</p> <p>vim ~/.git/.gitconfig</p> <pre><code>[alias]\n    l = log\n    st = status\n</code></pre> <p>Now, you can use </p> <pre><code>git l \ngit st\n</code></pre> <p>you can also use the CLI utility that helps you to achive the same(below)..</p> <pre><code>git config --global alias.l log \ngit config --global alias.st status\n</code></pre> <p>git alias references</p> <ul> <li> <p>The Ultimate Git Alias Setup</p> </li> <li> <p>Must Have Git Aliases</p> </li> <li> <p>Configure Git Alias</p> </li> </ul>"},{"location":"vcontroller/git/overview/#git-oneliners","title":"Git Oneliners","text":"<pre><code>git reset HEAD -- path/to/file -&gt; rm file from staged repo\n\ngit commit --amend -m 'created new files' --no-edit -&gt; modify recent commit\n\ngit reset --hard HEAD~1  -&gt; revert previous commit\n</code></pre>"},{"location":"vcontroller/git/overview/#references","title":"References","text":"<p>https://www.bogotobogo.com/cplusplus/Git/Git_GitHub_Express.php</p> <p>https://www.codementor.io/@alexershov/11-painful-git-interview-questions-and-answers-you-will-cry-on-lybbrqhvs`</p>"}]}