{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Sunil's Wiki","text":"<p>This wiki document serves as a comprehensive repository for technical references and personal learnings. It aims to capture a wide range of topics related to various domains, including programming languages, frameworks, tools, and concepts. The document's purpose is to facilitate knowledge and continuous learning for me. It covers practical examples, code snippets, best practices, troubleshooting tips, and insights gained from personal experiences.</p> <p>Programming Languages</p> <ul> <li>Explore about the programming languages, code patterns, clean code... etc</li> <li>Cover popular frameworks and libraries used in software development, web development</li> <li>Discuss essential tools, software, and utilities used in various technical fields. Include tutorials, tips, and tricks for using them effectively.</li> <li>Dive into fundamental concepts and principles in computer science, software engineering, networking, databases, algorithms, and more. Provide explanations, diagrams, and practical applications.</li> <li>Provide tips and techniques for identifying and resolving common errors, bugs, and issues in different programming environments and platforms.</li> <li>Share personal projects undertaken by contributors, detailing the challenges faced, lessons learned, and insights gained. Include code repositories, project documentation, and success stories.</li> </ul>"},{"location":"#myself","title":"Myself","text":"<p>I\u2019m a DevOps and SRE with 14+ years of experience, currently working at IBM where I focus on SRE automation and internal platform engineering.</p> <p>Over the last few years, I\u2019ve been designing and building a Slack-based internal automation platform that streamlines incident management, warroom creation, GitHub workflows, and operational tasks. This reduced manual incident handling time from minutes to seconds and significantly improved operational efficiency.</p> <p>My core strengths are infrastructure automation using Terraform, Kubernetes operations, CI/CD pipeline design, and building scalable automation systems in AWS and IBM Cloud.</p> <p>Before this, I worked extensively in Linux infrastructure environments managing 300+ servers, implementing Ansible-based automation, security hardening, and performance optimization.</p> <p>Overall, I specialize in reducing operational toil, improving reliability, and designing scalable systems with strong automation practices.</p>"},{"location":"#day2day","title":"Day2Day","text":"<p>On a day-to-day basis, I work on enhancing and maintaining our internal SRE automation platform.</p> <p>This includes developing new automation workflows in Python, improving existing Slack-based operational tools, integrating APIs like ServiceNow and GitHub, and ensuring the platform remains highly available and scalable.</p> <p>I also monitor system health using Prometheus and Grafana dashboards, analyze logs when issues arise, and troubleshoot Kubernetes-based deployments.</p> <p>A significant part of my work involves identifying repetitive operational tasks and converting them into automated workflows to reduce manual intervention and improve reliability.</p> <p>Additionally, I collaborate with SRE and development teams during incidents, help with root cause analysis, and continuously improve system resilience through better monitoring and automation.</p> <p>Documentation help</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"about/","title":"About","text":"<p>This wiki is a personal knowledge base that captures ongoing learning in software engineering, DevOps/SRE, cloud, and system design. Notes are organized by topic and focused on practical references: concepts, diagrams, checklists, and implementation details.</p>"},{"location":"about/#what-this-wiki-includes","title":"What this wiki includes","text":"<ul> <li>Programming fundamentals and patterns (Python, Go, clean code)</li> <li>DevOps and platform tooling (Docker, Kubernetes, CI/CD, Linux, monitoring)</li> <li>Reliability engineering (incident management, observability, chaos engineering)</li> <li>System design case studies and architecture notes</li> <li>Cloud and MLOps overviews</li> </ul>"},{"location":"about/#why-it-exists","title":"Why it exists","text":"<p>The goal is to keep a durable, searchable set of notes for quick recall and continuous improvement, and to share learnings in a structured way.</p>"},{"location":"cicd/overview/","title":"Overview Of CICD","text":""},{"location":"cicd/overview/#difference-between-cicd","title":"Difference between CI/CD","text":"<p>Continuous Delivery: Software can be deployed to customers at any time with the \"push of a button\" (i.e. by running a deployment script).</p> <p>Continuous Deployment: Software is automatically deployed to customers once it passes through the continuous integration system.</p> <p></p>"},{"location":"cicd/overview/#phases-of-continious-integrations","title":"Phases of Continious integrations","text":"<p>Each of these phases involves incremental improvements to the technical infrastructure as well as, perhaps more importantly, improvements in the practices and culture of the development team itself</p>"},{"location":"cicd/overview/#phase-1-no-build-server","title":"Phase 1 - No Build Server","text":"<p>Software is built manually on a developer's machine, but developers do not necessarily commit their changes on a regular basis. Some time before a release is scheduled, a developer manually integrates the changes</p>"},{"location":"cicd/overview/#phase-2-nightly-builds","title":"Phase 2 - Nightly Builds","text":"<p>the team has a build server, and automated builds are scheduled on a regular (typically nightly) basis. This build simply compiles the code, as there are no reliable or repeatable unit tests. Indeed, automated tests, if they are written, are not a mandatory part of the build process, and may well not run correctly at all. However developers now commit their changes regularly, at least at the end of every day</p>"},{"location":"cicd/overview/#phase-3-nightly-builds-and-basic-automated-tests","title":"Phase 3 - Nightly Builds and Basic Automated Tests","text":"<p>The build server is configured to kick off a build whenever new code is committed to the version control system, and team members are able to easily see what changes in the source code triggered a particular build, and what issues these changes address</p>"},{"location":"cicd/overview/#phase-4-enter-the-metrics","title":"Phase 4 - Enter the Metrics","text":"<p>Automated code quality and code coverage metrics are now run to help evaluate the quality of the code base and (to some extent, at least) the relevance and effectiveness of the tests. The code quality build also automatically generates API documentation for the application. </p>"},{"location":"cicd/overview/#phase-5-getting-more-serious-about-testing","title":"Phase 5 - Getting More Serious About Testing","text":"<p>Test-Driven Development are more widely practiced, resulting in a growing confidence in the results of the automated builds. The application is no longer simply compiled and tested, but if the tests pass, it is automatically deployed to an application server for more comprehensive end-to-end tests and performance tests</p>"},{"location":"cicd/overview/#phase-6-automated-acceptance-tests-and-more-automated-deployment","title":"Phase 6 - Automated Acceptance Tests and More Automated Deployment","text":"<p>Acceptance-Test Driven Development is practiced, guiding development efforts and providing high-level reporting on the state of project. These automated tests use Behavior-Driven Development and Acceptance-Test Driven Development tools to act as communication and documentation tools and documentation as mush as testing tools, publishing reports on test results in business terms that non-developers can understand. Since these high-level tests are automated at an early stage in the development process, they also provide a clear idea of what features have been implemented, and which remain to be done. The application is automatically deployed into test environments for testing by the QA team either as changes are committed, or on a nightly basis; a version can be deployed(or \"prompted\") to UAT and possibly production environments using a manually-triggered build when testers consider it ready. The team is also capable of using the build server to back out a release, rolling back to a previous release, if something goes horribly wrong.</p>"},{"location":"cicd/overview/#phase-7-continuous-deployment","title":"Phase 7 - Continuous Deployment","text":"<p>Confidence in the automated unit, integration and acceptance tests is now such that teams can apply the automated deployment techniques developed in the previous phase to push out new changes directly into production. The progression between levels here is of course somewhat approximate, and may not always match real-world situations</p>"},{"location":"cicd/overview/#poc","title":"POC","text":"<p>we would deploy an flask application as part of entrire CICD pipeline. so when the application commits from the app, and if the CI pipeline is successful, then it would be direcly deplpyed into an local cluster(minikube/rancher desktop..etc) using the Argo CD. </p>"},{"location":"cicd/overview/#pre-requsites","title":"Pre-requsites","text":"<p>Create two repos for CI and CD seperately.  Make sure those repos have token and access to RW for the repos and update those in secrets where we could access as variables.</p> <p>How to create MANIFESTS_REPO_PAT</p> <pre><code>GitHub profile \u2192\n  \u2192 Settings \u2192 Developer settings\u2192 Personal access tokens\n</code></pre> <p>Token name: manifests-repo-ci</p> <p>select <code>demo-ci</code> and <code>demo-ci</code> repository access.  Repository permissions: Contents: Read and write (least-privilege)</p> <p>Go to <code>demo-ci</code> and <code>demo-cd</code> repo ... </p> <pre><code>Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret\n</code></pre>"},{"location":"cicd/overview/#ci","title":"CI","text":"<p>CI: https://github.com/samperay/demo-ci</p> <p>You will have to create steps in <code>.github/workflows/ci.yaml</code></p> <p>https://github.com/samperay/demo-ci/blob/master/.github/workflows/ci.yml</p>"},{"location":"cicd/overview/#cd","title":"CD","text":"<p>CD: https://github.com/samperay/demo-cd</p> <p>Install Argo CD for local deployment.</p> <p>Make sure you have a local cluster running and update the below</p> <pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\nkubectl -n argocd port-forward svc/argocd-server 8081:443\nhttp://localhost:8081\nUsername: admin\nPassword: \n</code></pre> <p>Argo CD would take the file https://github.com/samperay/demo-cd/blob/master/apps/poc/argocd-app.yaml and any drift changes would update to the latest release. </p>"},{"location":"cicd/argocd/applications/","title":"applications","text":"<p>Its a resource object representing a deployed application instance in an environment. Its defined by two key pieces of information. </p> <ul> <li>source: desired state</li> <li>destination: reference to target cluster and namespace</li> </ul> <p>All the applications/projects can be created using below three methods.</p> <ul> <li>declarative(yaml), always recommened</li> <li>Web UI</li> <li>CLI</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: guestbook\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/argoproj/argocd-example-apps.git\n    targetRevision: HEAD\n    path: guestbook\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: guestbook\n</code></pre>"},{"location":"cicd/argocd/applications/#create-application","title":"Create application","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: guestbook\n  namespace: argocd\nspec: \n  destination: \n    namespace: guestbook\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n  syncPolicy:\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>\u279c  labs git:(master) \u2717 kubectl get application -n argocd\nNAME        SYNC STATUS   HEALTH STATUS\nguestbook   OutOfSync     Missing\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app list            \nNAME              CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                PATH       TARGET\nargocd/guestbook  https://kubernetes.default.svc  guestbook  default  OutOfSync  Missing  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\n\u279c  labs git:(master) \u2717 \n</code></pre> <p>Go to the UI and start \"sync\" which will then start your deployment by checking with your manifests and so on </p> <p></p> <p>Create application using CLI</p> <pre><code>\u279c  labs git:(master) \u2717 argocd app create app-2 --repo https://github.com/mabusaa/argocd-example-apps.git --revision master --path guestbook --dest-namespace app-2 --dest-server https://kubernetes.default.svc --sync-option CreateNamespace=true\n\napplication 'app-2' created\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app list\nNAME              CLUSTER                         NAMESPACE  PROJECT  STATUS     HEALTH   SYNCPOLICY  CONDITIONS  REPO                                                PATH       TARGET\nargocd/app-1      https://kubernetes.default.svc  app-1      default  Synced     Healthy  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\nargocd/app-2      https://kubernetes.default.svc  app-2      default  OutOfSync  Missing  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\nargocd/guestbook  https://kubernetes.default.svc  guestbook  default  Synced     Healthy  Manual      &lt;none&gt;      https://github.com/mabusaa/argocd-example-apps.git  guestbook  master\n\u279c  labs git:(master) \u2717 \n\n\u279c  labs git:(master) \u2717 argocd app sync app-2\nTIMESTAMP                  GROUP        KIND   NAMESPACE                  NAME    STATUS    HEALTH        HOOK  MESSAGE\n2025-04-11T08:56:13+05:30            Service       app-2          guestbook-ui  OutOfSync  Missing              \n2025-04-11T08:56:13+05:30   apps  Deployment       app-2          guestbook-ui  OutOfSync  Missing              \n2025-04-11T08:56:15+05:30          Namespace                             app-2   Running   Synced              namespace/app-2 created\n2025-04-11T08:56:15+05:30            Service       app-2          guestbook-ui    Synced  Healthy              \n2025-04-11T08:56:15+05:30            Service       app-2          guestbook-ui    Synced   Healthy              service/guestbook-ui created\n2025-04-11T08:56:15+05:30   apps  Deployment       app-2          guestbook-ui  OutOfSync  Missing              deployment.apps/guestbook-ui created\n2025-04-11T08:56:16+05:30   apps  Deployment       app-2          guestbook-ui    Synced  Progressing              deployment.apps/guestbook-ui created\n\nName:               argocd/app-2\nProject:            default\nServer:             https://kubernetes.default.svc\nNamespace:          app-2\nURL:                https://localhost:8080/applications/app-2\nSource:\n- Repo:             https://github.com/mabusaa/argocd-example-apps.git\n  Target:           master\n  Path:             guestbook\nSyncWindow:         Sync Allowed\nSync Policy:        Manual\nSync Status:        Synced to master (93860ce)\nHealth Status:      Progressing\n\nOperation:          Sync\nSync Revision:      93860cefec473c343718a38c99a2e099cc40d209\nPhase:              Succeeded\nStart:              2025-04-11 08:56:12 +0530 IST\nFinished:           2025-04-11 08:56:15 +0530 IST\nDuration:           3s\nMessage:            successfully synced (all tasks run)\n\nGROUP  KIND        NAMESPACE  NAME          STATUS   HEALTH       HOOK  MESSAGE\n       Namespace              app-2         Running  Synced             namespace/app-2 created\n       Service     app-2      guestbook-ui  Synced   Healthy            service/guestbook-ui created\napps   Deployment  app-2      guestbook-ui  Synced   Progressing        deployment.apps/guestbook-ui created\n\u279c  labs git:(master) \u2717 \n</code></pre>"},{"location":"cicd/argocd/applications/#projects","title":"Projects","text":"<ul> <li>you can create project for application logical grouping. </li> <li>access restrictions for multiple teams</li> <li>allow apps to be deployed into specific clusters and namespaces</li> <li>project roles, enables to create a role with set of policies \"permission\" to grant access to project applications i.e you can set JWT or ODIC etc </li> <li>Always it would create a default project </li> </ul>"},{"location":"cicd/argocd/applications/#create-projects","title":"Create projects","text":"<p>Declarative method</p> <ul> <li>Allow all sources</li> <li>Allow all destination</li> <li>Allow all cluster and namespace scopes resources</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: demo-project\n  namespace: argocd\nspec:\n  description: Demo Project\n  sourceRepos:\n  - '*'\n\n  destinations:\n  - namespace: '*'\n    server: '*'\n\n  clusterResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  namespaceResourceWhitelist:\n  - group: '*'\n    kind: '*'\n</code></pre> <p>Once the project is defined, then you would create application related to this project. </p> <pre><code>\u279c  labs git:(master) \u2717 kubectl get appproject -n argocd                         \nNAME      AGE\ndefault   47h\n\u279c  labs git:(master) \u2717 \n</code></pre> <p>TODO:</p> <ul> <li>[ ] Create a new project and deploy application in that project.</li> <li>[ ] Create a new project to be allowed only for particular namespace.</li> <li>[ ] create application in that namespace to chekc if deployment is successful</li> <li>[ ] Describe the logs for the application incase there's issue</li> <li>[ ] Modify the logs according to the namespace and you would not be able to run the app</li> </ul>"},{"location":"cicd/argocd/applications/#roles","title":"Roles","text":"<p>Enable you to create a role with set of policies \"permission\" to grant access to a project applications. fine-grained access control within a project using RBAC (Role-Based Access Control).</p> <p>You can:</p> <ul> <li>Create custom roles for a project</li> <li>Assign JWT tokens (like service accounts) for automation or scripts</li> <li>Define what actions are allowed on what applications</li> </ul> <p>Example: </p> <pre><code># ci-role.yaml\napiVersion: argoproj.io/v1alpha1\nkind: AppProject\nmetadata:\n  name: project-with-role\n  namespace: argocd\nspec:\n  description: project-with-role description\n  sourceRepos:\n  - '*'\n\n  destinations:\n  - namespace: '*'\n    server: '*'\n\n  clusterResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  namespaceResourceWhitelist:\n  - group: '*'\n    kind: '*'\n\n  roles:\n  - name: ci-role\n    description: Sync privileges for project-with-role\n    policies:\n    - p, proj:project-with-role:ci-role, applications, sync, project-with-role/*, allow\n</code></pre> <ul> <li>all sources</li> <li>all destinations</li> <li>all clusters and namespace scoped resources</li> <li>define role</li> <li>defined role with sync to all applications in the same project. </li> <li>create token related to this role</li> <li>try to delete application using the token which will be <code>denied</code> by cluster.</li> </ul> <pre><code>argocd proj role create-token ci-role\n</code></pre> <p>using cli, try to delete some other application using the above token, you will get \"action denied\" because this token don't have an access to delete.</p> <pre><code>argocd app list \nargocd app delete &lt;project&gt; --auth-token etrasdSaaweSxcese.....\n</code></pre> <p>TODO:</p> <ul> <li>[ ] run \"ci-role.yaml\" </li> <li>[ ] create token</li> <li>[ ] using the token, try deleting...</li> </ul>"},{"location":"cicd/argocd/applications/#repository","title":"repository","text":"<p>You can add your private repo as well to the Argocd, just make sure you pass your <code>git-creds</code> and <code>application id</code>. </p> <p>https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/#github-app-credential</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-https\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: https://github.com/mabusaa/argocd-example-apps-private.git\n  password: # password goes here, NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n  username: my-token\n</code></pre> <pre><code>#private-repo-creds-https.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-creds-https\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: https://github.com/mabusaa\n  password: # password goes here, NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n  username: my-token\n</code></pre> <pre><code># private-repo-ssh.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: private-repo-ssh\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\nstringData:\n  type: git\n  url: git@github.com:mabusaa/argocd-example-apps-private.git\n  sshPrivateKey: |\n    -----BEGIN OPENSSH PRIVATE KEY-----\n     # key goes here  NOTE: dont push secrets into git, use sealed secrets as a solution for secrets in gitops.\n    -----END OPENSSH PRIVATE KEY-----\n</code></pre> <p>TODO</p> <ul> <li>[ ] Clone the repo and make it private. i.e create empty repo, copy files and then push src code.</li> <li>[ ] Create a secret uing username password or API token in argocd namespace </li> <li>[ ] Go to UI and check for repository, you would see the repo getting connected..</li> <li>[ ] Create a new application providing the priavte repo url as source of manifest</li> <li>[ ] Try for adding HTTPS/SSH connection as well.  </li> <li>[ ] Validate if application created and OutOfsync from \"Web UI\"</li> </ul>"},{"location":"cicd/argocd/faq/","title":"faq","text":"# Interview Question Explanation / Answer 1 What is Argo CD and how does it fit into GitOps? Argo CD automates Kubernetes deployments using Git as the source of truth. It enables GitOps by syncing Git state with live cluster state. 2 How does Argo CD differ from Flux CD? Argo CD has a GUI, built-in RBAC, and better multi-cluster support. Flux is more CLI/operator-focused. 3 How do you manage secrets in Argo CD? Use tools like SOPS, Sealed Secrets, or External Secrets Operator. Avoid committing raw secrets to Git. 4 Difference between Manual and Auto Sync? Manual sync requires user action. Auto sync applies changes from Git to the cluster automatically. 5 How do you promote code from dev \u2192 stage \u2192 prod in Argo CD? Use Git branches, kustomize overlays, or Helm values per environment. Trigger promotion by merging branches. 6 What are ApplicationSets in Argo CD? A controller that dynamically generates Argo Applications using templates and generators (Git, Matrix, List, etc.). 7 How to implement RBAC and security best practices? Integrate with SSO, disable admin user, restrict resource access via RBAC, use NetworkPolicies. 8 How is drift detection handled? Argo CD continuously checks live vs. Git state. Drift shows as <code>OutOfSync</code>. Can use <code>syncOptions</code> to fine-tune sync behavior. 9 How do you rollback in Argo CD? Use <code>argocd app history</code> and <code>argocd app rollback</code> to revert to a previous revision. 10 How to monitor and audit Argo CD? Integrate Prometheus/Grafana, export logs, and use Argo CD audit logs or webhook notifications."},{"location":"cicd/argocd/faq/#real-time-issues","title":"real time issues","text":"# Issue Root Cause Solution / Fix 1 App stuck in OutOfSync due to Helm/Kustomize Rendering failed (missing values or wrong base) Validate with <code>helm template</code>, ensure <code>valuesFiles</code> or <code>kustomize.path</code> is correct. 2 Sync fails due to RBAC or NetworkPolicy Insufficient permissions or blocked API access Grant proper ClusterRoleBinding, fix NetworkPolicy, test with <code>kubectl auth can-i</code>. 3 Shared resource (e.g. Secret) is deleted during sync Prune deletes resources not scoped properly Use <code>Prune=false</code> or resource customizations in Argo CD config. 4 Excess Applications created by ApplicationSet Wrong generator or repo structure Use <code>preview</code> mode, validate Git paths, check controller logs. 5 Application stuck in Syncing state Failed pre/post sync hook or finalizer Check <code>sync hooks</code>, use retries, patch finalizers if needed. 6 Resource sync fails due to CRDs not installed yet CRDs are missing in cluster Install CRDs first or use <code>--validate=false</code> option in sync. 7 Secrets in Git exposed accidentally Raw secrets committed to Git Use encryption tools (SOPS), or external secret backends. 8 Multi-team setup with config drift Teams override shared manifests Use separate overlays per team/environment. Implement strict review via PRs. 9 UI shows stale sync status Controller not updated or cache issue Restart <code>argocd-application-controller</code>, refresh app. 10 Argo CD performance drops in large clusters High app count, long sync intervals Scale controller, increase cache size, split by project or namespace."},{"location":"cicd/argocd/install/","title":"install","text":"<p>You can install in <code>non-HA</code> using <code>minikube</code> or <code>docker-desktop</code> or <code>rancher-desktop</code></p> <pre><code>\u279c  ~ kubectl get nodes\nNAME                   STATUS   ROLES                  AGE    VERSION\nlima-rancher-desktop   Ready    control-plane,master   229d   v1.30.3+k3s1\n\n\u279c  ~ kubectl create ns argocd\nnamespace/argocd created\n\n\u279c  ~ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\ncustomresourcedefinition.apiextensions.k8s.io/applications.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/applicationsets.argoproj.io created\ncustomresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io created\nserviceaccount/argocd-application-controller created\nserviceaccount/argocd-applicationset-controller created\nserviceaccount/argocd-dex-server created\nserviceaccount/argocd-notifications-controller created\nserviceaccount/argocd-redis created\nserviceaccount/argocd-repo-server created\nserviceaccount/argocd-server created\nrole.rbac.authorization.k8s.io/argocd-application-controller created\nrole.rbac.authorization.k8s.io/argocd-applicationset-controller created\nrole.rbac.authorization.k8s.io/argocd-dex-server created\nrole.rbac.authorization.k8s.io/argocd-notifications-controller created\nrole.rbac.authorization.k8s.io/argocd-redis created\nrole.rbac.authorization.k8s.io/argocd-server created\nclusterrole.rbac.authorization.k8s.io/argocd-application-controller created\nclusterrole.rbac.authorization.k8s.io/argocd-applicationset-controller created\nclusterrole.rbac.authorization.k8s.io/argocd-server created\nrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-dex-server created\nrolebinding.rbac.authorization.k8s.io/argocd-notifications-controller created\nrolebinding.rbac.authorization.k8s.io/argocd-redis created\nrolebinding.rbac.authorization.k8s.io/argocd-server created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created\nclusterrolebinding.rbac.authorization.k8s.io/argocd-server created\nconfigmap/argocd-cm created\nconfigmap/argocd-cmd-params-cm created\nconfigmap/argocd-gpg-keys-cm created\nconfigmap/argocd-notifications-cm created\nconfigmap/argocd-rbac-cm created\nconfigmap/argocd-ssh-known-hosts-cm created\nconfigmap/argocd-tls-certs-cm created\nsecret/argocd-notifications-secret created\nsecret/argocd-secret created\nservice/argocd-applicationset-controller created\nservice/argocd-dex-server created\nservice/argocd-metrics created\nservice/argocd-notifications-controller-metrics created\nservice/argocd-redis created\nservice/argocd-repo-server created\nservice/argocd-server created\nservice/argocd-server-metrics created\ndeployment.apps/argocd-applicationset-controller created\ndeployment.apps/argocd-dex-server created\ndeployment.apps/argocd-notifications-controller created\ndeployment.apps/argocd-redis created\ndeployment.apps/argocd-repo-server created\ndeployment.apps/argocd-server created\nstatefulset.apps/argocd-application-controller created\nnetworkpolicy.networking.k8s.io/argocd-application-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-applicationset-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-dex-server-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-notifications-controller-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-redis-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-repo-server-network-policy created\nnetworkpolicy.networking.k8s.io/argocd-server-network-policy created\n\u279c  ~\n\n\u279c  ~ kubectl get all -n argocd\nNAME                                                   READY   STATUS    RESTARTS   AGE\npod/argocd-application-controller-0                    1/1     Running   0          102s\npod/argocd-applicationset-controller-cc68b7b7b-6ck72   1/1     Running   0          103s\npod/argocd-dex-server-555b55c97d-qrrbq                 1/1     Running   0          103s\npod/argocd-notifications-controller-65655df9d5-drn4n   1/1     Running   0          103s\npod/argocd-redis-764b74c9b9-bkdrs                      1/1     Running   0          103s\npod/argocd-repo-server-7dcbcd967b-bxcjd                1/1     Running   0          103s\npod/argocd-server-5b9cc8b776-bjnqd                     1/1     Running   0          102s\n\nNAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nservice/argocd-applicationset-controller          ClusterIP   10.43.100.205   &lt;none&gt;        7000/TCP,8080/TCP            104s\nservice/argocd-dex-server                         ClusterIP   10.43.83.92     &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   104s\nservice/argocd-metrics                            ClusterIP   10.43.158.10    &lt;none&gt;        8082/TCP                     104s\nservice/argocd-notifications-controller-metrics   ClusterIP   10.43.106.252   &lt;none&gt;        9001/TCP                     103s\nservice/argocd-redis                              ClusterIP   10.43.67.82     &lt;none&gt;        6379/TCP                     103s\nservice/argocd-repo-server                        ClusterIP   10.43.17.137    &lt;none&gt;        8081/TCP,8084/TCP            103s\nservice/argocd-server                             ClusterIP   10.43.40.122    &lt;none&gt;        80/TCP,443/TCP               103s\nservice/argocd-server-metrics                     ClusterIP   10.43.40.210    &lt;none&gt;        8083/TCP                     103s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argocd-applicationset-controller   1/1     1            1           103s\ndeployment.apps/argocd-dex-server                  1/1     1            1           103s\ndeployment.apps/argocd-notifications-controller    1/1     1            1           103s\ndeployment.apps/argocd-redis                       1/1     1            1           103s\ndeployment.apps/argocd-repo-server                 1/1     1            1           103s\ndeployment.apps/argocd-server                      1/1     1            1           103s\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argocd-applicationset-controller-cc68b7b7b   1         1         1       103s\nreplicaset.apps/argocd-dex-server-555b55c97d                 1         1         1       103s\nreplicaset.apps/argocd-notifications-controller-65655df9d5   1         1         1       103s\nreplicaset.apps/argocd-redis-764b74c9b9                      1         1         1       103s\nreplicaset.apps/argocd-repo-server-7dcbcd967b                1         1         1       103s\nreplicaset.apps/argocd-server-5b9cc8b776                     1         1         1       103s\n\nNAME                                             READY   AGE\nstatefulset.apps/argocd-application-controller   1/1     102s\n\u279c  ~\n</code></pre> <p>All the pods must be in running space </p> <pre><code>\u279c  ~ kubectl get pods -n argocd\nNAME                                               READY   STATUS    RESTARTS   AGE\nargocd-application-controller-0                    1/1     Running   0          2m16s\nargocd-applicationset-controller-cc68b7b7b-6ck72   1/1     Running   0          2m17s\nargocd-dex-server-555b55c97d-qrrbq                 1/1     Running   0          2m17s\nargocd-notifications-controller-65655df9d5-drn4n   1/1     Running   0          2m17s\nargocd-redis-764b74c9b9-bkdrs                      1/1     Running   0          2m17s\nargocd-repo-server-7dcbcd967b-bxcjd                1/1     Running   0          2m17s\nargocd-server-5b9cc8b776-bjnqd                     1/1     Running   0          2m16s\n\u279c  ~\n</code></pre> <p>You need to follow below steps for setup </p> <p>Get the initial password that is stored as secret in argocd namesapce convert secret from base64 into plain text</p> <pre><code>\u279c  ~ kubectl get secret -n argocd argocd-initial-admin-secret -o yaml\napiVersion: v1\ndata:\n  password: T2FDUFJ5d1pab0g1ODVuRQ==\nkind: Secret\nmetadata:\n  creationTimestamp: \"2025-04-09T04:17:55Z\"\n  name: argocd-initial-admin-secret\n  namespace: argocd\n  resourceVersion: \"775735\"\n  uid: aebfa38f-12fa-4a8d-adab-22ed8ed816ad\ntype: Opaque\n\u279c  ~\n\n\u279c  ~  echo \"T2FDUFJ5d1pab0g1ODVuRQ==\"|base64 -d\nOaCPRywZZoH585nE%                                                                                                                                   \u279c  ~\n</code></pre> <p>you could also use the below to get password</p> <pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <p>Expose the argocd-server to connect to UI by port-forwarding</p> <pre><code>\u279c  ~ kubectl get svc -n argocd\nNAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\nargocd-applicationset-controller          ClusterIP   10.43.100.205   &lt;none&gt;        7000/TCP,8080/TCP            8m38s\nargocd-dex-server                         ClusterIP   10.43.83.92     &lt;none&gt;        5556/TCP,5557/TCP,5558/TCP   8m38s\nargocd-metrics                            ClusterIP   10.43.158.10    &lt;none&gt;        8082/TCP                     8m38s\nargocd-notifications-controller-metrics   ClusterIP   10.43.106.252   &lt;none&gt;        9001/TCP                     8m37s\nargocd-redis                              ClusterIP   10.43.67.82     &lt;none&gt;        6379/TCP                     8m37s\nargocd-repo-server                        ClusterIP   10.43.17.137    &lt;none&gt;        8081/TCP,8084/TCP            8m37s\nargocd-server                             ClusterIP   10.43.40.122    &lt;none&gt;        80/TCP,443/TCP               8m37s\nargocd-server-metrics                     ClusterIP   10.43.40.210    &lt;none&gt;        8083/TCP                     8m37s\n\u279c  ~\n\n\u279c  ~ kubectl port-forward svc/argocd-server -n argocd 8080:443\nForwarding from [::1]:8080 -&gt; 8080\n\n[ leave one terminal as such .....]\n</code></pre> <p>Now, you can connect your localhost with the url http://localhost:8080</p> <p>Username:admin password:OaCPRywZZoH585nE</p>"},{"location":"cicd/argocd/install/#argocd-cli","title":"ArgoCD Cli","text":"<p>what can be done using cli</p> <ul> <li>manage application </li> <li>manage repos</li> <li>manage clusters</li> <li>admin tasks</li> <li>manage projects</li> <li>and more and more</li> </ul>"},{"location":"cicd/argocd/install/#installations","title":"installations","text":"<p>https://argo-cd.readthedocs.io/en/stable/cli_installation/</p> <pre><code>\u279c  ~ VERSION=$(curl --silent \"https://api.github.com/repos/argoproj/argo-cd/releases/latest\" | grep '\"tag_name\"' | sed -E 's/.*\"([^\"]+)\".*/\\1/')\n\u279c  ~ echo \"${VERSION}\"\nv2.14.9\n\u279c  ~ curl -sSL -o argocd-darwin-amd64 https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-darwin-amd64\n\u279c  ~ sudo install -m 555 argocd-darwin-amd64 /usr/local/bin/argocd ;rm argocd-darwin-amd64\n\u279c \n\n\u279c  ~ argocd help | head\nargocd controls a Argo CD server\n\nUsage:\n  argocd [flags]\n  argocd [command]\n\nAvailable Commands:\n  account     Manage account settings\n  admin       Contains a set of commands useful for Argo CD administrators and requires direct Kubernetes access\n  app         Manage applications\n\u279c  ~\n</code></pre> <p>Login to cli using admin</p> <pre><code>\u279c  ~ argocd login localhost:8080\nWARNING: server certificate had error: tls: failed to verify certificate: x509: certificate signed by unknown authority. Proceed insecurely (y/n)? y\nUsername: admin\nPassword:\n'admin:login' logged in successfully\nContext 'localhost:8080' updated\n\u279c  ~\n\n\u279c  ~ argocd cluster list\nSERVER                          NAME        VERSION  STATUS   MESSAGE                                                  PROJECT\nhttps://kubernetes.default.svc  in-cluster           Unknown  Cluster has no applications and is not being monitored.\n\u279c  ~\n\n</code></pre>"},{"location":"cicd/argocd/install/#note","title":"Note","text":"<pre><code>Make sure that ArgoCD server endpoint is accessible whether using port-forward or ingress or load balancer service.\n\nexample: kubectl port-forward svc/argocd-server -n argocd 8080:443\n\nRemember to login into  ArgoCD using command argocd login.\n\nyou need admin user and password. remember that you can get the initial admin password from k8s secret \"argocd-initial-admin-secret\"\n\n\nexample : argocd login localhost:8080 --insecure\n\nThen you can apply cli commands.\n</code></pre>"},{"location":"cicd/argocd/overview/","title":"ArgoCD","text":"<p>Argo CD (short for Argo Continuous Delivery) is a declarative, GitOps-based continuous delivery tool for Kubernetes. It automates the deployment of the desired application states defined in Git repositories to Kubernetes clusters(actual state).</p>"},{"location":"cicd/argocd/overview/#core-concepts","title":"Core Concepts","text":"Concept Description GitOps Uses Git as the single source of truth for declarative infrastructure and applications. Application A Kubernetes Custom Resource (CR) in Argo CD that defines the desired state of an app. Sync The process of aligning the live Kubernetes cluster state with what's defined in Git. Drift Detection Identifies any difference between the declared state in Git and the actual state in the cluster. Reconciliation Keeps the cluster in sync with Git either manually or through automated syncing."},{"location":"cicd/argocd/overview/#architecture","title":"Architecture","text":"<p>Argo CD follows a controller-based architecture. It continuously monitors a Git repository and ensures that the Kubernetes cluster's state matches the state defined in Git.</p> <p></p> Component Description API Server Provides the REST and gRPC API. Handles user authentication, RBAC, and serves the CLI/UI. Repository Server Clones Git repositories, renders manifests (Helm, Kustomize, etc.), and makes them available to the controller. Application Controller Monitors application resources. Detects drift between desired (Git) and live (cluster) state and triggers sync. Dex (Optional) An identity service that enables SSO integration with providers like GitHub, LDAP, Google, etc. CLI / Web UI User interfaces to interact with Argo CD \u2014 <code>argocd</code> CLI for terminal, Web UI for visual management."},{"location":"cicd/argocd/overview/#gitops","title":"Gitops","text":"<p>GitOps is a modern DevOps practice that uses Git as the single source of truth for managing infrastructure and application deployment. It enables automated delivery of code and configuration into Kubernetes or any other environment through Git workflows.</p>"},{"location":"cicd/argocd/overview/#core-principles","title":"Core principles","text":"Principle Description Git as the source of truth All infrastructure and application configuration is versioned and stored in Git repositories. Declarative configuration The desired state of the system is defined declaratively (e.g., YAML files). Automated reconciliation A GitOps controller ensures the live state matches the desired state in Git, and takes action if there is a drift. Pull-based delivery Changes are pulled from Git <p>Benefits of GitOps</p> <p>Auditability: Every change is tracked in Git</p> <p>Consistency: No drift between environments</p> <p>Security: Pull-based model keeps CI/CD tooling outside of your cluster</p> <p>Speed &amp; Agility: Quick, safe, and repeatable deployments</p> <p>Self-healing systems: Automated correction of configuration drift</p>"},{"location":"cicd/argocd/sync/","title":"sync","text":""},{"location":"cicd/argocd/sync/#automated-syncing","title":"Automated syncing","text":"<ul> <li>by default, argocd polls git repo every 3 minutes to detect changes to the manifests. This will sync the desired manifests automaically to the live state in the cluster. </li> </ul> <p>Note: </p> <ul> <li>automated sync will be performed only if the application is <code>OutOfSync</code>. it will not re-attemps if the automated sync has been failed against the same SHA Commmit.</li> <li>Rollback cannot be performed against an application with automated sync enabled. </li> </ul> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --sync-policy automated\n</code></pre> <pre><code>#automated-sync.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-sync-app\n  namespace: argocd\nspec:\n  destination:\n    namespace: auto-sync-app\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated: {}\n    syncOptions:\n      - CreateNamespace=true\n</code></pre>"},{"location":"cicd/argocd/sync/#automated-pruning","title":"Automated pruning","text":"<p>Default: No prune enabled. </p> <p>autosync when enabled, for safety purpose we have <code>auto prune</code> disabled so that when sync happends we will not delete any resources when detected for any changes. but it can be enabled.  When using production please don't enable it. </p> <pre><code># automated-prune.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-pruning-demo\n  namespace: argocd\nspec: \n  destination:\n    namespace: auto-pruning-demo\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated: \n      prune: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --auto-prune\n</code></pre>"},{"location":"cicd/argocd/sync/#automated-self-healing","title":"Automated self-healing","text":"<p>by default, changes that are made to the live cluster will not trigger automated sync. argocd has a feature to enable self healing when the live cluster state deviates from the git state. </p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata: \n  name: auto-selfheal-demo\n  namespace: argocd\nspec: \n  destination:\n    namespace: auto-selfheal-demo\n    server: \"https://kubernetes.default.svc\"\n  project: default\n  source: \n    path: guestbook-with-sub-directories\n    repoURL: \"https://github.com/mabusaa/argocd-example-apps.git\"\n    targetRevision: master\n    directory:\n      recurse: true\n  syncPolicy:\n    automated:\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n</code></pre> <pre><code>argocd app create application --repo https://... --revision 1.x.x.x --dest-namespace default --dest-server https://kubernetes.default.svc --self-heal\n</code></pre>"},{"location":"cicd/argocd/sync/#sync-options","title":"Sync options","text":""},{"location":"cicd/github_actions/cicd/","title":"CICD pipeline","text":""},{"location":"cicd/github_actions/cicd/#cicd-pipeline","title":"cicd pipeline","text":"<p>we would have complete version of the cicd pipeline. </p>"},{"location":"cicd/github_actions/cicd/#feature-to-develop-branch","title":"feature to develop branch","text":"<ul> <li>Developer would create a new <code>feature branch</code> and <code>push</code></li> <li>Once the push is detected, we would <code>run the workflow</code> against it. </li> <li>Once they are successful, you would <code>create a PR</code> for the <code>dev branch</code> where we run <code>workflow actions</code>.</li> <li>once the <code>job is successful</code>, we would <code>deploy code in the staging server</code>. </li> </ul>"},{"location":"cicd/github_actions/cicd/#develop-to-master-branch","title":"develop to master branch","text":"<ul> <li>we would create a<code>new PR from dev branch to master</code>, which makes the <code>actions to run</code></li> <li>On successful run, we would deploy code to the production server. </li> </ul>"},{"location":"cicd/github_actions/cicd/#post-deployments","title":"Post deployments","text":"<ul> <li>Job failures, we would create a new issue </li> <li>successful job, would send a slack message</li> <li>successful release, we would send a slack message</li> </ul>"},{"location":"cicd/github_actions/cicd/#pull_requests","title":"pull_requests","text":"<p>As we discussed earlier, we can't push to <code>master</code> or <code>develop</code> using the branch. hence we need to create a new <code>feature</code> branch and requires a <code>pull request</code> to the respective branches. You need to update settings on the <code>branch protection rules</code> for both <code>develop and master</code></p> <p></p> <p>Here is an example of python project, which builds and let you merge the pull request upon successful</p> <pre><code>name: ci\non:\n    pull_request: \n        branches: \n          - develop # PR created from feature to develop\n    push:\n        branches:\n          - develop # merge PR (push to develop branch)\njobs:\n    build:\n        runs-on: ubuntu-latest\n        steps:\n            - name: repository checkout\n              uses: actions/checkout@v3\n\n            - name: python3.10 setup \n              uses: actions/setup-python@v4\n              with:\n                python-version: '3.10'\n                check-latest: true\n\n            - name: install dependencies\n              run: |\n                pip install fastapi[\"all\"]\n                if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\n            - name: cache dependencies\n              uses: actions/cache@v3\n              with:\n                path: ~/.cache/pip\n                key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n                restore-keys: |\n                  ${{ runner.os }}-pip-\n\n            - name: code formating\n              run: flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n            - name: run testcases\n              run: pytest\n\n            - name: test coverage report\n              run: pytest --cov=./app ./tests\n\n            - name: upload artifacts test coverage report\n              uses: actions/upload-artifact@v1\n              with:\n                name: code-coverage\n                path: ./.coverage\n\n            - name: build fastapi docker image\n              if: github.event_name == 'push'\n              run: docker build -t '${{ secrets.DOCKER_LOGIN }}'/demo-fastapi:'${{ github.sha }}' .\n\n            - name: login to dockerhub\n              if: github.event_name == 'push'\n              run: docker login --username '${{ secrets.DOCKER_LOGIN }}' --password '${{ secrets.DOCKER_PASSWORD }}'\n\n            - name: push image to dockerhub\n              if: github.event_name == 'push'\n              run: docker push '${{ secrets.DOCKER_LOGIN }}'/demo-fastapi:'${{ github.sha }}'\n</code></pre>"},{"location":"cicd/github_actions/intro/","title":"Introduction","text":"<p>In this intro page, we would look for couple of examples that we learnt from the overview section. </p>"},{"location":"cicd/github_actions/intro/#examples","title":"examples","text":"<p>As part of testing, create a new file in <code>.github/workflows/example.yml</code> on push to repository. Check from your <code>actions</code> of the github repository to see the results overview. </p> <pre><code>name: example\nrun-name: example\non: push\njobs:\n    demo:\n        runs-on: ubuntu-latest\n        steps:\n            - name: first step\n              run: echo \"Hello world !\"\n\n            - name: second step\n              run: echo \"Hello World again !\"\n</code></pre> <p>For, any python application in the repository, you need to check for the <code>requirements.txt</code> file and add all your dependencies to it, which will do repository checkout, python setup, install libs, setup linter and unit testing.</p> <pre><code>name: python application\non:\n    push:\n        branches: [\"master\"]\njobs:\n    preinstall:\n        runs-on: ubuntu-latest\n        steps:\n            - name: repository checkout\n              uses: actions/checkout@v3\n\n            - name: python3.10 setup \n              uses: actions/setup-python@v4\n              with:\n                python-version: '3.10'\n                check-latest: true\n\n            - name: install dependencies\n              run: |\n                pip install fastapi[\"all\"]\n                if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\n            - name: install flake8\n              run: |\n                flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n            - name: test with pytest\n              run: |\n                pytest   \n</code></pre> <p>We can checkout our repository using the steps, however we can do the same things using <code>uses</code> or <code>checkout</code></p> <pre><code>name: checkout the git repo in the runner machine\non:\n  - push\njobs:\n    checkout_repo:\n    runs-on: ubuntu-latest\n    steps:\n      - name: download the current repo\n        run: |\n          git init \n          git remote add origin \"https://$GITHUB_ACTOR:${{ secrets.GITHIB_TOKEN }}@github.com/$GITHIB_REPOSITORY.git\"\n          git fetch origin \n          git checkout main\n\n      - name: list all the files from download repository\n        run: ls -a          \n</code></pre> <ul> <li>using <code>uses</code> to download any repo</li> </ul> <p><code>yaml   name: download the repo   on:     - push   jobs:       checkout_repo:       runs-on: ubuntu-latest       steps:         - name: download the repository           uses: actions/&lt;your repository name&gt;@[&lt;commit_id&gt;|&lt;tag&gt;|&lt;release_version&gt;]         - name: list the files from repository           run: ls -a</code></p> <ul> <li>using <code>checkout</code> to download the current repo, using <code>with</code> you can specify any repository you need to download</li> </ul> <p>```yaml     name: download the current repo     on:       - push     jobs:       checkout_repo_1:       runs-on: ubuntu-latest       steps:         - name: files before checkout repository           run: ls -a</p> <pre><code>    - name: download the repository\n      uses: actions/checkout@v3\n\n    - name: files after checkout repository\n      run: ls -a\n</code></pre> <p>```</p> <p>References: https://github.com/actions/checkout </p>"},{"location":"cicd/github_actions/matrix_strategy/","title":"Matrix stratergy","text":"<p>We will be discussing more on the Startergy, matrix and docker container in Jobs. </p>"},{"location":"cicd/github_actions/matrix_strategy/#matrix","title":"matrix","text":"<p>Incase we need to run with specific version, then we can use <code>uses</code> to defined the versions. </p> <pre><code>name: versions\non:\n    push\n\njobs:\n    node_version:\n        runs-on: ubuntu-latest\n        steps:\n            - name: log node version\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: 6\n\n            - name: log node version\n              run: node -v\n</code></pre> <p>If we need to run multiple jobs for multiple versions, then you need to use <code>strategy</code> and specify in the array to run all of those parallel. </p> <pre><code>name: strategy matrix\non:\n    push\n\njobs:\n    node_version:\n        strategy:\n            matrix:\n                node_version: [6, 7, 8]\n                os: [\"ubuntu-latest\",\"windows-latest\"]\n        runs-on: ${{ matrix.os }}\n        steps:\n            - name: log node version installed in ubuntu\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: ${{ matrix.node_version}}\n\n            - name: log node version currenly set to\n              run: node -v\n</code></pre> <p>Now, you will have totally 6 jobs running at once. </p> <p></p>"},{"location":"cicd/github_actions/matrix_strategy/#include-and-exclude-matrix","title":"include and exclude matrix","text":"<p>Incase we do get a situation, that we need to exclude some of the versions and include few version, then we can use <code>include</code> and <code>exclude</code> stratergy to run jobs.</p> <pre><code>name: strategy matrix\non:\n    push\n\njobs:\n    node_version:\n        strategy:\n            matrix:\n                node_version: [6, 7, 8]\n                os: [\"ubuntu-latest\",\"windows-latest\"]\n                exclude:\n                    - os: ubuntu-latest\n                      node_version: 6\n                    - os: ubuntu-latest\n                      node_version: 8\n                include:\n                    - os: ubuntu-latest\n                      node_version: 9\n                      is_ubuntu_9: 9\n\n        env:\n            IS_UBUNTU_9: ${{ matrix.is_ubuntu_9 }}\n        runs-on: ${{ matrix.os }}\n        steps:\n            - name: log node version installed in ubuntu\n              run: node -v\n\n            - uses: actions/setup-node@v1\n              with:\n                node-version: ${{ matrix.node_version}}\n\n            - name: log node version currenly set to\n              run: |\n                node -v\n                echo $IS_UBUNTU_9\n\n</code></pre> <p></p>"},{"location":"cicd/github_actions/matrix_strategy/#run-from-docker-containers","title":"run from docker containers","text":"<p>Till now, we had runners working from the virtual machines, but now we want docker container to initialize and we would run steps from inside container. </p> <pre><code>name: container\non: push \n\njobs:\n    node-docker:\n        runs-on: ubuntu-latest\n        container:\n            image: node:20-alpine3.17\n        steps:\n            - name: log node version\n              run: |\n                node -v \n                cat /etc/os-release\n</code></pre>"},{"location":"cicd/github_actions/overview/","title":"Overview","text":""},{"location":"cicd/github_actions/overview/#github-actions-components","title":"GitHub actions components","text":"<p>GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.</p> <p>You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container.</p>"},{"location":"cicd/github_actions/overview/#workflow","title":"Workflow","text":"<p>A workflow is a configurable automated process that will run one or more jobs, Workflows are defined by a YAML file checked in to your repository(.github/workflows) and will run when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule.</p>"},{"location":"cicd/github_actions/overview/#events","title":"Events","text":"<p>An event is a specific activity in a repository that triggers a workflow run.  e.g activity can originate from GitHub when someone creates a pull request, opens an issue, or pushes a commit to a repository. </p>"},{"location":"cicd/github_actions/overview/#jobs","title":"Jobs","text":"<p>A job is a set of steps in a workflow that is executed on the same runner. Each step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other. Since each step is executed on the same runner, you can share data from one step to another</p>"},{"location":"cicd/github_actions/overview/#actions","title":"Actions","text":"<p>An action is a custom application for the GitHub Actions platform that performs a complex but frequently repeated task.Use an action to help reduce the amount of repetitive code that you write in your workflow files</p>"},{"location":"cicd/github_actions/overview/#runners","title":"Runners","text":"<p>A runner is a server that runs your workflows when they're triggered. Each runner can run a single job at a time. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows; each workflow run executes in a fresh, newly-provisioned virtual machine.</p> <pre><code>name: learn-github-actions\nrun-name: ${{ github.actor }} is learning GitHub Actions\non: [push]\njobs:\n  check-bats-version:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3 # checkout code in runner.\n      - uses: actions/setup-node@v3 # install specified version 14 in runner and sets $PATH\n        with:\n          node-version: '14'\n      - run: npm install -g bats # run the command\n      - run: bats -v # run the command\n</code></pre> <p></p>"},{"location":"cicd/github_actions/overview/#references","title":"References","text":"<p>GitHub Actions Documentation</p>"},{"location":"cicd/github_actions/variables/","title":"Variables","text":""},{"location":"cicd/github_actions/variables/#default-custom-env-vars","title":"Default &amp; custom env vars","text":"<p>Variables provide a way to store and reuse non-sensitive configuration information.</p> <p>You can set your own custom variables or use the default environment variables that GitHub sets automatically. For more information, see Default environment variables.</p> <pre><code>name: custom vars\non:\n    push\nenv:\n    WF_ENV: env for workflow\n\njobs:\n    log-env:\n        runs-on: ubuntu-latest\n        env: \n            JOB_ENV: env for job\n        steps:\n            - name: display all envs\n              env:\n                STEP_ENV: env for step\n              run: |\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n\n            - name: display workflow and job env\n              run: |\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n    log-default-env:\n        runs-on: ubuntu-latest\n        steps:\n            - name: display all default envs\n              run: |\n                echo \"GITHUB_ACTOR: ${GITHUB_ACTOR}\"\n                echo \"GITHUB_JOB: ${GITHUB_JOB}\"\n                echo \"RUNNER_ARCH: ${RUNNER_ARCH}\"\n                echo \"RUNNER_OS: ${RUNNER_OS}\"\n                echo \"WF_ENV: ${WF_ENV}\"\n                echo \"JOB_ENV: ${JOB_ENV}\"\n                echo \"STEP_ENV: ${STEP_ENV}\"\n\n</code></pre> <p>References: about variables</p>"},{"location":"cicd/github_actions/variables/#secrets-variables","title":"secrets variables","text":"<p>incase you need to add secrets to the workflow, in your github, settings-&gt;Secrets and Variables-&gt;add secrets vars and save it. You can ue those values in your workflow, so that it won't be able to display in the output. </p> <pre><code>on:\n    push\nenv:\n    WF_ENV: ${{ secrets.WF_ENV }} # reference the secrets\n\njobs:\n    log-env:\n        runs-on: ubuntu-latest\n        env: \n            JOB_ENV: env for job\n</code></pre>"},{"location":"cicd/github_actions/variables/#calling-rest-api-to-create-issue","title":"Calling REST API to create issue","text":"<p>You can use the GITHUB_TOKEN to make authenticated API calls. This example workflow creates an issue using the GitHub REST API</p> <pre><code>jobs:\n  create_issue:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    steps:\n      - name: Create issue using REST API\n        run: |\n          curl --request POST \\\n          --url https://api.github.com/repos/${{ github.repository }}/issues \\\n          --header 'authorization: Bearer ${{ secrets.GITHUB_TOKEN }}' \\\n          --header 'content-type: application/json' \\\n          --data '{\n            \"title\": \"Automated issue for commit: ${{ github.sha }}\",\n            \"body\": \"This issue was automatically created by the GitHub Action workflow **${{ github.workflow }}**. \\n\\n The commit hash was: _${{ github.sha }}_.\"\n            }' \\\n          --fail\n</code></pre>"},{"location":"cicd/github_actions/variables/#pull-and-push-using-github_token-variable","title":"Pull and push using GITHUB_TOKEN variable","text":"<pre><code>jobs:\n    create_issue:\n        runs-on: ubuntu-latest\n        permissions:\n            write-all\n        steps:\n          - name: push a random file\n            run: |\n                git init \n                git remote add origin \"https://samperay:${{ secrets.GITHUB_TOKEN }}@github.com/$GITHUB_REPOSITORY.git\"\n                git config --global user.email \"samperay@gmail.com\"\n                git config --global user.name \"samperay\"\n                git fetch\n                git checkout master\n                git branch --set-upstream-to=origin/master\n                git pull \n                ls -la\n                echo $RANDOM &gt;&gt; random.txt\n                ls -la\n                git add -A\n                git commit -m \"added random file\"\n                git push \n</code></pre> <p>you can define access and scopes for the GITHUB_TOKEN</p>"},{"location":"cicd/github_actions/variables/#encrypting-and-decrypting-files","title":"Encrypting and Decrypting files","text":"<p>we could use the file to store credentials if data &gt;64KB where we store api_username or api_token.  we could encrypt those file and later in the workflow we can decrypt to use those. Create PASSPHRASE in the github secrets and use that for decryption for the files to get api_username or api_token. </p> <pre><code>vim secrets.json\n{\n    api_username: 'assasas'\n    api_token: 'as8qwe78qwshhshhwhsd_sd!e7we'\n}\n\ngpg --symmetric --cipher-algo AES256 /tmp/secrets.json\ngit add /tmp/secrets.json.gpg\ngit commit -m 'added secretfile'\ngit push\n</code></pre> <pre><code>on:\n    push\n\njobs:\n    decrypt: \n        runs-on: ubuntu-latest\n        steps:\n            - uses: actions/checkout@v1\n\n            - name: Decrypt file\n              run: gpg --quiet --batch --yes --decrypt --passphrase=\"$PASSPHRASE\" --output $HOME/secret.json secrets.json.gpg\n              env: \n                PASSPHRASE: ${{ secrets.PASSPHRASE }}\n\n            - name: print file secret contents\n              run: cat $HOME/secret.json\n</code></pre> <p>References: encrypted secrets</p>"},{"location":"cicd/github_actions/variables/#contexts","title":"Contexts","text":"<p>Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects.</p> <pre><code>name: Context testing\nruns-name: \"contexts example\"\non: push\n\njobs:\n  dump_contexts_to_log:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Dump GitHub context\n        env:\n          GITHUB_CONTEXT: ${{ toJson(github) }}\n        run: echo \"$GITHUB_CONTEXT\"\n      - name: Dump job context\n        env:\n          JOB_CONTEXT: ${{ toJson(job) }}\n        run: echo \"$JOB_CONTEXT\"\n      - name: Dump steps context\n        env:\n          STEPS_CONTEXT: ${{ toJson(steps) }}\n        run: echo \"$STEPS_CONTEXT\"\n      - name: Dump runner context\n        env:\n          RUNNER_CONTEXT: ${{ toJson(runner) }}\n        run: echo \"$RUNNER_CONTEXT\"\n      - name: Dump strategy context\n        env:\n          STRATEGY_CONTEXT: ${{ toJson(strategy) }}\n        run: echo \"$STRATEGY_CONTEXT\"\n      - name: Dump matrix context\n        env:\n          MATRIX_CONTEXT: ${{ toJson(matrix) }}\n        run: echo \"$MATRIX_CONTEXT\"\n\n</code></pre> <p>References: context</p>"},{"location":"cicd/github_actions/variables/#functions-and-expressions","title":"Functions and expressions","text":"<p>You can use expressions to programmatically set environment variables in workflow files and access contexts. An expression can be any combination of literal values, references to a context, or functions</p> <pre><code>  functions_expressions:\n    runs-on: ubuntu-latest\n    steps:\n        - name: expressions\n          run: |\n            echo ${{ contains(fromJSON('[\"push\", \"pull_request\"]'), github.event_name) }}\n            echo ${{ startsWith('Hello world', 'He') }}\n            echo ${{ endsWith('Hello world', 'ld') }}\n\n  func_status_checks:\n    runs-on: ubuntu-latest\n    steps:\n        - name: step to fail\n          run: echooo \"hello\"\n\n        - name: run even if failed\n          if: failure()\n          run: echo \"I need this step to execute even if previous step fails\"\n\n        - name: run always\n          if: always()\n          run: echo \"this step will always run, success or failed step\"\n\n        - name: run success\n          run: echo \"run when none of previous step failed\" \n          if: success()\n\n        - name: runs cancel\n          run: echo \"runs on cancel job\"\n          if: cancelled()\n</code></pre> <p>Note: line # 216, if we add <code>continue-on-error: true</code> we don't need line # 219.  Both functions work in same manner.</p> <p>References: expressions</p>"},{"location":"cicd/github_actions/workflow_triggers/","title":"Triggers","text":"<p>In this section, we will discuss about the Events, Schedules, External Events &amp; Filters which can trigger the workflow run. </p>"},{"location":"cicd/github_actions/workflow_triggers/#triggerring-workflow-with-events","title":"Triggerring workflow with events","text":"<p>We will create a new branch e.g(develop) and once created we would want the actions to run when we try to create a new PR. so we would add the event for <code>pull_request</code> which will trigger the workflow actions. </p> <p>There are types involved in the <code>pull_request</code> which tells as to what activities on PR would need your workflow to trigger run actions. e.g, when you <code>close</code> | <code>reopen</code> | <code>assign</code> ..etc</p> <p>here is the link for list of activities for pull_request</p> <p>we can also set the runners to run the terminal which we choose by default for entire workflow or for induvidual steps.  e.g even runnners from <code>windows-latest</code> or <code>ubuntu-latest</code> it would choose only which we set as default one's</p> <pre><code>name: actions workflow\nrun-name: actions workflow\non: \n    push:\n    pull_request: \n        types: [opened, closed, assigned, reopened] # you can specify the activity types.\n\n# choose the default shell as bash\ndefault:\n  run:\n    shell: bash    \n\njobs:\n    run-github-actions:\n        runs-on: ubuntu-latest\n        steps:\n            - name: list files\n              run: |\n                pwd\n                ls\n            - name: checkout\n              uses: actions/checkout@v1\n            - name: list files after checkout \n              run: |\n                pwd\n                ls -a\n            - name: simple JS actions\n              id: greet\n              uses: actions/hello-world-javascript-action@v1\n              with:\n                who-to-greet: John\n            - name: Log greeting time\n              run: echo \"${{ steps.greet.outputs.time }}\"\n\n    windows-actions:          \n      runs-on: windows-latest\n\n      # we are trying to overide the default shell from 'bash' to 'powershell'\n      default:\n        run:\n         shell: pwsh\n\n      steps: \n        - name: display dirrectory\n          run: dir\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#needs","title":"needs","text":"<p>Identify any jobs that must complete successfully before another job will run.</p> <p>e.g, when you have two or more jobs to run, and you have a scnerio that you must complete the first job and then it should start second job. in that case, you would use the key word <code>needs</code></p> <pre><code>jobs:\n  run-shell-cmds:\n    runs-on: ubuntu-latest\n    steps:\n      - name: echo string\n        run: echo \"hello world\"\n\n      - name: multiline script\n        run: |\n          node -v\n          npm -v \n\n      - name: python command \n        run: | \n          import platform\n          print(platform.processor())\n        shell: python\n\n  run-windows-cmds:\n    runs-on: windows-latest\n    needs: \n      - run-shell-cmds\n</code></pre> <p>References: needs</p>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-scheduler","title":"trigger using scheduler","text":"<p>you can use <code>cron</code> scheduler to run the action  workflow. </p> <p>Cron scheduler to run for every 5 mins, thats the minimum shortest interval you can run scheduled workflows.</p> <pre><code>name: actions workflow\nrun-name: actions workflow by @{{ github.actor}}, ${{ github.event_name }}\non: \n    schedule:\n      # you can defined multiple cron schedulers\n      - cron: \"*/5 * * * *\"\n      - cron: \"0 14 * * *\"\n    # push:\n    pull_request: \n      types: [opened, closed, assigned, reopened]\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-dispatch_events","title":"trigger using dispatch_events","text":"<p>You can use the GitHub API to trigger a webhook event called repository_dispatch when you want to trigger a workflow for activity that happens outside of GitHub. </p> <p>Create a new access token for the authorization for the repo, and you can trigger it using curl or postman, on which your build would run. </p> <p>Postman configurations:</p> <pre><code>Authorization -&gt; Select Basic Auth -&gt; Password: sdhdjhdhd\n\nHeaders: \n\nContent-Type: application/json\nAccept: application/json\n\nBody: raw -&gt; select JSON\n\n{\n    \"event_type\": \"build\",\n        \"client_payload\": {\n            \"env\" : \"production\"\n  }\n}\n\n</code></pre> <p>You can write any keyword for the <code>event_type</code> which you can call in the github action workflow.</p> <pre><code>on:\n  repository_dispatch:\n    types: [build]\n  pull_request: \n    types: [opened, closed, assigned, reopened]\n\njobs:\n    run-github-actions:\n        runs-on: ubuntu-latest\n        steps:\n            - name: webhook trigger build\n              run: echo \"${{ github.event.client_payload.env }}\"\n</code></pre> <p>On triggring the event in the postman, your build should be running.</p> <p>References: </p> <p>create-a-repository-dispatch-event</p> <p>repository_dispatch</p>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-using-branches-tags-paths","title":"trigger using branches, tags, paths","text":"<p>you can use specific branch, tags and paths to execute scripts based on the actions.  Note: You can't use including or excluding at same time</p> <p>including</p> <ul> <li>branches</li> <li>tags</li> <li>paths</li> </ul> <p>excluding</p> <ul> <li>branches-ignore</li> <li>tags-ignore</li> <li>paths-ignore</li> </ul> <p>while you merge changes form one branch to another, and incase your target branch is <code>dev</code> or <code>feature</code> then your PR would be running state. </p> <pre><code>on:\n  push: \n    branches: \n      - \"!dev1\" # don't run actions if push is from dev1 branch\n      - \"dev\" # run actions if push from dev branch\n\n  pull_request:\n    types:\n      - opened\n    branches:\n      - \"dev\"\n      - \"feature/*\" # feature/featureA, feature/featureB\n      - \"feature/**\" # feature/feat/A, feature/feat/B\n      - \"!dev1\" # don't run actions incase I create PR from current branch to dev1\n</code></pre> <p>References: Filter Usage</p>"},{"location":"cicd/github_actions/workflow_triggers/#skip-actions","title":"skip actions","text":"<p>incase you don't want to run your job while commiting, you need to update below message while <code>git commit</code></p> <pre><code>git commit -m 'your change[actions skip]'\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#display-runner-logs","title":"display runner logs","text":"<p>while you run the workflow, you need to mark the lines as errors for display purpose in the runner. We can also mask the secrets and group logs</p> <pre><code>steps:\n    - name: Display error\n      run: echo \"::error:: your error message\"\n\n    - name: Display warning\n      run: echo \"::warning:: your error message\"\n\n    - name: Display debug\n      run: echo \"::debug:: your error message\"\n\n    - name: Display notice\n      run: echo \"::notice:: your error message\"\n\n    - name: diplay group of logs\n      run: |\n        echo \"::group:: group title logs\n        echo \"write all the logs - 1\"\n        echo \"write up logs -2 \"\n        echo \"::endgroup::\"\n\n    - name: mask the secrets\n      run: echo \"::add-mask::my-password\"\n\n    # it would show as ***\n    - name: display my secret password\n      run: echo \"my-password\"\n</code></pre> <p>References: https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions</p>"},{"location":"cicd/github_actions/workflow_triggers/#parent-child-workflow-triggers","title":"parent child workflow triggers","text":"<p>When you have two jobs and you need to run the job after one workflow has been completed, you can let the new job know about it.</p> <p>Example as below</p> <pre><code>#simple.yml(parent workflow)\nname: simple\non:\n    push\njobs:\n    simple:\n        runs-on: ubuntu-latest\n        steps:\n            - name: parent workflow\n              run: echo \"hello world\"\n\n\n#deps.yml(child.yml)\nname: depsjod\non:\n    workflow_run:\n        workflows: [simple]\n        types: [completed]\njobs:\n    simple_deps_jobs:\n        runs-on: ubuntu-latest\n        steps:\n          - name: child workflow\n            run: echo \"running child job after parent job.\"\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#trigger-workflow-manually","title":"trigger workflow manually","text":"<p>You can configure custom-defined input properties, default input values, and required inputs for the event directly in your workflow. you need to go to the UI and run the job manually by providing the inputs</p> <pre><code>name: manual-runjob\non:\n    workflow_dispatch:\n      inputs:\n        logLevel:\n          description: 'Log level'\n          required: true\n          default: 'warning'\n          type: choice\n          options:\n          - info\n          - warning\n          - debug\n        tags:\n          description: 'Test scenario tags'\n          required: false\n          type: boolean\n        envs:\n          description: 'environments'\n          type: environment\n          required: true\njobs:\n    log-the-inputs:\n        runs-on: ubuntu-latest\n        steps:\n        - run: |\n            echo log_level: ${{ inputs.logLevel }}\n            echo tags: ${{ inputs.tags }}\n            echo env: ${{ inputs.envs }}\n</code></pre>"},{"location":"cicd/github_actions/workflow_triggers/#environment-files-and-env","title":"environment files and env","text":"<p>During the execution of a workflow, the runner generates temporary files that can be used to perform certain actions. The path to these files are exposed via environment variables.  </p> <p>You can make an environment variable available to any subsequent steps in a workflow job by defining or updating the environment variable and writing this to the GITHUB_ENV environment file. The step that creates or updates the environment variable does not have access to the new value, but all subsequent steps in a job will have access.</p> <pre><code>steps:\n  - name: Set the value\n    id: step_one\n    run: |\n      echo \"action_state=yellow\" &gt;&gt; \"$GITHUB_ENV\"\n  - name: Use the value\n    id: step_two\n    run: |\n      printf '%s\\n' \"$action_state\" # This will output 'yellow'\n\n## multiline string\n\nsteps:\n  - name: Set the value in bash\n    id: step_one\n    run: |\n      {\n        echo 'JSON_RESPONSE&lt;&lt;EOF'\n        curl https://example.com\n        echo EOF\n      } &gt;&gt; \"$GITHUB_ENV\"\n</code></pre>"},{"location":"cicd/jenkins/deploy/","title":"Virtual Server","text":"<p>Application: https://github.com/samperay/jenkins-project/tree/single-server-deploy</p> <p>Spinned up a new ec2 machine(prod-server) where the application is deployed. we would be configuring the application to have an Continious deployment. This is on a <code>single-server-deployment</code> </p> <pre><code>mkdir /home/ec2-user/\ncd /home/ec2-user/app\npython3 -m venv venv\nvi /etc/systemd/system/flaskapp.service\n\n[Unit]\nDescription=flask app \nAfter=network.target\n\n[Service]\nUser=ec2-user\nGroup=ec2-user\nWorkingDirectory=/home/ec2-user/app/\nEnvironment=\"PATH=/home/ec2-user/app/venv/bin\"\nExecStart=/home/ec2-user/app/venv/bin/python3 /home/ec2-user/app/app.py\n\n[Install]\nWantedBy=multi-user.target\n\n\nsudo systemctl enable flaskapp.service\nsudo systemctl start flaskapp.service\n</code></pre>"},{"location":"cicd/jenkins/deploy/#jenkins-server-installation","title":"Jenkins server installation","text":"<p>EC2 machine has been spinned <code>t2.medium</code> without which the jenkins server won't be able to schedule the job.</p>"},{"location":"cicd/jenkins/deploy/#installation","title":"Installation","text":"<pre><code>sudo yum update \u2013y\nsudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo\nsudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key\nsudo yum install java-17-amazon-corretto docker git pip pytest -y\nsudo yum install jenkins -y\nsudo systemctl enable jenkins\nsudo systemctl start jenkins\nsudo systemctl status jenkins\nsudo systemctl start docker.service\nsudo systemctl enable docker.service\nsudo cat /var/lib/jenkins/secrets/initialAdminPassword\nsudo usermod -a -G docker ec2-user\nsudo usermod -a -G docker jenkins\nsudo chown jenkins /var/run/docker.sock\n</code></pre>"},{"location":"cicd/jenkins/deploy/#pipelines","title":"Pipelines","text":"<p>In Jenkins configure the <code>pipeline</code> job, and run the build to see the deployed version. https://github.com/samperay/jenkins-project/tree/single-server-deploy</p> <pre><code>pipeline {\n    agent any \n\n    environment {\n        SERVER_IP = credentials('prod-server-ip')\n    }\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh \"pip install -r requirements.txt\"\n            }\n        }\n\n        stage('Test') {\n            steps {\n                sh \"pytest\"\n            }\n        }\n\n        stage('Package code') {\n            steps {\n                sh \"zip -r myapp.zip ./* -x '*.git*'\"\n                sh \"ls -lart\"\n            }\n        }\n\n        stage(\"Deploy to Prod\") {\n            steps {\n                withCredentials([sshUserPrivateKey(credentialsId: 'ssh-key', keyFileVariable: 'MY_SSH_KEY', usernameVariable: 'username')]) {\n                    sh '''\n                    scp -i $MY_SSH_KEY -o StrictHostKeyChecking=no myapp.zip ${username}@${SERVER_IP}:/home/ec2-user/\n                    ssh -i $MY_SSH_KEY -o StrictHostKeyChecking=no ${username}@${SERVER_IP} &lt;&lt; EOF \n                      unzip -o /home/ec2-user/myapp.zip -d /home/ec2-user/app/\n                      source app/venv/bin/activate\n                      cd /home/ec2-user/app\n                      pip install -r requirements.txt\n                      sudo systemctl restart flaskapp.service\n                    '''\n                }\n            }\n        }\n\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/deploy/#docker","title":"Docker","text":"<p>In Jenkins configure the <code>pipeline</code> job to build and push the docker image to hub.  https://github.com/samperay/jenkins-project/tree/docker-image</p> <pre><code>pipeline \n{\n    agent any\n\n    environment {\n        IMAGE_NAME='sunlnx/jenkins-flask-app'\n        IMAGE_TAG=\"${IMAGE_NAME}:${env.GIT_COMMIT}\"\n    }\n\n    stages \n    {\n\n        stage('Setup') \n        {\n            steps {\n                sh \"pip install -r requirements.txt\"\n            }\n        }\n\n        stage('Test')\n        {\n            steps {\n                sh \"pytest\"\n            }\n        }\n\n                stage('Build Docker Image') \n        {\n            steps {\n                sh 'docker build -t ${IMAGE_TAG} .'\n                echo \"Docker image build successfully\"\n                sh 'docker images ls'\n            }\n        }\n\n        stage('Login to Dockerhub') \n        {\n            steps {\n                withCredentials([usernamePassword(credentialsId: 'docker-creds', usernameVariable: 'USERNAME', passwordVariable: 'PASSWORD')]) {\n\n                sh 'echo ${PASSWORD} | docker login -u ${USERNAME} --password-stdin' \n                echo 'login successful'\n                }\n\n            }\n        }\n\n        stage('Push Docker image') \n        {\n            steps {\n                sh \"docker push ${IMAGE_TAG}\"\n                echo \"Docker image pushed successfully.\"\n            }\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"cicd/jenkins/deploy/#multipipeline-repo-setup","title":"multipipeline repo setup","text":""},{"location":"cicd/jenkins/overview/","title":"Overview","text":""},{"location":"cicd/jenkins/overview/#build-trigger","title":"Build Trigger","text":"<p>Trigger builds remotely</p> <p>Enable this option if you would like to trigger new builds by accessing a special predefined URL. you will pre-define the token and will pass that to the specific url to trigger build</p> <p><code>JENKINS_URL/job/python-app/build?token=TOKEN_NAME</code></p> <p>Build after other projects are built</p> <p>Set up a trigger so that when some other projects finish building, a new build is scheduled for this project</p> <p>Build periodically</p> <p>Cron job</p> <p>Build when a change is pushed to Gogs</p> <p>when we commit the change, this would trigger the build</p> <p>Poll SCM</p> <p>Configure Jenkins to poll changes in SCM. Costly ops as it should scan the entire workspace. </p> <p>Reference: https://wiki.jenkins.io/display/JENKINS/Building-a-software-project.html</p>"},{"location":"cicd/jenkins/overview/#sample-jenkinsfile","title":"Sample Jenkinsfile","text":"<p>Simple Jenkinsfile</p> <pre><code>pipeline {\n    agent any \n    stages{\n        stage('Bulid') {\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#environments-jenkinsfile","title":"Environments Jenkinsfile","text":"<p>Global environment variables set</p> <pre><code>pipeline {\n    agent any \n\n    // env defined globally for all the stages\n    // we must avoid env to be set global\n\n    environment {\n        DB_HOSTNAME = 'rds-db.example.com'\n        PORT=3023\n        DB_USERNAME = 'dbuser'\n        DB_PASSWORD = 'password123'\n    }\n    stages{\n        stage('Bulid') {\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"stage:test db_username: ${DB_USERNAME}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Define env vars locally </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('Bulid') {\n                environment {\n                DB_HOSTNAME = 'rds-db.example.com'\n                PORT=3023\n                DB_USERNAME = 'dbuser'\n                DB_PASSWORD = 'password123'\n            }\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        // this stage won't have access for the vars and hence the build will be failed.\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"stage:test db_username: ${DB_USERNAME}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Builtin env </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('Bulid') {\n                environment {\n                DB_HOSTNAME = 'rds-db.example.com'\n                PORT=3023\n                DB_USERNAME = 'dbuser'\n                DB_PASSWORD = 'password123'\n            }\n            steps {\n                echo \"Running build id {env.BUILD_ID} on jenkins url ${env.JENKINS_URl}\"\n                echo \"db_hostname: ${DB_HOSTNAME}\"\n                echo \"db_username: ${DB_USERNAME}\"\n                echo \"db_password: ${DB_PASSWORD}\"\n            }\n        }\n\n        stage('test') {\n            steps {\n                echo \"this is testing stage\"\n                echo \"build number ${env.BUILD_ID}\"\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://wiki.jenkins.io/display/JENKINS/Building-a-software-project.html#Buildingasoftwareproject-belowJenkinsSetEnvironmentVariables</p> <p>Accessing credentials</p> <p>There are two types of credentials</p> <ol> <li>System - Defined for specific projects etc </li> <li>Global - Defined for all the projects</li> </ol> <p>I am using <code>Global</code> I will create <code>username and password</code> with id as <code>prod-server</code> for the credentials and use them for the Jenkinsfile</p> <pre><code>pipeline {\n    agent any\n\n    environment {\n        PROD_SERVER = credentials('prod-server')\n    }\n\n    stages {\n        stage('Build') {\n            steps {\n                echo \"running build stage\"\n                echo \"${PROD_SERVER}\"\n                echo \"${PROD_SERVER_USR}\"\n                echo \"${PROD_SERVER_PSW}\"\n            }\n        }\n    }\n}\n</code></pre> <p>when used <code>PROD_SERVER_USR</code> and <code>PROD_SERVER_PSW</code> it will display username and password. but its not recommended to use as it would provide warning message in the jenkins console. </p> <p>we must be using Jenkins Binding Credentials. </p> <pre><code>pipeline {\n    agent any \n\n    stages{\n        stage('build') {\n            steps {\n                echo 'build stage'\n                withCredentials([usernamePassword(credentialsId: 'prod-server', usernameVariable: 'myusername', passwordVariable: 'mypassword')])\n                {\n                    sh '''\n                    echo username: ${myusername}\n                    echo password: ${mypassword}\n                    '''\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://www.jenkins.io/doc/pipeline/steps/credentials-binding/#withcredentials-bind-credentials-to-variables</p>"},{"location":"cicd/jenkins/overview/#nested-and-parallel-stages","title":"Nested and parallel stages","text":"<p>Nested</p> <pre><code>pipeline {\n    agent any\n    stages{\n        stage('Linting and Testing'){\n            stages {\n                stage('lint'){\n                    steps {\n                        echo \"linting stage\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        echo \"testing stage\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre> <p>Parallel</p> <p>when you write <code>parallel</code> in the stages, it would be executed at same time.</p> <pre><code>pipeline {\n    agent any\n    stages{\n        stage('Linting and Testing'){\n            parallel {\n                stage('lint'){\n                    steps {\n                        sh \"sleep 30\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        sh \"sleep 30\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#pipeline-options","title":"Pipeline Options","text":"<p>timeout - how long does the job needs to be run before timing out.</p> <pre><code>pipeline {\n    agent any\n\n    options {\n        timeout(time:1 ,unit: 'MINUTES')\n    }\n\n    stages{\n        stage('Linting and Testing'){\n            parallel {\n                stage('lint'){\n                    steps {\n                        sh \"sleep 70\"\n                    }\n                }\n                stage('testing'){\n                    steps{\n                        sh \"sleep 70\"\n                    }\n                }\n            }\n        }\n        stage('deploy') {\n            steps{\n                echo \"Deploy stage\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/jenkins/overview/#parameters","title":"parameters","text":"<p>You can customize the behaviour of the pipeline by passing in some data when we trigger a build. like env=dev/stage/prod..</p> <pre><code>pipeline {\n    agent any\n\n    parameters { \n        choice(name: 'ENVIRONMENT', choices: ['dev', 'stage', 'prod'], description: 'Deployment environment')\n        booleanParam(name: 'RUN_TESTS', defaultValue: true, description: 'Run tests in pipeline')\n\n    }\n\n    stages{\n        stage('Test'){\n            when {\n                expression {\n                    params.RUN_TESTS == true\n                }\n            }\n\n            steps {\n                echo \"RUN_TESTS is true, we will execute these steps\"\n            }\n        }\n\n        stage('Development Deploy Stage'){\n            when {\n                expression {\n                    params.ENVIRONMENT == \"dev\"\n                }\n            }\n\n            steps {\n                echo \"Deploy stage to development environment\"\n            }\n        }\n    }\n}\n</code></pre> <p>Reference: https://www.jenkins.io/doc/book/pipeline/syntax/#parameters</p>"},{"location":"cicd/jenkins/overview/#input","title":"input","text":"<pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Build'){\n            steps {\n                echo \"Build stage\"\n            }\n        }\n\n        stage('Deploy'){\n            steps {\n                echo \"Deploy stage for production\"\n            }\n            input {\n                message \"Please approve the deployment for production?\"\n                ok \"Approved\"\n            }\n\n        }\n    }\n}\n</code></pre>"},{"location":"cicd/tekton/concepts/","title":"concepts","text":""},{"location":"cicd/tekton/concepts/#cicd","title":"cicd","text":"<ul> <li>set of practices to streamline software development process.</li> <li>achievement of faster deployment cycles, reduce errors, enhance quality &amp; collaboration</li> </ul>"},{"location":"cicd/tekton/concepts/#ci","title":"ci","text":"<ul> <li>encourages developers to frequeently integrate their code into shared repository which detects integration issues, erors very early stage of development. </li> <li>Build is transparent and can be used by tools like 'mavev,docker,gradle' etc</li> <li>auotmated testing i.e unit test, integration testing etc </li> <li>linting, checkstyles etc for code quality</li> </ul>"},{"location":"cicd/tekton/concepts/#cd","title":"cd","text":"<ul> <li>new git release has been created </li> <li>delivery/deployment pipeline </li> <li>publish artifacts/images</li> <li>handle stage of release</li> <li>human approval process involmenet.</li> <li>operation transparent.</li> </ul>"},{"location":"cicd/tekton/concepts/#tekton-architecture","title":"Tekton architecture","text":""},{"location":"cicd/tekton/concepts/#definition","title":"Definition","text":"<p>steps: smallest units, its equivalent to container that executes on specific input to produce output.</p> <p>task: composed of one or more steps. represent higher level unit of work of specific job in your CI/CD process. </p> <p>pipelines series of tasks that should be executed in a specific order. they orchstrate the flow of work through the CICD process. </p> <p>^ ^ Complete process but only the blueprint(definition)</p>"},{"location":"cicd/tekton/concepts/#instatiation","title":"Instatiation","text":"<p>pipelineRun is the actual work done.. these components picked up by the tekton controller , this controller will create respoective pods </p> <p>eventhandling </p> <p>listens to external events such as code-commits and trigger piprlines executions. this is done by the HTTP which is configured to have a webhooks defined. </p> <p>Interceptors inspect events and filter external events.. they run through the pipeline so you can customize triggers.. only auth events pass..</p> <p>Trigger binding/templates: extract data from the  incoming event payloads to feed pipelineRun.reusable templates to trigger the pipelines. </p> <p></p>"},{"location":"cicd/tekton/overview/","title":"overview","text":"<p>Tekton is an open-source cloud native CICD solution</p> <p>Make sure you have any of the tool being installed for the kubernetes cluser. i.e Rancher/Podman/Docker Desktop etc..</p> <p>kind is a tool for installation local kubernetes cluster, make sure to install</p> <p>installing-from-release-binaries</p> <p>Verify cluster is created and working fine. </p> <pre><code>\u279c  ~ kind version\nkind v0.20.0 go1.20.4 darwin/amd64\n\u279c  ~ kind create cluster --name tekton-cluster\nCreating cluster \"tekton-cluster\" ...\n \u2713 Ensuring node image (kindest/node:v1.27.3) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing CNI \ud83d\udd0c\n \u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-tekton-cluster\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-tekton-cluster\n\nNot sure what to do next? \ud83d\ude05  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n\u279c  ~ kubectl config current-context\nkind-tekton-cluster\n\u279c  ~ kubectl create deployment nginx --image=nginx\ndeployment.apps/nginx created\n</code></pre> <p>install tekton pipelines - install necessary binaries/packages</p> <pre><code>\u279c  ~ kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.52.0/release.yaml\nnamespace/tekton-pipelines created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created\nclusterrole.rbac.authorization.k8s.io/tekton-events-controller-cluster-access created\nrole.rbac.authorization.k8s.io/tekton-pipelines-controller created\nrole.rbac.authorization.k8s.io/tekton-pipelines-webhook created\nrole.rbac.authorization.k8s.io/tekton-pipelines-events-controller created\nrole.rbac.authorization.k8s.io/tekton-pipelines-leader-election created\nrole.rbac.authorization.k8s.io/tekton-pipelines-info created\nserviceaccount/tekton-pipelines-controller created\nserviceaccount/tekton-pipelines-webhook created\nserviceaccount/tekton-events-controller created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-events-controller-cluster-access created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-leaderelection created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-leaderelection created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-info created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-events-controller created\nrolebinding.rbac.authorization.k8s.io/tekton-events-controller-leaderelection created\ncustomresourcedefinition.apiextensions.k8s.io/clustertasks.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/customruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/pipelines.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/pipelineruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/resolutionrequests.resolution.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/tasks.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/taskruns.tekton.dev created\ncustomresourcedefinition.apiextensions.k8s.io/verificationpolicies.tekton.dev created\nsecret/webhook-certs created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.pipeline.tekton.dev created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.pipeline.tekton.dev created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.pipeline.tekton.dev created\nclusterrole.rbac.authorization.k8s.io/tekton-aggregate-edit created\nclusterrole.rbac.authorization.k8s.io/tekton-aggregate-view created\nconfigmap/config-defaults created\nconfigmap/config-events created\nconfigmap/feature-flags created\nconfigmap/pipelines-info created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-observability created\nconfigmap/config-registry-cert created\nconfigmap/config-spire created\nconfigmap/config-tracing created\ndeployment.apps/tekton-pipelines-controller created\nservice/tekton-pipelines-controller created\ndeployment.apps/tekton-events-controller created\nservice/tekton-events-controller created\nnamespace/tekton-pipelines-resolvers created\nclusterrole.rbac.authorization.k8s.io/tekton-pipelines-resolvers-resolution-request-updates created\nrole.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created\nserviceaccount/tekton-pipelines-resolvers created\nclusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers created\nrolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created\nconfigmap/bundleresolver-config created\nconfigmap/cluster-resolver-config created\nconfigmap/resolvers-feature-flags created\nconfigmap/config-leader-election created\nconfigmap/config-logging created\nconfigmap/config-observability created\nconfigmap/git-resolver-config created\nconfigmap/hubresolver-config created\ndeployment.apps/tekton-pipelines-remote-resolvers created\nservice/tekton-pipelines-remote-resolvers created\nhorizontalpodautoscaler.autoscaling/tekton-pipelines-webhook created\ndeployment.apps/tekton-pipelines-webhook created\nservice/tekton-pipelines-webhook created\n\u279c  ~\n</code></pre> <p>Deletion:</p> <pre><code>kind delete cluster --name tekton-cluster\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/","title":"pipelines","text":""},{"location":"cicd/tekton/tasks_pipelines/#task","title":"Task","text":"<p>re-usale atomic work, a specific job that needs to be built or testing. it executes as a pod in the cluster. it has be initiated by taskRun.</p> <pre><code># tasks.yml\napiVersion: tekton.dev/v1beta1\nkind: Task\nmetadata:\n  name: 4-01-echo-task\nspec:\n  description: A simple example Tekton Task\n  steps:\n    - name: echo-step\n      image: alpine:3.14\n      script: |\n        #!/bin/sh\n        echo \"Message: Hello, Tekton!\"\n</code></pre> <pre><code>kubectl create -f tasks.yaml\nkubectl get tasks\nkubectl describe task 4-01-echo-task\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#taskrun","title":"TaskRun","text":"<ul> <li>individual runs of the tekton tasks. </li> <li>actual work carried out for CICD tasks. </li> <li>tasksRun are initated from the tasks for execution</li> <li>each taskRun is executed only once. </li> </ul> <pre><code># tasks_run.yml\napiVersion: tekton.dev/v1beta1\nkind: TaskRun\nmetadata:\n  name: 4-02-echo-task-run\nspec:\n  taskRef:\n    name: 4-01-echo-task\n</code></pre> <pre><code>kubectl create -f tasks_run.yml\nkubectl get pods \nkubectl logs &lt;pod_name&gt;\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#pipeline","title":"Pipeline","text":"<ul> <li>define and manage CICD pipelines</li> <li>seq of tasks together for CICD</li> <li>customize the execution conditions according to business needs</li> </ul> <pre><code># pipeline.yml\napiVersion: tekton.dev/v1beta1\nkind: Pipeline\nmetadata:\n  name: 4-03-example-pipeline\nspec:\n  tasks:\n    - name: echo-task\n      taskRef:\n        name: 4-01-echo-task\n</code></pre> <pre><code>kubectl create -f pipeline.yml \nkubectl get pipelines \nkubectl describe pipeline 4-03-example-pipeline\n</code></pre>"},{"location":"cicd/tekton/tasks_pipelines/#pipelinerun","title":"PipelineRun","text":"<ul> <li>instance of pipeline</li> <li>representatin of actual instantition of CICD workflow</li> <li>pipelineRuns are building constructs from those blueprints</li> <li>each pipelineRun is a unique execution of pipeline</li> </ul> <pre><code># pipeline_run.yml\napiVersion: tekton.dev/v1beta1\nkind: PipelineRun\nmetadata:\n  name: 4-04-example-pipeline-run\nspec:\n  pipelineRef:\n    name: 4-03-example-pipeline\n</code></pre> <pre><code>kubectl create -f pipeline_run.yml \nkubectl get pipelineruns  / kubectl get taskruns\nkubectl get pods \nkubectl logs 4-04-example-pipeline-run-echo-task-pod \n</code></pre> <p>if you are deleting pipelineRuns, assicoated taskRuns also deleted. </p>"},{"location":"cicd/tekton/tasks_pipelines/#input-parameterizing","title":"input parameterizing","text":"<ul> <li>instead of static, we would write re-usable code using input parameteriation.</li> <li>flow control is key benefit</li> <li>allows to make decision during the execution of your tasks/pipelines based on input values.</li> </ul>"},{"location":"cicd/tekton/tasks_pipelines/#results","title":"Results","text":"<ul> <li>tekton tasks and taskruns can accept input parameters</li> <li>flexibility using results of steps in and beyond tasks.  The tasks is able to emit results in simple case as string, objects, arrays etc</li> </ul>"},{"location":"cicd/tekton/tasks_pipelines/#workspaces","title":"workspaces","text":"<ul> <li>storage area that allows tasks and pipeines to share data and files</li> <li>configmap, secrets, pvc, pvc-templates etc</li> <li>why ?</li> <li>data sharing</li> <li>data persistance</li> <li>isolation</li> <li>task level </li> <li>definition where workspace reside on its step containers</li> <li>specifies how data should be stored and accessed during the execution of that specific task. </li> <li>pipeline level</li> <li>mamages data flow between thaks via worksapce collaboration</li> <li>task A downloads code, while task B needs artifact for compiling</li> </ul> <p>Who creates wokspace and who manages ?</p> <ul> <li>its reposibility of taskrun and taskrunpipeline. </li> <li>providing and managing the workspace</li> <li>when creating a taskrun or pipelinerun</li> <li>specficatin necessary which workspace to use under which volume</li> <li>tekton take care about creating and mounting volumes on pod level</li> </ul> <p>write task in one workspace and read from another workspace.  more info on examples: 4.0.7-*.yaml</p>"},{"location":"cicd/tekton/tasks_pipelines/#auth","title":"auth","text":""},{"location":"cicd/tekton/tasks_pipelines/#clustertask","title":"clustertask","text":""},{"location":"cicd/tekton/tasks_pipelines/#resolvers","title":"resolvers","text":""},{"location":"cloud/aws/practitioner/summary/","title":"Summary","text":""},{"location":"cloud/aws/practitioner/summary/#clf-c022026","title":"CLF-C022026","text":""},{"location":"cloud/aws/practitioner/summary/#shared-responsibility-model","title":"Shared responsibility Model","text":"<p>Official link Shared Responsibility Model</p>"},{"location":"cloud/aws/practitioner/summary/#iam","title":"IAM","text":""},{"location":"cloud/aws/practitioner/summary/#compute","title":"Compute","text":"<p>These are the instance types for EC2. https://aws.amazon.com/ec2/instance-types/</p> <p>SG: firewall attached to Ec2 instances</p> <p>Ec2 instance role: link to IAM roles</p> <p>Purchace options: On demand, spot, reserverd(standard+convertable), dedicated host, dedicated instance.</p> <p>Userdata: data to be bootstrappoed during the boot time. </p> <pre><code>#!/bin/bash\n# Use this for your user data (script from top to bottom)\n# install httpd (Linux 2 version)\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"&lt;h1&gt;Hello World from $(hostname -f)&lt;/h1&gt;\" &gt; /var/www/html/index.html\n</code></pre>"},{"location":"cloud/aws/practitioner/summary/#serverless-compute","title":"Serverless Compute","text":""},{"location":"cloud/aws/practitioner/summary/#storage","title":"Storage","text":""},{"location":"cloud/aws/practitioner/summary/#load-balancers-and-asg","title":"Load Balancers and ASG","text":"<p>Uses of load balancer</p> <ul> <li>Spread load across multiple downstream instances </li> <li>Expose a single point of access (DNS) to your application </li> <li>Seamlessly handle failures of downstream instances </li> <li>Do regular health checks to your instances </li> <li>Provide SSL termination (HTTPS) for your websites </li> <li>Enforce stickiness with cookies \u2022 High availability across zones </li> <li> <p>Separate public traffic from private traffic</p> </li> <li> <p>managed load balancer</p> </li> <li>AWS guarantees that it will be working</li> <li>AWS takes care of upgrades, maintenance, high availability</li> <li> <p>AWS provides only a few configuration knobs</p> </li> <li> <p>integrated with many AWS offerings / services</p> </li> <li>EC2, EC2 Auto Scaling Groups, Amazon ECS</li> <li>AWS Certificate Manager (ACM), CloudWatch</li> <li> <p>Route 53, AWS WAF, AWS Global Accelerator</p> </li> <li> <p>health checks</p> </li> <li>load balancer to know if instances it forwards traffic to are available to reply to requests</li> <li>health check is done on a port and a route (/health is common)</li> <li>response is not 200 (OK), then the instance is unhealthy</li> </ul> <p>Classic Load Balancer</p> <ul> <li>HTTP, HTTPS, TCP, SSL (secure TCP)</li> </ul> <p>Application Load Balancer</p> <ul> <li> <p>HTTP, HTTPS, WebSocket</p> </li> <li> <p>Operates at layer 7(Appliation layer)</p> </li> <li> <p>Routing tables to different target groups:</p> </li> <li>Routing based on path in URL (example.com/users &amp; example.com/posts)</li> <li>Routing based on hostname in URL (one.example.com &amp; other.example.com)</li> <li> <p>Routing based on Query String, Headers</p> </li> <li> <p>ALB are a great fit for micro services &amp; container-based application</p> </li> <li> <p>Target Groups:</p> </li> <li>EC2 instances (can be managed by an Auto Scaling Group)- HTTP</li> <li>ECS tasks (managed by ECS itself) \u2013 HTTP</li> <li>Lambda functions \u2013 HTTP request is translated into a JSON event</li> <li>ALB can route to multiple target groups</li> <li> <p>Health checks are at the target group level</p> </li> <li> <p>Fixed hostname</p> </li> <li> <p>The application servers don\u2019t see the IP of the client directly </p> </li> <li>The true IP of the client is inserted in the header X-Forwarded-For</li> <li>We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)</li> </ul> <p>Network Load Balancer - TCP, TLS (secure TCP), UDP - Operates at layer 4(Network layer)</p> <p>Gateway Load Balancer - Operates at layer 3 (Network layer) \u2013 IP Protocol</p> <p>registered target groups when doing the lab</p> <p></p> <p></p> <p></p>"},{"location":"cloud/aws/practitioner/summary/#s3","title":"S3","text":""},{"location":"cloud/aws/practitioner/summary/#infra-deploymentscale","title":"Infra Deployment@scale","text":""},{"location":"cloud/aws/practitioner/summary/#global-applications","title":"Global Applications","text":""},{"location":"cloud/aws/practitioner/summary/#cloud-integrations","title":"Cloud integrations","text":""},{"location":"cloud/aws/practitioner/summary/#monitoring","title":"Monitoring","text":""},{"location":"cloud/aws/practitioner/summary/#vpc","title":"VPC","text":"<p>https://www.ipaddressguide.com/cidr</p> <p>AWS resevres 5 ip address in each subnet, e.g 10.0.0.0/24</p> <p>10.0.0.0 - network address 10.0.0.1 - reserved by aws for vpc router 10.0.0.2 - reserved by aws for mapping to amazon provided dns 10.0.0.3 - reseced by aws for future aws 10.0.0.255 - netwrok broadcase address. aws does not suppose broacase in a vpc thefor reseverd.</p>"},{"location":"cloud/aws/practitioner/summary/#demovpc","title":"DemoVPC","text":"<p>Public Subnet:</p> <p></p> <p></p>"},{"location":"cloud/aws/practitioner/summary/#vpc-peering","title":"VPC Peering","text":""},{"location":"cloud/aws/practitioner/summary/#vpc-endpoint","title":"VPC endpoint","text":"<p>There are two types of endpoints are only for S3 and Dynamo DB which are HA and scalable. The main purpose of using this is you dont need to connect to internet to access the AWS services. since they are privately available you can use this service.</p> <ul> <li>Interface: these would have an cost and would be more useful incase of S2S connectivity or any on-prem</li> <li>Gateway: Free of cost and performs same like interface.</li> </ul> <p>Deploy Endpoints in the privatesubnet route entry. earlier this used to be NAT gateway, you can remove entry from NAT, which goes to IGW to get access to your s3 using public, instead remove those NAT and add private GW.</p> <p></p>"},{"location":"cloud/aws/practitioner/summary/#site-to-site-vpn-s2s-vpn","title":"Site-to-Site VPN (S2S VPN)","text":"<p>A secure IPsec VPN tunnel over the public internet between: Your on-prem data center +  Your AWS VPC</p>"},{"location":"cloud/aws/practitioner/summary/#customer-gateway-cgw","title":"Customer Gateway (CGW)","text":"<p>The on-prem side of the VPN. It can be  firewall / Router /VPN appliance (Cisco, FortiGate, Palo Alto, etc.)</p>"},{"location":"cloud/aws/practitioner/summary/#virtual-private-gateway-vgw","title":"Virtual Private Gateway (VGW)","text":"<p>The AWS side of the VPN attached to VPC</p> <p>Its main responsibility is to terminate VPN tunnels and advertise routes to AWS</p> <p></p> <p>```On-Prem Network      | [Customer Gateway]      | ==== Encrypted IPsec Tunnel ====      | [Virtual Private Gateway]      |    AWS VPC</p> <pre><code>\n### AWS Direct Connect (DX)\n\nA dedicated private physical connection(Private fiber link) from your data center to AWS\n\n![direct_link](direct_link.png)\n\n### Direct Connect Gateway (DXGW)\n\nA global gateway that allows one Direct Connect to connect to multiple VPCs (even across regions).\n\n- Without DXGW \u2192 DX is tied to one VPC\n- With DXGW \u2192 DX can serve many VPCs\n\nUse cases: \n- Multi-account\n- Multi-region\n- Hub-and-spoke networking\n\n### Transit Gateway (TGW)\n\nTransit Gateway = a central network hub for AWS \nA cloud router that connects many networks together\n\n\nit can connect:\n\n- Multiple VPCs\n- On-prem networks\n- VPNs\n- Direct Connect\n\nInstead of creating many peer-to-peer connections, everything connects to one hub.\n\n</code></pre> <p>Transit Gateway    \u251c\u2500 VPC Attachment    \u251c\u2500 VPN Attachment    \u251c\u2500 DX Gateway Attachment    \u2514\u2500 TGW Peering ```</p> <p></p>"},{"location":"cloud/aws/practitioner/summary/#egress-gw","title":"egress-gw","text":"<p>allow instances in your VPC outbound connections over ipv6 while preventing the internet to initiate an ipv6 connection to your instances</p> <p></p>"},{"location":"cloud/aws/practitioner/summary/#security","title":"security","text":""},{"location":"cloud/aws/practitioner/summary/#databaase-and-analytics","title":"Databaase and Analytics","text":""},{"location":"cloud/aws/practitioner/summary/#account-management-billing","title":"Account Management &amp; Billing","text":""},{"location":"cloud/aws/practitioner/summary/#other-aws-services","title":"Other AWS services","text":""},{"location":"cloud/aws/practitioner/summary/#aws-machine-learning","title":"AWS Machine learning","text":""},{"location":"cloud/ibm/application_deployment/","title":"IBM application Deployment","text":""},{"location":"cloud/ibm/application_deployment/#ibm-cloud-registry","title":"IBM Cloud Registry","text":"<p>A container registry is a service which provides a collection of repositories in which images can be stored. </p> <p>It can also have\u202f API\u202fpaths and\u202faccess control\u202frules.\u202f </p> <p>Container registries can be hosted publicly or privately.\u202f </p> <p></p>"},{"location":"cloud/ibm/application_deployment/#benefits-of-the-ibm-cloud-container-registry","title":"Benefits of the IBM Cloud Container Registry","text":"<p>HA and scalable.</p> <p>Push private images to conveniently run them in the IBM Cloud Kubernetes Service, Red Hat\u00ae OpenShift* Kubernetes Service, and other runtime environments. </p> <p>Images are checked for security issues, giving users full control and the ability to make informed decisions about their deployments.</p> <p>IBM Cloud Container Registry can be used by setting up an image namespace and pushing container images to the namespace.</p> <p>Image security compliance with vulnerable advisor</p> <p>When pushing images to IBM Cloud Container Registry, users can benefit from the built-in Vulnerability Advisor features that scan for potential security issues and vulnerabilities. </p> <p>Vulnerability Advisor\u202fchecks for vulnerable packages in specific Docker base images and known vulnerabilities in app configuration settings. When vulnerabilities are found, information about the vulnerability is provided. This information can be used to resolve security and configuration issues.\u202f </p> <p>Benefit from automatic scanning of images in a namespace. </p> <p>Review recommendations that are specific to the operating system to fix potential vulnerabilities and protect containers from being compromised.</p> <p>Quota limits for storage and pull traffic</p> <p>Benefit from free storage and pull traffic to private images until the free quota is reached. </p> <p>Set custom quota limits for storage and pull traffic per month to avoid exceeding the preferred payment level. </p>"},{"location":"cloud/ibm/application_deployment/#namespace","title":"namespace","text":"<p>A namespace in IBM Cloud Container Registry is a collection of repositories that store container images. IAM permissions can be granted to manage access control on namespaces, and retention policies can be set on namespaces in IBM Cloud Container Registry.</p> <pre><code>ibmcloud cr namespace-list\nibmcloud cr create yournamespace\nibmcloud cr namespace-rm &lt;my_namespace&gt;\n</code></pre>"},{"location":"cloud/ibm/application_deployment/#vulnerability-advisor","title":"vulnerability advisor","text":"<p>Vulnerability Advisor checks configuration settings in images that use supported operating systems (MySQL, NGINX, and Apache apps) and provides a link to any relevant security notices about the vulnerability. </p> <p>The table of vulnerabilities displays essential information about each issue, such as the Vulnerability ID, the policy status, the affected packages, and how to resolve the issue. </p>"},{"location":"cloud/ibm/application_deployment/#using-ibm-cloud-kubernetes-service","title":"Using IBM Cloud Kubernetes Service","text":"<p>Provides</p> <p>Intelligent scheduling  Horizontal scaling  Service discovery  Load balancing  Automated rollouts and rollbacks  Secret and configuration management  Self-healing features for apps</p> <p>Offers</p> <p>Advanced tools for secure and efficient management of cluster workloads  Built-in security and isolation features  Highly available and secure containerized app delivery on the public cloud  User-friendly interface</p> <p>IBM Cloud Kubernetes Service uses a split control plane and data plane model, which refers to the separation of the control plane components (such as the API server, scheduler, and controller manager) from the worker nodes that run the application workloads.  </p> <p>This separation allows for better scalability and fault tolerance, as well as improved security and ease of management. The control plane manages the cluster's overall state and makes decisions about where to place workloads, while the worker nodes handle the actual execution of the workloads. </p> <p>IBM Cloud offers two deployment options for running Kubernetes clusters: Classic and VPC (Virtual Private Cloud). The Classic option uses a shared, multi-tenant infrastructure, while the VPC option provides a dedicated, isolated virtual network for each client.  </p> <p>The VPC option offers greater security, control, and flexibility compared to Classic but also requires more management overhead. The choice between Classic and VPC depends on the specific needs and priorities of each organization</p>"},{"location":"cloud/ibm/application_deployment/#planning-app-deployment","title":"Planning App Deployment","text":"<p>Stateless Apps</p> <ul> <li>Stateless apps are preferred for cloud-native environments like Kubernetes.</li> <li>Stateless apps are easy to migrate and scale. </li> <li>Stateless apps declare dependencies and store configurations separately from the code. </li> <li>Stateless apps treat backing services such as databases as attached resources instead of coupled to the app. </li> <li>App pods don't require persistent data storage or a stable network IP address. </li> <li>Pods can be terminated, rescheduled, and scaled in response to workload demands. </li> <li>Stateless apps use a Database-as-a-Service for persistent data. </li> <li>Stateless apps use NodePort, load balancer, or Ingress services to expose the workload on a stable IP address.</li> </ul> <p>Stateful Apps</p> <ul> <li>Stateful apps are more complicated than stateless apps to set up, manage, and scale. </li> <li>Stateful apps require persistent data and a stable network identity. </li> <li>Stateful apps are often databases or other distributed, data-intensive workloads. </li> <li>Processing is more efficient closer to the data itself. </li> <li>To deploy a stateful app, persistent storage must be set up and mounted to the pod controlled by a StatefulSet object. </li> <li>File, block, or object storage can be added as persistent storage for a stateful set. </li> <li>Portworx can be installed on bare metal worker nodes to manage persistent storage for stateful apps as a highly - available software-defined storage solution. </li> <li>The Kubernetes documentation provides more information on how stateful sets work. </li> </ul>"},{"location":"cloud/ibm/automated_dev_tools/","title":"IBM Automation Tools","text":""},{"location":"cloud/ibm/automated_dev_tools/#continuous-delivery-services","title":"Continuous Delivery Services","text":"<p>DevOps is an increasingly common approach to agile software development that developers and operations teams use to build, test, deploy, and monitor applications with speed, quality, and control. DevOps is relevant to any kind of software project regardless of architecture, platform, or purpose. DevOps is a software development approach between development and operations that continuously delivers the required functionality. In the process, it also improves collaboration between the two teams. DevOps enables fast and automated deployment of services. </p> <p>Automate Continuous Integration is the effort required to integrate a system that increases exponentially with time. By integrating the system more frequently, integration issues are identified earlier, when they are easier to fix, and the overall integration effort is reduced. The result is a higher quality product and more predictable delivery schedules. </p> <p>Automated deployment is the practice of eliminating manual steps from the deployment of code to environments for software testing and delivery. </p> <p>Continuous delivery requires that code changes constantly flow from development all the way through to production. To continuously deliver in a consistent and reliable way, a team must break down the software delivery process into delivery stages and automate the movement of the code through the stages to create a delivery pipeline.</p> <p>The efficiency of working in small batches is one reason adopting agile development has benefited the software industry. Think about small batches in software development. The team writes and tests a small batch of code, such as a single user story, which provides an end-to-end capability that is valuable to a client. Everything is working, and you can demonstrate the capability to the client and get feedback quickly.</p> <p>Automate continuous testing to enable continuous delivery. An obvious benefit of automated testing, in contrast with manual testing, is that testing can happen quickly, repeatably, and on demand. It becomes a simple matter to verify that the software continues to run as it has before. In addition, using the practices of test-driven development(TDD) and behavior-driven development (BDD) to create test automation has been shown to improve coding quality and design.</p> <p>By using unattended automation to reduce the number of times people need to touch the systems, users can decrease the cost of the solution and improve profitability. Real profitability occurs when users can decrease the number of times people are required to touch the systems. This allows operators to focus on higher-value work and improves the operator-to-server ratio.</p> <p>Culture, Shift Left, Traceability Auditability, Security Education, and Visibility are the six best practices for DevSecOps.</p>"},{"location":"cloud/ibm/automated_dev_tools/#toolchains","title":"Toolchains","text":"<p>developers select each tool integration that they want to configure for their toolchain. Successful DevOps implementations generally rely on an integrated set of solutions or a \"toolchain\" to remove manual steps, reduce errors, increase team agility, and to scale beyond small, isolated teams. </p> <p>DevOps tools create integrated DevOps open toolchains to enable tool integrations that support development, deployment, and operations tasks. A toolchain is an integrated set of tools that developers can use to collaboratively develop, build, deploy, test, and manage applications and make operations repeatable and easier to manage. Toolchains can include open source tools, IBM Cloud services, such as IBM Cloud DevOps Insights, and third-party tools, such as GitHub, PagerDuty, and Slack. </p> <p>DevOps tools deliver continuously by using automated pipelines as well as automates builds, unit tests, deployments, and more. Capable of building, testing, and deploying in a repeatable way with minimal human intervention; ready to release into production at any time.</p> <p>DevOps Tools are able to edit and push code from anywhere by using the web-based Integrated Development Environment (IDE). They can create, edit, run, and debug, and complete source-control tasks in GitHub as well as move seamlessly from editing code to deploying it to production. </p> <p>DevOps tools allow for team collaboration and source code management with a Git repository (repos) and issue tracker that is hosted by IBM and built on GitLab Community Edition. This allows for the management of Git repos through fine-grained access controls that keep code secure. DevOps tools are able to review code and enhance collaboration through merge requests, track issues, and share ideas through the issue tracker, and they are able to document projects on the wiki system.</p>"},{"location":"cloud/ibm/automated_dev_tools/#functions-of-app-development","title":"Functions of App Development","text":"<p>Think: Plan application by creating bugs, tasks, or ideas using issue tracker.</p> <p>Code: implementation of application by providing a git as SCM.</p> <p>Deliver: configure the pipeline, allow users to specify automated build, deployment, testing code after developer pushes code the repository.</p> <p>Run:  run application in cloud env</p> <p></p> <p>IBM Cloud DevOps Insights is a tool that aggregates code, test, build, and deployment data to provide visibility of quality for all of your teams. DevOps Insights is a tool for continuous integration and continuous delivery (CI/CD). It evaluates builds to determine if they are safe to release. DevOps Insights is used to increase deployment quality and delivery control in continuous delivery. </p> <p>With DevOps Insights users can do the following: </p> <ul> <li>Maintain and improve the quality of their code in IBM Cloud.</li> <li>Monitor their deployments to identify risks before they are released. </li> <li>Analyze development changes for error probability.</li> <li>Improve the interactions of your team.</li> </ul> <p>Gates are the mechanism implemented in CI/CD tools to hold back the build if it does not meet the passing threshold. Gates compare the build with the policies configured for an application. A policy contains a set of rules.</p> <p>A rule is the passing criteria that users define for each type of test data they upload. For example, a policy can be created that contains a unit test rule that requires 100 percent success and a test coverage rule that requires 80 percent coverage.</p> <p>If the code does not meet or exceed a policy that is enacted at a particular gate, the deployment is stopped to prevent risky changes from being promoted to the next environment.</p>"},{"location":"cloud/ibm/automated_dev_tools/#open-source","title":"Open source","text":"<p>Tekton pipelines are able to build, test, and deploy in a repeatable way with minimal human intervention. It is an open source framework that is vendor neutral that can create continuous integration and delivery (CI/CD) systems. Tekton pipeline contains a Kubernetes-native framework and helps by modernizing continuous delivery. The following are three components of Tekton.</p> <p>Step: Platform management is one valid role for IBM Cloud Identity and Access Management</p> <p>Task: A task is a collection of steps in order. Tekton runs a task in the form of a Kubernetes pod, where each step becomes a running container in the pod.</p> <p>Pipeline: A pipeline is a collection of tasks in order. Tekton collects all the tasks, connects them in a directed acyclic graph (DAG), and executes the graph in sequence. In other words, it creates a number of Kubernetes pods and ensures that each pod completes running successfully as desired.</p> <p>A CI/CD workflow may contain speci\ufb01c executable actions like</p> <p>A PipelineRun is a speci\ufb01c execution of a pipeline. For example, a developer may ask Tekton to run a CI/CD workflow twice a day, and each execution becomes a PipelineRun resource trackable in the Kubernetes cluster. The status of the CI/CD workflow can be viewed and may include speci\ufb01cs of each task execution with PipelineRuns.</p> <p>A TaskRun is a speci\ufb01c execution of a task. TaskRuns are also available when a task is run outside a pipeline. The speci\ufb01cs of each step execution in a task may be viewed in the dashboard.</p> <p>Benefits and Features of Tekton</p> <p>Easier and fast deployment: Consistency and errors are reduced through automated processes. Quickly able to create cloud-native pipelines across multiple cloud providers or in hybrid environments.</p> <p>k8 native for agility and control: Tekton pipelines run on Kubernetes using their clusters as a first-class type, and they use containers as building blocks. </p> <p>Runs on any k8 cluster: Tekton is a true open source solution, which allows for the creation of continuous delivery pipelines to deploy apps to any Kubernetes environment. </p> <p>Serverless for greater efficiency: Cloud resources are used only when needed for execution of pipeline tasks. This enhances development team control and reduces costs. </p> <p>Shared pipelines reduce complexity: To reduce rework and speed up development, developers have access across projects and organizations through open source components that standardize CI/CD tooling.</p>"},{"location":"cloud/ibm/automated_dev_tools/#ibm-cloud-schematics","title":"IBM Cloud Schematics","text":""},{"location":"cloud/ibm/cloud_schematics/","title":"IBM Cloud Schematics","text":"<p>IBM Cloud Schematics provides automation by offering declarative Terraform templates to ensure a desired provisioned cloud infrastructure. Native integration with Red Hat\u00ae Ansible extends configuration, management, and provisioning to software and applications and integrates with other IBM Cloud Services. </p>"},{"location":"cloud/ibm/cloud_schematics/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>IBM Schematics executes IaC as a service which includes open source Ansible and Terraform amongst others (Git and Helm). IaC automation is hosted as-a-Service in the cloud and when adopted as an approach improves consistency, quickens deployment, and reduces manual errors through its provisioning and automation.    </p>"},{"location":"cloud/ibm/cloud_schematics/#features-and-key-capabilities","title":"Features and Key Capabilities","text":"<p>Hosted terraform workspace</p> <p>Able to easily provision cloud resources, allowing for focus to be on deployment of applications, configuration management, and subsequent daily upkeep in an automated approach for speed, consistency, and time to value benefits.  </p> <p>Native ansible actions</p> <p>Easily install software packages and application code on infrastructure.</p> <p>Collaborative environment</p> <p>Deploy and iterate infrastructure automation processes easily as a team.</p> <p>Integrate security</p> <p>Integrates easily with IBM Cloud IAM, Key Protect, IBM Log Analysis cloud service, and IBM Cloud Monitoring.</p>"},{"location":"cloud/ibm/cloud_schematics/#benefits-for-the-developer","title":"Benefits for the Developer","text":"<p>Faster provisioning Improved consistency Efficient development Improved ROI</p> <p></p>"},{"location":"cloud/ibm/cloud_schematics/#software-deployment","title":"Software Deployment","text":"<p>The IBM Software Solutions Catalog (opens in a new tab)has a wide range of software and infrastructure templates that developers can use to set up cloud resources, and to install IBM and third-party software in an IBM Cloud Kubernetes Service cluster, Red Hat\u00ae OpenShift\u00ae on IBM Cloud cluster, or a classic server or Virtual Servers for VPC. </p> <p></p>"},{"location":"cloud/ibm/cloud_schematics/#security-and-compliance","title":"Security and Compliance","text":"<ul> <li> <p>Monitor for controls and goals that pertain to IBM Cloud Schematics.</p> </li> <li> <p>Define rules for IBM Cloud Schematics that can help to standardize resource configuration.</p> </li> <li> <p>As a security or compliance focal, developers can use the IBM Cloud Schematics goals to help ensure that their organization is adhering to the external and internal standards for their industry. By using the Security and Compliance Center to validate resource configurations in their account, potential issues can be identified as they arise.</p> </li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/","title":"IBM Professional Cloud Developer Overview","text":""},{"location":"cloud/ibm/ibm_professional_cloud_developer/#objectives","title":"Objectives","text":"<ul> <li>Recall application development architectures such as full stack and microservices</li> <li>Apply working knowledge of IBM Cloud services: PaaS, IaaS, FaaS, and SaaS</li> <li>Demonstrate working knowledge of Kubernetes and Docker</li> <li>Identify concepts of IBM open innovation on IBM Cloud using Kubernetes and cloud-native, open-source work</li> <li>Apply concepts related to IBM security leadership associated with data encryption and configuration</li> <li>Identify variances between and across public, private, and hybrid-cloud enterprise grade offerings</li> <li>Identify techniques to move workloads between environments</li> <li>Recall concepts of open innovation design and implement an automated DevOps deployment</li> <li>Identify security concepts, like authentication, authorization, single sign-on, SSL/TLS, certificates, keys, encryption, Kubernetes config maps/secrets, and more.</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#major-topics","title":"Major topics:","text":"<ul> <li>IBM Professional Cloud Developer Overview</li> <li>IBM Cloud Features and Benefits</li> <li>Continuous Delivery Services</li> <li>IBM Cloud Schematics</li> <li>Essentials of IBM Cloud Professional Developer</li> <li>Application Deployment Choices</li> <li>Traditional Compute Options</li> <li>Identity Management</li> <li>IBM Key and Secrets Management</li> <li>Container Security</li> <li>Data Security</li> <li>IBM Cloud Professional Developer Principles</li> <li>Data Services</li> <li>Logging, Monitoring, and Event Management Tools</li> <li>Extending Applications through IBM Cloud Watson API Services</li> <li>Cloud Internet Services</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#cloud-developer-job-role","title":"Cloud Developer Job Role","text":"<p>An IBM Certified Cloud Developer designs and develops secure IBM Cloud applications, services, and products in an agile, collaborative environment. This includes, but is not limited to, back-end, front-end, full-stack, data and application integration, and cloud application deployment. </p> <p>The IBM Cloud Developer role is multi-functional and comprises one or more of the following sub-roles:</p>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#front-end","title":"Front end","text":"<ul> <li>Works closely with designers to take wireframes from conception to implementation</li> <li>Works closely with back-end developers to ensure implemented UI code is unit tested and production-code ready</li> <li>Codes user interfaces, including interactions, responsive layouts, and styling</li> <li>Uses and contributes to the IBM Design System to implement UIs</li> <li>Ensures user interfaces are accessible and enhance the performance of the application</li> <li>Works in programming languages like JavaScript, Node.js, or frameworks such as React, Vue, or Angular</li> <li>Skilled in UI development technologies such as HTML, CSS, JSON, and API usage</li> <li>Understands back-end concerns</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#backend","title":"Backend","text":"<ul> <li>Works with client-server architectures, networking protocols, application development, and databases</li> <li>Uses and develops RESTful APIs and web services</li> <li>Understands user and system requirements</li> <li>Develops using a combination of object-oriented and functional programming models</li> <li>Plans, analyzes, designs, and constructs databases</li> <li>Implements industry standards and best practices for database security and is capable of analyzing and - defining database and information security requirements</li> <li>Develops structured query language (SQL) queries, back-end database stored procedures, or NoSQL queries</li> <li>Works in programming languages like Ruby, Python, Java, Node.js, and server-side languages such as Go and Rust</li> <li>Understands front-end concerns</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#devsecops","title":"Devsecops","text":"<ul> <li>Works with development teams to enable a continuous integration environment that sustains high productivity levels and emphasizes defect-prevention techniques</li> <li>Ensures delivery pipeline from dev/test, staging, and production and performs A/B testing </li> <li>Automates, measures, and optimizes the system performance and processes</li> <li>Designs and implements tools for automated deployment and monitoring of multiple environments</li> <li>Works with tools like Jenkins, Maven, Ant, Gradle, Chef, Puppet, Docker, UrbanCode, Tekton, Terraform, and Ansible</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#qa-test-developers","title":"QA test Developers","text":"<ul> <li>Encourages application development that builds tests from the ground up</li> <li>Ensures the product is robust and failure scenarios are considered and refactored</li> <li>Collaborates with cross-functional team members on story development, from before definition through final  deployment</li> <li>Performs exploratory testing using industry-leading practices</li> <li>Discovers defects/bugs and works with coders and product owners to determine root cause and how to prevent similar issues from happening in the future</li> <li>Drives adoption of test automation - unit tests, integration tests, functional tests</li> </ul>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#skills-of-a-cloud-professional-developer","title":"Skills of a Cloud Professional Developer","text":"<p>Benefits for cloud</p> <p>Identify IBM Cloud features and benefits for cloud developers. </p> <p>Continious Delievery</p> <p>Identify fundamental concepts of Continuous Delivery Services Identify the features available when using Tekton pipelines on IBM Cloud Indicate the benefits when using DevOps Toolchains on IBM Cloud</p> <p>Cloud Schematics</p> <p>Identify the fundamental concepts of IBM Cloud Schematics</p> <p>Container registry &amp;&amp; Vulnerability advisor</p> <p>Indicate the purpose of IBM Cloud Container Registry</p> <p>Kubernetes service</p> <p>Recognize the considerations when deploying an application using IBM Cloud Kubernetes Service on IBM Cloud</p> <p>Openshift </p> <p>Recognize the considerations when deploying an application using Code Engine on IBM Cloud</p> <p>Code Engine</p> <p>Recognize the considerations when deploying an application using Red Hat\u00ae OpenShift\u00ae on IBM Cloud</p> <p>Traditional compute options</p> <p>Distinguish between IBM Cloud Platform compute options Identify appropriate use cases for VSIs in IBM Cloud Platform Identify appropriate use cases for using Bare Metal Servers on IBM Cloud Platform Identify appropriate use cases for VMware solutions on IBM Cloud Platform</p> <p>IAM</p> <p>Identify access controls when using IBM Cloud Identify and Access Management (IAM) Indicate the reasons an IBM Cloud Developer would use App ID when developing an application</p> <p>Key and secret management</p> <p>Indicate the purpose of secure key management on IBM Cloud Choose the best practices when using Secrets Manager</p> <p>Container security</p> <p>Identify the role of service accounts in container security Indicate the use of role-based access control (RBAC) in Kubernetes</p> <p>Data security</p> <p>Identify the key security aspects of cloud-native applications</p> <p>Data Services</p> <p>Determine the IBM Cloud Object Storage elements that secure data at rest, data in motion, and data access Distinguish between the IBM Cloud Databases services</p> <p>Logging, Monitoring, and Event management</p> <p>Understand the importance of IBM Log Analysis from a DevSecOps perspective</p> <p>Watson Services</p> <p>Indicate the capabilities of the IBM Watson Text to Speech API Recognize the functionality provided by IBM Watson\u00ae Assistant Identify the common input and output features of IBM Watson Speech to Text API Identify appropriate use cases for IBM Watson Natural Language Understanding Recognize the functionality of Watson Language Translator</p> <p>Internet Services</p> <p>Identify the use of Cloud Internet Services (CIS) on IBM Cloud</p>"},{"location":"cloud/ibm/ibm_professional_cloud_developer/#features-of-ibm-cloud-for-developers","title":"Features of IBM Cloud for Developers","text":"<ul> <li>Provide securely managed container orchestration for container needs. </li> <li>Secure continuous integration and continuous delivery services built with a security-first focus. </li> <li>Manage Infrastructure-as-code solutions enabling the use of Terraform and Ansible for provisioning and - configuration management needs.</li> <li>Provide cloud-wide robust identity and access management services. </li> <li>Provide enterprise-grade container runtimes and orchestration. </li> <li>Provide built-in container image scanning tools.</li> <li>Provide security and performance-boosting edge services.</li> </ul> <p>The DevOps Dashboard: Provides data sets for each application. The dashboard works within IBM Cloud, with multiple integrations and continuous delivery (CI/CD) tools. </p> <p>Toolchain Templates and Integrations: Our DevOps toolchain catalog includes many templates and third-party integrations like Slack to build powerful toolchains. </p> <p>Consistent, efficient, &amp; faster: Developers using IBM Cloud enjoy faster, consistent, and more efficient workflows. </p> <p>Improved ROI: IBM Cloud can help modernize workflow processes across the organization and its environment, lowering costs and increasing value. </p> <p>IBM Cloud Compliance: Multiple compliance programs are available, and all are certified with various regulatory and compliance standards. </p>"},{"location":"cloud/terraform/faq/","title":"Interview Questions","text":"<p>https://cloudchamp.notion.site/Terraform-Scenario-based-Interview-Questions-bce29cb359b243b4a1ab3191418bfaab</p> <p>You can create 3-tier architecture using the below codebase https://github.com/ajitinamdar-tech/three-tier-arch-aws-terraform</p> <ul> <li>How do you define tags which are common to all the resources which you create </li> </ul> <pre><code>variable \"common_tags\" {\n  type = map(string)\n  default = {\n    Owner       = \"sunil\"\n    Project     = \"core-infra\"\n    ManagedBy   = \"terraform\"\n    CostCenter  = \"cc-123\"\n  }\n}\n# \nlocals {\n  tags = merge(var.common_tags, \n        { Environment = var.env }\n  )\n}\n\nresource \"aws_vpc\" \"main\" {\n  cidr_block = \"10.0.0.0/16\"\n  tags       = merge(local.tags, { Name = \"main-vpc\" })\n}\n\noutput \"debug_tags\" {\n  value = local.tags\n}\n\n</code></pre> <ul> <li>How do you reference the list using terraform variable defined ?</li> </ul> <pre><code>variable \"subnet_ids\" {\n  type = list(string)\n  default = [\"subnet-1\"]\n}\n\n# index\nsubnet_id = var.subnet_ids[0]\n\n# loop\nresource \"aws_network_interface\" \"eni\" {\n  count     = length(var.subnet_ids)\n  subnet_id = var.subnet_ids[count.index]\n}\n</code></pre> <ul> <li>How to create 3 instances of the ec2 instance at once ?</li> </ul> <pre><code>resource \"aws_instance\" \"web\" {\n  count         = 3\n  ami           = \"ami-xxx\"\n  instance_type = \"t2.micro\"\n  tags          = { Name = \"web-${count.index}\" }\n}\n\noutput \"instances\" {\n  value = aws_instance.web.*\n}\n\n</code></pre> <ul> <li>Required to create 1 instance of t2.micro and t3.xlarge based on the dev / prod environment ?</li> </ul> <pre><code>variable \"env\" { type = string }\n\nlocals {\n  instance_type = var.env == \"prod\" ? \"t3.xlarge\" : \"t2.micro\"\n}\n\nresource \"aws_instance\" \"app\" {\n  ami           = var.ami_id\n  instance_type = local.instance_type\n  subnet_id     = var.subnet_id\n}\n\noutput \"env_instance\" {\n  value = aws_instance.app.*\n}\n</code></pre> <ul> <li>What do you do when you want to see the logs in details when you need to debug the terraform code ?</li> </ul> <pre><code>export TF_LOG=DEBUG  # TRACE, DEBUG, INFO, WARN, ERROR\nexport TF_LOG_PATH=./tf.log\nterraform apply\n</code></pre> <ul> <li>You have alredy defined the VPC Tags, based on the tags we need to create an public instance in the - defined subnet? How to achieve this ?</li> </ul> <p>Terraform can\u2019t \u201cquery AWS by tags\u201d natively unless you use a data source</p> <pre><code>data \"aws_subnets\" \"public\" {\n  filter {\n    name   = \"tag:Tier\"\n    values = [\"public\"]\n  }\n  filter {\n    name   = \"vpc-id\"\n    values = [aws_vpc.main.id]\n  }\n}\n\nresource \"aws_instance\" \"public_ec2\" {\n  ami                         = var.ami_id\n  instance_type               = \"t2.micro\"\n  subnet_id                   = data.aws_subnets.public.ids[0]\n  associate_public_ip_address = true\n}\n</code></pre> <ul> <li>How do I define the list of all the ports at once and add all of them using the resource group?</li> </ul> <pre><code>variable \"allowed_ports\" {\n  type    = list(number)\n  default = [22, 80, 443, 8080]\n}\n\nresource \"aws_security_group\" \"web_sg\" {\n  name   = \"web-sg\"\n  vpc_id = var.vpc_id\n\n  dynamic \"ingress\" {\n    for_each = var.allowed_ports\n    content {\n      from_port   = ingress.value\n      to_port     = ingress.value\n      protocol    = \"tcp\"\n      cidr_blocks = [\"0.0.0.0/0\"]\n    }\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre> <ul> <li>What are the provisioners available in terraform and can you illustrate each of them as when and where - to be used ?</li> </ul> <p>local-exec: runs on your machine (CI runner)</p> <pre><code>provisioner \"local-exec\" {\n  command = \"echo ${self.public_ip} &gt;&gt; ips.txt\"\n}\n</code></pre> <p>remote-exec: runs on the created resource (via SSH/WinRM)</p> <pre><code>provisioner \"remote-exec\" {\n  inline = [\"sudo yum install -y nginx\", \"sudo systemctl enable --now nginx\"]\n\n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n}\n</code></pre> <p>file: copies files to the remote instance</p> <pre><code>provisioner \"file\" {\n  source      = \"app.conf\"\n  destination = \"/tmp/app.conf\"\n  connection {\n    type        = \"ssh\"\n    user        = \"ec2-user\"\n    private_key = file(var.ssh_key_path)\n    host        = self.public_ip\n  }\n}\n</code></pre> <ul> <li>How do you define modules and how do you reference in terraform ?</li> </ul> <p>A module is just a folder with main.tf, variables.tf, outputs.tf.</p> <pre><code>module \"vpc\" {\n  source = \"./modules/vpc\" \n  cidr   = \"10.0.0.0/16\"\n  tags   = local.tags\n}\n\noutput \"vpc_id\" {\n  value = module.vpc.vpc_id\n}\n</code></pre> <ul> <li>How do you use the modulues which are already created by Hashicorp and use in the terraform script ?</li> </ul> <pre><code>module \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~&gt; 5.0\"\n\n  name = \"main\"\n  cidr = \"10.0.0.0/16\"\n  azs  = [\"us-east-1a\", \"us-east-1b\"]\n  public_subnets  = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  private_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n\n  tags = local.tags\n}\n</code></pre> <ul> <li>What are workspaces and how do you create environment using the workspaces ?</li> </ul> <p>Workspaces let you keep separate state files for same code</p> <pre><code>terraform workspace new dev\nterraform workspace new prod\nterraform workspace select dev\n</code></pre> <pre><code>locals {\n  env = terraform.workspace\n}\n\nresource \"aws_instance\" \"app\" {\n  instance_type = local.env == \"prod\" ? \"t3.xlarge\" : \"t2.micro\"\n}\n</code></pre> <ul> <li>What is remote state management and how do you use it ?</li> </ul> <p>Store state in a shared backend (S3 + DynamoDB lock is common).</p> <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"my-tf-state-bucket\"\n    key            = \"envs/dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"my-tf-locks\"\n    encrypt        = true\n  }\n}\n</code></pre> <ul> <li>How do you create multiple resources in different regions using multiple providers ?</li> </ul> <pre><code>provider \"aws\" {\n  region = \"us-east-1\"\n}\n\nprovider \"aws\" {\n  alias  = \"west\"\n  region = \"us-west-2\"\n}\n\nresource \"aws_s3_bucket\" \"east\" {\n  bucket = \"sunil-east-bkt\"\n}\n\nresource \"aws_s3_bucket\" \"west\" {\n  provider = aws.west\n  bucket   = \"sunil-west-bkt\"\n}\n</code></pre> <ul> <li>How do you enusure your password are kept secret using the terraform ?</li> </ul> <pre><code>variable \"db_password\" {\n  type      = string\n  sensitive = true\n}\n\nresource \"aws_db_instance\" \"rds\" {\n  password = var.db_password\n}\n</code></pre> <ul> <li>How do you set the terraform to be in-specific versions in the code and what happens if its not been set ?</li> </ul> <pre><code>terraform {\n  required_version = \"~&gt; 1.6\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n</code></pre> <ul> <li>what happens when you do \"terraform init\" ?</li> <li>Downloads providers</li> <li>Initializes backend (remote state)</li> <li>Downloads modules</li> <li> <p>Creates .terraform/ and lock file terraform.lock.hcl</p> </li> <li> <p>what is the difference between \"terraform plan\" and \"terraform refresh\" ?</p> </li> </ul> <p>terraform plan: computes changes needed to match config with real infra.</p> <p>terraform refresh (legacy): updates state from real resources (doesn\u2019t change infra).</p> <p>Modern approach: terraform apply -refresh-only.</p> <ul> <li>I do need to destroy certain resources in the terraform, how do I do it in particular ?</li> </ul> <pre><code>terraform destroy -target=aws_instance.web[0]\nterraform destroy -target=module.vpc\n</code></pre> <ul> <li>I do already have a terraform instance running, however I have lost the terraform file, Can I re-create - the terraform state file ?</li> </ul> <p>If you used remote backend, state is already there (good).</p> <p>If you lost local state and code:</p> <p>You cannot reliably reconstruct full original config from state alone.</p> <p>You can:</p> <p>Pull state (if backend exists): terraform state pull</p> <p>Rebuild minimal config and then terraform import resources</p> <p>Use tools like Terraformer (best-effort), but imports still required.</p> <p>If you still have resources in AWS but no state:</p> <p>Create new project \u2192 terraform import each resource into new state.</p> <ul> <li>What is the difference between \"variable.tf\" and \"terraform.tfvars\" ?</li> </ul> <p>variables.tf: declares variables (type, description, defaults)</p> <p>terraform.tfvars: assigns values to variables (per environment)</p> <pre><code># variables.tf\nvariable \"env\" { type = string }\n\n# terraform.tfvars\nenv = \"dev\"\n</code></pre> <ul> <li>I would required to create an RDS instance before by AWS instance, How do I do it ?</li> </ul> <p>Terraform builds a dependency graph automatically. To force order:</p> <p>Reference RDS outputs in EC2 (best)</p> <pre><code>resource \"aws_db_instance\" \"rds\" { ... }\n\nresource \"aws_instance\" \"app\" {\n  # example: pass RDS endpoint to user_data\n  user_data = templatefile(\"${path.module}/user_data.sh\", {\n  db_host = aws_db_instance.rds.address\n  })\n}\n</code></pre> <p>Or depends_on (explicit)</p> <pre><code>resource \"aws_instance\" \"app\" {\n  depends_on = [aws_db_instance.rds]\n  ...\n}\n</code></pre> <ul> <li>I have certain resources created using terraform and how do I keep this file in consistent so that it won't be corrupted or so?</li> </ul> <p>Use remote backend with locking (S3 + DynamoDB)</p> <p>Commit terraform.lock.hcl</p> <p>Pin Terraform + provider versions</p> <p>Run in CI: terraform fmt, validate, plan</p> <p>Avoid manual changes in cloud (or use refresh-only + import)</p> <p>Protect state bucket (versioning + encryption + restricted IAM)</p> <p>Separate environments (workspaces or separate backends)</p>"},{"location":"cloud/terraform/overview/","title":"Terraform","text":"<p>Terraform is not a configuration management tool, its a declarative language(HCL). It supports a syntax that is JSON compatible and primarily designed on immutable infrastructure principles.</p> <p>Terraform analyses any expressions within a resource block to find references to other objects and treats those references as implicit ordering requirements when creating, updating, or destroying resources.</p>"},{"location":"cloud/terraform/overview/#iac","title":"IaC","text":"<ul> <li>IaC makes it easy to provision and apply infrastructure configurations, saving time. It standardizes workflows across different infrastructure providers (e.g., VMware, AWS, Azure, GCP, etc.) by using a common syntax across all of them.</li> <li>Makes infra more reliable, IaC makes changes idempotent, consistent, repeatable, and predictable.</li> <li>With IaC, we can test the code and review the results before the code is applied to our target environments.</li> <li>Since code is checked into version control systems such as GitHub, GitLab, BitBucket, etc., it is possible to review how the infrastructure evolves over time.</li> <li>Makes infra more manageable, IaC provides benefits that enable mutation, when necessary</li> </ul> <p>Terraform use case</p> <ul> <li>Heroku App Setup</li> <li>Multi-Tier Applications</li> <li>Self-Service Clusters</li> <li>Software Demos</li> <li>Disposable Environments</li> <li>Software Defined Networking</li> <li>Resource Schedulers</li> <li>Multi-Cloud Deployment </li> </ul> <p>Terraform is cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. This simplifies management and orchestration, helping operators build large-scale multi-cloud infrastructures.</p>"},{"location":"cloud/terraform/overview/#manage-infra","title":"Manage infra","text":"<p>terraform init: will download and initialize any providers that are not already initialized and are installed in the current working directory.</p> <p>To upgrade to latest version of all terraform modules <code>terraform init -upgrade</code></p> <p>you can also use provider constraints using \"version\" within a provider block but that declares a new provider configuration that may cause problems particularly when writing shared modules. Hence recommended using required_providers</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      # Version constraint\n      version = \"~&gt; 5.0\" \n    }\n  }\n}\n\nprovider \"aws\" {\n  # Provider-specific configuration\n  region = \"us-east-1\" \n}\n</code></pre> <p>statefile uses:</p> <p>Mapping to the Real World, When you have a resource resource \"aws_instance\" \"foo\" in your configuration, Terraform uses this map to know that instance i-abcd1234 is represented by that resource.</p> <p>Metadata, track resource dependencies(implicit/explicit)</p> <p>Performance, When running a terraform plan, Terraform must know the current state of resources in order to effectively determine the changes that it needs to make to reach your desired configuration.</p> <p>Syncing, Terraform can use remote locking as a measure to avoid two or more different users accidentally running Terraform at the same time, and thus ensure that each Terraform run begins with the most recent updated state.</p> <pre><code>terraform {\n  backend \"s3\" {\n    # Name of the S3 bucket\n    bucket         = \"my-terraform-state-bucket\"\n    # Path to the state file in the bucket   \n    key            = \"prod/terraform.tfstate\"      \n    region         = \"us-east-1\"                   \n    encrypt        = true \n    # Optional: For state locking                         \n    dynamodb_table = \"terraform-lock-table\"        \n  }\n}\n</code></pre> <p>provisioners</p> <p>Provisioners can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service. https://www.terraform.io/docs/provisioners/#provisioners-are-a-last-resort</p>"},{"location":"cloud/terraform/overview/#terraform-workflow","title":"terraform workflow","text":"<p>terraform init: will download automatically(only Hasicorp) and initialize any providers that are not already initialized and are installed in the current working directory.</p> <ul> <li> <p>Backend Initialization: the root configuration directory is consulted for backend configuration and the chosen backend is initialized using the given configuration settings.</p> </li> <li> <p>Child Module Installation: the configuration is searched for module blocks, and the source code for referenced modules is retrieved from the locations given in their source arguments.</p> </li> <li> <p>Plugin Installation:, For providers distributed by HashiCorp, init will automatically download and install plugins if necessary. Plugins can also be manually installed in the user plugins directory, located at ~/.terraform.d/plugins on most operating systems.</p> </li> </ul> <p>terraform validate Validate runs checks that verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. It is thus primarily useful for general verification of reusable modules, including correctness of attribute names and value types.</p> <p>terraform plan The terraform plan command is used to create an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are necessary to achieve the desired state specified in the configuration files.</p> <p>By default, plan requires no flags and looks in the current directory for the configuration and state file to refresh.</p> <pre><code>exitcodes: Return a detailed exit code when the command exits\n0 = Succeeded with empty diff (no changes)\n1 = Error\n2 = Succeeded with non-empty diff (changes present)\n\nparallelism=n - Limit the number of concurrent operation as Terraform walks the graph. Defaults to 10.\n</code></pre> <p>Terraform itself does not encrypt the plan file. It is highly recommended to encrypt the plan file if you intend to transfer it or keep it at rest for an extended period of time.</p> <p>terraform apply The terraform apply command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan.</p> <p>terraform destroy The terraform destroy command is used to destroy the Terraform-managed infrastructure.</p> <p>The -target flag, instead of affecting \"dependencies\" will instead also destroy any resources that depend on the target(s) specified. The behavior of any terraform destroy command can be previewed at any time with an equivalent. terraform plan -destroy command</p> <p>terraform force-unlock Manually unlock the state for the defined configuration. This command removes the lock on the state for the current configuration. The behavior of this lock is dependent on the backend being used. Local state files cannot be unlocked by another process.</p> <p>terraform fmt command is used to rewrite Terraform configuration files to a canonical format and style</p> <pre><code>-list=false - Don't list the files containing formatting inconsistencies.\n-write=false - Don't overwrite the input files. (This is implied by -check or when the input is STDIN.)\n-diff - Display diffs of formatting changes\n-check - Check if the input is formatted. Exit status will be 0 if all input is properly formatted and non-zero otherwise.\n-recursive - Also process files in subdirectories. By default, only the given directory (or current directory) is processed.\n</code></pre> <p>terraform taint command manually marks a Terraform-managed resource as tainted, forcing it to be destroyed and recreated on the next apply. This command will not modify infrastructure, but does modify the state file in order to mark a resource as tainted.</p> <p>terraform state command is used for advanced state management, Terraform usage becomes more advanced, there are some cases where you may need to modify the terraform.tfstate</p> <p>terraform workspaces Terraform starts with a single workspace named \"default\". This workspace is special both because it is the default and can't delete.</p> <p>For local state, Terraform stores the workspace states in a directory called <code>terraform.tfstate.d</code>. some teams commit these files to version control, although using a remote backend and is always recommented when there are multiple collaborators.</p> <p>For remote state, the workspaces are stored directly in the configured backend. Multiple workspaces are currently supported by the following backends</p> <pre><code>AzureRM |  Consul | COS | COS | GCS | Local | Postgres | Remote | S3 | Manta\n</code></pre> <p>Note: Workspaces, managed with the terraform workspace command, isn't the same thing as Terraform Cloud's workspaces. Terraform Cloud workspaces act more like completely separate working directories.</p> <p>terraform import Import will find the existing resource from ID and import it into your Terraform state at the given ADDRESS. ADDRESS must be a valid resource address.</p> <p>Terraform will attempt to load configuration files that configure the provider being used for import. If no configuration files are present or no configuration for that specific provider is present, Terraform will prompt you for access credentials. You may also specify environmental variables to configure the provider.</p> <p>The only limitation Terraform has when reading the configuration files is that the import provider configurations must not depend on non-variable inputs.</p> <pre><code>terraform import aws_instance.foo i-abcd1234\nterraform import 'aws_instance.baz[0]' i-abcd1234\nterraform import 'aws_instance.baz[\"example\"]' i-abcd1234\n</code></pre> <p>terraform output command is used to extract the value of an output variable from the state file.</p> <p>terraform refresh command is used to reconcile the state Terraform knows about (via its statefile) with the real-world infrastructure. This can be used to detect any drift from the last-known state, and to update the state file. This does not modify infrastructure, but does modify the state file. If the state is changed, this may cause changes to occur during the next plan or apply.</p> <p>terraform show The terraform show command is used to provide human-readable output from a state or plan file. This can be used to inspect a plan to ensure that the planned operations are expected, or to inspect the current state as Terraform sees it.</p>"},{"location":"cloud/terraform/overview/#modules","title":"modules","text":"<p>Terraform Registry makes it simple to find and use modules.(https://registry.terraform.io/) The syntax for referencing a registry module // (hashicorp/consul/aws) <p>You can also use modules from a private registry of the form /// (app.terraform.io/example_corp/vpc/aws) <p>Module versioning We recommend explicitly constraining the acceptable version numbers for each external module to avoid unexpected or unwanted changes</p> <pre><code>module \"consul\" {\n  source  = \"hashicorp/consul/aws\"\n  version = \"0.0.5\"\n  servers = 3\n}\n</code></pre> <p>variables</p> <p>variables on the command line can be associated like below</p> <pre><code>terraform apply -var=\"image_id=ami-abc123\"\nterraform apply -var='image_id_list=[\"ami-abc123\",\"ami-def456\"]'\nterraform apply -var='image_id_map={\"us-east-1\":\"ami-abc123\",\"us-east-2\":\"ami-def456\"}'\n</code></pre> <p>Terraform also automatically loads a number of variable definitions files if they are present, It's more convenient to specify their values in a variable definitions file terraform.tfvars or terraform.tfvars.json or names ending with .auto.tfvars or .auto.tfvars.json</p> <pre><code>terraform apply -var-file=\"custom.tfvars\"\n</code></pre> <p>Terraform searches the environment of its own process for environment variables named TF_VAR_ followed by the name of a declared variable.</p> <pre><code>export TF_VAR_ami_id=\"abd\"\n</code></pre> <p>If a root module variable uses a type constraint to require a complex value (list, set, map, object, or tuple), Terraform will instead attempt to parse its value using the same syntax used within variable definitions files.</p> <pre><code>export TF_VAR_availability_zone_names='[\"us-west-1b\",\"us-west-1d\"]'\n</code></pre> <p>Vars precedence</p> <ol> <li>env vars (TF_VAR_) <li>The terraform.tfvars file, if present.</li> <li>The terraform.tfvars.json file, if present.</li> <li>Any .auto.tfvars or .auto.tfvars.json files, processed in lexical order of their filenames.</li> <li>Any -var and -var-file options on the command line, in the order they are provided.</li>"},{"location":"cloud/terraform/overview/#resource-data-blocks","title":"Resource data blocks","text":""},{"location":"cloud/terraform/overview/#resources","title":"Resources","text":"<p>Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances..etc</p> <p>A resource block declares a resource of a given type (\"aws_instance\") with a given local name (\"web\"). The name is used to refer to this resource from elsewhere in the same Terraform module, but has no significance outside of the scope of a module</p>"},{"location":"cloud/terraform/overview/#meta-arguments","title":"Meta-Arguments","text":"<ul> <li>depends_on -  for specifying hidden dependencies</li> <li>count -  for creating multiple resource instances according to a count</li> <li>for_each -  to create multiple instances according to a map, or set of strings</li> <li>provider -  for selecting a non-default provider configuration</li> <li>lifecycle -  for lifecycle customizations</li> <li>provisioner and connection - for taking extra actions after resource creation</li> </ul> <p>Explicitly specifying a dependency is only necessary when a resource relies on some other resource's behavior but doesn't access any of that resource's data in its arguments.</p> <p>e.g, creation of s3 bucket before aws_instance is provisioned.</p>"},{"location":"cloud/terraform/overview/#data-references","title":"Data references","text":"<p>Data sources allow data to be fetched or computed for use elsewhere in Terraform configuration.</p>"},{"location":"cloud/terraform/overview/#named-values","title":"Named values","text":"<ul> <li><code>&lt;RESOURCE TYPE&gt;.&lt;NAME&gt;</code> is an object representing a managed resource of the given type and name.</li> <li><code>var.&lt;NAME&gt;</code> is the value of the input variable of the given name.</li> <li><code>local.&lt;NAME&gt;</code> is the value of the local value of the given name.</li> <li><code>module.&lt;MODULE NAME&gt;.&lt;OUTPUT NAME&gt;</code> is the value of the specified output value from a child module called by the current module.</li> <li><code>data.&lt;DATA TYPE&gt;.&lt;NAME&gt;</code> is an object representing a data resource of the given data source type and name. If the resource has the count argument set, the value is a list of objects representing its instances. If the resource has the for_each argument set, the value is a map of objects representing its instances.</li> <li><code>path.module</code> is the filesystem path of the module where the expression is placed.</li> <li><code>path.root</code> is the filesystem path of the root module of the configuration.</li> <li><code>path.cwd</code> is the filesystem path of the current working directory. In normal use of Terraform this is the same as path.root, but some advanced uses of Terraform run it from a directory other than the root module directory, causing these paths to be different.</li> <li><code>terraform.workspace</code> is the name of the currently selected workspace.</li> </ul>"},{"location":"cloud/terraform/overview/#local-values","title":"local values","text":"<ul> <li>count.index - in resources that use the count meta-argument.</li> <li>each.key / each.value -  in resources that use the for_each meta-argument.</li> <li>self - in provisioner and connection blocks.</li> </ul>"},{"location":"cloud/terraform/overview/#resource-references","title":"resource references","text":"<p>The most common reference type is a reference to an attribute of a resource which has been declared either with a resource or data block</p> <pre><code>\ndata aws_resource_ami ubuntu_image {\n  ...\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = aws_resource_ami.ubuntu_image\n  instance_type = \"var.image\"\n</code></pre>"},{"location":"cloud/terraform/overview/#dynamic-blocks","title":"dynamic blocks","text":"<p>Within top-level block constructs like resources, expressions can usually be used only when assigning a value to an argument using the name = expression form.</p> <pre><code>resource \"aws_elastic_beanstalk_environment\" \"tfenvtest\" {\n  name                = \"tf-test-name\"\n  application         = \"${aws_elastic_beanstalk_application.tftest.name}\"\n  solution_stack_name = \"64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\"\n\n  dynamic \"setting\" {\n    for_each = var.settings\n    content {\n      namespace = setting.value[\"namespace\"]\n      name = setting.value[\"name\"]\n      value = setting.value[\"value\"]\n    }\n  }\n}\n</code></pre>"},{"location":"cloud/terraform/overview/#type-constraints","title":"Type Constraints","text":"<p>https://www.terraform.io/docs/configuration/types.html </p>"},{"location":"cloud/terraform/overview/#primitive-type","title":"primitive type","text":"<p>simple type that isn't made from any other types.(string | number | bool)</p>"},{"location":"cloud/terraform/overview/#complex-types","title":"Complex types","text":"<ul> <li> <p>Collection Types: Collection type allows multiple values of one other type to be grouped together as a single value. The type of value within a collection is called its element type. <code>list | map | set</code></p> </li> <li> <p>Structural types: A structural type allows multiple values of several distinct types to be grouped together as a single value. <code>object | tuple</code></p> </li> </ul> <p>Built-in-functions</p> <p>https://www.terraform.io/docs/configuration/functions.html</p> <p><code>string| chomp| format| formatlist| indent| join| lower| regex| regexall| replace| split| strrev| substr| title| trim| trimprefix| trimsuffix| trimspace| upper</code></p> <p>Numberic - <code>abs| ceil| floor| log| max| min| parseint| pow| signum</code></p>"},{"location":"cloud/terraform/overview/#state-management","title":"State Management","text":""},{"location":"cloud/terraform/overview/#state-locking","title":"state locking","text":"<p>If supported by your backend, Terraform will lock your state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting your state. State locking happens automatically on all operations that could write state.</p>"},{"location":"cloud/terraform/overview/#force-unlock","title":"force-unlock","text":"<p>Be very careful with this command. If you unlock the state when someone else is holding the lock it could cause multiple writers. Force unlock should only be used to unlock your own lock in the situation where automatic unlocking failed.</p>"},{"location":"cloud/terraform/overview/#sensitive-data-in-state","title":"Sensitive Data in State","text":"<p>Terraform state can contain sensitive data, depending on the resources in use and your definition of \"sensitive.\" The state contains resource IDs and all resource attribute. When using local state, state is stored in plain-text JSON files.</p> <p>When using remote state, state is only ever held in memory when used by Terraform. It may be encrypted at rest, but this depends on the specific remote state backend.</p> <p>Terraform Cloud always encrypts state at rest and protects it with TLS in transit. Terraform Cloud also knows the identity of the user requesting state and maintains a history of state changes. This can be used to control access and track activity. Terraform Enterprise also supports detailed audit logging.</p> <p>The S3 backend supports encryption at rest when the encrypt option is enabled. IAM policies and logging can be used to identify any invalid access. Requests for the state go over a TLS connection.</p>"},{"location":"cloud/terraform/overview/#backends","title":"backends","text":"<p>A \"backend\" in Terraform determines how state is loaded and how an operation such as apply is executed.</p>"},{"location":"cloud/terraform/overview/#benefits","title":"Benefits","text":"<ul> <li> <p>Working in a team - Backends can store their state remotely and protect that state with locks to prevent corruption</p> </li> <li> <p>Keeping sensitive information off disk - State is retrieved from backends on demand and only stored in memory</p> </li> <li> <p>Remote operations -  For larger infrastructures or certain changes, terraform apply can take a long, long time.</p> </li> <li> <p>Interactively -  Terraform will interactively ask you for the required values, unless interactive input is disabled.</p> </li> <li> <p>File: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init.</p> </li> <li> <p>Command-line key/value pairs: Key/value pairs can be specified via the init command line. </p> </li> </ul> <p>Note that many shells retain command-line flags in a history file, so this isn't recommended for secrets. To specify a single key/value pair, use the -backend-config=\"KEY=VALUE\" option when running terraform init.</p>"},{"location":"cloud/terraform/overview/#configuration-change","title":"Configuration Change","text":"<p>You can change your backend configuration at any time. You can change both the configuration itself as well as the type of backend (for example from \"consul\" to \"s3\").</p> <p>Terraform will automatically detect any changes in your configuration and request a reinitialization. As part of the reinitialization process, Terraform will ask if you'd like to migrate your existing state to the new configuration. This allows you to easily switch from one backend to another.</p>"},{"location":"cloud/terraform/overview/#unconfiguring","title":"Unconfiguring","text":"<p>If you no longer want to use any backend, you can simply remove the configuration from the file.</p>"},{"location":"cloud/terraform/overview/#local-backend","title":"local backend","text":"<p>The local backend stores state on the local filesystem, locks that state using system APIs, and performs operations locally.</p> <pre><code>terraform {\n  backend \"local\" {\n    path = \"relative/path/to/terraform.tfstate\"\n  }\n}\n\n</code></pre> <p>Render your data from the path of the terraform.tfstate that exists locally.</p> <pre><code>data \"terraform_remote_state\" \"foo\" {\n  backend = \"local\"\n\n  config = {\n    path = \"${path.module}/../../terraform.tfstate\"\n  }\n}\n</code></pre>"},{"location":"cloud/terraform/overview/#backend-types","title":"Backend Types**","text":"<ul> <li>Standard</li> <li>Enhanced</li> </ul> <p>Manual State Pull/Push You can still manually retrieve the state from the remote state using the terraform state pull command. You can also manually write state with terraform state push. This is extremely dangerous and should be avoided if possible. This will overwrite the remote state. This can be used to do manual fixups if necessary.</p>"},{"location":"cloud/terraform/overview/#debug","title":"Debug","text":"<p>Terraform has detailed logs which can be enabled by setting the TF_LOG environment variable to any value. This will cause detailed logs to appear on stderr</p> <p>You can set TF_LOG to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. TRACE is the most verbose and it is the default if TF_LOG is set to something other than a log level name.</p> <p>To persist logged output you can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled</p>"},{"location":"cloud/terraform/overview/#understand-terraform-cloud-and-enterprise","title":"Understand Terraform Cloud and Enterprise","text":"<p>Terraform Cloud is an application that helps teams use Terraform together. It manages Terraform runs in a consistent and reliable environment, and includes easy access to shared state and secret data, access controls for approving changes to infrastructure, a private registry for sharing Terraform modules, detailed policy controls for governing the contents of Terraform configurations.</p>"},{"location":"cloud/terraform/overview/#terraform-cloud-features","title":"terraform cloud features","text":"<ul> <li>Terraform Workflow</li> <li>Remote Terraform Execution</li> <li>Workspaces for Organizing Infrastructure</li> <li>Remote State Management, Data Sharing, and Run Triggers</li> <li>Version Control Integration</li> <li>Command Line Integration</li> <li>Private Module Registry</li> </ul>"},{"location":"cloud/terraform/overview/#terraform-cloud-integrations","title":"terraform cloud integrations","text":"<ul> <li>Full API</li> <li>Notifications</li> </ul>"},{"location":"cloud/terraform/overview/#acl-and-governance","title":"ACL and Governance","text":"<ul> <li>Team based permissions systems</li> <li>Sentinel policies</li> <li>Cost Estimations</li> </ul> <p>Terraform Cloud supports the following VCS providers.</p> <p><code>GitHub | GitHub.com (OAuth) | GitHub Enterprise | GitLab.com | GitLab EE and CE | Bitbucket Cloud | Bitbucket Server | Azure DevOps Server | Azure DevOps Services</code></p> <p>Terraform Enterprise, our self-hosted distribution of Terraform Cloud. It offers enterprises a private instance of the Terraform Cloud application, with no resource limits and with additional enterprise-grade architectural features like audit logging and SAML single sign-on.</p>"},{"location":"cloud/terraform/overview/#sentinel-overview","title":"Sentinel Overview","text":"<p>It enables fine-grained, logic-based policy decisions, and can be extended to use information from external sources.</p> <p>Sentinel with Terraform Cloud involves</p> <p>Defining the policies: Policies are defined using the policy language with imports for parsing the Terraform plan, state and configuration.</p> <p>Managing policies for organizations: Users with permission to manage policies can add policies to their organization by configuring VCS integration or uploading policy sets through the API</p> <p>Enforcing policy checks on runs:  Policies are checked when a run is performed, after the terraform plan but before it can be confirmed or the terraform apply is executed</p> <p>Mocking Sentinel Terraform data: Terraform Cloud provides the ability to generate mock data for any run within a workspace</p>"},{"location":"db/postgresql/install/","title":"setup and install","text":""},{"location":"db/postgresql/install/#development-environment-setup","title":"Development Environment Setup","text":"<p>In order to setup an database for development environment, we are using here an docker container and would add an sample database from postgresql-sample-database</p>"},{"location":"db/postgresql/install/#installation","title":"Installation","text":"<p>Create a new directory and file called <code>docker-compose-postgres.yml</code>.  Copy the below content into newly created file. </p> <pre><code>version: '3.8'\nservices:\n  db:\n    image: sunlnx/postgresql:v1\n    restart: always\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n    ports:\n      - '5432:5432'\n    volumes: \n      - db:/var/lib/postgresql/data\nvolumes:\n  db:\n    driver: local\n</code></pre> <p>Save the file, bring up the postgres container using  <code>docker-compose -f docker-compose-postgres.yml -d up</code> that starts the container in the background. Verify container status by <code>docker ps</code> </p>"},{"location":"db/postgresql/install/#database-restoration-using-pgadmin4","title":"Database Restoration using pgadmin4","text":"<p>You can load postgreSQL sample database as described in the documentation.</p>"},{"location":"docker_k8/docker/faq/","title":"FAQ","text":""},{"location":"docker_k8/docker/faq/#docker-networking","title":"Docker networking","text":"<p>Explain how Docker networking works.</p> <p>Docker networking provides different network drivers:</p> <ul> <li>Bridge (default for standalone containers): Containers can communicate within the same bridge network.</li> <li>Host: Removes network isolation between the container and the host.</li> <li>Overlay: Used in Swarm mode for multi-host networking.</li> <li>Macvlan: Assigns a MAC address to containers for direct communication with the physical network.</li> <li>None: No network access.</li> </ul> <p>How would you connect multiple containers in a production environment?</p> <ul> <li> <p>Use Docker Compose: Define a docker-compose.yml file to manage multi-container communication.</p> </li> <li> <p>Use Custom Networks:</p> </li> </ul> <pre><code>docker network create my_network\ndocker run --network=my_network --name=app1 my_app\ndocker run --network=my_network --name=db my_db\n</code></pre> <p>This allows containers to resolve each other using container names.</p>"},{"location":"docker_k8/docker/faq/#persistent-storage","title":"Persistent storage","text":"<p>How would you persist data in Docker containers to ensure it is not lost when the container restarts?</p> <p>Docker provides Volumes and Bind Mounts:</p> <p>Volumes (Preferred for Docker-managed storage)</p> <pre><code>docker volume create my_volume\ndocker run -d -v my_volume:/data --name my_container my_image\n</code></pre> <p>Bind Mounts (Maps a host directory to a container)</p> <pre><code>docker run -d -v /host/path:/container/path --name my_container my_image\n</code></pre> <p>For databases, use named volumes:</p> <pre><code>docker run -d -v db_data:/var/lib/mysql --name mysql_container mysql\n</code></pre>"},{"location":"docker_k8/docker/faq/#docker-img-optimize","title":"Docker img optimize","text":"<p>Your team is building a large Docker image that takes a long time to build and deploy. How would you optimize it?</p> <p>Best Practices to Optimize Docker Images:</p> <ul> <li> <p>Use a minimal base image <code>FROM python:3.9-alpine</code></p> </li> <li> <p>Leverage multi-stage builds</p> </li> </ul> <pre><code>FROM golang:1.18 AS builder\nWORKDIR /app\nCOPY . .\nRUN go build -o myapp\n\nFROM alpine:latest\nCOPY --from=builder /app/myapp /myapp\nCMD [\"/myapp\"]\n</code></pre> <ul> <li>Minimize layers and avoid unnecessary packages</li> <li>Use <code>.dockerignore</code>to exclude unnecessary files (e.g., logs, .git)</li> </ul>"},{"location":"docker_k8/docker/faq/#docker-security","title":"Docker security","text":"<ul> <li>Use minimal base images: Avoid bloated images.</li> <li>Scan images for vulnerabilities <code>docker scan my_image</code></li> <li>Run containers as non-root users</li> </ul> <pre><code>RUN addgroup -S appgroup &amp;&amp; adduser -S appuser -G appgroup\nUSER appuser\n</code></pre> <ul> <li>Restrict container privileges </li> </ul> <pre><code>docker run --security-opt no-new-privileges --read-only --cap-drop=ALL my_app\n</code></pre> <ul> <li>Limit network exposure: Use internal networking instead of exposing unnecessary ports.</li> </ul>"},{"location":"docker_k8/docker/faq/#container-failures","title":"Container Failures","text":"<p>Your application runs in Docker containers. A container crashes unexpectedly. How do you debug it?</p> <ul> <li>Check logs: <code>docker logs container_name</code></li> <li>Inspect container state : <code>docker inspect container_name</code></li> <li>Check for OOM (Out-of-Memory) issues: <code>docker stats</code> incase it has update it <code>docker run -m 512m --memory-swap 1G my_app</code></li> <li>Restart Policy: <code>docker run --restart=always my_app</code></li> </ul>"},{"location":"docker_k8/docker/faq/#cicd-docker","title":"CICD Docker","text":"<p>How would you integrate Docker into a CI/CD pipeline?</p> <p>Use GitHub Actions, Jenkins, GitLab CI/CD:</p> <pre><code>name: Docker Build &amp; Push\non:\n  push:\n    branches:\n      - main\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v2\n      - name: Build Docker image\n        run: docker build -t myrepo/myapp:latest .\n      - name: Login to Docker Hub\n        run: echo \"${{ secrets.DOCKER_PASSWORD }}\" | docker login -u \"${{ secrets.DOCKER_USERNAME }}\" --password-stdin\n      - name: Push Docker image\n        run: docker push myrepo/myapp:latest\n</code></pre> <p>Jenkinsfile</p> <pre><code>pipeline {\n    agent any\n\n    environment {\n        DOCKER_USERNAME = credentials('docker-username')  // Stored as a Jenkins credential\n        DOCKER_PASSWORD = credentials('docker-password')  // Stored as a Jenkins credential\n        IMAGE_NAME = 'myrepo/myapp:latest'\n    }\n\n    stages {\n        stage('Checkout Code') {\n            steps {\n                checkout scm\n            }\n        }\n\n        stage('Build Docker Image') {\n            steps {\n                script {\n                    sh \"docker build -t ${IMAGE_NAME} .\"\n                }\n            }\n        }\n\n        stage('Login to Docker Hub') {\n            steps {\n                script {\n                    sh \"echo ${DOCKER_PASSWORD} | docker login -u ${DOCKER_USERNAME} --password-stdin\"\n                }\n            }\n        }\n\n        stage('Push Docker Image') {\n            steps {\n                script {\n                    sh \"docker push ${IMAGE_NAME}\"\n                }\n            }\n        }\n    }\n}\n\n</code></pre>"},{"location":"docker_k8/docker/faq/#troubleshooting-issues","title":"Troubleshooting Issues","text":"<p>A container is running but you cannot access the application inside it. How do you troubleshoot?</p> <ul> <li>Check container status: <code>docker ps -a</code></li> <li>check logs: <code>docker logs my_container</code></li> <li>Inspect container networking: <code>docker inspect my_container | grep \"IPAddress\"</code></li> <li>Exec into the container: <code>docker exec -it my_container /bin/sh</code></li> <li>check port binding: <code>docker run -p 8080:80 my_app</code></li> </ul> <p>\"/bin/bash\" Exited few seconds ago </p> <p>A container lives as long as a process within it is running. If an application in a container crashes, container exits.</p> <p>unlike in other application programs like httpd, nginx, mysqld bash is not a process which is running. infact its a shell process which is listening for the input, when it don't get, it would exit the container.</p> <p>If we want to make the shell listen to some command for execution, you can find the below one. </p> <p>so we can make the container to sleep 30 seconds. </p> <pre><code>docker run ubuntu:18.04 sleep 30s\n</code></pre>"},{"location":"docker_k8/docker/overview/","title":"Overview","text":""},{"location":"docker_k8/docker/overview/#docker-architecture","title":"Docker architecture","text":"<ul> <li>Docker Daemon</li> <li>Docker Client</li> <li>Docker Registries</li> <li>Docker Objects</li> </ul> <p>Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers</p>"},{"location":"docker_k8/docker/overview/#docker-daemon","title":"Docker Daemon","text":"<p>The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>"},{"location":"docker_k8/docker/overview/#docker-client","title":"Docker Client","text":"<p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon. </p>"},{"location":"docker_k8/docker/overview/#docker-registries","title":"Docker registries","text":"<p>A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.</p>"},{"location":"docker_k8/docker/overview/#docker-objects","title":"Docker Objects","text":"<p>When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects</p> <ul> <li> <p>Images     An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization.</p> <p>Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.</p> </li> <li> <p>Containers</p> <p>A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.</p> <p>By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine.</p> <p>A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear. </p> <p>Docker defines certain policies to restart the container</p> <ul> <li>On-failure: container restarts only when a failure that occurred is not due to the user,</li> <li>Unless-stopped: container restarts only when a user executes the command to stop it,</li> <li>Always: the container is always restarted irrespective of error or other issues.</li> </ul> </li> </ul>"},{"location":"docker_k8/docker/overview/#docker-run","title":"Docker Run","text":"<ul> <li> <p>Pulls the  image: Docker checks for the presence of the ubuntu image and, if it doesn't exist locally on the host, then Docker downloads it from Docker Hub. If the image already exists, then Docker uses it for the new container.  <li> <p>Creates a new container: Once Docker has the image, it uses it to create a container. </p> </li> <li> <p>Allocates a filesystem and mounts a read-write layer: The container is created in the file system and a read-write layer is added to the image. </p> </li> <li> <p>Allocates a network / bridge interface: Creates a network interface that allows the Docker container to talk to the local host. </p> </li> <li> <p>Sets up an IP address: Finds and attaches an available IP address from a pool. </p> </li> <li> <p>Executes a process that you specify: Runs your application, and; </p> </li> <li> <p>Captures and provides application output: Connects and logs standard input, outputs and errors for you to see how your application is running. </p> </li>"},{"location":"docker_k8/docker/overview/#docker-storage","title":"Docker Storage","text":"<p>Lets discuss about the how containers are run and their association with volume mounts</p> <p>Once the Dockerfile all the commands, and when trying to build, it will create each layer of containers and finally makes a complete readonly snapshot of the image. These layes are called as image layers and they are in read-only. These intermidiate containers are stored in a cache, so incase if next build uses the same image it would be fetched from these containers, hence it will be taken very less time tp build.</p> <p>Once the image is built, we will run docker run image which will copy the executable from the image layer and writes to an read-write layer. These are called as copy-on-write. when the container runs, the storage is created in the run time and persists only until the contaniner is up. once the container is destroyed, its volume mounts are destroyed.</p> <p>In order to make containers retain their data, we would be using something called as persistant volumes, where we would explictly say to mount the data of our local paths to container paths. These are available in /var/lib/docker/volumes/.</p> <p>docker run -d -p hostport:containerport -v localdata:container image</p> <p>These types of mount are called as volume mounts</p> <p>Newer version you would be using the same using mount bind and they are called as volume binds</p> <p>docker run -d -p hostport:containerport --mount -bind src=localdata,destination=container image</p> <p>Different mount types available:</p> <ul> <li>Bind mounts: These can be stored anywhere on the host system</li> <li>Volume mount: they are managed by Docker and are stored in a part of the host filesystem.</li> <li>tmpfs mount: they are stored in the host system's memory. These mounts can never be written to the host's filesystem.</li> </ul>"},{"location":"docker_k8/docker/overview/#docker-container-lifecycle","title":"Docker Container lifecycle","text":"<ul> <li>Create phase</li> <li>Running phase</li> <li>Paused phase/unpause phase</li> <li>Stopped phase</li> <li>Killed phase</li> </ul>"},{"location":"docker_k8/docker/overview/#stateful-or-stateless","title":"stateful or stateless","text":"<p>Stateless applications should be preferred over a Stateful application for Docker Container. We can create one container from our application and take out the app's configurable state parameters. Once it is one, we can run the same container with different production parameters and other environments. Through the Stateless application, we can reuse the same image in distinct scenarios. It is also easier to scale a Stateless application than a Stateful application when it comes to Docker Containers.</p>"},{"location":"docker_k8/docker/overview/#docker-networks","title":"Docker Networks","text":"<ul> <li>bridge: Default network which the containers connect to if the network is not specified otherwise</li> <li>none: Connects to a container-specific network stack lacking a network interface</li> <li>host: Connects to the host\u2019s network stack</li> </ul> <p>default docker network</p> <pre><code>docker container run -d -p 8088:80 --name nginx-server1 nginx:alpine\ndocker inspect nginx-server1\ndocker container ps\ncurl http://localhost:&lt;port&gt;\n</code></pre> <p>custom docker network</p> <pre><code>docker network create -d bridge my-bridge-network\ndocker container run -d -p 8788:80 --network=\"my-bridge-network\" --name nginx-server2 nginx:alpine\ndocker container ps\ncurl http://localhost:&lt;port&gt;\ndocker inspect nginx-server2\n</code></pre>"},{"location":"docker_k8/docker/overview/#cmd-vs-entrypoint","title":"CMD Vs ENTRYPOINT","text":"<p>CMD provides a default arguments for the container also can be overridden when its running. </p> <pre><code>FROM Ubuntu:20.04 \nCMD [\"echo\", \"Hello from CMD\"]\n\n# docker build -t cmd-example . \n# docker run cmd-example # Output: Hello from CMD\n# docker run cmd-example \"hi there\" # Output: hi there\n</code></pre> <p>An ENTRYPOINT provides a fixed comamnd to run when container starts. its harder to override. Arguments passed during <code>docker run</code> are appended to ENTRYPOINT</p> <pre><code>FROM ubuntu:20.04\nENTRYPOINT [\"echo\", \"hello from ENTRYPOINT\"]\n\n# docker build -t entrypoint-example . \n# docker run entrypoint-example # output hello from ENTRYPOINT\n# docker run entrypoint-example \"hi there\" # output: hello from ENTRYPOINT hi there\n</code></pre> <p>Containers are meant to run a task or a process. A container lives as long as a process within it is running. If an application in a container crashes, container exits.</p> <p>Difference between the CMD and ENTRYPOINT with related to the supplied to the \"docker run\" command. While the CMD will be completely over-written by the supplied command (or args), for the ENTRYPOINT, the supplied command will be appended to it.</p> <pre><code># Dockerfile\nFROM ubuntu:20.04\n\nENTRYPOINT [\"echo\"]\nCMD [\"Hello from CMD\"]\n\ndocker build -t combined-example .\ndocker run combined-example                    # Output: Hello from CMD\ndocker run combined-example \"Custom Message\"   # Output: Custom Message\n</code></pre>"},{"location":"docker_k8/docker/overview/#example","title":"Example","text":"<p>Let's understand it with an ubuntu-sleeper example. </p> <p>When you want to make a container run, it would check for the CMD to run the process, but when we have a bash which is just a listening terminal to get the input and if we don't provide it, it would just exit the conatiner. </p> <pre><code>docker run ubutu:20.04 sleep 30 # provide an input to bash terminal for 30 sec.\n</code></pre> <p>Let's make a docker equivalent file for above command</p> <pre><code>FROM ubuntu:20.04\nCMD [\"sleep\", \"30\"]\n\ndocker build -t ubuntu-sleep .\ndocker run ubuntu-sleep\n</code></pre> <p>Container always sleep 30 sec once it started !  So what if we need to change the time ? i.e sleep 10 ?  Since its hardcoded, we would now want to make it parametrized.. </p> <pre><code>FROM ubuntu:20.04\nCMD [\"30\"]\nENTRYPOINT [\"sleep\"]\n\ndocker build -t ubuntu-sleep .\ndocker run ubuntu-sleep # sleep for 30s when no args are passed\ndocker run ubuntu-sleep 10 # sleep for 10 sec # observe that CMD has been overwritten for ENTRYPOINT\n</code></pre> <p>incase you want to override the command itself in the ENTRYPOINT, then..</p> <pre><code>docker run --entrypoint new-sleep-command ubuntu-sleep 60\n</code></pre>"},{"location":"docker_k8/docker/overview/#dockerfile","title":"Dockerfile","text":"<ul> <li> <p>FROM -  sets the base image for subsequent instructions, especially easier to start by pulling an image. </p> </li> <li> <p>MAINTAINER -  Author field of the generated images</p> </li> <li> <p>RUN - execute any commands in a new layer on top of the current image and commit the results. The resulting committed image will be used for the next step in the Dockerfile.. <code>RUN [ \"echo\", \"$HOME\" ]</code> will not do variable substitution on $HOME as exec won't invoke any command shell. if you want shell processing you need to specify the shell <code>RUN [ \"sh\", \"-c\", \"echo\", \"$HOME\" ]</code></p> </li> <li> <p>CMD - Command that needs to be executed while running container. The main purpose of a CMD is to provide defaults for an executing. These defaults can include an executable, or they can omit the executable, in which case we must specify an ENTRYPOINT instruction as well.</p> <p>CMD It has 3 forms:</p> <p>CMD [\"executable\",\"param1\",\"param2\"] (exec form, this is the preferred form) CMD [\"param1\",\"param2\"] (as default parameters to ENTRYPOINT) CMD command param1 param2 (shell form)</p> </li> <li> <p>WORDDIR AND ENV - The WORKDIR instruction sets the working directory for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile. The WORKDIR instruction can resolve environment variables previously set using ENV. The ENV instruction sets the environment variable to the value . This value will be passed to all future RUN instructions. The environment variables set using ENV will persist when a container is run from the resulting image.</p> </li> <li> <p>ADD copies new files, directories or remote file URLs from and adds them to the filesystem of the container at the path .</p> </li> <li> <p>ENTRYPOINT allows us to configure a container that will run as an executable.</p> </li> </ul>"},{"location":"docker_k8/docker/overview/#example_1","title":"Example","text":"<p>lets create Dockerfile and check above actions </p> <pre><code>FROM ubuntu:20.04\nMAINTAINER samperay\n\nRUN apt-get update &amp;&amp; apt-get install htop\nWORKDIR /root\nENV TAG Dev\n\n# build image\n\ndocker build -t demo . \ndocker images\ndocker run -it --rm demo -- /bin/bash\n\n# insise docker image\n$ pwd\n/root \n$ echo $TAG\nDev\n$ \n</code></pre> <p>Now, lets create a script and make it to run from the container</p> <pre><code># run.sh\n\n#!/bin/sh\necho \"The current directory : $(pwd)\"\necho \"The Tag variable : $TAG\"\necho \"There are $# arguments: $@\"\n</code></pre> <pre><code>FROM ubnutu:20.04\nWORKDIR /root\nENV TAG Dev\nADD run.sh /root/run.sh\nRUN chmod +x ./root/run.sh\nCMD [\"./run.sh\"]\n\n\ndocker build -t demo1 . \ndocker run -it --rm demo1\ndocker container run -it --rm demo1 ./run.sh Hello Sunil\n\n# outputs \necho \"The current directory : $(pwd)\"\necho \"The Tag variable : $TAG\"\necho \"There are $# arguments: $@\"\n</code></pre> <p>Let's discuss about the <code>CMD</code> and <code>ENTRYPOINT</code> in above code. since we pass arguments, we can use that using CMD options to provide as an input to ENTRYPOINT.</p> <pre><code>FROM ubnutu:20.04\nWORKDIR /root\nENV TAG Dev\nADD run.sh /root/run.sh\nRUN chmod +x ./root/run.sh\nENTRYPOINT [\"./run.sh\"]\nCMD [\"arg1\"]\n\n\ndocker build -t demo2 . \ndocker run -it --rm demo2  \n\n# Output: \necho \"The current directory : /root\"\necho \"The TAG variable : Dev\"\necho \"There are 1 arguments: arg1\"\n\n\ndocker container run -it --rm demo1 /bin/bash\n\necho \"The current directory : /root\"\necho \"The TAG variable : Dev\"\necho \"There are 1 arguments: /bin/bash\"\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/","title":"Istio","text":""},{"location":"docker_k8/istio/istio_overview/#monolithic","title":"Monolithic","text":"<p>Includes all the infos of security/auth/code and so on in a big giant of code. Each service depends on a specific version of another, requiring the whole package to be deployed simultaneously and often involving database scripts. </p> <p>Since all modules use the same programming language (Java) and share a single database, any issue\u2014such as the Ratings module struggling with heavy data load\u2014affects the entire system. Even minor updates require a full redeployment, making scalability and independent upgrades challenging.</p> <p> </p> <p></p> <p></p> <p>benefits of micro services over monolithic</p> Benefit Description Independent Scaling The Ratings module can scale based on customer load. Faster Releases Independent deployments lead to smaller, less risky releases. Technological Flexibility Teams can choose different programming languages for each service. Enhanced Resilience Loose coupling increases the overall system resilience, simplifying monitoring, updates, and rollbacks. Manageability Maintaining smaller, autonomous applications reduces the risk of developing a \"big ball of mud.\" <p>pro of microservices:</p> <ul> <li>scalbility</li> <li>faster, smaller releases</li> <li>tech and lan agnostic dev lifecycls</li> <li>system resilience and isolcation</li> <li>indndependent  and easy to underdtand services.</li> </ul> <p></p> <p>The above one id problem as all the security modules are to be tied in 1 place proviing a very fat code. </p> <p>Cons of mictoservices</p> <ul> <li>complex service netwirking</li> <li>security [ serv ice to service is problem as there are independednt]</li> <li>observlity ( different languages and different metrics)</li> <li>overload of traditional ops models ( diff languages)</li> </ul>"},{"location":"docker_k8/istio/istio_overview/#service-mesh","title":"service mesh","text":"<p>A service mesh is a dedicated, configurable infrastructure layer designed to manage service-to-service communications in microservices architectures without requiring modifications to your business code. With a service mesh, these tasks are offloaded to sidecar proxies deployed alongside every microservice. The network communication between services is managed by these proxies, which also form the data plane. The network communication between services is managed by these proxies, which also form the data plane. The proxies communicate with a central server-side component known as the Control Plane. The Control Plane oversees and directs all traffic entering and leaving the services, ensuring a larger, cohesive system.</p> <p></p> Capability Description Benefit Service Discovery Automatically identifies the IP addresses and ports where services are exposed. Simplifies inter-service communication without manual setup. Health Checks Continuously monitors the status of services and maintains a pool of healthy instances. Improves resilience and fault tolerance. Load Balancing Routes traffic intelligently toward healthy instances, isolating or bypassing failing ones. Optimizes resource usage and minimizes downtime. <p>Responsible for ..</p> <ul> <li>Traffic mgmt </li> <li>security</li> <li>observability</li> <li>service discovery</li> <li>discovery</li> <li>health check </li> <li>load balancing</li> </ul>"},{"location":"docker_k8/istio/istio_overview/#istio_1","title":"istio","text":"<p>Istio's architecture is divided into two main parts: the data plane and the control plane and can be run only in kubernetes cluster.</p> <p>Data plane: </p> <p>consists of Envoy proxies that are deployed alongside each service instance (or pod). These proxies handle crucial functions such as load balancing, security, and observability</p> <p>control plane</p> <p>control plane manages and configures the proxies to route traffic, enforce policies, and collect telemetry data</p> <p>It has three main components</p> <ul> <li>Citadel - Responsible for generating and managing certificates for secure communications.</li> <li>Pilot - Handles service discovery and maintains routing configurations.</li> <li>Galley - Validates configuration files to ensure correct settings.</li> </ul> <p>Above 3 combined in single service called <code>istiod</code></p> <p></p> <p>Within each pod, an Istio agent works in tandem with the Envoy proxy. The agent is responsible for delivering configuration secrets and other necessary data to ensure that the proxy operates correctly.</p>"},{"location":"docker_k8/istio/istio_overview/#install-istion","title":"install istion","text":"<ul> <li>istioctl </li> <li>istio operator install </li> <li>helm chats</li> </ul> <p>It will install istiod along with 2 other components </p> <ul> <li>istio-ingressgateway </li> <li>istio-egressgateway</li> </ul> <p>Download link - https://istio.io/latest/docs/setup/getting-started/#download</p> <pre><code>curl -L https://istio.io/downloadIstio | sh -\ncd istio-&lt;version-number&gt;\nexport PATH=$PWD/bin:$PATH\nistioctl install --set profile=demo -y\n\n\u2714 Istio core installed \u26f5\ufe0f                                                                                                                     \n\u2714 Istiod installed \ud83e\udde0                                                                                                                         \n\u2714 Egress gateways installed \ud83d\udeeb                                                                                                               .\n\u2714 Ingress gateways installed \ud83d\udeec                                                                                                              \n\u2714 Installation complete     \n\n</code></pre> <pre><code>root@controlplane ~/istio-1.28.0 \u279c  kubectl get pods -n istio-system\nNAME                                   READY   STATUS    RESTARTS   AGE\nistio-egressgateway-78bfb5f9d8-9jz8b   1/1     Running   0          52s\nistio-ingressgateway-74c94955d-q9scw   1/1     Running   0          51s\nistiod-6f789f76fc-smdxm                1/1     Running   0          60s\n\nroot@controlplane ~/istio-1.28.0 \u279c \n\n\nroot@controlplane ~/istio-1.28.0 \u279c  istioctl version\nclient version: 1.28.0\ncontrol plane version: 1.28.0\ndata plane version: 1.28.0 (2 proxies)\n\nroot@controlplane ~/istio-1.28.0 \u279c  \n\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/#deploy-application","title":"Deploy application","text":"<pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n\n\nroot@controlplane ~ \u279c  kubectl create -f istio-sample.yml \nservice/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\n\n\nroot@controlplane ~ \u2716 istioctl analyze\nInfo [IST0102] (Namespace default) The namespace is not enabled for Istio injection. Run 'kubectl label namespace default istio-injection=enabled' to enable it, or 'kubectl label namespace default istio-injection=disabled' to explicitly mark it as not needing injection.\n\nroot@controlplane ~ \u279c \n\n\nroot@controlplane ~ \u279c  kubectl delete -f istio-sample.yml \nservice \"details\" deleted\nserviceaccount \"bookinfo-details\" deleted\ndeployment.apps \"details-v1\" deleted\nservice \"ratings\" deleted\nserviceaccount \"bookinfo-ratings\" deleted\ndeployment.apps \"ratings-v1\" deleted\nservice \"reviews\" deleted\nserviceaccount \"bookinfo-reviews\" deleted\ndeployment.apps \"reviews-v1\" deleted\ndeployment.apps \"reviews-v2\" deleted\ndeployment.apps \"reviews-v3\" deleted\nservice \"productpage\" deleted\nserviceaccount \"bookinfo-productpage\" deleted\ndeployment.apps \"productpage-v1\" deleted\n\nroot@controlplane ~ \u279c \n\n\nroot@controlplane ~ \u279c  kubectl label namespace default istio-injection=enabled\nnamespace/default labeled\n\nroot@controlplane ~ \u279c  \n\nroot@controlplane ~ \u279c  \n\nroot@controlplane ~ \u279c  kubectl create -f istio-sample.yml \nservice/details created\nserviceaccount/bookinfo-details created\ndeployment.apps/details-v1 created\nservice/ratings created\nserviceaccount/bookinfo-ratings created\ndeployment.apps/ratings-v1 created\nservice/reviews created\nserviceaccount/bookinfo-reviews created\ndeployment.apps/reviews-v1 created\ndeployment.apps/reviews-v2 created\ndeployment.apps/reviews-v3 created\nservice/productpage created\nserviceaccount/bookinfo-productpage created\ndeployment.apps/productpage-v1 created\n\nroot@controlplane ~ \u279c  \n\n\n\nroot@controlplane ~ \u279c  kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\ndetails-v1-67894999b5-md2ns       2/2     Running   0          30s\nproductpage-v1-7bd5bd857c-v8nc9   2/2     Running   0          30s\nratings-v1-676ff5568f-bgs6q       2/2     Running   0          30s\nreviews-v1-f5b4b64f-gwm4c         2/2     Running   0          30s\nreviews-v2-74b7dd9f45-jbjgn       2/2     Running   0          30s\nreviews-v3-65d744df5c-9m7h9       2/2     Running   0          30s\n\nroot@controlplane ~ \u279c  \n\n\nroot@controlplane ~ \u279c  istioctl analyze\n\n\u2714 No validation issues found when analyzing namespace: default.\n\nroot@controlplane ~ \u279c  \n\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/#visualize-kiali","title":"visualize - Kiali","text":"<p>kiali can automatically generate istio configuration.</p> <p>Visualisation of a service mesh.</p> <p>Health check of a service mesh.</p> <p>Logs and metric of a service mesh.</p> <pre><code>root@controlplane ~ \u279c  kubectl apply -f /root/istio-1.20.8/samples/addons\nserviceaccount/grafana created\nconfigmap/grafana created\nservice/grafana created\ndeployment.apps/grafana created\nconfigmap/istio-grafana-dashboards created\nconfigmap/istio-services-grafana-dashboards created\ndeployment.apps/jaeger created\nservice/tracing created\nservice/zipkin created\nservice/jaeger-collector created\nserviceaccount/kiali created\nconfigmap/kiali created\nclusterrole.rbac.authorization.k8s.io/kiali-viewer created\nclusterrole.rbac.authorization.k8s.io/kiali created\nclusterrolebinding.rbac.authorization.k8s.io/kiali created\nrole.rbac.authorization.k8s.io/kiali-controlplane created\nrolebinding.rbac.authorization.k8s.io/kiali-controlplane created\nservice/kiali created\ndeployment.apps/kiali created\nserviceaccount/loki created\nconfigmap/loki created\nconfigmap/loki-runtime created\nservice/loki-memberlist created\nservice/loki-headless created\nservice/loki created\nstatefulset.apps/loki created\nserviceaccount/prometheus created\nconfigmap/prometheus created\nclusterrole.rbac.authorization.k8s.io/prometheus created\nclusterrolebinding.rbac.authorization.k8s.io/prometheus created\nservice/prometheus created\ndeployment.apps/prometheus created\n\nroot@controlplane ~ \u279c \n\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/#k8-network","title":"k8 network","text":""},{"location":"docker_k8/istio/istio_overview/#kubernetes-services","title":"kubernetes services","text":"<p>For internal cluster communication, it\u2019s crucial that pods can reliably locate and interact with each other. Although each pod is assigned its own IP, these addresses are temporary. The challenge then becomes how to enable a front-end, for instance, to consistently reach a back-end service even as individual pod IPs change. This is where Kubernetes Services become essential. </p> <p>A backend service can be configured to target a set of backend pods. Because the service itself receives a stable IP address, there is no longer any need to monitor the dynamic IPs of the individual pods.</p> <p>ClusterIP: The default and most common service type, ClusterIP, exposes the service on an internal IP address within the cluster. This type is ideal for enabling communication between applications within the same cluster.</p> <p>NodePort: NodePort exposes the service on a specific port across all nodes in the cluster. This makes it possible to access the service externally, directly via the node IP addresses.</p> <p>LoadBalancer: This service type provisions an external load balancer (supported by select cloud providers) which routes traffic to the service. It extends the functionality of NodePort by providing enhanced traffic distribution and integration with cloud load-balancing solutions.</p>"},{"location":"docker_k8/istio/istio_overview/#sidecar","title":"sidecar","text":"<p>Understanding sidecars is essential when constructing multi-container Pods, as they enable auxiliary functionalities that support the main application container.</p> <p>the primary container runs the core business logic of your application, the sidecar container is dedicated to handling tasks such as:</p> <p>Log shipping Monitoring File loading Proxying</p> <p>Understanding and effectively utilizing sidecars in your Kubernetes deployments can lead to more resilient and maintainable applications by offloading supportive tasks from the main application container</p>"},{"location":"docker_k8/istio/istio_overview/#envoy","title":"envoy","text":"<p>A proxy acts as an intermediary between a user and an application. Instead of embedding additional functionalities\u2014such as TLS encryption, authentication, and request retries\u2014directly into your application, these tasks can be offloaded to a proxy. This approach enables developers to concentrate on the core business logic while the proxy handles supplementary operations.</p> <p>Envoy operates both as a proxy and as a communication bus with advanced routing capabilities. Typically, Envoy is deployed as a sidecar container alongside your primary application containers. This design ensures that all inbound and outbound pod traffic is managed by Envoy, which enhances communication handling and offloads additional features from your application.</p>"},{"location":"docker_k8/istio/istio_overview/#minikube-installation","title":"Minikube installation","text":"<pre><code># minik8.sh\n\n#!/usr/bin/env bash\n\nset -Eeuo pipefail\n\n# Description: start/stop local k8 cluster\n# Usage: ./script.sh {start/stop}\n# Author: Sunil Amperayani\n# Version: v0.0.1\n\n#action_id=\"$1\"\nscript_name=\"${0##*/}\"\n\nlog() {\n  local level=\"$1\"\n  shift\n  echo \"$(date '+%Y-%m-%d %H:%M:%S') - ${level} - ${1}\"\n}\n\nusage() {\ncat &lt;&lt;EOF\nUsage: $script_name &lt;start|stop|status|restart&gt;\n\nCommands:\nstart:   start minikube\nstop:    stop minikube\nstatus:  show cluster status\nrestart: stop then start\n\ne.g:\n$script_name start\n$script_name stop\nEOF\n}\n\nstart_cluster() {\n  log INFO \"starting minikube..\"\n  minikube start --driver=vfkit --network=nat\n  log INFO \"start complete..!\"\n}\n\nstop_cluster() {\n  log INFO \"stopping minikube..\"\n  minikube stop\n  log INFO \"stopped\"\n\n}\n\nmain() {\n  if [[ $# -lt 1 ]]; then\n    usage\n    exit 1\n  fi\n\n  local action_id=\"$1\"\n\n  case \"${action_id}\" in\n    start)          start_cluster;;\n    stop)           stop_cluster;;\n    -h|--help|help) usage;;\n     *)\n       log Error \"unknown command: ${action_id}\"\n       usage\n       exit 2\n   esac\n}\n\nmain \"$@\"\n\n</code></pre> <p>\u279c  ~ minikube addons enable ingress \ud83d\udca1  ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub. You can view the list of minikube maintainers at: https://github.com/kubernetes/minikube/blob/master/OWNERS     \u25aa Using image registry.k8s.io/ingress-nginx/controller:v1.13.2     \u25aa Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2     \u25aa Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2 \ud83d\udd0e  Verifying ingress addon... \ud83c\udf1f  The 'ingress' addon is enabled \u279c  ~</p>"},{"location":"docker_k8/istio/istio_overview/#install-demo-setup","title":"Install demo setup","text":"<pre><code>kubectl label namespace default istio-injection=enabled\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\nkubectl get pods\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/#kiali-dashboard","title":"Kiali dashboard","text":"<pre><code>istio-1.20.8\nkubectl apply -f istio-1.20.8/samples/addons\nkubectl get pods -n istio-system\n\nkubectl get svc kiali -n istio-system\nNAME    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE\nkiali   ClusterIP   10.107.21.94   &lt;none&gt;        20001/TCP,9090/TCP   119s\n\n\n\u279c  istio-1.28.0 istioctl dashboard kiali\nhttp://localhost:20001/kiali &lt;- it will default to kiali dashboard\n</code></pre>"},{"location":"docker_k8/istio/istio_overview/#traffic-generation-kiali","title":"Traffic generation kiali","text":"<p>Configure gateway to accept traffic to service mesh from outside the cluster </p> <pre><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml\ngateway.networking.istio.io/bookinfo-gateway created\nvirtualservice.networking.istio.io/bookinfo created\n\n\n\u279c  istio-1.28.0 istioctl analyze\n2025-11-14T04:45:08.984270Z error   kube    translation function for core/v1alpha1/MeshConfig not found controller=analysis-controller\n2025-11-14T04:45:08.984648Z error   kube    translation function for core/v1alpha1/MeshNetworks not found   controller=analysis-controller\n\n\u2714 No validation issues found when analyzing namespace: default.\n\u279c  istio-1.28.0\n\n\n\u279c  istio-1.28.0 export INGRESS_HOST=$(minikube ip)\n\u279c  istio-1.28.0 echo $INGRESS_HOST\n192.168.106.3\n\u279c  istio-1.28.0\n\n\u279c  istio-1.28.0 export INGRESS_PORT=$(kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.spec.ports[?(@.name==\"http2\")].nodePort}')\n\u279c  istio-1.28.0 echo $INGRESS_PORT\n32541\n\u279c  istio-1.28.0\n\n\u279c  istio-1.28.0 curl \"http://$INGRESS_HOST:$INGRESS_PORT/productpage\"\n\n\nKeep generating traffic to view services in kiali dashboard.\n\nwhile sleep 1; do curl -sS curl 'http://'\"${INGRESS_HOST}\"':'\"${INGRESS_PORT}\"'/productpage' &amp;&gt;/dev/null; done\n\n</code></pre> <p></p> <p>we deleted the application to check how it would show in the kiali dashboard </p> <pre><code>\u279c  istio-1.28.0 kubectl get deployments\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ndetails-v1       1/1     1            1           58m\nproductpage-v1   1/1     1            1           83s\nratings-v1       1/1     1            1           58m\nreviews-v1       1/1     1            1           58m\nreviews-v2       1/1     1            1           58m\nreviews-v3       1/1     1            1           58m\n\u279c  istio-1.28.0 kubectl delete deployment productpage-v1\ndeployment.apps \"productpage-v1\" deleted from default namespace\n\u279c  istio-1.28.0 kubectl get deployments\nNAME         READY   UP-TO-DATE   AVAILABLE   AGE\ndetails-v1   1/1     1            1           58m\nratings-v1   1/1     1            1           58m\nreviews-v1   1/1     1            1           58m\nreviews-v2   1/1     1            1           58m\nreviews-v3   1/1     1            1           58m\n\u279c  istio-1.28.0\n</code></pre> <p></p> <p></p> <p>Get application back online...</p> <pre><code>istio-1.28.0 kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\nservice/details unchanged\nserviceaccount/bookinfo-details unchanged\ndeployment.apps/details-v1 unchanged\nservice/ratings unchanged\nserviceaccount/bookinfo-ratings unchanged\ndeployment.apps/ratings-v1 unchanged\nservice/reviews unchanged\nserviceaccount/bookinfo-reviews unchanged\ndeployment.apps/reviews-v1 unchanged\ndeployment.apps/reviews-v2 unchanged\ndeployment.apps/reviews-v3 unchanged\nservice/productpage unchanged\nserviceaccount/bookinfo-productpage unchanged\ndeployment.apps/productpage-v1 created\n\u279c  istio-1.28.0\n</code></pre>"},{"location":"docker_k8/istio/traffic_mgmt/","title":"Traffic Mgmt","text":""},{"location":"docker_k8/istio/traffic_mgmt/#gateway","title":"Gateway","text":"<p>Istio Gateways function as load balancers at the edge of the mesh, handling both inbound and outbound traffic. When Istio is deployed on a cluster, it automatically installs both the Istio Ingress Gateway and Istio Egress Gateway.</p> <p>the Istio Ingress Gateway intercepts all inbound traffic using Envoy proxies. Every service in the mesh is paired with an Envoy sidecar proxy, while the gateways themselves are standalone proxies positioned at the edge of the mesh.</p> <p></p> <p>Our objective is to capture all traffic arriving at the Istio Ingress Gateway for the hostname \"bookinfo.app\" and forward it to the product page service.</p> <p>Create gateway object to accept HTTP traffic on port 80 for specified hostname.</p> <pre><code># bookinfo-gateway.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: bookinfo-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"192.168.106.3\"\n</code></pre> <p>At this point, the bookinfo Gateway is configured to capture traffic coming through the default Istio Ingress Gateway for the URL \"bookinfo.app\". The following step is to define Virtual Services to correctly route this traffic to the product page service, which will be covered in a subsequent article.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#virtual-services","title":"virtual services","text":"<p>Virtual Services allow you to configure routing rules, directing incoming traffic through the Ingress Gateway to the appropriate service in your service mesh.</p> <p>When a user visits http://bookinfo.app, the request first reaches the bookinfo.app Gateway. From there, Istio routes the traffic based on defined rules. For example, you can direct all traffic for the URL bookinfo.app/productpage</p> <p>Virtual Services offer flexibility by allowing you to specify hostnames, manage traffic among different service versions, and use both standard and regex URI paths. Once a Virtual Service is created, the Istio control plane disseminates the configuration to all Envoy sidecars in the mesh.</p> <pre><code># virtual-service1.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n    - \"192.168.106.3\"\n  gateways:\n    - bookinfo-gateway\n  http:\n    - match:\n        - uri:\n            exact: /productpage\n        - uri:\n            prefix: /static\n        - uri:\n            exact: /login\n        - uri:\n            exact: /logout\n        - uri:\n            prefix: /api/v1/products\n      route:\n        - destination:\n            host: productpage\n            port:\n              number: 9080\n</code></pre> <p>This configuration instructs Istio to forward any traffic passing through the bookinfo-gateway with the host bookinfo.app and matching the specified URL patterns to the productPage service on port 9080.</p> <p></p> <p>Traffic Routing With Istio Virtual Services</p> <p>Istio overcomes these limitations by decoupling traffic routing from pod count. Using Virtual Services in conjunction with destination rules (which define subsets like v1 and v2), you can precisely control traffic percentages. For example, the following Virtual Service configuration directs 99% of traffic to subset v1 and 1% to subset v2:</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n    - route:\n        - destination:\n            host: reviews\n            subset: v1\n          weight: 99\n        - destination:\n            host: reviews\n            subset: v2\n          weight: 1\n</code></pre> <p>Even if the reviews v2 deployment scales up with more pods, the Virtual Service configuration continues to manage the traffic distribution independently. 99% of the traffic is directed to the subset \"v1\" and 1% to the subset \"v2\".  Subsets used in Virtual Services are defined in Destination Rules. These rules allow you to apply specific  configurations to traffic after it has been routed to a service.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#destination-rules","title":"Destination rules","text":"<p>Destination Rules enable you to define policies that are applied after traffic is routed to a specific service, ensuring controlled distribution and effective load balancing.</p> <p>Subsets represent groups of service instances identified by labels on the respective pods.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews-destination\nspec:\n  host: reviews\n  subsets:\n    - name: v1\n      labels:\n        version: v1\n    - name: v2\n      labels:\n        version: v2\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: reviews-v1\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: reviews\n        version: v1\n</code></pre>"},{"location":"docker_k8/istio/traffic_mgmt/#customizing-load-balancing-policies","title":"Customizing Load Balancing Policies","text":"<p>By default, Envoy uses a round-robin load-balancing strategy. However, you can modify this behavior by specifying a traffic policy within a Destination Rule</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: reviews-destination\nspec:\n  host: reviews\n  trafficPolicy:\n    loadBalancer:\n      simple: PASSTHROUGH\n  subsets:\n    - name: v1\n      labels:\n        version: v1\n    - name: v2\n      labels:\n        version: v2\n</code></pre> <p>Other types of alogorithms;</p> <ul> <li>PASSTHROUGH </li> <li>ROUND_ROBIN </li> <li>RANDOM </li> <li>LEAST_CONN </li> </ul> <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\nkubectl get gateway\nkubectl delete gateway\nkubectl create -f bookinfo-gateway.yaml\nkubectl apply -f virtual-service1.yaml\ncurl -s -HHost:bookinfo.app http://$INGRESS_HOST:$INGRESS_PORT/productpage &lt;- works internally&gt;\n\nFrom, browser: \n\nhttp://192.168.106.3:$INGRESS_PORT/productpage\n\n</code></pre> <p>Create diffrent versions of revisions </p> <pre><code>\u279c  istio-1.28.0 kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml\ndestinationrule.networking.istio.io/productpage created\ndestinationrule.networking.istio.io/reviews created\ndestinationrule.networking.istio.io/ratings created\ndestinationrule.networking.istio.io/details created\n\u279c  istio-1.28.0\n\n\u279c  istio-1.28.0 kubectl get destinationrules\nNAME          HOST          AGE\ndetails       details       48s\nproductpage   productpage   48s\nratings       ratings       48s\nreviews       reviews       48s\n\n\u279c  istio-1.28.0 kubectl describe destinationrules reviews\nName:         reviews\nNamespace:    default\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  networking.istio.io/v1\nKind:         DestinationRule\nMetadata:\n  Creation Timestamp:  2025-11-14T11:27:30Z\n  Generation:          1\n  Resource Version:    46306\n  UID:                 79566adc-5d6f-4e2a-afa1-a79686954955\nSpec:\n  Host:  reviews\n  Subsets:\n    Labels:\n      Version:  v1\n    Name:       v1\n    Labels:\n      Version:  v2\n    Name:       v2\n    Labels:\n      Version:  v3\n    Name:       v3\nEvents:         &lt;none&gt;\n\u279c  istio-1.28.0\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: reviews\nspec:\n  hosts:\n    - reviews\n  http:\n    - route:\n        - destination:\n            host: reviews\n            subset: v1\n          weight: 75\n        - destination:\n            host: reviews\n            subset: v2\n          weight: 25\n</code></pre> <pre><code>\u279c  istio-1.28.0 kubectl apply -f review-service.yaml\nvirtualservice.networking.istio.io/reviews created\n\n\u279c  istio-1.28.0 kubectl get virtualservice\nNAME       GATEWAYS               HOSTS               AGE\nbookinfo   [\"bookinfo-gateway\"]   [\"192.168.106.3\"]   6h58m\nreviews                           [\"reviews\"]         23s\n\u279c  istio-1.28.0\n</code></pre> <pre><code>\u279c  istio-1.28.0 kubectl get pods -l app=reviews,version=v1\nNAME                         READY   STATUS    RESTARTS   AGE\nreviews-v1-8cf7b9cc5-2df2j   2/2     Running   0          7h16m\n\u279c  istio-1.28.0\n\u279c  istio-1.28.0 kubectl get pods -l app=reviews,version=v2\nNAME                          READY   STATUS    RESTARTS   AGE\nreviews-v2-67d565655f-7b88c   2/2     Running   0          7h16m\n\u279c  istio-1.28.0\n\u279c  istio-1.28.0 kubectl get pods -l app=reviews,version=v3\nNAME                         READY   STATUS    RESTARTS   AGE\nreviews-v3-d587fc9d7-f9fnm   2/2     Running   0          7h16m\n\u279c  istio-1.28.0\n</code></pre> <p></p> <p></p> <p></p> <p>if you want only specific user to get the reviews page or route traffic.. </p> <pre><code>\u279c  istio-1.28.0 kubectl get virtualservice reviews -o yaml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: reviews\n  namespace: default\nspec:\n  hosts:\n  - reviews\n  http:\n  - match:\n    - headers:\n        end-user:\n          exact: sunil\n    route:\n    - destination:\n        host: reviews\n        subset: v2\n  - route:\n    - destination:\n        host: reviews\n        subset: v1\n</code></pre>"},{"location":"docker_k8/istio/traffic_mgmt/#fault-injection","title":"fault injection","text":"<p>Fault injection is a testing strategy designed to simulate errors and validate the resilience of your error handling mechanisms. Fault injection in Istio supports the simulation of two primary error types in Virtual Services:</p> <p>Delays Aborts</p>"},{"location":"docker_k8/istio/traffic_mgmt/#delays","title":"delays","text":"<p>how to inject a delay fault into a Virtual Service. In this configuration, a delay of 5 seconds is applied to 10% of the requests routed to the service.</p> <pre><code># details-virtual-service.yml\napiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: details\nspec:\n  hosts:\n  - details\n  http:\n  - fault:\n      delay:\n        percentage:\n          value: 70\n        fixedDelay: 7s\n    route:\n    - destination:\n        host: details\n        subset: v1\n</code></pre> <pre><code>kubectl apply -f details-virtualservice.yml\nvirtualservice.networking.istio.io/details created\n</code></pre> <p>refresh from web browsers. </p> <p></p>"},{"location":"docker_k8/istio/traffic_mgmt/#aborts","title":"aborts","text":"<p>you can configure abort faults to simulate scenarios where requests are rejected with specific error codes. Abort faults help test how your service behaves under error conditions, ensuring that fallback mechanisms and error handling policies are effective.</p> <p>By incorporating both delay and abort fault injections, you can thoroughly assess the robustness of your application and ensure that it remains resilient even under adverse conditions.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#timeouts","title":"timeouts","text":"<p>In a distributed microservices system, services may experience delays or failures due to various reasons. Such delays can propagate throughout the network, causing a chain reaction that impacts user experience. Timeouts prevent a single slow service from adversely affecting the overall system. When a dependent service exceeds a configured waiting period, it automatically fails and returns an error, keeping the rest of the network responsive.</p> <p>For example, if the rating service slows down, requests may get queued at the review service and the product page service. Similarly, if the details service becomes unresponsive, the product page service experiences delays, ultimately affecting all users. Implementing timeouts ensures that a service will not wait indefinitely for a slow response, thereby isolating faults and preserving system resilience.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#configure-timeouts","title":"Configure timeouts","text":"<p>Consider a scenario with a details service and a booking service that routes traffic to the product page. To prevent the system from waiting indefinitely for a response from the product page service, you can configure a timeout such that if the service takes longer than three seconds to respond, the request is automatically rejected. This is accomplished by adding a timeout option in the service configuration.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: bookinfo\nspec:\n  hosts:\n    - \"bookinfo.app\"\n  gateways:\n    - bookinfo-gateway\n  http:\n    - match:\n        - uri:\n            exact: /productpage\n        - uri:\n            prefix: /static\n      route:\n        - destination:\n            host: productpage\n            port:\n              number: 9080\n      timeout: 3s\n</code></pre> <p>details service is as-usual and does not include a timeout, its setup to route traffic normally.</p> <p>simulate fault injections </p> <p>To test how the system handles delayed responses, you can simulate a fault. For instance, introduce a fixed delay of five seconds for 50% of the traffic directed to the details service. This fault injection approach helps in validating the resilience of the system by forcing the product page service's three-second timeout to trigger. By configuring timeouts and simulating delays through fault injection, you can ensure that your microservices architecture remains agile and responsive even when individual components face issues.</p> <pre><code>apiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: details\nspec:\n  hosts:\n  - details\n  http:\n  - fault:\n      delay:\n        percentage:\n          value: 100\n        fixedDelay: 5s\n    route:\n    - destination:\n        host: details\n        subset: v1\n</code></pre> <pre><code>kubectl apply -f details-virtualservice.yml\nvirtualservice.networking.istio.io/details configured\n</code></pre>"},{"location":"docker_k8/istio/traffic_mgmt/#retries","title":"retries","text":"<p>retries work with Virtual Services to improve service resiliency. By offloading retry logic to your Virtual Services configuration, you avoid embedding complex error-handling code within your application, resulting in cleaner and more maintainable code. Retries enable your service to automatically attempt to reconnect when a connection failure occurs between services. This helps mitigate transient network issues without modifying your application code.</p> <p>When one service fails to connect to another, Virtual Services can be set up to automatically retry the operation. The main parameters for configuration are:</p> <p>Attempts: The number of times Istio will try to route the request. Per Try Timeout: The timeout duration for each individual retry.</p> <p>By default, Istio is configured with a 25-millisecond interval between retries and retries twice before returning an error. However, these default settings can be customized to better match your environment's requirements.</p> <pre><code>retry-virtual-service.yml\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: my-service\nspec:\n  hosts:\n  - my-service\n  http:\n  - route:\n    - destination:\n        host: my-service\n        subset: v1\n    retries:\n      attempts: 3 # The number of retry attempts before giving up.\n      perTryTimeout: 2s # The maximum duration to wait for each retry attempt.\n</code></pre> <p>By adjusting the retry settings, you can control how long to wait between attempts and when to stop retrying, thereby enhancing the overall resilience of your application.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#circuit-breaking","title":"circuit breaking","text":"<p>Circuit breaking plays a vital role in preventing cascading failures within a microservices ecosystem. When a service\u2014like the product service\u2014relies on other services such as reviews or details, any degradation in performance or failures in these dependencies can lead to a buildup of requests. For example, if the details service becomes unresponsive or slow, it causes requests from the product service to accumulate, leading to increased delays and possible system strain.</p> <p>Circuit breaking proactively fails requests that depend on an unresponsive service, avoiding long wait times and improving overall system resilience.</p> <p>In a microservices architecture, when one service fails or becomes slow, the subsequent requests are not allowed to pile up. Instead, the circuit breaker immediately interrupts the request flow, marking them as failed. This approach not only protects the application from being overwhelmed but also serves to limit the number of concurrent requests that can be sent to a particular service endpoint.</p>"},{"location":"docker_k8/istio/traffic_mgmt/#implementing-circuit-breaking","title":"Implementing Circuit Breaking","text":"<p>circuit breaking is configured through Destination Rules. The following example demonstrates the configuration for the Product Page Destination Rule, which restricts the number of concurrent TCP connections to three:</p> <pre><code>circuit_breaking.yaml\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: productpage\nspec:\n  host: productpage\n  subsets:\n    - name: v1\n      labels:\n        version: v1\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 3\n</code></pre> <pre><code>apiVersion: networking.istio.io/v1\nkind: VirtualService\nmetadata:\n  name: productpage\nspec:\n  hosts:\n  - productpage\n  http:\n  - route:\n    - destination:\n        host: productpage\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/","title":"Application Life Cycle Management","text":"<p>Understanding the container lifecycle is crucial ..</p> <ol> <li>Image Pulling - pulls the container image (e.g., nginx:latest) from the image registry to the node. With correct registry credentials, this step completes successfully</li> <li>Container Configuration - creates the container configuration by setting environment variables, command arguments, resource limits, volume mounts, network settings, security contexts..etc (incase failed - CreateContainerConfigError )</li> <li>Container Creation - The container runtime (e.g., containerd or Docker) creates the container using the pulled image. This involves establishing the filesystem and Linux namespaces. (incase failed - CreateContainerError)</li> <li>Container Start - the container's process starts by executing the defined command or entry point. Errors occurring at this stage are often referred to as run container errors, signaling problems with the process startup.</li> </ol>"},{"location":"docker_k8/k8/applifecyclemgmt/#rollout-and-versioning-in-a-deployment","title":"Rollout and Versioning in a Deployment","text":"<p>when you first create deployment, it will create a rollout which trigger deployment ( e.g v1), future when the application is upgraded meaning when the container version is updated to a new one a new rollout is triggered and a new deployment revision is created named revision (v2). These revisions help you track changes and enable rollbacks to previous versions if issues arise.</p> <p>Events indicate that the old ReplicaSet is scaled down to zero before scaling up the new ReplicaSet.</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#deployment-strategies","title":"Deployment Strategies","text":"<p>There are 2 types of deployment strategies</p> <ul> <li>Recreate</li> <li>RollingUpdate (Default Strategy)</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#recreate","title":"Recreate","text":"<p>One way to upgrade these to a newer version is to destroy all of these and then create newer versions of application instances meaning first destroy the running instances and then deploy the new instances of the new application version.</p> <p>The problem with this as you can imagine is that during the period after the older versions are down and before any newer version is up the application is down and inaccessible to users this strategy is known as the Recreate strategy.</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#rollingupdate","title":"RollingUpdate","text":"<p>Instead we take down the older version and bring up a newer version one by one. This way the application never goes down and the upgrade is seamless. In other words rolling update is the default deployment strategy so we talked about upgrades.</p> <p>How exactly do you update your deployment when you say update.</p> <p>It could be different things such as updating your application version by updating the version of docker containers used, updating their labels or updating the number of replicas etc. Since we already have a deployment definition file it is easy for us to modify this file once we make the necessary changes.</p> <p>we run the kubectl apply command to apply the changes. A new rollout is triggered and a new revision after deployment is created.</p> <pre><code>$ kubectl describe deployment nginxdeps\n$ kubectl rollout status deployment/nginxdeps\n$ kubectl rollout history deployment/nginxdeps\n$ kubectl rollout undo deployment/nginxdeps\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#configuring-applications","title":"Configuring Applications","text":"<ul> <li>Configuring Command and Arguments on applications</li> <li>Configuring Environment Variables</li> <li>Configuring Secrets</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#configuring-command-and-arguments-on-applications","title":"Configuring Command and Arguments on applications","text":"<p>when you specify a command field in the Pod spec, it completely replaces both the Docker ENTRYPOINT and CMD. It doesn't append to or modify the ENTRYPOINT.</p> <p></p> <p><code>python app.py --color green</code> - This would only happen if the Pod used args instead of command.  <code>python app.py</code> - the Pod manifest\u2019s command: [\"--color\",\"green\"] overrides the Dockerfile\u2019s ENTRYPOINT (python app.py), not just the CMD</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#configure-env","title":"Configure env","text":"<p>To set an environment variable set an env property in pod defination file.</p> <p>There are other ways of setting the environment variables such as, - ConfigMaps - Secrets</p> <pre><code>containers:\n- name: ubuntu-sleeper\n  image: ubuntu-sleeper\n  command: [\"sleep2.0\"]\n  args: [\"10\"]\nenv:\n- name: APP_COLOR\n  value: pink\n</code></pre>"},{"location":"docker_k8/k8/applifecyclemgmt/#configmaps","title":"ConfigMaps","text":"<p>There are 2 phases involved in configuring ConfigMaps.</p> <ol> <li>create the configMaps</li> </ol> <p>config_maps</p> <ol> <li>Inject then into the pod.</li> </ol> <p>config_maps_pod</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#secrets","title":"Secrets","text":"<p>How Kubernetes handles secrets ?</p> <ul> <li>A secret is only sent to a node if a pod on that node requires it.</li> <li>Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.</li> <li>Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.</li> </ul> <p>secrets</p> <p>encoding: echo -n 'mysql' | base64 decoding: echo -n 'bXlzcWw=' | base64 --decode</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#multicontainer-pod","title":"MultiContainer Pod","text":"<p>There are at times you need to have an container which has two or more services required to be available ( e.g webserver &amp; log agent). They need to co-exists and hence they are create/deleted at the same time. Such cases we are required to have multi container pods. They would share same namespaces, network isolations etc but we are not sure as to which would start first. </p> <p>There are 3 common patterns, when it comes to designing multi-container PODs. </p> <ul> <li>regular co-existing containers (app and db)</li> </ul> <p>multicontainerpods</p> <ul> <li>init containers (start before main application as helper and dies once done. (api startup or db ready))</li> </ul> <p>initcontainer</p> <p>multi_init_containers</p> <ul> <li>sidecar containers (starts before the application and continues to run.(istio, file beat etc))</li> </ul> <p>sidecar</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#autoscaling","title":"autoscaling","text":"<p>Two primary scaling strategies in Kubernetes:</p> <ul> <li>Scaling workloads \u2013 adding or removing containers (Pods) in the cluster.</li> <li>Horizontal scaling: Create more Pods.</li> <li> <p>Vertical scaling: Increase resource limits and requests for existing Pods.</p> </li> <li> <p>Scaling the underlying cluster infrastructure \u2013 adding or removing nodes (servers) in the cluster.</p> </li> <li>Horizontal scaling: Add more nodes to the cluster.</li> <li>Vertical scaling: Increase resources (CPU, memory) on existing nodes</li> </ul>"},{"location":"docker_k8/k8/applifecyclemgmt/#hpa","title":"HPA","text":"<p>Horizontal Pod Autoscalar</p> <p>hpa</p>"},{"location":"docker_k8/k8/applifecyclemgmt/#vpa","title":"VPA","text":"<p>Vertical Pod Autoscalar</p>"},{"location":"docker_k8/k8/architecture/","title":"Kubernetes Overview","text":""},{"location":"docker_k8/k8/architecture/#k8-architecture-components","title":"k8 Architecture components","text":"<p>Kubernetes consists of nodes which are physcial or cloud which hosts applications on the nodes in form of containers.</p> <p>The master node in the Kubernetes cluster is responsible for managing the Kubernetes cluster storing information regarding the different nodes planning which containers cause where monitoring the notes and containers on them etc.</p> <p>The Master node does all of these using a set of components together known as the control plane components.</p>"},{"location":"docker_k8/k8/architecture/#etcd","title":"etcd","text":"<p>details about the container informations like when it was loaded, which container is placed on which nodes etc. These are stored as DB in key/value format.</p> <ul> <li>The ETCD Datastore stores information regarding the cluster such as Nodes, PODS, Configs, Secrets, Accounts, Roles, Bindings and Others.</li> <li>Every information you see when you run the kubectl get command is from the ETCD Server.</li> </ul>"},{"location":"docker_k8/k8/architecture/#kube-api","title":"kube-api","text":"<p>The kube-apiserver is the primary management component of kubernetes. The kube-api server is responsible for orchestrating all operations within the cluster. it exposes kubernetes API which is used by external users to perform mgmt operations on the cluster as well as the various controllers to monitor the state of the cluster and make the necessary changes as required and by the worker nodes to communicate with the server.</p> <p>Kube-apiserver is responsible for authenticating, validating requests, retrieving and Updating data in ETCD key-value store. In fact kube-apiserver is the only component that interacts directly to the etcd datastore. The other components such as kube-scheduler, kube-controller-manager and kubelet uses the API-Server to update in the cluster in thier respective areas.</p>"},{"location":"docker_k8/k8/architecture/#kubernetes-components-overview","title":"Kubernetes Components Overview","text":"Component Role Command/Action Example kubectl CLI tool to send API requests <code>kubectl get nodes</code> Kube API Server Central component for processing, authenticating, and validating requests Processes API requests and interacts with etcd Scheduler Monitors API Server for unassigned pods and assigns them to worker nodes Automatically assigns node to newly created pods Kubelet Runs on worker nodes to manage pod lifecycle and report status Interacts with container runtime to deploy images etcd Distributed key-value store used for saving cluster configuration Stores all cluster state data"},{"location":"docker_k8/k8/architecture/#control-managers","title":"Control Managers","text":"<p>Kube Controller Manager manages various controllers in kubernetes.</p> <p>In kubernetes terms, a controller is a process that continously monitors the state of the components within the system and works towards bringing the whole system to the desired functioning state.</p> <ul> <li> <p>Node-controller : The node-controller takes care of nodes. They're responsible for onboarding new nodes to the cluster handling situations where nodes become unavailable or get gets destroyed.</p> <ul> <li>Monitoring Nodes</li> <li>Node Monitor Period 5s</li> <li>Node Monitoring Grace Period Status 40s</li> <li>POD Eviction timeout 5m</li> </ul> </li> <li> <p>Replication-Controller : Ensures that the desired number of containers are running at all times in your replication group.</p> </li> </ul>"},{"location":"docker_k8/k8/architecture/#kube-scheduler","title":"kube-scheduler","text":"<p>kube-scheduler is responsible for scheduling pods on nodes.The kube-scheduler is only responsible for deciding which pod goes on which node. It doesn't actually place the pod on the nodes, thats the job of the kubelet. - Decision factors   - Resource requirements and Limits   - Taints and Tolerations   - Node Selectors and Affinity</p>"},{"location":"docker_k8/k8/architecture/#kubelet","title":"kubelet","text":"<p>Kubelet is the sole point of contact for the kubernetes cluster</p> <p>A kubelet is an agent that runs on each node in a cluster. It listens for instructions from the kube-api server and deploys or destroys containers on the nodes as required. The kube-api server periodically fetches status reports from the kubelet to monitor the state of nodes and containers on them.</p>"},{"location":"docker_k8/k8/architecture/#kube-proxy","title":"kube proxy","text":"<p>Within Kubernetes Cluster, every pod can reach every other pod, this is accomplish by deploying a pod networking cluster to the cluster. Kube-Proxy is a process that runs on each node in the kubernetes cluster.</p> <p>The Kube-proxy service ensures that the necessary rules are in place on the worker nodes to allow the containers running on them to reach each other.</p>"},{"location":"docker_k8/k8/architecture/#communication-on-kubernetes-mgmt-plane","title":"Communication on kubernetes mgmt plane","text":"<p>when user types kubectl get nodes, kube-api server authenticates the requests, validates it and gets information by etcd which stores all the dats using key/value pair. when new pod is created, kubeapi server authenticates first, and then validated. it then creates a pod without assigning it to a node, updates information in etcd and etcd reverts back to kube-api that information has been updated.</p> <p>Now, you have a container which is not assigned to any node, this is being seen by kube-scheduler  which verifies the requirements of container and would assign to right nodes and it would update the kube-api as to which nodes this container has to be scheduled, then kube-api would be send this request to etcd to update the inforamtion as to which nodes this container needs to be scheduuled, after which kube-api would pass this information to kubelet to start the container in appropriate worker node. kubelet then creates pod on the node and instructs the container runtime engine to deploy the image. Once done kubelet updates back inforamtion to kube-api which then sends back information to etcd</p>"},{"location":"docker_k8/k8/clustermaintenance/","title":"Cluster Maintenance","text":""},{"location":"docker_k8/k8/clustermaintenance/#os-upgrades","title":"OS Upgrades","text":"<p>if there are any deployments that are on the nodes, we would like to move to another node hence we <code>cordon</code> the node and <code>drain</code> it, so that the existing applications(deployments) would be re-created into another node.</p> <pre><code>kubectl drain node01\nkubectl describe node node01 | grep -i taint\nkubectl cordon node01\nkubectl describe node node01 | grep -i taint\nkubectl drain --ignore-daemonsets --force\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#cluster-upgrade-process","title":"Cluster Upgrade process","text":"<p>first, check how many nodes does exists and nodestatus.</p> <pre><code>kubectl describe node node01 | grep -i taint\nkubectl describe node master | grep -i taint\n</code></pre> <p>Check for the latest version of the cluster using <code>kubeadm</code> tool</p> <pre><code>kubeadm upgrade plan\n</code></pre> <p>Once you know it has to be upgraded, upgrade one node followed by another.</p> <pre><code>kubectl cordon master\nkubectl drain master --ignore-daemonsets\n</code></pre> <p>On master,</p> <pre><code>apt update\napt install kubeadm=1.19.0-00\nkubeadm upgrade apply v1.19.0\napt install kubelet=1.19.0-00\nsystem restart kubelet\n</code></pre> <pre><code>kubectl uncordon master\nkubectl describe node master | grep -i taint\n</code></pre> <p>then, worker node</p> <pre><code>apt update\napt install kubeadm=1.19.0-00\napt install kubelet=1.19.0-00\nsystem restart kubelet\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#etcd-backup","title":"etcd backup","text":"<pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt\n--cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key\nsnapshot save /opt/snapshot-pre-boot.db\n</code></pre>"},{"location":"docker_k8/k8/clustermaintenance/#etcd-restore","title":"etcd restore","text":"<pre><code>ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup\nsnapshot restore /opt/snapshot-pre-boot.db\n</code></pre> <p>Modify, /etc/kubernetes/manifests/etcd.yaml and in the hostPath mention: <code>/var/lib/etcd-from-backup</code> wait for few minutes, then check the applications. </p>"},{"location":"docker_k8/k8/cmdref/","title":"kube reference","text":""},{"location":"docker_k8/k8/cmdref/#imperative-commands","title":"imperative commands","text":"<p>The imperative approach in Kubernetes involves executing specific commands to create, update, or delete objects. This method instructs Kubernetes on both what needs to be done and how it should be done. Use this method for rapid execution when you need to quickly create or modify Kubernetes objects,</p> <pre><code>kubectl run --image=nginx nginx\nkubectl create deployment --image=nginx nginx\nkubectl expose deployment nginx --port 80\nkubectl edit deployment nginx\nkubectl scale deployment nginx --replicas=5\nkubectl set image deployment nginx nginx=nginx:1.18\nkubectl create -f nginx.yaml\nkubectl replace -f nginx.yaml\nkubectl delete -f nginx.yaml\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#declarative","title":"Declarative","text":"<p>The declarative approach enables you to specify the desired state of your infrastructure through configuration files (typically written in YAML).</p> <p>This approach is recommended for complex, long-term management scenarios. It enables a systematic management of configurations via YAML files, ensuring every change is recorded and version-controlled.</p>"},{"location":"docker_k8/k8/cmdref/#pods","title":"pods","text":"<p>kubectl run nginx --dry-run=client --image nginx -o yaml &gt; nginx.yaml kubectl create service nodeport nginx-service --tcp=80:80 --dry-run=client -o yaml &gt;&gt;nginx.yaml</p> <pre><code># nginx.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: nginx\n    app: frontend\n  name: nginx-application\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: frontend\n    run: nginx\n  name: nginx-service\nspec:\n  ports:\n  - name: nginx-port-80\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: frontend\n  type: NodePort\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#pod-scheduler","title":"pod scheduler","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    run: nginx\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n  nodeName: node01\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#taint-and-tolerations","title":"taint and tolerations","text":"<pre><code>\nkubectl taint node node01 node-role.kubernetes.io/control-plane:NoSchedule\nkubectl taint node node01 node-role.kubernetes.io/control-plane:NoSchedule-\n\n\nkubectl taint node node02 spray=mortein:NoSchedule # imperative method\n\n# pod definition using declarative method.\ntolerations:\n  - key: spray\n    value: mortein\n    effect: NoSchedule\n    operator: Equal\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#deployment","title":"Deployment","text":"<p>kubectl create deployment --image=nginx nginxdeployment --replicas=2 --dry-run=client -o yaml &gt; nginxdeps.yaml kubectl create svc nodeport nginxdeps --tcp=80:80 -o yaml &gt;&gt;nginxdeps.yaml</p> <pre><code># nginxdeps.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginxdeps\n  name: nginxdeps\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginxdeps\n  strategy: {}\n  template:\n    metadata:\n      labels:\n        app: nginxdeps\n    spec:\n      containers:\n      - image: sunlnx/mac4linux:v1\n        name: nginx\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginxdeps\n  name: nginxdeps\n  namespace: default\nspec:\n  ports:\n  - name: 80-80\n    port: 80 # virtual port on the service within the cluster\n    protocol: TCP\n    targetPort: 80 # port on the Pod where the application listens \n    nodePort: 30008 # defined the static nodeport\n  selector:\n    app: nginxdeps\n  type: NodePort # maps the external request to the specific port on the node\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#services","title":"Services","text":"<pre><code>            +-----------------------+\n            |     Your Browser      |\n            +-----------------------+\n                      |\n                      | 1. Access using Node IP + NodePort\n                      v\n            http://&lt;Node-IP&gt;:&lt;NodePort&gt;\n                      |\n                      v\n    +-------------------------------------------+\n    |              Kubernetes Node              |\n    |-------------------------------------------|\n    |                                           |\n    |   NodePort Service (auto: 30000\u201332767)    |\n    |       Example: NodePort 31333             |\n    |                                           |\n    +-------------------------------------------+\n                      |\n                      | 2. NodePort forwards traffic\n                      v\n    +-------------------------------------------+\n    |                Service (ClusterIP)         |\n    |-------------------------------------------|\n    | Name: nginxdeps                            |\n    | port: 80                                   |\n    | selector: app=nginxdeps                    |\n    +-------------------------------------------+\n                      |\n                      | 3. Service forwards to pod's targetPort\n                      v\n    +-------------------------------------------+\n    |                  Pod (nginx)              |\n    |-------------------------------------------|\n    | containerPort / targetPort: 80            |\n    | Image: nginx:latest                       |\n    +-------------------------------------------+\n                      |\n                      v\n               nginx serves response\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#labels-and-selectors","title":"labels and selectors","text":"<pre><code>kubectl get pods -l app=nginx\nkubectl get pods -l app=nginx tier=frontend\nkubectl get all -l app=nginx tier=frontend\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#cmd-and-args","title":"cmd and args","text":"<pre><code># command\napiVersion: v1 \nkind: Pod \nmetadata:\n  name: ubuntu-sleeper-2 \nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu\n    command:\n      - \"sleep\"\n      - \"5000\"\n</code></pre> <pre><code># green.yaml\napiVersion: v1 \nkind: Pod \nmetadata:\n  name: webapp-green\n  labels:\n      name: webapp-green \nspec:\n  containers:\n  - name: simple-webapp\n    image: kodekloud/webapp-color\n    command: [\"python\", \"app.py\"]\n    args: [\"--color\", \"green\"] \n</code></pre>"},{"location":"docker_k8/k8/cmdref/#dockerfile-and-kubernetes-args","title":"Dockerfile and kubernetes args","text":"<pre><code>FROM python:3.6-alpine\nRUN pip install flask\nCOPY . /opt/\nEXPOSE 8080\nWORKDIR /opt\nENTRYPOINT [\"python\", \"app.py\"]\nCMD [\"--color\", \"red\"]\n</code></pre> <pre><code>apiVersion: v1 \nkind: Pod \nmetadata:\n  name: webapp-green\n  labels:\n      name: webapp-green \nspec:\n  containers:\n  - name: simple-webapp\n    image: kodekloud/webapp-color\n    command: [\"--color\",\"green\"]\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#env-variables","title":"env variables","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: webapp-color\n  name: webapp-color\n  namespace: default\nspec:\n  containers:\n    - name: webapp-color\n      image: kodekloud/webapp-color\n      env:\n        - name: APP_COLOR\n          value: green\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#config-maps","title":"config maps","text":"<p>kubectl create configmap webapp-color --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=green kubectl create configmap dbconfig --from-file=dbconfig.properties</p> <pre><code># configmap.yaml\napiVersion: v1\ndata:\n  APP_COLOR: darkblue\n  APP_OTHER: green\nkind: ConfigMap\nmetadata:\n  name: webapp-color\n  namespace: default\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#configmap-into-pod","title":"configmap into pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: webapp-color\n  name: webapp-color\n  namespace: default\nspec:\n  containers:\n    - name: webapp-color\n      image: kodekloud/webapp-color\n      env:\n        - name: APP_COLOR\n          valueFrom: \n            configMapKeyRef:\n              name: webapp-config-map\n              key: APP_COLOR\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#secrets","title":"secrets","text":"<p>kubectl create secret generic app-secret  --from-literal=DB_Host=mysql kubectl create secret generic app-secret --from-file=app_secret.properties</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-secret\ndata:\n  DB_Host: c3FsMDE=\n  DB_User: cm9vdA==\n  DB_Password: cGFzc3dvcmQxMjM=\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#inject-secrets-into-pod","title":"inject secrets into pod","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: simple-webapp-color\n  labels:\n    name: simple-webapp-color\nspec:\n  containers:\n  - name: simple-webapp-color\n    image: simple-webapp-color\n    ports:\n    - containerPort: 8080\n    envFrom:\n    - secretRef:\n        name: db-secret\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#multicontainer-pods","title":"Multicontainer pods","text":""},{"location":"docker_k8/k8/cmdref/#reguler-multipod-containers","title":"reguler multipod containers","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-redis\n  labels:\n    name: nginx-redis\nspec:\n  containers:\n      - name: nginx\n        image: nginx:latest\n      - name: redis\n        image: redis\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#initcontainer","title":"initcontainer","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: initcontainer\n  labels:\n    name: initcontainer\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'git clone &lt;some-repository-that-will-be-used-by-application&gt; ; done;']\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#multi_init","title":"multi_init","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi_init\n  labels:\n    name: multi_init\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#sidecar","title":"sidecar","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: app\n  name: app\n  namespace: elastic-stack\nspec:\n  initContainers:\n  - name: sidecar\n    image: kodekloud/filebeat-configured\n    restartPolicy: Always\n    volumeMounts:\n      - name: log-volume\n        mountPath: /var/log/event-simulator\n\n  containers:\n  - image: kodekloud/event-simulator\n    name: app\n    resources: {}\n    volumeMounts:\n    - mountPath: /log\n      name: log-volume\n\n  volumes:\n  - hostPath:\n      path: /var/log/webapp\n      type: DirectoryOrCreate\n    name: log-volume\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#flask-deployment-app","title":"flask deployment app","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-web-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-app\n  template:\n    metadata:\n      labels:\n        app: flask-app\n    spec:\n      containers:\n      - name: flask\n        image: rakshithraka/flask-web-app\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: flask-web-app-service\nspec:\n  type: ClusterIP\n  selector:\n    app: flask-app\n  ports:\n   - port: 80\n     targetPort: 80 \n</code></pre>"},{"location":"docker_k8/k8/cmdref/#hpa","title":"HPA","text":"<p>kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80 kubectl get hpa kubectl event hpa</p> <p>This command configures the \"my-app\" deployment to maintain 50% CPU utilization, scaling the number of pods between 1 and 10: kubectl autoscale deployment my-app --cpu-percent=50 --min=1 --max=10</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 7\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        resources:\n         requests: # manual setting hpa limits\n           cpu: 100m  \n         limits:\n           cpu: 200m\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: nginx-deployment\nspec:\n  minReplicas: 1\n  maxReplicas: 3\n  metrics:\n  - resource:\n      name: cpu\n      target:\n        averageUtilization: 80\n        type: Utilization\n    type: Resource\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment\nstatus:\n  currentMetrics: null\n  desiredReplicas: 0\n  currentReplicas: 0\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#csr","title":"csr","text":"<pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: akshay\nspec:\n  groups:\n  - system:authenticated\n  request: &lt;cat akshay.csr | base64 -w 0&gt;\n  usages:\n  - client auth\n</code></pre> <pre><code>kubectl get csr \n# akshay would be in pending state. we need to approve it. \n\nkubectl certificate approve akshay\nkubectl certificate deny agent-smith\nkubectl delete csr agent-smith\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#kubeconfig","title":"kubeconfig","text":"<pre><code># kubectl config -h\n\n  current-context   Display the current-context\n  delete-cluster    Delete the specified cluster from the kubeconfig\n  delete-context    Delete the specified context from the kubeconfig\n  delete-user       Delete the specified user from the kubeconfig\n  get-clusters      Display clusters defined in the kubeconfig\n  get-contexts      Describe one or many contexts\n  get-users         Display users defined in the kubeconfig\n  rename-context    Rename a context from the kubeconfig file\n  set               Set an individual value in a kubeconfig file\n  set-cluster       Set a cluster entry in kubeconfig\n  set-context       Set a context entry in kubeconfig\n  set-credentials   Set a user entry in kubeconfig\n  unset             Unset an individual value in a kubeconfig file\n  use-context       Set the current-context in a kubeconfig file\n  view              Display merged kubeconfig settings or a specified kubeconfig\nfile\n</code></pre> <pre><code># /root/my-kube-config\napiVersion: v1\nkind: Config\n\nclusters:\n- name: production\n  cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: development\n  cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: kubernetes-on-aws\n  cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: test-cluster-1\n  cluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\ncontexts:\n- name: test-user@development\n  context:\n    cluster: development\n    user: test-user\n\n- name: aws-user@kubernetes-on-aws\n  context:\n    cluster: kubernetes-on-aws\n    user: aws-user\n\n- name: test-user@production\n  context:\n    cluster: production\n    user: test-user\n\n- name: research\n  context:\n    cluster: test-cluster-1\n    user: dev-user\n\nusers:\n- name: test-user\n  user:\n    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt\n    client-key: /etc/kubernetes/pki/users/test-user/test-user.key\n- name: dev-user\n  user:\n    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt\n    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key\n- name: aws-user\n  user:\n    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt\n    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key\n\ncurrent-context: test-user@development\npreferences: {}\n</code></pre> <pre><code>kubectl config --kubeconfig=/root/my-kube-config use-context research\nkubectl config --kubeconfig=/root/my-kube-config current-context\n</code></pre> <p>If you need to set another kubeconfig file in the bash environment.. you need to expose KUBECONFIG in ~/.bashrc</p> <pre><code>vim ~/.bashrc\nexport KUBECONFIG=/root/my-kube-config \nsource ~/.bashrc\nkubectl config get-contexts\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#role-and-rolebindings","title":"role and rolebindings","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: developer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"list\", \"create\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: dev-user-binding\nsubjects:\n- kind: User\n  name: dev-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <pre><code>kubectl get roles\nkubectl get rolebindings\nkubectl describe role developer\nkubectl describe rolebinding devuser-developer-binding\n\nkubectl auth can-i create deployments\nkubectl auth can-i create pods --as dev-user\nkubectl auth can-i create pods --as dev-user -n namespace\n</code></pre> <p>Limited permission on the resources</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: developer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"create\", \"update\"]\n  resourceNames: [\"blue\", \"orange\"] # restrict access \"blue\" and \"orange\" pods\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#rbac-practice-examples","title":"RBAC practice examples","text":""},{"location":"docker_k8/k8/cmdref/#example-1-read-only-access-to-pods-in-a-namespace","title":"Example 1 : Read-only access to Pods in a namespace","text":"<pre><code>kubectl create namespace demo\n\n# pod-reader-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: demo\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n\nkubectl apply -f pod-reader-role.yaml\n\nkubectl create sa demo-user -n demo\n\n# bind-sa.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods-binding\n  namespace: demo\nsubjects:\n  - kind: ServiceAccount\n    name: demo-user\n    namespace: demo\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n\nkubectl apply -f bind-sa.yaml\n\nkubectl auth can-i list pods --as=system:serviceaccount:demo:demo-user -n demo\n\nkubectl auth can-i create pods --as=system:serviceaccount:demo:demo-user -n demo\n\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-2-give-access-to-all-namespaces-clusterrole","title":"Example 2 : Give access to ALL namespaces (ClusterRole)","text":"<pre><code># cluster-role-pod-reader.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-reader-all\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"get\", \"list\"]\n\n\nkubectl apply -f cluster-role-pod-reader.yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-pods-everywhere\nsubjects:\n  - kind: ServiceAccount\n    name: demo-user\n    namespace: demo\nroleRef:\n  kind: ClusterRole\n  name: pod-reader-all\n  apiGroup: rbac.authorization.k8s.io\n\n\nkubectl apply -f cluster-role-binding.yaml\n\nkubectl auth can-i get pods --as=system:serviceaccount:demo:demo-user -A\n\n\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-3-deny-access-default-behavior","title":"Example 3: Deny access (default behavior)","text":"<pre><code>\nkubectl auth can-i delete nodes\nkubectl auth can-i create deployments\n\n\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-4-admin-role-for-a-namespace","title":"Example 4: Admin role for a namespace","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: namespace-admin\n  namespace: demo\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"*\"]\n    verbs: [\"*\"]\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: demo-admin-binding\n  namespace: demo\nsubjects:\n  - kind: User\n    name: sunil\nroleRef:\n  kind: Role\n  name: namespace-admin\n  apiGroup: rbac.authorization.k8s.io\n\n\nkubectl auth can-i create deployments --as sunil -n demo\n\nkubectl auth can-i delete pods --as bob -n demo \n\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-5-create-a-limited-serviceaccount-for-a-deployment-best-practice","title":"Example 5: Create a limited ServiceAccount for a Deployment (BEST PRACTICE)","text":"<pre><code>\nkubectl create sa app-sa -n demo\n\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: cm-reader\n  namespace: demo\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"get\"]\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: cm-reader-binding\n  namespace: demo\nsubjects:\n  - kind: ServiceAccount\n    name: app-sa\nroleRef:\n  kind: Role\n  name: cm-reader\n  apiGroup: rbac.authorization.k8s.io\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  namespace: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      serviceAccountName: app-sa\n      containers:\n        - name: app\n          image: nginx\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-6-create-new-user-to-get-access-to-the-nodes","title":"Example 6 Create new user to get access to the nodes","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-admin\nrules:\n  - apiGroups: [\" \"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: all-node-status\nsubjects:\n  - kind: User\n    name: michelle\nroleRef:\n  kind: ClusterRole\n  name: node-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#example-6-create-new-user-to-get-access-to-the-storage","title":"Example 6 Create new user to get access to the storage","text":"<pre><code>---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: storage-admin\nrules:\n- apiGroups: [\"\"]\n  resources: [\"persistentvolumes\"]\n  verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n- apiGroups: [\"storage.k8s.io\"]\n  resources: [\"storageclasses\"]\n  verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"]\n\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: michelle-storage-admin\nsubjects:\n- kind: User\n  name: michelle\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: storage-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#real-world-rbac-scenarios-used-by-companies","title":"Real-World RBAC Scenarios Used by Companies","text":"<p>\ud83d\udfe3 Scenario 1: Dev team should deploy only in dev namespace \u274c Cannot touch Production \u274c Cannot see Secrets \u274c Cannot delete resources \u2714 Can create/update deployments</p> <p>Solution:</p> <p>Create a Role in dev namespace:</p> <p>verbs: [create, update, list, get]</p> <p>resources: deployments, pods, services</p> <p>Bind to Dev team group dev-team-group</p> <p>No permissions in prod namespace</p> <p>\ud83d\udfe3 Scenario 2: SRE team has full cluster-wide access \u2714 Manage nodes \u2714 Restart kube-system pods \u2714 Apply cluster CRDs \u2714 Debug any namespace</p> <p>Solution:</p> <p>ClusterRole = cluster-admin</p> <p>ClusterRoleBinding \u2192 SRE group (sre-admins)</p> <p>This is identical to real enterprise SRE teams.</p> <p>\ud83d\udfe3 Scenario 3: CI/CD Pipeline needs limited access</p> <p>Jenkins or GitHub Actions needs:</p> <p>Create deployments</p> <p>Patch existing deployments</p> <p>Cannot delete</p> <p>Cannot list secrets</p> <p>Can read configmaps</p> <p>Cannot access nodes</p> <p>Solution:</p> <p>Create a ServiceAccount ci-bot and bind:</p> <p>verbs: [create, update, patch] resources: [deployments, pods]</p> <p>This prevents pipeline mistakes from harming the cluster.</p> <p>\ud83d\udfe3 Scenario 5: Application Pods need only ConfigMap read access</p> <p>A microservice should:</p> <p>Read its own ConfigMap</p> <p>NOT read other ConfigMaps</p> <p>NOT read Secrets</p> <p>NOT list pods</p> <p>Solution:</p> <p>Role:</p> <p>resources: [\"configmaps\"] verbs: [\"get\"]</p> <p>Bind to its service account app-sa.</p> <p>\ud83d\udfe3 Scenario 6: Auditors need read-only access to everything</p> <p>Read pods, logs, nodes, events</p> <p>Read cluster scope resources</p> <p>Cannot modify anything</p> <p>Cannot exec into pods</p> <p>This is used by Governance/Risk/Compliance (GRC) teams.</p> <p>Solution: ClusterRole:</p> <p>verbs: [get, list, watch] resources: [\"*\"]</p> <p>Binding: auditor-group</p> <p>\ud83d\udfe3 Scenario 7: Break-glass access for emergencies</p> <p>For Sev1 incident:</p> <p>Temporary full access</p> <p>Scoped to certain on-call users</p> <p>Automatically expires</p> <p>Solution:</p> <p>Create time-bound ClusterRoleBinding</p> <p>Use automation to revoke it after X minutes</p> <p>\ud83d\udfe3 Scenario 8: Multi-Tenant Cluster Access</p> <p>Teams A, B, C working in same cluster.</p> <p>Each gets its own namespace</p> <p>Each gets admin in only their namespace</p> <p>No cross-namespace access</p> <p>Companies like IBM, Red Hat, AWS use this pattern.</p> <p>\ud83d\udfe3 Scenario 9: Developer wants to use kubectl logs but NOT exec</p> <p>This is common for security.</p> <p>Role:</p> <p>resources:   - pods   - pods/log       # allow logs verbs:   - get   - list</p> <p>Deny exec:</p> <p>resources: [\"pods/exec\"] verbs: []</p> <p>Since exec is not permitted \u2192 denied automatically.</p> <p>\ud83d\udfe3 Scenario 10: ArgoCD / Flux GitOps RBAC</p> <p>GitOps controllers need:</p> <p>Read namespaces</p> <p>Create deployments</p> <p>Manage CRDs</p> <p>Watch events</p> <p>Patch resources</p> <p>But must NOT:</p> <p>Delete PVCs</p> <p>Delete Namespaces</p> <p>Delete CRDs</p> <p>Fine-grained RBAC ensures safe reconciliation loops.</p> <p>\ud83d\udfe3 Scenario 11: Database Operator (Postgres/MySQL operator)</p> <p>Needs:</p> <p>Create StatefulSets</p> <p>Manage PVCs</p> <p>Watch CRDs</p> <p>Update secrets (DB credentials)</p> <p>Must not modify unrelated workloads</p> <p>Operators require very specific ClusterRoles.</p> <p>\ud83d\udfe3 Scenario 12: Ingress Controller RBAC</p> <p>Nginx/Istio needs:</p> <p>Read Ingress objects</p> <p>Watch Services and Endpoints</p> <p>Update Status fields</p> <p>Read ConfigMaps for config</p> <p>But cannot modify workloads.</p> <p>\ud83d\udfe3 Scenario 13: Logs/ELK/EFK Stack</p> <p>Fluentd/Fluentbit needs:</p> <p>Read Pod metadata</p> <p>Access /var/log</p> <p>Watch namespaces &amp; containers</p> <p>ClusterRole:</p> <p>verbs: [get, list, watch] resources: [pods, namespaces]</p> <p>\ud83d\udfe3 Scenario 14: Vault Injector needs access to Secrets only to read</p> <p>Secret Injector (Vault Agent) needs:</p> <p>Read only a specific secret</p> <p>Write into container volume</p> <p>Access only its namespace</p> <p>Role:</p> <p>resources: [\"secrets\"] resourceNames: [\"app-secret\"] verbs: [\"get\"]</p> <p>This is used in secure enterprises.</p>"},{"location":"docker_k8/k8/cmdref/#pod-and-container-security","title":"pod and container security","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-pod\nspec:\n  # pod level security \n  securityContext: \n    runAsUser: 1001\n  containers:\n  -  image: ubuntu\n     name: web\n     command: [\"sleep\", \"5000\"]\n     # container security\n     securityContext:\n      runAsUser: 1002\n      capabilities:\n        add: [\"SYS_TIME\", \"NET_ADMIN\"]\n\n  -  image: ubuntu\n     name: sidecar\n     command: [\"sleep\", \"5000\"]\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#blue-green-deployment","title":"Blue Green deployment.","text":"<pre><code>kubectl create ns demo\n\n# app-v1.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-v1\n  namespace: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo-app\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: demo-app\n        version: v1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n        env:\n        - name: APP_VERSION\n          value: \"v1\"\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: html\n        configMap:\n          name: app-v1-html\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-v1-html\n  namespace: demo\ndata:\n  index.html: |\n    &lt;h1 style=\"color:blue\"&gt;BLUE Version v1&lt;/h1&gt;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-v1-svc\n  namespace: demo\nspec:\n  selector:\n    app: demo-app\n    version: v1\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <pre><code># app-v2.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-v2\n  namespace: demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo-app\n      version: v2\n  template:\n    metadata:\n      labels:\n        app: demo-app\n        version: v2\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n        env:\n        - name: APP_VERSION\n          value: \"v2\"\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: html\n        configMap:\n          name: app-v2-html\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-v2-html\n  namespace: demo\ndata:\n  index.html: |\n    &lt;h1 style=\"color:green\"&gt;GREEN Version v2&lt;/h1&gt;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-v2-svc\n  namespace: demo\nspec:\n  selector:\n    app: demo-app\n    version: v2\n  ports:\n  - port: 80\n    targetPort: 80\n</code></pre> <pre><code>kubectl apply -f app-v1.yaml\nkubectl apply -f app-v2.yaml\n</code></pre> <p>Ingress controller</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-ingress\n  namespace: demo\n  annotations:\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n  - host: demo.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app-v1-svc   # BLUE (current)\n            port:\n              number: 80\n</code></pre> <p>kubectl apply -f bluegreen-ingress.yaml</p> <p>Add incase in /etc/hosts. </p> <p>127.0.0.1 demo.local</p> <p>curl http://demo.local # Blue version v1</p> <pre><code># bluegreen-ingress.yaml\n        backend:\n          service:\n            name: app-v3-svc   # Switch to GREEN\n            port:\n              number: 80\n</code></pre>"},{"location":"docker_k8/k8/cmdref/#canary-deployment","title":"Canary deployment","text":"<pre><code># canary-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: demo-canary\n  namespace: demo\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"20\"  # 20% traffic to v2\nspec:\n  rules:\n  - host: demo.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: app-v2-svc\n            port:\n              number: 80\n\n</code></pre> <p>curl -s http://demo.local | grep Version</p>"},{"location":"docker_k8/k8/core/","title":"core concepts","text":""},{"location":"docker_k8/k8/core/#deployments","title":"Deployments","text":"<ul> <li>Deploy multiple instances of your application (like a web server) to ensure high availability and load balancing.</li> <li>Seamlessly perform rolling updates for Docker images so that instances update gradually, reducing downtime.</li> <li>Quickly roll back to a previous version if an upgrade fails unexpectedly.</li> <li>Pause and resume deployments, allowing you to implement coordinated changes such as scaling, version updates, or resource modifications.</li> </ul> <p><code>kubectl create deployment --image=nginx nginxdeployment --replicas=2</code></p> <p>When you create a deployment, Kubernetes automatically creates an associated ReplicaSet.</p> <pre><code>kubectl get rs\nkubectl describe rs &lt;&gt;\n</code></pre>"},{"location":"docker_k8/k8/core/#pods","title":"Pods","text":"<p>With Kubernetes, the goal is to run containers on worker nodes, but rather than deploying containers directly, Kubernetes encapsulates them within an object called a pod. A pod represents a single instance of an application and is the smallest deployable unit in Kubernetes.</p>"},{"location":"docker_k8/k8/core/#deploy-pods","title":"Deploy pods","text":"<pre><code>kubectl run nginx --image=nginx\nkubectl get pods -o wide\n</code></pre>"},{"location":"docker_k8/k8/core/#replication-set","title":"Replication Set","text":"<p>Replica Set ensures that a specified number of pod replicas are running at any one time. makes sure that a pod or a homogeneous set of pods is always up and available  It was earlier used to be called as replication controller.</p> <pre><code>kubectl create -f replicaset.yml\nkubectl get rs\nkubectl describe rs\n</code></pre>"},{"location":"docker_k8/k8/core/#services","title":"Services","text":"<p>Kubernetes Services enables communication between various components within an outside of the application.</p> <p>There are 3 types of service types in kubernetes</p> <ul> <li>NodePort: A NodePort service maps requests arriving at a designated node port (like 30008) to the Pod\u2019s target port.</li> </ul> <p>+-------------+        NodePort        +--------------------+        Pod |   Client    | ---&gt;  NodeIP:30008 ---&gt;| Kubernetes Node    | ---&gt;  nginx:80 +-------------+                        +--------------------+      (10.244.x.x)</p> <ul> <li>ClusterIP: The service creates a Virtual IP inside the cluster to enable communication between different services such as a set of frontend servers to a set of backend  servers</li> </ul> <p>+-------------------+         Service ClusterIP          +------------------+ | Front-End Service |  ---&gt;  10.96.0.15:80  ---&gt;         |   Pod: nginx     | +-------------------+                                    |   10.244.x.x:80  |                                                          +------------------+</p> <ul> <li>LoadBalancer: Provisions a load balancer for our application in supported cloud providers.<pre><code>  Internet\n      |\n</code></pre> <p>+----------------+  | External LB    |  | 35.201.10.22   |  +----------------+           |      NodePort(s)           | +--------------------------+ |  Kubernetes Nodes        | +--------------------------+           |      ClusterIP           |      nginx Pod</p> </li> </ul>"},{"location":"docker_k8/k8/core/#namespace","title":"Namespace","text":"<p>Namespaces are intended for use in environments with many users spread across multiple teams, or projects. Mainly used for isolation of the resources. While deployments and ReplicaSets work together seamlessly, deployments provide additional functionalities such as rolling updates, rollbacks, and the ability to pause/resume changes.</p> <p>When you create a Service, it creates a corresponding DNS entry. This entry is of the form ..svc.cluster.local, which means that if a container just uses , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN). <p>identify which namespace you need your default namespace to be, then you can set your context accordingly.</p> <pre><code>kubectl config set-context $(kubectl config current-context) --namespace=default\nkubectl config set-context $(kubectl config current-context) --namespace=dev\n</code></pre> <p>if you need to limit the resources based on the namespace...</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: dev\nspec:\n  hard:\n    pods: \"10\"\n    requests.cpu: \"4\"\n    requests.memory: 5Gi\n    limits.cpu: \"10\"\n    limits.memory: 10Gi\n</code></pre>"},{"location":"docker_k8/k8/networking/","title":"Networking","text":""},{"location":"docker_k8/k8/networking/#network-namespaces","title":"Network Namespaces","text":"<p>Once we have our containers created, it is isolated between the host systems's network, process, which means the process running inside the container is not visible to pysical host.</p> <p>We can create as such in below examples.</p>"},{"location":"docker_k8/k8/networking/#physical-host","title":"Physical Host","text":"<pre><code>ps aux\nroute\narp\nip netns\n</code></pre>"},{"location":"docker_k8/k8/networking/#virtualcontainers","title":"Virtual/Containers","text":"<pre><code>ip netns exec &lt;namespace&gt; ps aux\nip netns exec &lt;namespace&gt; route\nip netns exec &lt;namespace&gt; arp\nip netns exec &lt;namespace&gt; ip netns\n</code></pre>"},{"location":"docker_k8/k8/networking/#operations-network-namespace","title":"Operations - Network Namespace","text":"<pre><code>ip netns add red\nip netns add blue\nip netns exec red ip link\nip netns exec blue ip link\nip link add veth-red type veth peer name veth-blue\nip link set veth-red netns red\nip link set veth-blue netns blue\nip -n red addr add 192.168.15.1/24 dev veth-red\nip -n blue addr add 192.168.15.2/24 dev veth-blue\nip -n red link set veth-red up\nip -n blue link set veth-blue up\nip netns exec red ping 192.168.15.2\n</code></pre>"},{"location":"docker_k8/k8/networking/#cluster-networking","title":"Cluster Networking","text":""},{"location":"docker_k8/k8/networking/#conrol-plane-nodes","title":"Conrol plane nodes","text":"<p>Protocol    Direction   Port Range  Purpose                Used By TCP       Inbound     6443*     Kubernetes API server      All TCP       Inbound     2379-2380 etcd server client API   kube-apiserver, etcd TCP       Inbound     10250     Kubelet API Self,        Control plane TCP       Inbound     10251     kube-scheduler           Self TCP       Inbound     10252     kube-controller-manager   Self</p>"},{"location":"docker_k8/k8/networking/#worker-nodes","title":"Worker nodes","text":"<p>Protocol    Direction   Port Range  Purpose             Used By TCP       Inbound     10250       Kubelet API           Self, Control plane TCP       Inbound     30000-32767   NodePort Services\u2020  All</p>"},{"location":"docker_k8/k8/networking/#useful-commands","title":"Useful commands","text":"<pre><code>ip link\nip addr\nip addr add 192.168.56.10/24 dev eth0\nip route\nip route add 192.168.56.0/24 via 192.168.56.1\ncat /proc/sys/net/ipv4/ip_forward\narp\nnetstat -plnt\nroute\n</code></pre>"},{"location":"docker_k8/k8/networking/#pod-networking","title":"Pod Networking","text":""},{"location":"docker_k8/k8/networking/#cni-in-kubernetes","title":"CNI in Kubernetes","text":""},{"location":"docker_k8/k8/networking/#coredns","title":"CoreDNS","text":""},{"location":"docker_k8/k8/networking/#ingress","title":"Ingress","text":""},{"location":"docker_k8/k8/scheduling/","title":"Scheduling","text":""},{"location":"docker_k8/k8/scheduling/#nodename","title":"NodeName","text":"<p>You can make the pod run in a specific node using <code>nodeName</code>. </p> <p>pod scheduler</p>"},{"location":"docker_k8/k8/scheduling/#labels-selectors","title":"Labels &amp; Selectors","text":"<p>Labels and Selectors are standard methods to group things together. Labels are properties attach to each item.  Selectors help you to filter these items</p> <p>selctors</p>"},{"location":"docker_k8/k8/scheduling/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Taints are applied on nodes.</p> <p>Tolerations are applied on pods.</p> <p>\u2714 Taints repel pods # Only people with a matching pass can enter \u2714 Tolerations allow pods to schedule on tainted nodes .</p> <p>Pods must have the correct toleration to \u201cpass through\u201d the taint.</p> <p>there are 3 policies of taints:</p> <p>The taint effect defines what would happen to the pods if they do not tolerate the taint.</p> <ul> <li>NoSchedule - Pod will NOT schedule unless it tolerates this taint</li> <li>PreferNoSchedule - Avoid scheduling but not strictly enforced</li> <li>NoExecute - New pods cannot schedule, existing pods are evicted</li> </ul> <p>The default policy of the any node is no taints so that any pod can be placed in any of the nodes.</p> <p>Default, master nodes have a NoSchedule so that there won't be any pods running on the master nodes. We would for the testing purpose untaint the node and then revert back</p> <p>taints and tolerations</p>"},{"location":"docker_k8/k8/scheduling/#nodeselector","title":"NodeSelector","text":"<p>We add new property called Node Selector to the spec section and specify the label. The scheduler uses these labels to match and identify the right node to place the pods onto the nodes.</p> <p>Label the nodes to ensure your pod is getting scheduled on a desired right nodes by kube-scheduler, however you can't really guarantee that the pods does only gets scheduled on the label being added. sometimes it might also get scheduled to other nodes as well. So, to fix this we would add nodeAffinity on the pods sections.</p> <p>If there are more complex operations(==, !=, etc) are involved then we might not be able to achieve using nodeSelector, so we need to use nodeAffinity and anti-affinity</p> <p>default labels of the node</p> <pre><code>get nodes node01 --show-labels\nkubectl label nodes kubenode01 size=small\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#node-affinity","title":"Node Affinity","text":"<p>The primary feature of Node Affinity is to ensure that the pods are hosted on particular nodes.</p>"},{"location":"docker_k8/k8/scheduling/#node-affinity-types","title":"Node Affinity Types","text":""},{"location":"docker_k8/k8/scheduling/#available","title":"Available","text":"<ul> <li>requiredDuringSchedulingIgnoredDuringExecution</li> <li>preferredDuringSchedulingIgnoredDuringExecution</li> </ul>"},{"location":"docker_k8/k8/scheduling/#planned","title":"Planned","text":"<ul> <li>requiredDuringSchedulingRequriedDuringExecution</li> <li>preferredDuringSchedulingRequiredDuringExecution</li> </ul>"},{"location":"docker_k8/k8/scheduling/#resource-limits","title":"Resource &amp; Limits","text":"<p>Each node has a set of CPU, Memory and Disk resources available. By default, K8s assume that a pod or container within a pod requires 0.5 CPU and 256Mi of memory. This is known as the Resource Request for a container.</p> <p>If your application within the pod requires more than the default resources, you need to set them in the pod definition file.</p> <pre><code>resources:\n     requests:\n      memory: \"1Gi\"\n      cpu: \"1\"\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#limits","title":"Limits","text":"<p>By default, k8s sets resource limits to 1 CPU and 512Mi of memory You can set the resource limits in the pod definition file.</p> <pre><code>limits:\n  memory: \"2Gi\"\n  cpu: \"2\"\n</code></pre> <p>Check the pod limitrange.yaml</p> <p>Note: Remember Requests and Limits for resources are set per container in the pod.</p>"},{"location":"docker_k8/k8/scheduling/#daemonset","title":"DaemonSet","text":"<p>DaemonSets are like replicaSet, as it helps in to deploy multiple instances of pod. But it runs one copy of your pod on each node in your cluster.</p> <p>Daemonset use cases: - Monitoring solution - logviewer - Helper pods for applications - Networking pods (weave net)</p>"},{"location":"docker_k8/k8/scheduling/#static-pods","title":"Static pods","text":"<p>When you need to create pods without any interference from the kubeapi server or kubernetes management control plane, you can write your yaml files and place in the kubelet directory and create pods. these kind of pods which have no interference from the any of the kubernetes components are called as static pod</p> <p>What if we run the static pods and if these nodes are part of kubernetes manage control plane, would it be known to kubeapi server? Yes kubeapi server would be known that there is pod running in the node as kubelet provides a mirror object in the kubeapi as read-only object.</p>"},{"location":"docker_k8/k8/scheduling/#first-method","title":"First Method","text":"<p>You can configure the kubelet to read the pod definition files from a directory on the server designated to store information about pods.The designated directory can be any directory on the host and the location of that directory is passed in to the kubelet as an option while running the service. --pod-manifest-path</p> <pre><code>systemctl status kubelet.service\ncat /lib/systemd/system/kubelet.service\nExecStart=/usr/local/bin/kubelet \\\\\n.\n.\n--pod-manifest-path=/etc/kubernetes/manifests\n.\n</code></pre> <p>Create a static pod</p> <pre><code>kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 &gt; /etc/kubernetes/manifests/static-busybox.yaml\n\nkubectl get pods\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#second-method","title":"Second Method","text":"<p>Incase if you have created kubeadmin way of configuring the cluster, your manifests will be in below location.</p> <pre><code>systemctl status kubelet.service\ncat /var/lib/kubelet/config.yaml\n\nlook for \"staticPodPath: /etc/kubernetes/manifests\"\n</code></pre>"},{"location":"docker_k8/k8/scheduling/#multiple-schedulers","title":"Multiple schedulers","text":"<p>https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/</p> <p>my-scheduler pod</p> <p>we could ask nginx to create a pod using my-scheduler</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-my-scheduler\n  labels:\n    name: nginx-pod\nspec:\n  schedulerName: my-scheduler &lt;== custom scheduler.\n  containers:\n  - name: nginx\n    image: nginx\n</code></pre>"},{"location":"docker_k8/k8/security/","title":"Security","text":"<p>The Kube API server is at the heart of Kubernetes operations because all cluster interactions</p>"},{"location":"docker_k8/k8/security/#authentication","title":"Authentication","text":"<p>Authentication verifies the identity of a user or service before granting access to the API server.</p> <ul> <li>Static user IDs and passwords(users/admins)</li> </ul> <p>One simple method for authentication is using a static password file. This CSV file contains user details and is structured with three columns: password, username, and user ID. An optional fourth column can be added to assign users to specific groups.</p> <pre><code>kube-apiserver --basic-auth-file=user-details.csv\n</code></pre> <ul> <li>Tokens(service accounts)</li> </ul> <p>you can authenticate using a static token file. This file includes a token, username, user ID, and an optional group assignment</p> <pre><code>kube-apiserver --token-auth-file=user-token-details.csv\ncurl -v -k https://master-node-ip:6443/api/v1/pods --header \"Authorization: Bearer &lt;token&gt;\"\n</code></pre> <p>Note: not recommented due to security issues, instead use RBAC.</p> <ul> <li>Client certificates</li> <li>Integration with external authentication providers (e.g., LDAP)</li> <li>Service accounts</li> </ul>"},{"location":"docker_k8/k8/security/#authorization","title":"Authorization","text":"<p>After authentication, authorization determines what actions a user or service is allowed to perform.</p> <ul> <li>Role-Based Access Control (RBAC) - default</li> </ul> <p>RBAC simplifies user permission management by defining roles instead of directly associating permissions with individual users. For example, you can create a \"developer\" role that encompasses only the necessary permissions for application deployment. Developers are then associated with this role, and modifications in user access can be handled by updating the role, affecting all associated users immediately.</p> <ul> <li>Attribute-Based Access Control (ABAC)</li> </ul> <p>Attribute-based authorization associates specific users or groups with a defined set of permissions.  For example, you can grant a user called \"dev-user\" permissions to view, create, and delete pods. This is achieved by creating a policy file in JSON format and passing it to the API server. Each time security requirements change, you must manually update this policy file and restart the Kube API Server. This manual process can be tedious and set the stage for more streamlined methods such as Role-Based Access Control (RBAC).</p> <ul> <li>Node Authorization</li> </ul> <p>The Kubernetes API Server is the central component accessed by both management users and internal components, such as kubelets, which retrieve and report metadata about services, endpoints, nodes, and pods. </p> <p>Requests from kubelets\u2014typically using certificates with names prefixed by \"system:node\" as part of the system:nodes group\u2014are authorized by a special component known as the node authorizer.</p> <ul> <li>Webhook authorization</li> </ul> <p>Methods to authorize..</p> <ul> <li>static file</li> <li>static token file</li> <li>certificates</li> <li>identity services</li> </ul> <p>Note:  both these methods are not recommended due to security issues.  RBAC is the most recommened method.</p> <pre><code># static file method\ncurl -v -k http://master-node-ip:6443/api/v1/pods -u \"user1:password123\" \n\n# token method\ncurl -v -k http://master-node-ip:6443/api/v1/pods --header \"Authorization: Bearer &lt;Token&gt;\"\n</code></pre> <p>we will study more on the RBAC.</p>"},{"location":"docker_k8/k8/security/#tls-certificates","title":"TLS Certificates","text":"<p>Secure communications between Kubernetes components are enabled via TLS encryption. This ensures that data transmitted between key components remains confidential and tamper-proof. </p> <ul> <li>Communication within the etcd cluster</li> <li>Interactions between the Kube Controller Manager and Kube Scheduler</li> <li>Links between worker node components such as the Kubelet and Kube Proxy</li> </ul> <p>Communication between the applications within cluster is defined by network policies</p> <ul> <li>What are TLS certificates?</li> <li>How does kubernetes use certificates?</li> <li>How to generate them?</li> <li>How to configure them?</li> <li>How to view them?</li> <li>How to troubleshoot issues related to certificates</li> </ul> <p>A certificate is used to gurantee trust between 2 parties during a transaction. tls certificates ensure that the communication between them is encrypted.</p> <p>Symmetric - uses single key to encrypt and decrypt the data and the key has to be exchanged between the sender and the receiver. Asymmetric - instead of using single key, asymmetric encryption uses a pair of keys, private key(*.pem) and a public key(*.crt).</p> <p>Asymmetric encryption is used only during the handshake(securely agree on a shared secret), to encrypt all actual website data you use  Symmetric. </p> <ul> <li> <p>Browser connects to server (https://example.com)</p> </li> <li> <p>Server sends public key inside an SSL certificate</p> </li> <li> <p>Browser verifies certificate (CA trust)</p> </li> <li> <p>Browser creates a random Session Key (AES)</p> </li> <li> <p>Browser encrypts the session key with server\u2019s public key</p> </li> <li> <p>Server decrypts using private key</p> </li> <li> <p>Both now share the same symmetric key</p> </li> <li> <p>All further communication is encrypted with fast AES symmetric encryption</p> </li> </ul> <pre><code># Generate a private key\nopenssl genrsa -out my-bank.key 1024\n\n# Extract the public key\nopenssl rsa -in my-bank.key -pubout &gt; mybank.pem or mybank.crt\n</code></pre> <pre><code>\n                  HTTPS Communication Flow\n       -----------------------------------------------------\n\n            (1) Client Hello\n       Browser ----------------------------------&gt; Server\n\n            (2) Server sends Certificate (Public Key)\n       Browser &lt;----------------------------------- Server\n                          |\n                          | Public Key (from cert)\n                          |\n                    Certificate verified?\n                       (Trusted CA)\n\n            (3) Browser generates \"Session Key\" (AES)\n            (4) Browser encrypts Session Key with\n                Server's Public Key\n       Browser ----------------------------------&gt; Server\n              Encrypted(SessionKey, ServerPublicKey)\n\n\n            (5) Server decrypts Session Key using\n                Private Key\n\n       Server: SessionKey = decrypt_with_private_key(encrypted_key)\n\n            Both now have SAME Session Key!\n\n       -----------------------------------------------------\n       FROM HERE ON:\n       All data encrypted with AES (Symmetric Encryption)\n       -----------------------------------------------------\n\n       Browser &lt;======== TLS Encrypted Traffic ========&gt; Server\n\n</code></pre> <p>Browsers rely on Certificate Authorities (CAs) to sign and validate certificates. Renowned CAs, such as Symantec, DigiCert, Komodo, and GlobalSign, use their private keys to sign certificate signing requests (CSRs). When you generate a CSR for your web server, it is sent to a CA for signing:</p> <p><code>openssl req -new -key my-bank.key -out my-bank.csr -subj \"/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com\"</code></p>"},{"location":"docker_k8/k8/security/#kubernetes-tls-certificates","title":"Kubernetes TLS Certificates","text":"<p>Since, the method is being used for creation cluster is kubeadm, all the certificates are stored in below location <code>/etc/kubernetes/pki/</code></p> <ul> <li> <p>Server Certificates for Servers</p> </li> <li> <p>Client Certificates for Clients</p> </li> <li> <p>Public keys</p> </li> </ul> <pre><code>apiserver-etcd-client.crt  \napiserver-kubelet-client.crt  \napiserver.crt\nca.crt\nfront-proxy-ca.crt  \nfront-proxy-client.crt\n</code></pre> <ul> <li>Private keys</li> </ul> <pre><code>apiserver-etcd-client.key  \napiserver-kubelet-client.key  \napiserver.key\nca.key\nfront-proxy-ca.key  \nfront-proxy-client.key  \nsa.key\n</code></pre> <p>Viewing, certiticate details : <code>openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout</code></p> <p>Fields to be viewed :  <code>Issuer - Validity - Subject - Subject Alternative Names</code></p> <p>Troubleshooting, kubeadm installation method.</p> <pre><code>kubectl logs etcd-master\ndocker ps -a\ndocker logs &lt;container-id&gt;\n</code></pre>"},{"location":"docker_k8/k8/security/#certificate-api","title":"Certificate API","text":"<p>The CA is really just the pair of key and certificate files that we have generated, whoever gains access to these pair of files can sign any certificate for the kubernetes environment. Kubernetes has a built-in certificates API that can do this for you.</p> <ul> <li>CertificateSigningRequest</li> <li>Review Requests</li> <li>Approve Requests</li> <li>Share to Users</li> </ul> <p>All the certificate releated operations are carried out by the controller manager.</p> <p>Certificate signing requests</p>"},{"location":"docker_k8/k8/security/#kubeconfig","title":"kubeconfig","text":"<p>Client uses the certificate file and key to query the kubernetes Rest API for a list of pods using curl.</p> <p><code>kubectl get pods --kubeconfig config</code></p> <p>The kubeconfig file has 3 sections</p> <ul> <li>Clusters</li> <li>Contexts</li> <li>Users</li> </ul> <p>kubectl config references</p>"},{"location":"docker_k8/k8/security/#api-groups","title":"API Groups","text":"<p>Kubernetes organizes its API into multiple groups based on specific functionality. These groups help in managing versioning, health metrics, logging, and more. For instance, the /version endpoint provides cluster version data, while endpoints like /metrics and /healthz offer insights into the cluster\u2019s performance and health.</p> <p>These APIs are catagorized into two.</p> <p>Core API Group: Contains the essential features of Kubernetes such as namespaces, pods, replication controllers, events, endpoints, nodes, bindings, persistent volumes, persistent volume claims, config maps, secrets, and services.</p> <p>Named API Groups: Provides an organized structure for newer features. These groups include apps, extensions, networking, storage, authentication, and authorization.</p> <p>Every API group includes various resources along with associated actions (verbs) such as list, get, create, delete, update, and watch</p> <pre><code>curl http://localhost:6443 -k # query API Server\ncurl http://localhost:6443/apis -k\n</code></pre>"},{"location":"docker_k8/k8/security/#rbac","title":"RBAC","text":"<pre><code>                   ( WHO? )\n   +---------------------------------------------+\n   |        Users / Groups / ServiceAccounts     |\n   +-------------------------+-------------------+\n                             |\n                             |  RoleBinding / ClusterRoleBinding\n                             v\n    +-------------------------------------------------------------+\n    |          RBAC BINDINGS (connect WHO \u2194 WHAT)                |\n    |  - RoleBinding (namespaced)                                |\n    |  - ClusterRoleBinding (cluster-wide)                       |\n    +-------------------------+-----------------------------------+\n                              |\n                              | points to\n                              v\n   +-------------------------------------------------------------+\n   |          Roles / ClusterRoles (WHAT actions allowed)        |\n   |  rules:                                                     |\n   |   - apiGroups                                              |\n   |   - resources                                              |\n   |   - verbs                                                  |\n   +-------------------------+-----------------------------------+\n</code></pre> <p>RBAC Object Summary</p> <p>+----------------------+----------------------+----------------------------------+ | RBAC Object          | Scope               | Purpose                          | +----------------------+----------------------+----------------------------------+ | Role                 | Namespace            | Define permissions within a NS   | | ClusterRole          | Cluster-wide         | Define global permissions        | | RoleBinding          | Namespace            | Bind Role to subject             | | ClusterRoleBinding   | Cluster-wide         | Bind ClusterRole to subject      | +----------------------+----------------------+----------------------------------+</p> <p>Resource Types &amp; API Groups</p> <p>Core API Group (\"\"): pods, configmaps, secrets, services, nodes, persistentvolumes</p> <p>apps API Group (\"apps\"): deployments, daemonsets, replicasets, statefulsets</p> <p>rbac API Group (\"rbac.authorization.k8s.io\"): roles, clusterroles, bindings</p> <p>batch API Group (\"batch\"): jobs, cronjobs</p> <p>Verbs Cheat Sheet</p> <p>get        \u2192 Read a single object list       \u2192 List all objects watch      \u2192 Watch changes create     \u2192 Create objects update     \u2192 Modify patch      \u2192 Partial modification delete     \u2192 Delete object deletecollection \u2192 Delete multiple exec       \u2192 Execute command in pods logs       \u2192 View logs (pods/log subresource)</p> <p>Test RBAC</p> <pre><code>kubectl auth can-i &lt;verb&gt; &lt;resource&gt; --as &lt;user&gt;\n\nkubectl auth can-i create pods --as dev-user -n dev\nkubectl auth can-i delete nodes --as audit-bot\nkubectl auth can-i list secrets --as=system:serviceaccount:prod:app-sa\n\n\n</code></pre> <p>All the examples are found here.. </p> <p>role|rolebinding|clusterrole|clusterrolebindings|examples</p>"},{"location":"docker_k8/k8/security/#role","title":"Role","text":"<p>Namespace-scoped, defines permission inside a namespace.</p> <p>Roles can be described with below three parameters. - apiGroups - resources - verbs [ get, list, watch, create, update, patch, delete ] </p> <p>linking user to the role is called rolebinding</p>"},{"location":"docker_k8/k8/security/#clusterrole","title":"ClusterRole","text":"<p>Not namespaced,  Used for:   Cluster-wide resources (nodes, CRDs) Or reusable Role across namespaces</p>"},{"location":"docker_k8/k8/security/#rolebinding","title":"RoleBinding","text":"<p>Binds a Role \u2192 Subject (user / group / service account) within a namespace</p>"},{"location":"docker_k8/k8/security/#clusterrolebinding","title":"ClusterRoleBinding","text":"<p>Binds a ClusterRole \u2192 Subject cluster-wide</p>"},{"location":"docker_k8/k8/security/#images-security","title":"Images Security","text":"<p>when we pull image from \"nginx\" we are pulling from the nginx/nginx of the private docker.io hub. so you need to login to your hub.docker.com so that kubelet will try to pull image from the repository and deploy applications in the worker nodes.</p> <pre><code>containers:\n- name: nginx\n  image: nginx\n</code></pre> <p>what if the repository is private, then you need to specify the DNS name prefixed with the docker image.</p> <pre><code>containers:\n- name: nginx\n  image: private-repo/nginx:latest\n</code></pre> <p>If you have to modify the docker containers which has to be provided with the secrets, kubernets have secrets where you can store your credentials.</p> <pre><code>kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com\n</code></pre> <p>you would be using these credentials for the pod in applications to be up and running.</p> <pre><code>spec:\n  containers:\n  - name: private-reg-container\n    image: &lt;your-private-image&gt;\n  imagePullSecrets:\n  - name: private-reg-cred\n</code></pre>"},{"location":"docker_k8/k8/security/#secrurity-contexts","title":"Secrurity Contexts","text":"<p>You may choose to configure the security settings at a container level or at a pod level.</p> <p>pod and container security</p>"},{"location":"docker_k8/k8/storage/","title":"Storage Concepts","text":""},{"location":"docker_k8/k8/storage/#introduction-of-docker-storage","title":"Introduction of Docker Storage","text":"<p>Docker stores data on the local file system, it creates this directory structures at /var/lib/docker, where it has all the folders and directories called (aufs, containers, image, volumes ). this is place where it stores data by default.</p> <p>All files related to containers are stored under the containers directory and the files related to images are stored under the image directory. Any volumes created by the Docker containers are created under the volumes directory.</p>"},{"location":"docker_k8/k8/storage/#layered-architecture","title":"Layered Architecture","text":"<p>When docker builds images, it builds these in a layered architecture. Each line of instruction in the Docker file creates a new layer in the Docker image with just the changes from the previous layer.</p> <p>Advantage of layered architecture: When image builds, docker is not going to build first three layers instead of it reuses the same three layers it built for the first application from the cache. This way, Docker builds images faster and efficiently saves disk spaces. Once the build is complete, you cannot modify the contents of these layers and so they are read-only and you can only modify them by initiating a new build.</p> <p>When you run a container based off of this image, using the Docker run command, Docker creates a container based off of these layers and creates a new writeable layer on top of the image layer. The writeable layer is used to store data created by the container such as log files written by the applications, any temporary files generated by the container.</p> <p>Examples:</p> <ul> <li>Let's take an example of our application code. Since we bake our code into the image, the code is part of the image and as such, its read-only. After running a container, what if I wish to modify the source code.</li> <li>Yes, I can still modify this file, but before I saved the modified file, Docker automatically creates a copy of the file in the read-write layer and I will then be modifying a different version of the file in the read-write layer. All future modifications will be done on this copy of the file in the read-write layer. This is called copy-on-right mechanism.</li> <li>The Image layer being a read-only just means that the files in these layers will not be modified in the image itself. So, the image will remain the same all the time until you rebuild the image using the Docker build command. If container destroyed then all of the data that was stored in the container layer also gets deleted.</li> </ul> <p>Container run time engine: Kubernetes used Docker alone as the container runtime engine, and all the code to work with Docker was embedded within the Kubernetes source code. The Container Runtime Interface is a standard that defines how an orchestration solution like Kubernetes would communicate with container runtimes like Docker</p>"},{"location":"docker_k8/k8/storage/#persistent-volume","title":"Persistent Volume","text":"<p>In the large environment, with a lot of users deploying a lot of pods, the users would have to configure storage every time for each Pod.</p> <p>A Persistent Volume is a cluster-wide pool of storage volumes configured by an administrator to be used by users deploying application on the cluster. The users can now select storage from this pool using Persistent Volume Claims.</p> <pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: pv-vol1\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  capacity:\n   storage: 1Gi\n  hostPath:\n   path: /tmp/data\n</code></pre> <pre><code>kubectl create -f pv.yaml\nkubectl get pv\nkubectl delete pv pv-vol1\n</code></pre>"},{"location":"docker_k8/k8/storage/#persistent-volume-claim","title":"Persistent Volume Claim","text":"<p>Now we will create a Persistent Volume Claim to make the storage available to the node. Volumes and Persistent Volume Claim are two separate objects in the Kubernetes namespace. Once the Persistent Volume Claim created, Kubernetes binds the Persistent Volumes to claim based on the request and properties set on the volume.</p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim1\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  resources:\n   requests:\n     storage: 300m\n</code></pre> <p>Kubernetes binds only one PVC to one PV. if there are no volumes for claims, then then claims will remain in the pending state. you can also use 'labels' or 'SelectLabels' to defined which volumes to be used for 'claims' These can be described based on parameters like - Sufficient capacity - Access Modes - Volume Modes - Storage Class - Selector</p> <p>There are different accessModes that can be mounted. - ReadWriteOnce -- the volume can be mounted as read-write by a single node - ReadOnlyMany -- the volume can be mounted read-only by many nodes - ReadWriteMany -- the volume can be mounted as read-write by many nodes</p> <p>In above example, though you might have an PV with 1G, if the claim is 300M it would select PV with 1GB. though if there are other claims that exists since it can't find the volumes it will be still pending.</p> <p>If the claims are deleted, we could set parameter in the PV to do these - Retain ( default ) - though claims are deleted, this will retain the volumes - Delete - claims are deleted, volumes are deleted - Recycle - volumes are scrubbed and can be re-used for claims</p>"},{"location":"docker_k8/k8/storage/#storage-class","title":"Storage Class","text":"<p>We created Persistent Volume but before this if we are taking a volume from Cloud providers like GCP, AWS, Azure. We need to first create disk.</p>"},{"location":"docker_k8/k8/storage/#static-provisioning","title":"Static Provisioning","text":"<p>We need to create manually each time when we define in the Pod definition file. that's called Static Provisioning.</p>"},{"location":"docker_k8/k8/storage/#dynamic-provisioning","title":"Dynamic Provisioning","text":"<p>No we have a Storage Class, So we no longer to define Persistent Volume. It will create automatically when a Storage Class is created. It's called Dynamic Provisioning</p> <pre><code>sc-definition.yaml\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: io1\n  iopsPerGB: \"10\"\n  fsType: ext4\n</code></pre> <pre><code>kubectl create -f sc-definition.yaml\nkubectl get sc\n</code></pre> <pre><code>pvc-definition.yaml\n\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: myclaim\nspec:\n  accessModes: [ \"ReadWriteOnce\" ]\n  storageClassName: aws-ebs      \n  resources:\n   requests:\n     storage: 500Mi\n</code></pre> <pre><code>pod-definition.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: frontend\n      image: nginx\n      volumeMounts:\n      - mountPath: \"/var/www/html\"\n        name: web\n  volumes:\n    - name: web\n      persistentVolumeClaim:\n        claimName: myclaim\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/issues/","title":"issues","text":""},{"location":"docker_k8/k8/troubleshooting/issues/#image-pull-error","title":"Image pull error","text":"<ul> <li>incorrect image</li> <li>incorect image tags</li> <li>incorrect secrets</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#crashing-pods","title":"Crashing pods","text":"<p>CrashLoopBackOff means Kubernetes is persistently trying to restart a failing container.  it restarts bcoz of the pod definition file has <code>restartPolicy: Always</code>. It can also be set as <code>never</code> or <code>on failure</code></p> <ul> <li>not providing ENV variable it can be <code>secrets</code> or <code>configmaps</code></li> <li>permission issues for the image while starting up app by scripts.</li> <li>missing file/volume for applications/pod</li> <li>Memory limits - OOOMKilled.</li> <li>liveness or rediness probes endpoint</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#pending-pods","title":"pending pods","text":"<ul> <li>insufficient cpu</li> <li>taints on the node</li> <li>node selector or missing labels </li> <li>missing toleration on nodes</li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#missing-pods","title":"missing pods","text":"<ul> <li>resource quota on your namespace</li> <li>necessary resource accounts or dependencies on your namespace</li> </ul> <p>Always, check the events i.e <code>kubectl get events -n &lt;namespace&gt;</code></p>"},{"location":"docker_k8/k8/troubleshooting/issues/#schrodingers-deployment","title":"Schr\u00f6dinger's Deployment","text":"<p>Always use unique labels in the deployment when created would get the same match for the pods/services, to forward traffic to the cluster nodes. Overlapping selectors can cause unexpected behavior and intermittent failures that are challenging to diagnose in large-scale environments</p>"},{"location":"docker_k8/k8/troubleshooting/issues/#create-container-error","title":"Create container error","text":""},{"location":"docker_k8/k8/troubleshooting/issues/#createcontainerconfigerror","title":"CreateContainerConfigError","text":"<ul> <li>missing env/configmaps/secrets etc </li> <li>env varaible refereced being missed. </li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#createcontainererror","title":"CreateContainerError","text":"<ul> <li>this happens during the start of container when you have no rntry point defined in the image </li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#runtimecontainer","title":"runtimeContainer","text":"<ul> <li>invalid command </li> </ul>"},{"location":"docker_k8/k8/troubleshooting/issues/#config-out-of-date","title":"Config out of date.","text":"<p>configuration changes (such as updates to ConfigMaps or Secrets) are not immediately reflected in running pods Once you change any config maps or secrets make sure you restart deployment as good practice. <code>kubectl rollout restart deploymnet &lt;deployment-name&gt;</code></p> <p>Note: you can use a reloader by adding annotating in the deployment file which will restart the pods.</p>"},{"location":"docker_k8/k8/troubleshooting/issues/#endless-termination","title":"endless termination","text":"<p>sometimes a resource may not terminate correctly. Some pods remain in the Terminating state. This behavior is often due to background operations or cleanup tasks (similar to garbage collection) that must complete before the resource is fully removed.</p> <p><code>kubectl delete pod shipping-api-57cdd984bc-grq7g --force</code></p> <p>Removing Finalizers - Finalizers ensure that specific cleanup tasks\u2014such as persistent volume or namespace protection actions\u2014are completed before the resource is deleted</p> <p>To allow the pod to be fully deleted, remove or set the finalizers to null and save the changes.</p>"},{"location":"docker_k8/k8/troubleshooting/issues/#enableservicelinks","title":"enableServiceLinks","text":"<p>enableServiceLinks is crucial for controlling how service-related environment variables are injected into your pods</p> <p>standard_init_linux.go:228: exec user process caused: argument list too long</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-frontend\n  namespace: staging\n  annotations:\n    deployment.kubernetes.io/revision: \"1\"\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"metadata\":{\"name\":\"app-frontend\",\"namespace\":\"staging\"},\"spec\":{\"replicas\":1,\"selector\":{\"matchLabels\":{\"app\":\"frontend\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"frontend\"}},\"spec\":{\"containers\":[{\"image\":\"nginx:1.19\",\"name\":\"app-frontend\",\"ports\":[{\"containerPort\":80}]}]}}}}\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      enableServiceLinks: false &lt;=======\n      containers:\n        - image: nginx:1.19\n          name: app-frontend\n          ports:\n            - containerPort: 80\n</code></pre> <p>enableServiceLinks controls how Kubernetes injects service environment variables into your pods. By setting enableServiceLinks to false, you can improve resource efficiency and avoid errors like \"argument list too long,\" especially when many services coexist in the same namespace. Leveraging DNS via CoreDNS for service discovery is a best practice that renders these environment variables unnecessary.</p>"},{"location":"docker_k8/k8/troubleshooting/overview/","title":"Troubleshooting","text":"<p>Basic commands required to troubleshoot application</p> <pre><code>kubeclt get all \nkubectl get all -A\nkubectl get -n &lt;namespace&gt; deployments -o yaml\nkubectl get pods -A\nkubectl get pods -n &lt;namespace&gt;\nkubectl describe pod &lt;podname&gt; -n &lt;namespace&gt;\nkubectl describe node # check for events\nkubectl logs &lt;podname&gt; -n namespace\nkubectl get events -n &lt;namespace&gt;\nkubectl logs -n &lt;namespace&gt; --all-containers\nkubectl logs &lt;podname&gt; -c &lt;container&gt;\nkubectl logs -l app=webtier\nkubectl logs podname --timestamps \nkubectl logs podname since=5s\nkubectl logs -n &lt;namespace&gt; &lt;pod&gt; --since=1h # logs since 1hr\nkubectl logs &lt;podname&gt; -n namespace -f\nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -- ls \nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -c &lt;container&gt; --ls\nkubectl exec -n &lt;namespace&gt; &lt;podname&gt; -c &lt;container&gt; -it bash\nkubectl port-forward -n &lt;namespace&gt; svc/app.service laptop:listeningport\nkubectl explain &lt;resource&gt;.spec # documentation\nkubectl top node # memory/CPU\nkubectl auth can-i list pods -n namespace\nkubectl auth whoami\nkubectl diff\n\nkubectl rollout restart deployment -n &lt;namespace&gt; &lt;deployment&gt;\nkubectl get events -n &lt;namespace&gt;\n\nkubectl debug - why ?\n- minimize pod disruptions\n- distroless images\n- crashed container\n</code></pre> <p>ephemeral containers useful</p> <ul> <li>When using distroless images</li> <li>Images doesn\u2019t contain debugging utilities</li> <li>Debugging container in a crashloop</li> </ul> <p>Create debug pod for troubleshooting</p> <pre><code>kubectl debug -it ephemeral --image=busybox:1.28 --target=ephemeral\n</code></pre> <pre><code>kubectl debug -it ephemeral --image=busybox:1.28 --target=ephemeral\nTargeting container \"ephemeral\". If you don't see processes from this container it may be because the container runtime doesn't support this feature.\n--profile=legacy is deprecated and will be removed in the future. It is recommended to explicitly specify a profile, for example \"--profile=general\".\nDefaulting debug container name to debugger-gvbx8.\nIf you don't see a command prompt, try pressing enter.\n/ # ps aux\nPID   USER     TIME  COMMAND\n    1 root      0:00 /pause\n   56 root      0:00 sh\n  112 root      0:00 ps aux\n/ # \n</code></pre> <p>upon exiting the shell, the container gets terminated. </p>"},{"location":"docker_k8/k8/troubleshooting/overview/#application-failures","title":"Application Failures","text":"<ul> <li>Application/Service status of the webserver</li> <li>endpoint of the service and compare it with the selectors</li> <li>status and logs of the pod</li> <li>logs of the previous pod</li> </ul> <pre><code>curl http://web-service-ip:node-port\nkubectl describe service web-service\nkubectl get pod\nkubectl describe pod web\nkubectl logs web\nkubectl logs web -f --previous\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#control-plane-failure","title":"Control Plane Failure","text":"<ul> <li>Nodes in the cluster, are they Ready or NotReady, If they are NotReady then check the LastHeartbeatTime of the node to find out the time when node might have crashed</li> <li>Possible CPU and MEMORY using top and df -h</li> <li>Status and the logs of the kubelet for the possible issues.</li> <li>Check the kubelet Certificates, they are not expired, and in the right group and issued by the right CA</li> </ul> <pre><code>kubectl get nodes\nkubectl describe node worker-1\ntop\ndf -h\nserivce kubelet status\nsudo journalctl \u2013u kubelet\nopenssl x509 -in /var/lib/kubelet/worker-1.crt -text\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#worker-nodes-failure","title":"Worker nodes failure","text":"<ul> <li>Status of the Nodes in the cluster, are they Ready or NotReady</li> <li>If they are NotReady then check the LastHeartbeatTime of the node</li> <li>Check the possible CPU and MEMORY using top and df -h</li> <li>Check the status and the logs of the kubelet for the possible issues.</li> <li>Check the kubelet Certificates, they are not expired, and in the right group and issued by the right CA.</li> </ul> <pre><code>kubectl get nodes\nkubectl describe node worker-1\nserivce kubelet status\nsudo journalctl \u2013u kubelet\nopenssl x509 -in /var/lib/kubelet/worker-1.crt -text\n</code></pre>"},{"location":"docker_k8/k8/troubleshooting/overview/#network-failures","title":"Network Failures","text":""},{"location":"ds_ai_ml/ai/chatbot/","title":"chatbot","text":"<p>There are 3 components for the architecture...</p> <ul> <li> <p>access the LLM in the aws bedrock service if the AWS(i.e integrate LLM bedrock). make sure your <code>.aws/config</code> is been configured with access creds and regions </p> </li> <li> <p>In oder to store the conversation you need to buffer the pervious conversation i.e <code>Langchain - conversationalBufferMempry</code></p> </li> <li> <p><code>LangChain-ConversationChain</code> will be used to store <code>prompt store</code> + <code>Lanfchain-conversationchain</code> + <code>langchain - conversationbuffermemory</code></p> </li> </ul>"},{"location":"ds_ai_ml/ai/movie_poster/","title":"movie poster","text":""},{"location":"ds_ai_ml/ai/movie_poster/#lambda-function","title":"lambda function:","text":"<p>Pre-requsites</p> <ul> <li>Create a S3 bucket name</li> <li>Create a lambda function by providing roles for both Amazon Bedrock and S3 bucket.</li> <li>Ensure pre-signed S3 url would be provided back to API gateway</li> </ul> <pre><code>import json\nimport boto3\nimport base64\nimport datetime\n\n\ndef lambda_handler(event, context):\n    client_s3 = boto3.client('s3')\n    client_bedrock = boto3.client('bedrock-runtime')\n\n    input_prompt = event['prompt']\n    print(\"input_prompt: \", input_prompt)\n\n    response_bedrock = client_bedrock.invoke_model(\n        contentType='application/json',\n        accept='application/json',\n        modelId='amazon.nova-canvas-v1:0',\n        body=json.dumps({\n            \"textToImageParams\":{\"text\":input_prompt},\n                \"taskType\":\"TEXT_IMAGE\",\n                \"imageGenerationConfig\":{\n                    \"cfgScale\":6.5,\n                    \"seed\":12,\n                    \"width\":1280,\n                    \"height\":720,\n                    \"numberOfImages\":1\n                }\n        })\n    )\n\n    raw = response_bedrock[\"body\"].read()\n    payload = json.loads(raw.decode(\"utf-8\"))\n\n    # 3) Extract base64 from either schema\n    imgs = payload.get(\"images\") or []\n    first = imgs[0]\n    if isinstance(first, str):\n        b64_data = first\n        mime_hint = \"image/png\"\n    elif isinstance(first, dict):\n        b64_data = first.get(\"b64\") or first.get(\"base64\") or first.get(\"bytes\") or first.get(\"image\")\n        mime_hint = (first.get(\"mimeType\") or \"image/png\").lower()\n    else:\n        raise RuntimeError(f\"Unrecognized image element type: {type(first)}\")\n\n    if not b64_data:\n        raise RuntimeError(f\"No base64 data found in image object: {first}\")\n\n    # 4) Decode and persist\n    response_bedrock_finalimage = base64.b64decode(b64_data)\n    poster_name = 'posterName'+ datetime.datetime.today().strftime('%Y-%M-%D-%M-%S')\n\n    # upload image to S3\n    response_s3=client_s3.put_object(\n        Bucket='ai-create-image',\n        Body=response_bedrock_finalimage,\n        Key=poster_name)\n\n    # generate pre-signed url - temporary, secure URL that gives time-limited access to a specific S3 object \n    # \u2014 without making that object public\n    generate_presigned_url = client_s3.generate_presigned_url('get_object', Params={'Bucket':'ai-create-image','Key':poster_name}, ExpiresIn=3600)\n\n    # make sure you return the url so it would respond to API Gateway\n    return {\n        'statusCode': 200,\n        'body': generate_presigned_url\n    }\n</code></pre>"},{"location":"ds_ai_ml/ai/movie_poster/#api-gateway","title":"API Gateway","text":"<ul> <li> <p>Create a resource with method as GET. </p> <ul> <li>Method request - Validate query string parameters and headers</li> <li>URL query string parameters - prompt set to True</li> <li>Integration request settings -&gt; integrate with lambda<ul> <li>Mapping templates -  <code>{\"prompt\":\"$input.params('prompt')\"}</code></li> </ul> </li> </ul> </li> <li> <p>Deploy - always create a stage called \"Dev\" and re-deploy couple of times to work properly.</p> <ul> <li>Test method - Query strings -&gt; prompt=image of a cat</li> </ul> </li> </ul> <p>You would get a pre-signed url as response, click on that, image is downloaded from the s3 bucket.</p>"},{"location":"ds_ai_ml/ai/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/ai/overview/#gen-ai-context","title":"Gen AI context","text":""},{"location":"ds_ai_ml/ai/overview/#ai-overvierw","title":"AI Overvierw","text":"<p>Making intelligient machines especially computer programs that simulate human intelligience and decision making. e.g: self drivinf cars, recomendation engines from netwfils or ammazon</p>"},{"location":"ds_ai_ml/ai/overview/#ml-overvierw","title":"ML Overvierw","text":"<p>Algorithm is trained using historical data to make predictions on the new data.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#supervisor-learning","title":"supervisor learning","text":"<p>algorithms(classic/regression) are trained to use labelled data. i.e raw data you annotate which will be provided to algorithm.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#unsupervisoed-learning","title":"unsupervisoed learning","text":"<p>algorithms(Clustering/Association) are trained to use unlabelled data. i.e raw data you annotate which will be provided to algorithm.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#reinforcements","title":"reinforcements","text":"<p>train algorith on trail and error approach. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#deep-learning-and-artifical-neural-networks","title":"Deep learning and Artifical neural networks","text":"<p>Artifical neural networks - mimic the structure of human brain</p> <p>Deep leanring - subset of ML, that focus on building artifical neural networks that can learn from data. </p> <p></p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#generative-ai","title":"Generative AI","text":"<p>model in GenAI fundational model which is <code>[ data + trained alogorithm ]</code> which generates text, images, videos from trained data. e.g ChatGPT/GPT</p> <p></p> <p>traditional machine learning models have disadvantage which is over come by fondational models</p> <p></p> <p>key characteristic of foundational models and its uses. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#working","title":"Working","text":"<p>What are tokens, parameters and temperature ?</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#use-cases","title":"use-cases","text":""},{"location":"ds_ai_ml/ai/overview/#categorization","title":"categorization","text":""},{"location":"ds_ai_ml/ai/overview/#aws-ai-services","title":"AWS AI services","text":""},{"location":"ds_ai_ml/ai/overview/#ec2-compute-ai-service","title":"EC2 Compute AI service","text":"<p>AWS Tranium and Inferentia - Train and deploy models using EC2 machines.</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#sagemaker-ai","title":"SageMaker AI","text":"<p>Pre-generative traditional AI/ML service to help you build, train and deploy machine learning models.  you are now using Amazon bedrock service instead of this traditional mode, but this model can be used for analytics. </p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#bedrock","title":"bedrock","text":""},{"location":"ds_ai_ml/ai/overview/#q-business","title":"Q business","text":""},{"location":"ds_ai_ml/ai/overview/#q-developer","title":"Q developer","text":""},{"location":"ds_ai_ml/ai/overview/#aws-bedrock-arch","title":"AWS Bedrock Arch","text":""},{"location":"ds_ai_ml/ai/overview/#randomness-and-diversity","title":"Randomness and diversity","text":""},{"location":"ds_ai_ml/ai/overview/#length","title":"Length","text":""},{"location":"ds_ai_ml/ai/overview/#repetations","title":"repetations","text":"<p>Available only in few models</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>Synchronous invocation</li> <li>Async invocation</li> <li>Stream based(polling)</li> </ul>"},{"location":"ds_ai_ml/ai/overview/#aws-api-gateway","title":"AWS API Gateway","text":""},{"location":"ds_ai_ml/ai/overview/#vectors","title":"vectors","text":""},{"location":"ds_ai_ml/ai/overview/#embeddings-model","title":"embeddings model","text":""},{"location":"ds_ai_ml/ai/overview/#data-chunking","title":"Data Chunking","text":""},{"location":"ds_ai_ml/ai/overview/#vector-store","title":"Vector store","text":""},{"location":"ds_ai_ml/ai/overview/#vector-search","title":"Vector search","text":""},{"location":"ds_ai_ml/ai/overview/#cosing-similarity","title":"Cosing similarity","text":"<p>alogorith: cosine</p> <p></p>"},{"location":"ds_ai_ml/ai/overview/#k-nearest-neighbor-search","title":"k-nearest neighbor search","text":""},{"location":"ds_ai_ml/ai/text_summariztion/","title":"text summarization","text":""},{"location":"ds_ai_ml/ai/text_summariztion/#aws-demo-services","title":"AWS Demo Services","text":"<pre><code>import json, boto3\n\ndef lambda_handler(event, context):\n    client = boto3.client(\"bedrock-runtime\")\n    prompt = event.get(\"prompt\", \"Explain VPC peering simply.\")\n\n    body = {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": [{\"text\": prompt}]}\n        ],\n        \"inferenceConfig\": {\n            \"maxTokens\": 256,\n            \"temperature\": 0.7,\n            \"topP\": 0.9\n        }\n    }\n\n    resp = client.invoke_model(\n        modelId=\"amazon.nova-lite-v1:0\",\n        contentType=\"application/json\",\n        accept=\"application/json\",\n        body=json.dumps(body),\n    )\n\n    data = json.loads(resp[\"body\"].read())\n    # Nova chat response:\n    text = (data.get(\"output\", {})\n                .get(\"message\", {})\n                .get(\"content\", [{}])[0]\n                .get(\"text\", \"\"))\n\n    return {\"statusCode\": 200, \"body\": json.dumps({\"response\": text})}\n</code></pre>"},{"location":"ds_ai_ml/ai/text_summariztion/#api-gateway","title":"API Gateway","text":"<ul> <li> <p>Create a resource with method as GET. </p> <ul> <li>Method request - Validate query string parameters and headers</li> <li>URL query string parameters - prompt set to True</li> <li>Integration request settings -&gt; integrate with lambda<ul> <li>Mapping templates -  <code>{\"prompt\":\"$input.params('prompt')\"}</code></li> </ul> </li> </ul> </li> <li> <p>Deploy - always create a stage called \"Dev\" and re-deploy couple of times to work properly.</p> <ul> <li>Test method - Query strings -&gt; prompt=image of a cat</li> </ul> </li> </ul>"},{"location":"ds_ai_ml/ds/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/ds/overview/#data-science-methodology","title":"Data Science Methodology","text":"<p>Module 1: From Problem to Approach</p> <p>Business Understanding Analytic Approach</p> <p>Module 2: From Requirements to Collection</p> <p>Data Requirements Data Collection</p> <p>Module 3: From Understanding to Preparation</p> <p>Data Understanding Data Preparation</p> <p>Module 4: From Modeling to Evaluation</p> <p>Modeling Evaluation</p> <p>Module 5: From Deployment to Feedback</p> <p>Deployment Feedback</p>"},{"location":"ds_ai_ml/ds/overview/#business-understanding","title":"Business understanding","text":"<p>Data science methodology begins with spending the time to seek clarification, to attain what can be referred to as a business understanding. Having this understanding is placed at the beginning of the methodology because getting clarity around the problem to be solved, allows you to determine which data will be used to answer the core question.</p> <p>Establishing a clearly defined question starts with understanding the GOAL of the person who is asking the question. For example, if a business owner asks:  \"How can we reduce the costs of performing an activity?\"</p> <p>We need to understand, is the goal to improve the efficiency of the activity? Or is it to increase the businesses profitability?</p> <p>Once the goal is clarified, the next piece of the puzzle is to figure out the objectives that are in support of the goal. By breaking down the objectives, structured discussions can take place where priorities can be identified in a way that can lead to organizing and planning on how to tackle the problem.Depending on the problem, different stakeholders will need to be engaged in the discussion to help determine requirements and clarify questions.</p> <p>Case study for \"Business Understanding\" In the case study, the question being asked is: What is the best way to allocate the limited healthcare budget to maximize its use in providing quality care?</p> <p>This question is one that became a hot topic for an American healthcare insurance provider. As public funding for readmissions was decreasing, this insurance company was at risk of having to make up for the cost difference,which could potentially increase rates for its customers. Knowing that raising insurance rates was not going to be a popular move, the insurance company sat down with the health care authorities in its region and brought in IBM data scientists to see how data science could be applied to the question at hand. Before even starting to collect data, the goals and objectives needed to be defined. After spending time to determine the goals and objectives, the team prioritized \"patient readmissions\" as an effective area for review. With the goals and objectives in mind, it was found that approximately 30% of individuals who finish rehab treatment would be readmitted to a rehab center within one year; and that 50% would be readmitted within five years. After reviewing some records, it was discovered that the patients with congestive heart failure were at the top of the readmission list. It was further determined that a decision-tree model could be applied to review this scenario, to determine why this was occurring. To gain the business understanding that would guide the analytics team in formulating and performing their first project, the IBM Data scientists, proposed and delivered an on-site workshop to kick things off. The key business sponsors involvement throughout the project was critical, in that the sponsor:</p> <ul> <li>Set overall direction</li> <li>Remained engaged and provided guidance.</li> <li>Ensured necessary support, where needed.</li> <li>Finally, four business requirements were identified for whatever model would be built, i.e<ul> <li>Predicting readmission outcomes for those patients with Congestive Heart Failure</li> <li>Predicting readmission risk.</li> <li>Understanding the combination of events that led to the predicted outcome</li> <li>Applying an easy-to-understand process to new patients, regarding their readmission risk.</li> </ul> </li> </ul>"},{"location":"ds_ai_ml/ds/overview/#analytic-approach","title":"Analytic Approach","text":"<p>Once the problem to be addressed is defined, the appropriate analytic approach for the problem is selected in the context of the business requirements.</p> <p>Once a strong understanding of the question is established, the analytic approach can be selected. This means identifying what type of patterns will be needed to address the question most effectively. </p> <ul> <li> <p>Question is to determine probabilities of an action, then a predictive model might be used.</p> </li> <li> <p>Question is to show relationships, a descriptive approach maybe be required. This would be one that would look at clusters of similar activities based on events and preferences.</p> </li> <li> <p>Question is to Statistical analysis applies to problems that require counts.</p> </li> <li> <p>Question requires a yes/ no answer, then a classification approach to predicting a response would be suitable.</p> </li> </ul> <p>Machine Learning can be used to identify relationships and trends in data that might otherwise not be accessible or identified.</p> <p>In the case where the question is to learn about human behaviour, then an appropriate response would be to use Clustering Association approaches.</p> <p>Case study related to applying Analytic Approach.,  a decision tree classification model was used to identify the combination of conditions leading to each patient's outcome.</p> <p>In this approach, examining the variables in each of the nodes along each path to a leaf, led to a respective threshold value. This means the decision tree classifier provides both the predicted outcome, as well as the likelihood of that outcome, based on the proportion at the dominant outcome, yes or no, in each group.</p> <p>From this information, the analysts can obtain the readmission risk, or the likelihood of a yes for each patient. If the dominant outcome is yes, then the risk is simply the proportion of yes patients in the leaf. If it is no, then the risk is 1 minus the proportion of no patients in the leaf.</p> <p>A decision tree classification model is easy for non-data scientists to understand and apply, to score new patients for their risk of readmission. Clinicians can readily see what conditions are causing a patient to be scored as high-risk and multiple models can be built and applied at various points during hospital stay. This gives a moving picture of the patient's risk and how it is evolving with the various treatments being applied. For these reasons, the decision tree classification approach was chosen for building the Congestive Heart Failure readmission model.</p>"},{"location":"ds_ai_ml/ds/overview/#data-requirements","title":"Data Requirements","text":"<p>Building on the understanding of the problem at hand, and then using the analytical approach selected, the Data Scientist is ready to get started.</p> <p>Now let's look at some examples of the data requirements within the data science methodology.</p> <p>Prior to undertaking the data collection and data preparation stages of the methodology, it's vital to define the data requirements for decision-tree classification. This includes identifying the necessary data content, formats and sources for initial data collection.</p> <p>So now, let's look at the case study related to applying \"Data Requirements\".</p> <p>In the case study, the first task was to define the data requirements for the decision tree classification approach that was selected. This included selecting a suitable patient cohort from the health insurance providers member base. In order to compile the complete clinical histories, three criteria were identified for inclusion in the cohort.</p> <p>First, a patient needed to be admitted as in-patient within the provider service area, so they'd have access to the necessary information. Second, they focused on patients with a primary diagnosis of congestive heart failure during one full year. Third, a patient must have had continuous enrollment for at least six months, prior to the primary admission for congestive heart failure, so that complete medical history could be compiled.</p> <p>Congestive heart failure patients who also had been diagnosed as having other significant medical conditions, were excluded from the cohort because those conditions would cause higher-than-average re-admission rates and, thus, could skew the results. Then the content, format, and representations of the data needed for decision tree classification were defined. This modeling technique requires one record per patient, with columns representing the variables in the model. To model the readmission outcome, there needed to be data covering all aspects of the patient's clinical history. This content would include admissions, primary, secondary, and tertiary diagnoses, procedures, prescriptions, and other services provided either during hospitalization or throughout patient/doctor visits. </p> <p>Thus, a particular patient could have thousands of records, representing all their related attributes. To get to the one record per patient format, the data scientists rolled up the transactional records to the patient level, creating a number of new variables to represent that information.</p> <p>This was a job for the data preparation stage, so thinking ahead and anticipating subsequent stages is important.</p>"},{"location":"ds_ai_ml/ds/overview/#data-collection","title":"Data Collection","text":"<p>Collecting data requires that you know the source or, know where to find the data elements that are needed.</p> <p>After the initial data collection is performed, an assessment by the data scientist takes place to determine whether or not they have what they need.</p> <p>Once the data ingredients are collected, then in the data collection stage, the data scientist will have a good understanding of what they will be working with. Techniques such as descriptive statistics and visualization can be applied to the data set, to assess the content, quality, and initial insights about the data. Gaps in data will be identified and plans to either fill or make substitutions will have to be made. In essence, the ingredients are now sitting on the cutting board.</p> <p>So now, let's look at the case study related to applying \"Data Collection\".</p> <p>demographic, clinical and coverage information of patients, provider information, claims records, as well as pharmaceutical and other information related to all the diagnoses of the congestive heart failure patients.</p> <p>For this case study, certain drug information was also needed, but that data source was not yet integrated with the rest of the data sources. This leads to an important point: It is alright to defer decisions about unavailable data, and attempt to acquire it at a later stage.</p> <p>For example, this can even be done after getting some intermediate results from the predictive modeling. If those results suggest that the drug information might be important in obtaining a good model, then the time to try to get it would be invested. As it turned out though, they were able to build a reasonably good model without this drug information.</p> <p>DBAs and programmers often work together to extract data from various sources, and then merge it. This allows for removing redundant data, making it available for the next stage of the methodology, which is data understanding.</p> <p>At this stage, if necessary, data scientists and analytics team members can discuss various ways to better manage their data, including automating certain processes in the database, so that data collection is easier and faster.</p>"},{"location":"ds_ai_ml/ds/overview/#data-understanding","title":"Data Understanding","text":"<p>Data understanding encompasses all activities related to constructing the data set. Essentially, the data understanding section of the data science methodology answers the question:  Is the data that you collected representative of the problem to be solved?</p> <p>Case Study:</p> <p>In order to understand the data related to congestive heart failure admissions, descriptive statistics needed to be run against the data columns that would become variables in the model.</p> <ul> <li> <p>First, these statistics included Hearst, univariates, and statistics on each variable, such as mean, median, minimum, maximum, and standard deviation.</p> </li> <li> <p>Second, pairwise correlations were used, to see how closely certain variables were related, and which ones, if any, were very highly correlated, meaning that they would be essentially redundant, thus making only one relevant for modeling.</p> </li> <li> <p>Third, histograms of the variables were examined to understand their distributions. Histograms are a good way to understand how values or a variable are distributed, and which sorts of data preparation may be needed to make the variable more useful in a model.</p> </li> </ul> <p>For example, for a categorical variable that has too many distinct values to be informative in a model, the histogram would help them decide how to consolidate those values. The univariates, statistics, and histograms are also used to assess data quality.</p> <p>From the information provided, certain values can be re-coded or perhaps even dropped if necessary, such as when a certain variable has many missing values. The question then becomes, does \"missing\" mean anything? Sometimes a missing value might mean \"no\", or \"0\" (zero), or at other times it simply means \"we don't know\". Or, if a variable contains invalid or misleading values, such as a numeric variable called \"age\" that contains 0 to 100 and also 999, where that \"triple-9\" actually means \"missing\", but would be treated as a valid value unless we corrected it.</p> <p>Initially, the meaning of congestive heart failure admission was decided on the basis of a primary diagnosis of congestive heart failure. But working through the data understanding stage revealed that the initial definition was not capturing all of the congestive heart failure admissions that were expected, based on clinical experience. This meant looping back to the data collection stage and adding secondary and tertiary diagnoses, and building a more comprehensive definition of congestive heart failure admission.</p> <p>This is just one example of the interactive processes in the methodology. The more one works with the problem and the data, the more one learns and therefore the more refinement that can be done within the model, ultimately leading to a better solution to the problem.</p>"},{"location":"ds_ai_ml/ds/overview/#data-preparation","title":"Data Preparation","text":"<p>In a sense, data preparation is similar to washing freshly picked vegetables in so far as unwanted elements, such as dirt or imperfections, are removed. Together with data collection and data understanding, data preparation is the most time-consuming phase of a data science project, typically taking seventy percent and even up to even ninety percent of the overall project time. Automating some of the data collection and preparation processes in the database, can reduce this time to as little as 50 percent.</p> <p>Case Study:</p> <p>Transforming data in the data preparation phase is the process of getting the data into a state where it may be easier to work with. Specifically, the data preparation stage of the methodology answers the question: What are the ways in which data is prepared?</p> <p>To work effectively with the data, it must be prepared in a way that addresses missing or invalid values and removes duplicates, toward ensuring that everything is properly formatted.</p> <p>Feature engineering is also part of data preparation. It is the process of using domain knowledge of the data to create features that make the machine learning algorithms work. A feature is a characteristic that might help when solving a problem. Features within the data are important to predictive models and will influence the results you want to achieve.</p> <p>Feature engineering is critical when machine learning tools are being applied to analyze the data. When working with text, text analysis steps for coding the data are required to be able to manipulate the data. The data scientist needs to know what they're looking for within their dataset to address the question. </p> <p>The text analysis is critical to ensure that the proper groupings are set, and that the programming is not overlooking what is hidden within. The data preparation phase sets the stage for the next steps in addressing the question. While this phase may take a while to do, if done right the results will support the project. If this is skipped over, then the outcome will not be up to par and may have you back at the drawing board. It is vital to take your time in this area, and use the tools available to automate common steps to accelerate data preparation. Make sure to pay attention to the detail in this area. After all, it takes just one bad ingredient to ruin a fine meal.</p>"},{"location":"ds_ai_ml/ds/overview/#modelling","title":"Modelling","text":"<p>Modelling is the stage in the data science methodology where the data scientist has the chance to sample the sauce and determine if it's bang on or in need of more seasoning!</p> <p>two key questions: First, what is the purpose of data modeling, and second, what are some characteristics of this process?</p> <p>Data Modelling focuses on developing models that are either descriptive or predictive.</p> <p>An example of a descriptive model might examine things like: if a person did this, then they're likely to prefer that.</p> <p>A predictive model tries to yield yes/no, or stop/go type outcomes.</p> <p>These models are based on the analytic approach that was taken, either statistically driven or machine learning driven. The data scientist will use a training set for predictive modelling. A training set is a set of historical data in which the outcomes are already known. The training set acts like a gauge to determine if the model needs to be calibrated. In this stage, the data scientist will play around with different algorithms to ensure that the variables in play are actually required.</p> <p>The success of data compilation, preparation and modelling, depends on the understanding of the problem at hand, and the appropriate analytical approach being taken. The data supports the answering of the question, and like the quality of the ingredients in cooking, sets the stage for the outcome. Constant refinement, adjustments and tweaking are necessary within each step to ensure the outcome is one that is solid.</p> <p>In John Rollins' descriptive Data Science Methodology, the framework is geared to do 3 things:  First, understand the question at hand.  Second, select an analytic approach or method to solve the problem, and third, obtain, understand, prepare, and model the data.</p> <p>The end goal is to move the data scientist to a point where a data model can be built to answer the question. With dinner just about to be served and a hungry guest at the table, the key question is: Have I made enough to eat? Well, let's hope so.</p> <p>In this stage of the methodology, model evaluation, deployment, and feedback loops ensure that the answer is near and relevant. This relevance is critical to the data science field overall, as it \u00eds a fairly new field of study, and we are interested in the possibilities it has to offer. The more people that benefit from the outcomes of this practice, the further the field will develop.</p>"},{"location":"ds_ai_ml/ds/overview/#evaluation","title":"Evaluation","text":"<p>A model evaluation goes hand-in-hand with model building as such, the modeling and evaluation stages are done iteratively. Model evaluation is performed during model development and before the model is deployed.</p> <p>Evaluation allows the quality of the model to be assessed but it's also an opportunity to see if it meets the initial request. Evaluation answers the question: Does the model used really answer the initial question or does it need to be adjusted?</p> <p>Model evaluation can have two main phases.</p> <p>The first is the diagnostic measures phase, which is used to ensure the model is working as intended. If the model is a predictive model, a decision tree can be used to evaluate if the answer the model can output, is aligned to the initial design. It can be used to see where there are areas that require adjustments. If the model is a descriptive model, one in which relationships are being assessed, then a testing set with known outcomes can be applied, and the model can be refined as needed.</p> <p>The second phase of evaluation that may be used is statistical significance testing. This type of evaluation can be applied to the model to ensure that the data is being properly handled and interpreted within the model. This is designed to avoid unnecessary second guessing when the answer is revealed.</p> <p>Case Study:</p> <p>Let's look at one way to find the optimal model through a diagnostic measure based on tuning one of the parameters in model building. Specifically we'll see how to tune the relative cost of misclassifying yes and no outcomes. As shown in this table, four models were built with four different relative misclassification costs. As we see, each value of this model-building parameter increases the true-positive rate, or sensitivity, of the accuracy in predicting yes, at the expense of lower accuracy in predicting no, that is, an increasing false-positive rate.</p> <p>The question then becomes, which model is best based on tuning this parameter? For budgetary reasons, the risk-reducing intervention could not be applied to most or all congestive heart failure patients, many of whom would not have been readmitted anyway. On the other hand, the intervention would not be as effective in improving patient care as it should be, with not enough high-risk congestive heart failure patients targeted.</p> <p>So, how do we determine which model was optimal? As you can see on this slide, the optimal model is the one giving the maximum separation between the blue ROC curve relative to the red base line. We can see that model 3, with a relative misclassification cost of 4-to-1, is the best of the 4 models. And just in case you were wondering, ROC stands for receiver operating characteristic curve, which was first developed during World War II to detect enemy aircraft on radar. It has since been used in many other fields as well. Today it is commonly used in machine learning and data mining. The ROC curve is a useful diagnostic tool in determining the optimal classification model.</p> <p>This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied. In this case, the criterion is a relative misclassification cost. By plotting the true-positive rate against the false-positive rate for different values of the relative misclassification cost, the ROC curve helped in selecting the optimal model.</p>"},{"location":"ds_ai_ml/ds/overview/#deployment","title":"Deployment","text":"<p>While a data science model will provide an answer, the key to making the answer relevant and useful to address the initial question, involves getting the stakeholders familiar with the tool produced.</p> <p>In a business scenario, stakeholders have different specialties that will help make this happen, such as the solution owner, marketing, application developers, and IT administration. Once the model is evaluated and the data scientist is confident it will work, it is deployed and put to the ultimate test. Depending on the purpose of the model, it may be rolled out to a limited group of users or in a test environment, to build up confidence in applying the outcome for use across the board.</p> <p>Case Study</p> <p>In preparation for solution deployment, the next step was to assimilate the knowledge for the business group who would be designing and managing the intervention program to reduce readmission risk. In this scenario, the business people translated the model results so that the clinical staff could understand how to identify high-risk patients and design suitable intervention actions.</p> <p>The goal, of course, was to reduce the likelihood that these patients would be readmitted within 30 days after discharge. During the business requirements stage, the Intervention Program Director and her team had wanted an application that would provide automated, near real-time risk assessments of congestive heart failure. It also had to be easy for clinical staff to use, and preferably through browser-based application on a tablet, that each staff member could carry around. This patient data was generated throughout the hospital stay. It would be automatically prepared in a format needed by the model and each patient would be scored near the time of discharge. Clinicians would then have the most up-to-date risk assessment for each patient, helping them to select which patients to target for intervention after discharge. As part of solution deployment, the Intervention team would develop and deliver training for the clinical staff. Also, processes for tracking and monitoring patients receiving the intervention would have to be developed in collaboration with IT developers and database administrators, so that the results could go through the feedback stage and the model could be refined over time.</p> <p>This map is an example of a solution deployed through a Cognos application. In this case, the case study was hospitalization risk for patients with juvenile diabetes. Like the congestive heart failure use case, this one used decision tree classification to create a risk model that would serve as the foundation for this application. The map gives an overview of hospitalization risk nationwide, with an interactive analysis of predicted risk by a variety of patient conditions and other characteristics. This slide shows an interactive summary report of risk by patient population within a given node of the model, so that clinicians could understand the combination of conditions for this subgroup of patients. And this report gives a detailed summary on an individual patient, including the patient's predicted risk and details about the clinical history, giving a concise summary for the doctor.</p>"},{"location":"ds_ai_ml/ds/overview/#feedback","title":"Feedback","text":"<p>Once in play, feedback from the users will help to refine the model and assess it for performance and impact. The value of the model will be dependent on successfully incorporating feedback and making adjustments for as long as the solution is required. Throughout the Data Science Methodology, each step sets the stage for the next. Making the methodology cyclical, ensures refinement at each stage in the game. The feedback process is rooted in the notion that, the more you know, the more that you'll want to know.</p> <p>Once the model is evaluated and the data scientist is confident it'll work, it is deployed and put to the ultimate test: actual, real-time use in the field.</p> <p>Case Study</p> <p>The plan for the feedback stage included these steps:</p> <p>First, the review process would be defined and put into place, with overall responsibility for measuring the results of a \"flying to risk\" model of the congestive heart failure risk population. Clinical management executives would have overall responsibility for the review process.</p> <p>Second, congestive heart failure patients receiving intervention would be tracked and their re-admission outcomes recorded.</p> <p>Third, the intervention would then be measured to determine how effective it was in reducing re-admissions.</p> <p>For ethical reasons, congestive heart failure patients would not be split into controlled and treatment groups. Instead, readmission rates would be compared before and after the implementation of the model to measure its impact. After the deployment and feedback stages, the impact of the intervention program on re-admission rates would be reviewed after the first year of its implementation. Then the model would be refined, based on all of the data compiled after model implementation and the knowledge gained throughout these stages. Other refinements included: Incorporating information about participation in the intervention program, and possibly refining the model to incorporate detailed pharmaceutical data.</p> <p>If you recall, data collection was initially deferred because the pharmaceutical data was not readily available at the time. But after feedback and practical experience with the model, it might be determined that adding that data could be worth the investment of effort and time. We also have to allow for the possibility that other refinements might present themselves during the feedback stage. Also, the intervention actions and processes would be reviewed and very likely refined as well, based on the experience and knowledge gained through initial deployment and feedback.</p> <p>Finally, the refined model and intervention actions would be redeployed, with the feedback process continued throughout the life of the Intervention program.</p>"},{"location":"ds_ai_ml/mcp/Overview/","title":"Overview","text":""},{"location":"ds_ai_ml/mcp/Overview/#model-context-protocolmcp","title":"Model Context Protocol(MCP)","text":"<p>Model Context Protocol (MCP), a standard that enables AI agents to move beyond simple text generation and perform real-world actions. While traditional large language models are limited to providing information, agents use MCP to securely interact with third-party APIs, such as booking flights or managing databases. The documentation details how this protocol acts as a universal translator, allowing diverse applications to communicate without custom manual coding for every integration. </p> <p>The Model Context Protocol (MCP) enables AI agents to perform complex actions by serving as a standardised guide that helps them navigate and interact with diverse third-party platforms. While Large Language Models (LLMs) can natively generate text or images, they cannot take real-world actions on their own. MCP bridges this gap by providing agents with the necessary context to make the correct API calls to external services.</p> <p>The protocol facilitates complex tasks through several key mechanisms:</p> <p>\u2022 Standardising API Interactions: Different services, such as various airlines, often use unique API formats, URL paths, and data structures. Instead of requiring developers to write custom \"adapter code\" for every single service, MCP provides a universal standard that informs the agent about a service's specific capabilities, as well as its required input and output formats.</p> <p>\u2022 A Client-Server Architecture: MCP operates on a client-server model where the AI agent acts as a client that communicates with various MCP servers. These servers act as intermediaries for specific applications\u2014such as MongoDB, Google BigQuery, or flight systems\u2014allowing the agent to retrieve information and make data modifications without interacting with the raw API directly.</p> <p>\u2022 Capability Discovery: Through MCP, an agent can \"discover\" what a tool is capable of doing. For example, an MCP server for an airline might tell an agent it has \"search flights\" and \"book flights\" functions, specifying exactly what passenger details are required to complete a booking.</p> <p>\u2022 Enabling Multi-Step Workflows: Unlike a simple chatbot that provides a single response, an agent using MCP can perform a sequence of multiple calls. It can interact with a database, scan a codebase, check a terminal, and review transaction history across different sources until a complex task\u2014like troubleshooting a software bug or booking a complete trip\u2014is finished.</p> <p>\u2022 Agent-to-Agent Collaboration: For extremely complex scenarios, MCP and the agent-to-agent model allow specialised agents to communicate. A \"flight agent\" can discover the capabilities of a \"hotel agent,\" tasking it with finding accommodation while sharing relevant context and results back and forth to complete a multi-faceted request.</p> <p>In essence, if an AI agent is like a traveller, the Model Context Protocol is its universal translator and guidebook, providing the exact instructions needed to interact with the unique \"laws\" and \"languages\" of every different service it visits.</p>"},{"location":"ds_ai_ml/mcp/Overview/#mcp-components","title":"MCP Components","text":"<p>M - Model (AI LLM) C - Context (Giving AI context ) P - Prtocol ( Define set of standards)</p> <p></p> <p>https://modelcontextprotocol.io/docs/getting-started/intro</p>"},{"location":"ds_ai_ml/mcp/Overview/#mcp-architecture","title":"MCP Architecture","text":"<p>The architecture of an MCP server is designed around a client-server model where the server acts as a standardised intermediary between an AI agent (the client) and external data or services. Instead of an agent interacting with a third-party API directly, it communicates with the MCP server, which provides the necessary context and interface for the agent to function</p> <p>Core Functional Components</p> <ul> <li> <p>Tools: These are specific executable functions that allow the agent to take actions, such as \"search flights\" or \"book flights\". The server defines the exact input structure (e.g., passenger name, email) and output structure the agent should expect (function calls and returns data i.e decorators kind of)</p> </li> <li> <p>Resources: These provide the agent with data or files it can read and use as context, such as database records or local file structures   (just a function call)</p> </li> <li> <p>Prompts: These are predefined templates or instructions that guide the LLM on how to interact with the server's specific data or tools effectively</p> </li> </ul>"},{"location":"ds_ai_ml/mcp/Overview/#build-mcp-server","title":"Build MCP Server","text":"<p>You just need SDK to build an MCP server. </p>"},{"location":"ds_ai_ml/mcp/Overview/#mcp-inspector","title":"MCP Inspector","text":"<p>In order to test the MCP set, you use inspector</p>"},{"location":"ds_ai_ml/mcp/Overview/#mcp-client","title":"MCP client","text":"<ul> <li> <p>Roots - folders to see the MCP servers can see..</p> </li> <li> <p>Sampling - server to interact with LLM, so it won't directly connect with LLM instead ask client to check with LLM</p> </li> <li> <p>Elicitiation - sometims server wants information from user, so it would ask the MCP client to check with user for more information</p> </li> </ul>"},{"location":"ds_ai_ml/mlops/data/","title":"Data","text":""},{"location":"ds_ai_ml/mlops/data/#collection","title":"Collection","text":""},{"location":"ds_ai_ml/mlops/data/#gathering","title":"gathering","text":""},{"location":"ds_ai_ml/mlops/models/","title":"Models","text":""},{"location":"ds_ai_ml/mlops/models/#development","title":"Development","text":""},{"location":"ds_ai_ml/mlops/models/#training","title":"training","text":""},{"location":"ds_ai_ml/mlops/models/#deployment","title":"deployment","text":""},{"location":"ds_ai_ml/mlops/models/#serving","title":"serving","text":""},{"location":"ds_ai_ml/mlops/overview/","title":"Overview","text":""},{"location":"ds_ai_ml/mlops/security_gov/","title":"Security","text":""},{"location":"ds_ai_ml/mlops/security_gov/#security-intro","title":"Security Intro","text":""},{"location":"ds_ai_ml/mlops/security_gov/#governance","title":"Governance","text":""},{"location":"jobdesc/devops/","title":"devops","text":""},{"location":"jobdesc/devops/#accenture","title":"Accenture","text":"<p>Summary:As a DevOps Engineer, you will be responsible for building and setting up new development tools and infrastructure utilizing knowledge in continuous integration, delivery, and deployment (CI/CD), Cloud technologies, Container Orchestration and Security. You will build and test end-to-end CI/CD pipelines, ensuring that systems are safe against security threats. Your typical day will involve working on developing and implementing CI/CD pipelines, collaborating with cross-functional teams, and ensuring the security and efficiency of the development process.</p> <p>Roles &amp; Responsibilities:</p> <ul> <li>Expected to perform independently and become an SME.</li> <li>Required active participation/contribution in team discussions.</li> <li>Contribute in providing solutions to work related problems.</li> <li>Develop and implement CI/CD pipelines for efficient software development and deployment.</li> <li>Collaborate with cross-functional teams to ensure smooth integration and delivery of software.</li> <li>Ensure the security and integrity of systems by implementing security measures and best practices.</li> <li>Monitor and optimize the performance of CI/CD pipelines.</li> <li>Stay updated with the latest trends and technologies in DevOps and implement them in the development process.</li> </ul> <p>Professional &amp; Technical Skills:</p> <ul> <li>Must To Have Skills:Proficiency in SAP UI5 Development.</li> <li>Strong understanding of continuous integration, delivery, and deployment (CI/CD) principles.</li> <li>Experience with Cloud technologies and platforms such as AWS or Azure.</li> <li>Knowledge of container orchestration tools like Kubernetes or Docker.</li> <li>Solid understanding of security best practices and implementing security measures.</li> <li>Experience with scripting languages such as Python or Shell scripting.</li> </ul>"},{"location":"jobdesc/devops/#hcl","title":"HCL","text":"<ul> <li>Hands on experience in scripting (Shell, Python)</li> <li>Deep knowledge of the IBM Workload Scheduler</li> <li> <p>Design, develop automation suite and integrate with continuous integration process through Jenkins</p> </li> <li> <p>Implement automation and orchestration process framework for CI/CD pipeline</p> </li> <li>Knowledge of Scrum and Test Driven Development practices.</li> <li>Proficient in test automation using Java, Python and any scripting languages.</li> <li>Hands-on experience in writing, executing and monitoring automated test suites</li> <li>Profound knowledge in code deployment tools like Ansible, Chef</li> <li>Working knowledge of database, SQL queries and maintain Java web applications</li> <li>Good knowledge in container concepts, docker and kubernetes hands on experience is a value add</li> <li>Strong knowledge and hands on experience in unix OS</li> <li>Experience in network, server, application status monitoring and troubleshooting</li> <li>Possess good problem solving and debugging skills. Troubleshoot issues and coordinate with development team to - streamline code deployment to generate build</li> <li>Good understanding of build and deployment process for both on-premise and cloud</li> <li>Apply cloud computing skills to deploy the product in AWS, Azure, GCP</li> <li>Optimise the cloud computing architecture</li> <li>Conduct system tests for security, performance and availability</li> <li>Collaborate with team members to improve the process, tools, security</li> <li>Implement the best practices of CI/CD framework to increase the efficiency of development lifecycle</li> <li>Fulfilling all the commitments by timely delivering the deliverables</li> <li>Design, develop automation suite and integrate with continuous integration process through Jenkins</li> </ul>"},{"location":"jobdesc/srdevops/","title":"srdevops","text":""},{"location":"jobdesc/srdevops/#job-description-12-yrs","title":"job description 12 yrs","text":"<p>naukri senior devops job desc</p>"},{"location":"jobdesc/srdevops/#iconium-consulting","title":"Iconium Consulting","text":"<ol> <li>DevOps Implementation: \u2022 Lead the design and implementation of DevOps practices, including CI/CD pipelines, automation, and infrastructure as code (IaC). \u2022 Collaborate with software development and operations teams to streamline workflows and optimize processes. \u2022 Ensure the efficient and reliable delivery of software releases.</li> <li>Automation and CI/CD: \u2022 Create and maintain automation scripts and workflows for building, testing, and deploying applications. \u2022 Design and manage continuous integration and continuous deployment (CI/CD) pipelines. \u2022 Implement best practices for code versioning, branching, and deployment strategies.</li> <li>Infrastructure Management: \u2022 Manage and optimize cloud-based and on-premises infrastructure resources. \u2022 Implement Infrastructure as Code (IaC) principles using tools like Terraform, Ansible, or CloudFormation. \u2022 Ensure the availability, scalability, and security of infrastructure components.</li> <li>Containerization and Orchestration: \u2022 Architect and manage containerization solutions using Docker and container orchestration platforms such as Kubernetes. \u2022 Optimize container deployments for resource utilization and scalability. \u2022 Implement best practices for managing containerized applications.</li> <li>Security and Compliance: \u2022 Collaborate with security teams to implement and enforce security best practices within the DevOps processes. \u2022 Integrate security testing and vulnerability scanning into CI/CD pipelines. \u2022 Ensure compliance with industry standards and regulations.</li> <li>Monitoring and Performance: \u2022 Implement monitoring and alerting solutions to proactively identify and address issues. \u2022 Analyze system performance metrics to optimize resource usage. \u2022 Respond to incidents and troubleshoot issues to minimize downtime.</li> <li>Documentation and Knowledge Sharing: \u2022 Maintain comprehensive documentation for DevOps processes, configurations, and best practices. \u2022 Share knowledge and provide mentoring to junior team members. \u2022 Promote a culture of continuous learning and improvement.</li> </ol>"},{"location":"jobdesc/srdevops/#vco","title":"VCO","text":"<ul> <li>Design and support Infrastructure as Code automation and deployment processes</li> <li>Define, implement, and manage the provisioning, software configuration and release process for all applications and - environments</li> <li>Serve as a gatekeeper for infrastructure and system deployments; establish controls and</li> <li>processes around these functions</li> <li>Deliver new features in conjunction with the application team by providing highly available and scalable - infrastructure</li> <li>Improve infrastructure and reliability of systems through monitoring and metric collection</li> <li>Make continuous improvements to security and costs of infrastructure</li> <li>Maintain Operations Environment and Tools</li> <li>Provide production support</li> <li>Work with an Agile Development team responsible for designing, building, testing of high traffic Supply Chain platforms.</li> </ul> <p>Must Have Experience:</p> <ul> <li>Bachelors degree in computer science or equivalent experience</li> <li>Good verbal and written communication skills</li> <li>5+ years of cloud DevOps experience.</li> <li>6-8 years of cloud DevOps experience.</li> <li>Must have in-depth knowledge of Terraform, Kubernetes, AWS,Jenkins</li> <li>Nice to have: Monitoring and Metrics (Nagios, Prometheus, Grafana,New Relic)</li> <li>OS experience : Windows, Linux, Unix</li> <li>Cloud experience (AWS)</li> <li>Containerization (Docker, Kubernetes, ECR, EKS)</li> </ul>"},{"location":"jobdesc/srdevops/#next-sphere","title":"next sphere","text":"<ul> <li>Collaborating with coworkers to conceptualize, develop, and release software.</li> <li>Conducting quality assurance to ensure that the software meets prescribed guidelines.</li> <li>Rolling out fixes and upgrades to software, as needed.</li> <li>Securing software to prevent security breaches and other vulnerabilities.</li> <li>Collecting and reviewing customers' feedback to enhance user experience.</li> <li>Suggesting alterations to workflow in order to improve efficiency and success.</li> <li>Pitching ideas for projects based on gaps in the market and technological advancements.</li> </ul>"},{"location":"jobdesc/srdevops/#engminds","title":"Engminds","text":"<p>Engineersmind is focused on providing the greatest customer experience possible and hiring the most brilliant individuals to study and build technology that enhances the lives of people worldwide, all while maintaining a growing client base.</p> <p>About You and this Role We are looking for a Senior DevOps Engineer with talent to manage our group of DevOps Consultants in maintaining and improving our clients' deployments. Along with the internal team, clients, and outside IT specialists, you will collaborate to deploy, automate, and manage the software infrastructure. As a Senior DevOps engineer, you will also be responsible for setting up cloud infrastructure, monitoring schedules, and CI/CD pipelines.</p> <p>Key Responsibilities:</p> <ul> <li>Preferred to have certification - AWS associate level and Azure (good to have)</li> <li>Deep practical knowledge of AWS services i.e. S3, EC2, API Gateway, Load balancer, Secrets manager and route53, - AMI, RDS Pand Auroa services etc.</li> <li>Good to have knowledge of Azure data platform especially their build process Datalake and Power BI with DevOps - perspective, this is crucial</li> <li>Expert with terraform and CloudFormation</li> <li>Must know the best security practices for both cloud and on Prem as we work mostly with healthcare and financial - clients and security is paramount for us.</li> <li>Take charge of Continuous Deployment (CD), Continuous Improvement (CI) Github action, Argo cd, DevOps, and System - Integration. Interacting with project teams, the internal engineering department, and external customers.</li> <li>Manage and enhance cloud infrastructure such as AWS, Azure, and GCP.</li> <li>Discuss operating requirements for software solutions with management SDLC.</li> <li>Exchange information regarding the dangers, operational impact, and alternatives for information systems.</li> <li>As junior software engineers gain experience and assume responsibility for DevOps tasks, mentor them.</li> <li>Supervise the installation and configuration of the solution.</li> <li>Collaborate with developers on software specifications and evaluate test stage outcomes.</li> <li>Creating interface simulators and designing automated module deployments</li> <li>Completely update scripts and code and fix any problems with product implementation.</li> <li>Oversee routine maintenance procedures and perform diagnostic tests.</li> <li>Document processes and monitor performance indicators.</li> <li>Follow recommended practices for network administration and cybersecurity.</li> <li>Assist our customer in implementing the same infrastructure-as-code tools that we use internally.</li> <li>Work independently to proactively pinpoint system flaws, shortcomings, and opportunities for development.</li> <li>Keep track of problem solving, record solutions used, and produce troubleshooting manuals.</li> <li>As needed, communicate with the product development team, clients, partners and integrators of the solution, and other cross-functional teams.</li> <li>Use regular execution procedures and methods with efficacy.</li> </ul> <p>Technical Expertise:</p> <ul> <li>Proficiency in source control management, CI/CD, DevOps, and GitOps.</li> <li>Competence with Maven, Jenkins, Artifactory, Ansible, and Git.</li> <li>Competence with containerization and Kubernetes.</li> <li>Strong background in managing systems based on RedHat Linux.</li> <li>Familiarity with logging and monitoring systems like Datadog's etc</li> <li>Very good with databases both AWS and Azure</li> <li>Must know about Vue, React, TypeScript or JavaScript for modern web clients, and Restful APIs and how to do - automated deployment.</li> <li>Experience with cloud infrastructure management and automation technologies, as well as familiarity with a - comprehensive range of AWS infrastructure solutions (EBS, EC2, RDS, S3, EC2, CloudFront, Route 53, Secret Manager, ACM, Lambda, VPC).</li> </ul> <p>Secondary Skills:</p> <ul> <li>It is necessary to take full responsibility and accountability for all duties, including obtaining requirements, - finishing technical work, documenting, and assisting with delivery.</li> <li>Self-starter who is at ease compiling data from many sources.</li> <li>Outstanding analytical and debugging abilities, including the ability to troubleshoot complicated systems with - several tiers of technology.</li> <li>Demonstrated aptitude at picking up new tools, languages, and software development techniques.</li> <li>Excellent and clear communication abilities, both written and spoken.</li> <li>Dedicated, meticulous, customer-focused, and a team player.</li> <li>Excellent interpersonal abilities.</li> </ul>"},{"location":"jobdesc/srdevops/#yamaha-motor-co","title":"Yamaha Motor Co","text":"<p>Role</p> <p>As a DevOps Engineer at Yamaha  you will work with the Engineering and Data Science teams to deliver amazing new features and take our product to the next level. You will be a part of a highly creative, agile, human centred team who values your happiness, growth and personal development. You\u2019ll be a self-starter and highly motivated thinker who loves to learn new things and enjoys when no two days are the same. The role involves working in an agile engineering team, assisting in the development and continuous improvement of our cloud infrastructure and systems.</p> <p>To be successful in this role you will have:</p> <p>\u2022 Strong written and verbal communication skills</p> <p>\u2022 Scripting experience in Python and Bash</p> <p>\u2022 Hands on experience with Linux and Windows systems</p> <p>\u2022 Knowledge of Git commands and branching</p> <p>\u2022 Experience managing in Azure networking concepts including but not limited to virtual networks, route tables, Hub and Spoke network design.</p> <p>\u2022 Experience with the following Azure services:</p> <p>o Azure Firewall</p> <p>o Azure Kubernetes Service</p> <p>o Azure VM Scale Sets</p> <p>o Azure Functions</p> <p>o Event Hubs</p> <p>o Virtual Networks</p> <p>o Cosmos DB</p> <p>o Azure API Management o Azure Storage (ADLS Gen 2) o Azure AD &amp; DNS \u2022 Experience with Terraform, Jenkins CI/CD Declarative Pipelines, Grafana</p> <p>\u2022 Experience with Databricks</p> <p>\u2022 Experience with Azure DevOps pipelines</p> <p>\u2022 Experience with Kubernetes Administration and Management, minimum CKA certification or equivalent experience</p> <p>\u2022 Experience with Active Directory and DNS management</p> <p>Skills that will be highly desired but not essential:</p> <p>\u2022 Experience with SAML integration using Okta as IDP</p> <p>\u2022 Experience with Okta administration</p> <p>\u2022 Experience working with or deploying SEIM/Infrastructure Logging solutions.</p> <p>Roles and Responsibilities Responsibilities</p> <p>There is no typical working week at the Yield. But here are some of the things you might be working on, dayto-day;</p> <p>\u2022 Working with software and data science teams to ensure efficient release of software and update/create CI/CD automation to enable repeatable releases.</p> <p>\u2022 Build and deploy infrastructure to support business requirements.</p> <p>\u2022 Follow and contribute towards engineering and design guidelines for developing highly scalable, available and fault tolerant systems.</p> <p>\u2022 Contributing to a cohesive, diverse, and high-performing team that is genuinely inclusive and genderbalanced.</p> <p>\u2022 Establishing good internal and external relationships, communicating verbally and in writing with key stakeholders</p> <p>\u2022 Being flexible and able to work independently, we need people who will do whatever it takes to get the job done.</p> <p>\u2022 Focus on secure infrastructure and development practices that extends to reviewing new and existing infrastructure with a focus on ensuring configuration is secure and follows principle of least privilege</p>"},{"location":"jobdesc/srdevops/#bounteous","title":"Bounteous","text":"<p>Key Responsibilities:</p> <ul> <li>Pipeline Development and Optimisation:</li> <li>Design, develop, and maintain CI/CD pipelines in GitLab from scratch</li> <li>Optimise pipelines for performance, reliability, and scalability</li> <li>Collaborate with development teams to integrate GitLab CI/CD pipelines into their workflows</li> </ul> <p>Deployment Management:</p> <ul> <li>Write deployment scripts and YAML files to automate the deployment of applications and services</li> <li>Ensure seamless deployment processes across various environments (development, testing, production)</li> <li>Testing Integration and Optimisation:</li> <li>Integrate automated unit, integration, and regression tests written in Cucumber into the GitLab pipelines</li> <li>Optimise the performance of running automated tests to ensure quick feedback cycles</li> </ul> <p>Monitoring and Tooling:</p> <ul> <li>Build and implement monitoring tools within GitLab to track pipeline performance, failures, and bottlenecks</li> <li>Develop dashboards and alerting mechanisms to provide visibility into pipeline health</li> </ul> <p>Must Have:</p> <ul> <li>Proven experience as a DevOps Engineer, with a strong focus on GitLab CI/CD</li> <li>Proficient in shell scripting and Windows PowerShell</li> <li>Experience in writing deployment scripts and YAML files for automation and configuration management</li> <li>Strong experience in integrating automated unit, integration, and regression tests written in Cucumber into CI/CD pipelines</li> <li>Strong knowledge of monitoring and logging tools (Splunk, Grafana, Dx-APM)</li> <li>Strong knowledge of Terraform, Kubernetes, Docker, Cactus, ArgoCD,</li> </ul>"},{"location":"jobdesc/sre/","title":"sre","text":""},{"location":"jobdesc/sre/#job-description-12-yrs","title":"job description 12 yrs","text":"<p>naukri senior sre job desc</p>"},{"location":"jobdesc/sre/#ibm-sre","title":"IBM-SRE","text":"<p>The ideal candidate will have 5 to 8 years of experience in ensuring the reliability, availability, and performance of critical services and systems. This role involves building and maintaining infrastructure, automating processes, and responding to incidents to ensure our systems run smoothly and efficiently.</p> <p>Infrastructure Management:</p> <ul> <li>Design, build, and maintain scalable, resilient infrastructure using cloud platforms (AWS and Azure).</li> <li>Manage and optimize Kubernetes clusters, containers, and microservices.</li> <li>Implement Infrastructure as Code (IaC) using tools like Terraform (Must), Ansible (Good to have), or CloudFormation(Good to have).</li> </ul> <p>Automation &amp; CI/CD:</p> <ul> <li>Maintain automated CI/CD pipelines to ensure rapid, safe, and reliable delivery of software.</li> <li>Automate repetitive tasks, processes, and workflows to increase efficiency and reduce human error.</li> <li>Implement and maintain monitoring, logging, and alerting systems to ensure visibility into system performance.</li> </ul> <p>Cost Optimization:</p> <ul> <li>Set up monitoring and reporting tools to track cloud spending in real-time.</li> <li>Regularly review the architecture and operations to identify areas where costs can be reduced. This includes - evaluating new tools, services, or practices that could lead to further cost savings.</li> <li>Collaborate with development teams to ensure that cost-efficient practices are followed in software design and - deployment.</li> <li>Recommend and manage the purchase of reserved instances, savings plans, or other discounts offered by cloud providers to reduce costs for long-term workloads.</li> </ul> <p>Incident Response &amp; Troubleshooting:</p> <ul> <li>Respond to and resolve incidents in a timely manner, ensuring minimal downtime and impact on customers.</li> <li>Perform root cause analysis and post-mortem reviews to prevent recurrence of issues.</li> <li>Collaborate with development teams to improve system reliability through proactive issue identification and resolution.</li> </ul> <p>Performance Optimization:</p> <ul> <li>Monitor system performance and capacity, and implement improvements to optimize efficiency and scalability.</li> <li>Analyze and improve application performance, ensuring high availability and low latency.</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>Ensure security best practices are followed across the infrastructure.</li> <li>Implement security controls and monitoring to protect against vulnerabilities and threats.</li> <li>Work with compliance teams to ensure systems adhere to regulatory requirements.</li> </ul> <p>Collaboration &amp; Communication:</p> <ul> <li>Work closely with software engineers, product managers, platform team, Global Support and other stakeholders to - ensure system reliability aligns with business goals.  </li> <li>Provide guidance and mentorship to junior SREs and other team members.</li> <li>Document processes, procedures, and best practices for the broader team.</li> <li>Required Technical and Professional Expertise</li> <li>5-8 years of experience in Site Reliability Engineering, DevOps, or a similar role.</li> <li>Strong experience with cloud platforms (AWS and Azure) and cloud-native technologies.</li> <li>Proficiency in scripting languages (e.g., Python, Bash) and automation tools.</li> <li>Experience with containerization (Docker, Kubernetes) and orchestration.</li> <li>Knowledge of networking, security, and infrastructure best practices.</li> <li>Familiarity with monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack).</li> <li>Strong problem-solving skills and ability to work under pressure.</li> <li>Excellent communication and collaboration skills.</li> <li>Preferred Technical and Professional Expertise</li> <li>Experience with database management (SQL).</li> <li>Knowledge of software development practices and experience with Agile methodologies.</li> <li>Certifications in cloud platforms (AWS Certified, Azure Certified, K8s certification etc.)</li> </ul>"},{"location":"jobdesc/sre/#sre-automation-developer","title":"SRE Automation Developer","text":"<p>Job description</p> <p>The candidate will support the Siemens Xcelerator platform and will be responsible for identifying, managing, improving, and reporting on availability, resiliency, reliability, and stability efficiencies. This includes providing technical guidance and leadership to drive solutions, create &amp; enhance processes that deliver excellence. A strong relationship with the various product teams of the Xcelerator platform is necessary to support core objectives. This roles success will be defined by product teams within DISW business units meeting their SLAs.</p> <p>Responsibilities/Tasks</p> <ul> <li>Provide &amp; lead the design, deployment, automation, and scripting solutions to drive new capabilities, visibility, - and efficiency</li> <li>Collaborate with other technical platforms and partners to engineer automated and integrated solutions between - tools, services, teams that increase availability, reliability, and performance.</li> <li>Own and ensure the internal and external SLA s meet and exceed expectations</li> <li>Be part of maintaining a 24x7, global, highly available SaaS environment</li> <li>Participate in an on-call rotation that supports our production infrastructure</li> <li>Troubleshoot production availability incidents that often span across multiple teams and services.</li> <li>Lead production incident post-mortems, and contribute to solutions to prevent problem recurrence; with the goal of - automated response to all non-exceptional service conditions</li> <li>Communicate to business and technical partners on incidents as they occur when they impact system performance or availability at a critical level</li> </ul> <p>Required Knowledge/Skills, Education, and Experience</p> <p>Bachelor s Degree with at least 2+ years of IT experience or equivalent experience.</p> <ul> <li>4+ years experience with automation via scripting &amp; API development</li> <li>3+ years experience with software development in the cloud</li> <li>2+ years experience with observability tools</li> <li>(Datadog, CloudWatch, CloudTrail, Elastic Stack, Grafana, or equivalent tools)</li> <li>2+ years experience with containerization, specifically Kubernetes</li> <li>2+ years experience with Amazon Web Services (AWS) services</li> <li>2+ years experience Terraform, CloudFormation, Ansible, or equivalent tools</li> <li>2+ years experience with Python</li> <li>Preferred Knowledge/Skills, Education, and Experience</li> </ul> <p>Siemens Teamcenter software - Desired certifications include: Datadog, Kubernetes, AWS or Azure certification - 2+ years experience as a Site Reliability Engineer or equivalent role - 2+ years experience with issue/incident tracking tool - (ServiceNOW, ServiceDesk, Jira or equivalent tools) - 2+ years with log management tools (ie ELK Stack) - 2+ years experience Enterprise IT environment with distributed environments - Networking concepts, including firewalls, VPN, routing, load balancers, security and DNS - Senior level system administration experience, including troubleshooting, support, mentorship/training, and oversight Attachments</p>"},{"location":"kafka/overview/","title":"Overview","text":""},{"location":"kafka/overview/#components","title":"Components","text":"<p>Producers: producers are applications that send records to kafka topics. they are responsible for choosing which partition within the topic the record has to be sent </p> <p>Consumer:: applications that read from kafka topics. they subscribe to topics and process message in real time</p> <p>Broker: server that runs kafka. they receive messages from producers, store then in disk, serve them to consumers. they have multiple brokers to ensure load balancing and fault tolerance. </p> <p>Topic: logical channel to which producers send records and from which consumers read. they are partitoned for scalability and parallelism</p> <p>Partition:: each topic divided into partitions which are ordered, immutable and sequence of records. partitions allow kafka to scale horizontally and maintain the order of records withtin each partition. </p> <p>Zookeeper: kafka uses this for distributed coridination, config mgmt, and leader election for kafka brokers and topics. </p>"},{"location":"kafka/overview/#kafka-role","title":"Kafka role","text":"<p>Real-Time Data Ingestion: Kafka is widely used for ingesting real-time data from various sources such as logs, sensors, and user interactions. It provides a scalable and fault-tolerant way to collect and store large volumes of data.</p> <p>Stream Processing: Kafka integrates seamlessly with stream processing frameworks like Apache Flink, Apache Spark, and Kafka Streams. This allows organizations to process and analyze data in real time, enabling use cases like fraud detection, recommendation engines, and monitoring.</p> <p>Data Integration: Kafka acts as a central hub for data integration, enabling the movement of data between different systems and applications. It supports connectors for various data sources and sinks, making it easy to build data pipelines.</p> <p>Event Sourcing: Kafka is used for event sourcing, where state changes in an application are logged as a sequence of events. This approach provides a reliable and auditable way to track changes over time.</p> <p>Message Queue: Kafka can function as a distributed message queue, enabling asynchronous communication between different parts of an application. It supports decoupling of producers and consumers, which enhances the scalability and resilience of applications.</p> <p>Log Aggregation: Kafka is commonly used for log aggregation, where logs from multiple services are collected, centralized, and processed. This helps in monitoring, troubleshooting, and gaining insights from log data.</p> <p>Metrics Collection and Monitoring: Kafka can be used to collect and aggregate metrics from various systems, enabling real-time monitoring and alerting. This helps in maintaining the health and performance of applications and infrastructure.</p>"},{"location":"kafka/overview/#use-cases","title":"Use Cases","text":"<p>Real-Time Analytics: Companies use Kafka to analyze data in real time, providing insights and enabling proactive decision-making.</p> <p>Event-Driven Architectures: Kafka enables event-driven architectures where different services communicate through events, enhancing scalability and decoupling. </p> <p>Microservices: Kafka facilitates communication between microservices, allowing them to exchange messages asynchronously and reliably.</p> <p>Log Aggregation and Monitoring: Kafka is widely used for collecting and aggregating logs from various services, enabling centralized monitoring and alerting.</p> <p>Data Integration: Kafka serves as a backbone for data integration, moving data between different systems and ensuring consistency and reliability.</p>"},{"location":"kafka/overview/#kafka-usp","title":"Kafka USP","text":""},{"location":"kafka/overview/#high-throughput-and-low-latency","title":"High Throughput and Low Latency","text":"<p>Kafka is designed to handle high-throughput, real-time data streams with minimal latency. It achieves this through efficient disk storage mechanisms and high-performance networking capabilities. Kafka\u2019s architecture allows it to process millions of messages per second, making it suitable for applications requiring high throughput.</p>"},{"location":"kafka/overview/#scalability","title":"Scalability","text":"<p>Horizontal Scalability: Kafka\u2019s distributed nature allows it to scale horizontally by adding more brokers to a cluster. Each topic in Kafka is partitioned, and these partitions can be spread across multiple brokers. This architecture ensures that Kafka can handle increasing loads without degradation in performance.</p> <p>Elasticity: Kafka\u2019s partition-based architecture allows for dynamic scaling. As the load increases, more partitions and brokers can be added without downtime, providing elastic scalability.</p>"},{"location":"kafka/overview/#durability-and-fault-tolerance","title":"Durability and Fault Tolerance","text":"<p>Replication: Kafka replicates data across multiple brokers, ensuring data durability and availability. This replication mechanism guarantees that even if one or more brokers fail, the data remains accessible.</p> <p>Log-Based Storage: Kafka\u2019s use of log-based storage ensures that data is persisted on disk in an append-only fashion. This approach minimizes data corruption and allows for efficient data recovery.</p>"},{"location":"kafka/overview/#flexibility-and-versatility","title":"Flexibility and Versatility","text":"<p>Multiple Use Cases: Kafka is versatile and supports various use cases, including real-time analytics, event sourcing, log aggregation, metrics collection, and stream processing. Its ability to handle a wide range of scenarios makes it a preferred choice for many organizations.</p> <p>Integration with Ecosystem: Kafka integrates seamlessly with a wide array of tools and frameworks, such as Kafka Connect for data integration, Kafka Streams for stream processing, and external processing frameworks like Apache Flink and Apache Spark. This extensibility makes it a central component of many data architectures.</p>"},{"location":"kafka/overview/#message-ordering-and-guarantee","title":"Message Ordering and Guarantee","text":"<p>Message Ordering: Kafka ensures strict ordering of messages within a partition, which is crucial for applications where the order of events is important.</p> <p>Delivery Semantics: Kafka supports various delivery semantics, including at-most-once, at-least-once, and exactly-once delivery. This flexibility allows developers to choose the appropriate level of guarantee based on their application requirements.</p>"},{"location":"kafka/overview/#high-availability","title":"High Availability","text":"<p>Leader-Follower Architecture: Kafka\u2019s leader-follower model ensures high availability. Each partition has one leader and multiple followers. If the leader fails, one of the followers automatically takes over, ensuring continuous availability without manual intervention.</p>"},{"location":"kafka/overview/#cost-efficiency","title":"Cost Efficiency","text":"<p>Efficient Resource Utilization: Kafka\u2019s design allows for efficient use of resources, both in terms of storage and compute. Its log-structured storage mechanism minimizes disk I/O, and its distributed nature ensures load balancing across the cluster.</p> <p>Open Source: As an open-source project, Kafka eliminates licensing costs associated with proprietary messaging systems, making it a cost-effective solution for many organizations.</p>"},{"location":"kafka/overview/#active-community-and-support","title":"Active Community and Support","text":"<p>Vibrant Community: Kafka has a large and active open-source community. This community continuously contributes to the platform, ensuring it evolves with new features, performance improvements, and bug fixes.</p> <p>Commercial Support: Organizations like Confluent offer commercial support and additional features, making Kafka a viable choice for enterprises that require professional support and enhanced capabilities.</p>"},{"location":"kafka/overview/#stream-processing-capabilities","title":"Stream Processing Capabilities","text":"<p>Kafka Streams: Kafka provides a native stream processing library called Kafka Streams, which allows for building real-time processing applications directly within the Kafka ecosystem. This integration simplifies the development and deployment of stream processing applications.</p> <p>KSQL: Kafka also offers KSQL, a SQL-like language for stream processing. KSQL enables users to perform stream processing tasks using SQL queries, making it accessible to users who are more familiar with SQL than with traditional programming languages.</p>"},{"location":"linux/admin/basics/","title":"basic","text":""},{"location":"linux/admin/basics/#linux-boot-process","title":"linux boot process","text":"<p>BIOS</p> <p>upon booting the system, it would perform system integrity checks and searches, loads and executes the body loader program (e.g CDROM, Floppy, Network..etc). Once the boot loader is detached and loaded into the memory, BIOS gives control to it, BIOS would then load the MBR boot loader.</p> <p>MBR</p> <p>located in the first sector of the bootlabel disk(/dev/hda or /dev/sda) which is 512MB, i.e 446Bytes of primary boot loader info, partition table of 64 bytes and last 2 bytes of validation checks, which contains info about the GRUB or LILO. MBR loads and executes GRUB boot loader.</p> <p>GRUB</p> <p>It has the complete knowledge of the system information and would load the file in /etc/grub/grub.conf which loads the kernel images and initrd images.</p> <p>kernel</p> <p>Mounts the root file system specified in the grub and executes /sbin/init program which is the first process in the linux system and loads all the necessary drivers compilied inside which helps to access the hard drive and other hardware devices.</p> <p>init</p> <p>looks /etc/inittab file to decide the runlevel and loads all the appropriate programs from it.</p> <p>runlevel</p> <p>depending on your runlevel from the /etc/inittab all the services from the /etc/rc3.d/ which start from 'S' will be executed which has all the services coming up."},{"location":"linux/admin/basics/#login-process","title":"login process","text":"<p>once the init process completes the run-level and finally it would be executing /etc/rc.local. it will start process called getty(get terminal) which initiates the login command and opens termial device, initializes it and prints login: and waits for user to to enter username.</p> <p>once user enters his name it starts /etc/login and prompts for the password which is hidden. it would next checks the credentials by verifying with /etc/passwd and /etc/shadow password matches then it initiates user properties gathering and starts users shell. If password doesn\u2019t match then getty terminates login process and re-initiates again with new prompt.</p> <p>In next stage the getty process reads the user properties (username, UID, GID, home directory, user shell etc.) from /etc/passwd file. After that it reads <code>/etc/motd</code> file for displaying content banner message.</p> <p>shell related properties, user aliases and variables getty process reads appropriate system wide default files /etc/profile or /etc/bashrc . After the system wide default files are read the shell reads user specific login files <code>.profile</code> or <code>.login</code></p> <p>At last stage it reads shell specific configuration files (.bashrc, .bash_profile etc. for bash shell) of that user which it gets on the users home directory.</p>"},{"location":"linux/admin/basics/#job-scheduler","title":"job scheduler","text":"<p>Cron</p> <p>Field    Description    Allowed Value MIN      Minute field    0 to 59 HOUR     Hour field      0 to 23 DOM      Day of Month    1-31 MON      Month field     1-12 DOW      Day Of Week     0-6 CMD      Command         Any command to be executed.</p> <pre><code>- schedule a cron to execute at 2 am daily\n0 2 * * * /bin/sh backup.sh\n\n- Schedule a cron to execute twice a day.\n0 5,17 * * * /scripts/script.sh\n\n- Schedule a cron to execute every Sunday at 5 PM\n0 17 * * sun  /scripts/script.sh\n\n- Schedule a cron to execute every 10 minutes\n*/10 * * * * /scripts/monitor.sh\n\n- Schedule a cron to execute on selected months.\n\\* \\* \\* jan,may,aug *  /script/script.sh\n\n- Schedule a cron to execute on selected days\n0 17 * * sun,fri  /script/script.sh\n\n- Schedule a cron to execute on the first Sunday of every month.\n0 2 * * sun  [ $(date +%d) -le 07 ] &amp;&amp; /script/script.sh\n\n- Schedule a cron to execute every four hours.\n0 */4 * * * /scripts/script.sh\n\n- Schedule a cron to execute twice every Sunday and Monday\n0 4,17 * * sun,mon /scripts/script.sh\n\n- Schedule tasks to execute yearly/monthly/weekly/daily/hourly\n@yearly/@monthly/@weekly/@daily/@hourly /bin/script.sh\n</code></pre> <p>at</p> <p>Jobs created with at command are executed only once.</p> <p>Schedule a job for the coming Monday at a time twenty minutes later than the current time <code>at Monday +20 minutes</code></p> <p>Schedule a job to run at 1:45 Aug 12 2020 <code>at 1:45 081220</code></p> <p>Schedule a job to run at 3pm four days from now <code>at 3pm + 4 days</code></p> <p>Schedule a job to shutdown the system at 4:30 today <code>echo \"shutdown -h now\" | at -m 4:30</code></p> <p>Schedule a job to run five hour from now <code>at now +5 hours</code></p>"},{"location":"linux/admin/basics/#login-non-login-shells","title":"login &amp; non-login shells","text":"<p>login</p> <p>When a user successfully logs in to a Linux system via terminal/SSH/switches to a user with the su - command, a login shell is created.</p> <p>echo $0 prints hypen then its a non-login shell</p> <ul> <li>login shell invokes <code>/etc/profile</code></li> <li><code>/etc/profile</code> invokes scripts in <code>/etc/profile.d/*.sh</code>, then executes users <code>~/.bash_profile</code></li> <li><code>~/.bash_profile</code> invokes users <code>~/.bashrc</code></li> <li><code>~/.bashrc</code> invokes <code>/etc/bashrc</code></li> </ul> <p>non-login shell</p> <p>A non-login shell is started by a login shell. shell that you start from another shell or from a program is a non-login shell.</p> <p>echo $0 prints no-hypen then its a non-login shell</p> <ul> <li>Non login shell first executes ~/.bashrc</li> <li>Then ~/.bashrc executes /etc/bashrc</li> <li>/etc/bashrc calls the scripts in /etc/profile.d</li> </ul>"},{"location":"linux/admin/basics/#softlinks-vs-hardlinks","title":"softlinks Vs hardlinks","text":"<ul> <li> <p><code>hard link</code>can only be created for a file but cannot be created for directories, where as <code>soft link</code> can link to a directory.</p> </li> <li> <p>Removing the original file that hard link points to does not remove the hardlink itself, the hardlink still provides the content of the underlying file.</p> </li> <li> <p>If we remove the hard link or the symlink itself, the original file will stay intact.</p> </li> </ul>"},{"location":"linux/admin/basics/#user-addition-process","title":"user addition process","text":"<p>When invoked, useradd creates a new user account according to the options specified on the command line and the default values set in the /etc/default/useradd</p> <p>useradd also reads the content of the /etc/login.defs file. This file contains configuration for the shadow password suite such as password expiration policy, ranges of user IDs used when creating system and regular users, and more..The command adds an entry to the /etc/passwd, /etc/shadow, /etc/group and /etc/gshadow files</p> <p>password fields:   - Login username    - Hash password    - Number of days since password has changed 01-01-1979   - Mininum days for a user to change his password   - Days available for password expiry   - Password expiry warining   - Empty. </p>"},{"location":"linux/admin/basics/#umask","title":"umask","text":"<p>umask lets you create the default permission for the files and folders, user can choose to restrict permissions by using a permission masks.</p> <pre><code>0    ---    No permission\n1    --x    Execute\n2    -w-    Write\n3    -wx    Write and execute\n4    r--    Read\n5    r-x    Read and execute\n6    rw-    Read and write\n7    rwx    Read, write, and execute\n</code></pre> <p>system default permission values are 777 (rwxrwxrwx) for folders and 666 (rw-rw-rw-) for files</p> <ul> <li>The default mask for a non-root user is 002, changing the folder permissions to 775 (rwxrwxr-x), and file permissions to 664 (rw-rw-r--).</li> <li>The default mask for a root user us 022, changing the folder permissions to 755 (rwxr-xr-x), and file permissions to 644 (rw-r--r--).</li> </ul>"},{"location":"linux/admin/basics/#file-special-permissions","title":"file special permissions","text":"<p>special permission bit are set on file or folder, thereby permitting only the owner or root user of the file or folder to modify, rename or delete the concerned directory or file. No other user would be permitted to have these privileges on a file which has a sticky bit.</p>"},{"location":"linux/admin/basics/#suid","title":"suid","text":"<p>we set SUID(set-user-ID) bit on the executable this behavior can be changed, then the file will always run with privileges of the owner of the file, no matter who runs the executable. instead of 'x' in the file permission you would be getting 's' in the user's permission</p> <p>Octal String: 4</p> <p>chmod: u+s</p> <p>e.g /usr/bin/passwd, /usr/bin/gpasswd</p>"},{"location":"linux/admin/basics/#sgid","title":"sgid","text":"<p>Set-group-ID bit on a file: Set-group-ID (SGID) is similar to SUID except that, an executable with SGID bit set runs with the privileges of the group which owns of the file</p> <p>Octal String: 2</p> <p>chmod : g+s</p>"},{"location":"linux/admin/basics/#sticky","title":"sticky","text":"<p>This special permission is useful to prevent users from deleting other user\u2019s file inside a shared folder where everyone has read, write, and execute access</p> <p>Octal String: 1</p> <p>chmod: 't'</p>"},{"location":"linux/admin/basics/#swap-size","title":"swap size","text":"<pre><code>sudo swapoff -a\nsudo dd if=/dev/zero of=/swapfile bs=1G count=8\nsudo mkswap /swapfile\nsudo swapon\n</code></pre>"},{"location":"linux/admin/basics/#etcfstab-fields","title":"/etc/fstab fields","text":"<p>The block device The mountpoint The filesystem type Mount options:     rw (read-write);     suid (respect setuid and setgid bits);     dev (interpret characters and block devices on the filesystem);     exec (allow executing binaries and scripts);     auto (mount the filesystem when the -a option of the mount command is used);     nouser(make the filesystem not mountable by a standard user);     async (perform I/O operations on the filesystem asynchronously).</p> <p>file system dump Fsck order:     default = 0     root = 1     non-root = 2</p>"},{"location":"linux/admin/basics/#password-policy","title":"password policy","text":"<p>/etc/login.defs  - all the password related informations can be found here.</p> <pre><code>PASS_MAX_DAYS    99999\nPASS_MIN_DAYS    0\nPASS_MIN_LEN    5\nPASS_WARN_AGE    7\n</code></pre>"},{"location":"linux/admin/basics/#cpu-utilization-calculation","title":"cpu utilization calculation","text":"<p>Linux load averages are \"system load averages\" that show the running thread (task) demand on the system as an average number of running plus waiting threads. This measures demand, which can be greater than what the system is currently processing. Most tools show three averages, for 1, 5, and 15 minutes</p> <p>The \u201cload\u201d is not the utilization but the total queue length.</p> <p>When load averages first appeared in Linux, they reflected CPU demand, as with other operating systems. But later on Linux changed them to include not only runnable tasks, but also tasks in the uninterruptible state (TASK_UNINTERRUPTIBLE or nr_uninterruptible). This state is used by code paths that want to avoid interruptions by signals, which includes tasks blocked on disk I/O and some locks. You may have seen this state before: it shows up as the \"D\" state in the output ps and top. The ps(1) man page calls it \"uninterruptible sleep (usually IO)\".</p> <p>Adding the uninterruptible state means that Linux load averages can increase due to a disk (or NFS) I/O workload, not just CPU demand</p>"},{"location":"linux/admin/basics/#use-of-nohup","title":"use of nohup","text":"<p>Nohup (stands for no hangup) is a command that ignores the HUP signal. You might be wondering what the HUP signal is. It is basically a signal that is delivered to a process when its associated shell is terminated. Usually, when we log out, then all the running programs and processes are hangup or stopped. If we want to continue running the process even after logout or disconnection from the current shell, we can use the nohup command</p> <p><code>nohup command &amp;</code></p> <ul> <li>how do you set priority for the process ?</li> <li>Explain how DNS works in Linux ?</li> <li>What is NFS and how would you configure it ?</li> <li>what is autofs and how's it configured ?</li> <li>what are TCP wrappers ?</li> <li>Explain about the IPtables and its chains ?</li> <li>How will you find out the system activity ?</li> <li>How will you trace all the system calls ?</li> </ul>"},{"location":"linux/admin/basics/#search-the-largest-or-empty-file","title":"search the largest or empty file","text":"<pre><code>-exec CMD: The file being searched which meets the above criteria and returns 0 for as its exit status for successful command execution.\n-ok CMD : It works same as -exec except the user is prompted first.\n-inum N : Search for files with inode number \u2018N\u2019.\n-links N : Search for files with \u2018N\u2019 links.\n-name demo : Search for files that are specified by \u2018demo\u2019.\n-newer file : Search for files that were modified/created after \u2018file\u2019.\n-perm octal : Search for the file if permission is \u2018octal\u2019.\n-print : Display the path name of the files found by using the rest of the criteria.\n-empty : Search for empty files and directories.\n-size +N/-N : Search for files of \u2018N\u2019 blocks; \u2018N\u2019 followed by \u2018c\u2019can be used to measure size in characters; \u2018+N\u2019 means size &gt; \u2018N\u2019 blocks and \u2018-N\u2019 means size &lt; \u2018N\u2019 blocks.\n-user name : Search for files owned by user name or ID \u2018name\u2019.\n\\(expr \\) : True if \u2018expr\u2019 is true; used for grouping criteria combined with OR or AND.\n! expr : True if \u2018expr\u2019 is false.\n</code></pre> <p>examples:</p> <ul> <li>Search a file with pattern.  <code>find ./GFG -name *.txt</code></li> <li>How to find and delete a file with confirmation. <code>find ./GFG -name sample.txt -exec rm -i {} \\;</code></li> <li>Search for empty files and directories <code>find ./GFG -empty</code></li> <li>Search for file with entered permissions <code>find ./GFG -perm 664</code></li> <li>Search text within multiple files <code>find ./ -type f -name \"*.txt\" -exec grep 'Geek'  {} \\;</code></li> </ul>"},{"location":"linux/admin/basics/#open-files-in-linux","title":"open files in linux","text":"<p>Graceful shutdown of relevant process would have not done for an On Linux or Unix systems, deleting a file via rm or through a file manager application will unlink the file from the file system's directory structure; however, if the file is still open (in use by a running process) it will still be accessible to this process and will continue to occupy space on disk. Therefore such processes may need to be restarted before that file's space will be cleared up on the filesystem.</p> <p><code>lsof | egrep \"deleted|COMMAND\"</code></p> <p>After a file has been identified, free the file used space by shutting down the affected process. If a graceful shutdown does not work, then issue the kill command to forcefully stop it by referencing the PID.</p> <p>link</p>"},{"location":"linux/admin/basics/#diff-yum-vs-rpm","title":"diff yum Vs rpm","text":"<p>Difference between RPM and YUM.</p> parameter RPM YUM Depencies resolvement no yes Multiple package instllations no yes automatic upgrades no yes online repo support no yes Autonomy RPM is autonomous and utilizes its own database to keep information about the packages on the system. YUM is a front-end utility that uses the RPM package manager for package management. The utility also uses the RPM database in the backend. Ease of use RPM package management and handling gets complicated at times. It is the easiest way to manage RPM packages. Rollback RPM doesn't support change rollback. YUM allows any changes to be rolled back."},{"location":"linux/admin/basics/#configure-local-yum-repo","title":"configure local yum repo","text":"<p>/etc/yum.conf - Main config file for yum.</p> <pre><code>[main]\ncachedir=/var/cache/yum/$basearch/$releasever\nkeepcache=0\ndebuglevel=2\nlogfile=/var/log/yum.log\nexactarch=1\nobsoletes=1\ngpgcheck=1\nplugins=1\ninstallonly_limit=3\n\n[comments abridged]\n\n# PUT YOUR REPOS HERE OR IN separate files named file.repo\n# in /etc/yum.repos.d\n</code></pre> <p>/etc/yum.repos.d/redhat.repo</p> <pre><code>[redhat]\nname = Red Hat Enterprise Linux\nbaseurl = file:/// or http:///\nenabled = 1\ngpgcheck = 1\ngpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\nsslverify = 1\nsslcacert = /etc/rhsm/ca/redhat-uep.pem\nsslclientkey = /etc/pki/entitlement/key.pem\nsslclientcert = /etc/pki/entitlement/11300387955690106.pem\n</code></pre>"},{"location":"linux/admin/basics/#recover-corrupted-rpmdb","title":"Recover corrupted rpmdb","text":"<pre><code>tar -zcvf /backups/rpmdb-$(date +\"%d%m%Y\").tar.gz  /var/lib/rpm\nrm -f /var/lib/rpm/__db*\n/usr/lib/rpm/rpmdb_verify /var/lib/rpm/Packages\n</code></pre> <p>In case the above operation fails, meaning you still encounter errors, then you should dump and load a new database</p> <pre><code># cd /var/lib/rpm/\n# mv Packages Packages.back\n# /usr/lib/rpm/rpmdb_dump Packages.back | /usr/lib/rpm/rpmdb_load Packages\n# rpm -qa\n# rpm -vv --rebuilddb\n# /usr/lib/rpm/rpmdb_verify Packages\n</code></pre>"},{"location":"linux/admin/basics/#webserver-issues","title":"Webserver issues","text":"<ul> <li>Application is suffering from the performance problem issues, how would you troubleshoot ?</li> <li>Need to create an FTP server to my localteam to help them setup yum server by creating a new ext3 volume ?</li> <li>user is unable to login to the system, what all could be the issues ?</li> <li>How do you set the user quota enabled in linux for user's homed irectory ?</li> <li>One of the NFS client is unable to access the share, how would you troubleshoot ?</li> <li>How would configure to authenticate nginx/httpd server ?</li> <li>Webserver is unable to access, explain the troubleshooting steps ?</li> </ul>"},{"location":"linux/admin/basics/#references","title":"References","text":"<p>linuxatemyram</p> <p>dns client issue troubleshooting</p>"},{"location":"linux/admin/networking/","title":"network","text":""},{"location":"linux/admin/networking/#ositcp-ip","title":"OSI/TCP IP","text":"<p>TCP/IP model more accurately represents the suite of protocols that are deployed in modern networks.</p> <p></p> <p>The layers in the TCP/IP network model, in order, include:</p> <p>Layer 5: Application  Layer 4: Transport  Layer 3: Network/Internet Layer 2: Data Link Layer 1: Physical</p>"},{"location":"linux/admin/networking/#layer-1-the-physical-layer","title":"Layer 1: The physical layer","text":"<p>Identify the cable has been plugged in, but we can easily troubleshoot physical layer problems from the Linux command line.</p> <pre><code>ip link show\n</code></pre> <p>Any indication of DOWN in the above output for the eth0 interface. This result means that Layer 1 isn\u2019t coming up.</p> <p>We might try troubleshooting by bringing up the interface(eth0) just to rule out that network interface disabled can be ruled out. </p> <pre><code>ip link set eth0 up\nip link show\nip -br link show # prints output in more readable format\nip -s link show eth0 # prints additional statistics about interface\n</code></pre> <p>ethtool utility is an excellent option. A particularly good use case for this command is checking to see if an interface has negotiated the correct speed. </p>"},{"location":"linux/admin/networking/#layer-2-the-data-link-layer","title":"Layer 2: The data link layer","text":"<p>The data link layer is responsible for local network connectivity. The most relevant Layer 2 protocol for most sysadmins is the Address Resolution Protocol (ARP), which maps Layer 3 IP addresses to Layer 2 Ethernet MAC addresses. When a host tries to contact another host on its local network (such as the default gateway), it likely has the other host\u2019s IP address, but it doesn\u2019t know the other host\u2019s MAC address. ARP solves this issue and figures out the MAC address for us. </p> <p>If your localhost can\u2019t successfully resolve its gateway\u2019s Layer 2 MAC address, then it won\u2019t be able to send any traffic to remote networks. This problem might be caused by having the wrong IP address configured for the gateway, or it may be another issue, such as a misconfigured switch port.</p> <p><code>ip neighbor show</code></p> <p>Linux caches the ARP entry for a period of time, so you may not be able to send traffic to your default gateway until the ARP entry for your gateway times out.</p> <pre><code> # ip neighbor show\n192.168.122.170 dev eth0 lladdr 52:54:00:04:2c:5d REACHABLE\n192.168.122.1 dev eth0 lladdr 52:54:00:11:23:84 REACHABLE\n# ip neighbor delete 192.168.122.170 dev eth0\n# ip neighbor show\n192.168.122.1 dev eth0 lladdr 52:54:00:11:23:84 REACHABLE\n</code></pre>"},{"location":"linux/admin/networking/#layer-3-the-networkinternet-layer","title":"Layer 3: The network/internet layer","text":"<p>Layer 3 involves working with IP addresses. IP addressing provides hosts with a way to reach other hosts that are outside of their local network</p> <pre><code>ip -br address show\n</code></pre> <p>check the interfaces and see it has ipaddress associated with the interface.  The lack of an IP address can be caused by a local misconfiguration, such as an incorrect network interface config file, or it can be caused by problems with DHCP.</p> <p>Layer 3 is the ping utility. Ping sends an ICMP Echo Request packet to a remote host, and it expects an ICMP Echo Reply in return. If you\u2019re having connectivity issues to a remote host, ping is a common utility to begin your troubleshooting</p> <p>Many might have blocked the ping hence you use traceroute. As with ICMP, intermediate routers may filter the packets that traceroute relies on, such as the ICMP Time-to-Live Exceeded message. But more importantly, the path that traffic takes to and from a destination is not necessarily symmetric, and it\u2019s not always the same.</p> <p>Another common issue that you\u2019ll likely run into is a lack of an upstream gateway for a particular route or a lack of a default route. When an IP packet is sent to a different network, it must be sent to a gateway for further processing. The gateway should know how to route the packet to its final destination. The list of gateways for different routes is stored in a routing table.</p> <pre><code>ip route show\n</code></pre>"},{"location":"linux/admin/networking/#layer-4-the-transport-layer","title":"Layer 4: The transport layer","text":"<p>The transport layer consists of the TCP and UDP protocols, with TCP being a connection-oriented protocol and UDP being connectionless. Applications listen on sockets, which consist of an IP address and a port. Traffic destined to an IP address on a specific port will be directed to the listening application by the kernel.</p> <p>The first thing that you may want to do is see what ports are listening on the localhost</p> <p>Another common issue occurs when a daemon or service won\u2019t start because of something else listening on a port</p> <pre><code>ss -tunlp4\n</code></pre> <p>The telnet command attempts to establish a TCP connection with whatever host and port you give it. This feature is perfect for testing remote TCP connectivity</p> <pre><code>telnet database.example.com 3306\ntelnet nfs.example.com 2049\n</code></pre> <p>The netcat utility can be used for many other things, including testing TCP connectivity.Note that netcat may not be installed on your system, and it\u2019s often considered a security risk to leave lying around. You may want to consider uninstalling it when you\u2019re done troubleshooting</p> <p>similarly, nmap which is capable of doing ..</p> <ul> <li>TCP and UDP port scanning remote machines.</li> <li>OS fingerprinting.</li> <li>Determining if remote ports are closed or simply filtered.</li> </ul>"},{"location":"linux/admin/networking/#how-examplecom-works","title":"How example.com works","text":"<ul> <li>The client types www.example.com in his browser</li> <li>The operating system looks at /etc/host file,first for the ip address of www.example.com(this can be changed from /etc/nsswitch), then looks /etc/resolv.conf for the DNS server IP for that machine</li> <li>the dns server will search its database for the name www.example.com, if it finds it will give that back, if not it will query the root server(.) for the information.</li> <li>root server will return a referral to the .com TLD name server(these TLD name servers knows the address of name servers of all SLD's).In our case we searched for www.example.com so root server will give us referral to .com TLD servers.</li> <li>Now One of the TLD servers of .com will give us the referral to the DNS server resposible for example.com domain.</li> <li>The dns server for example.com domain will now give the client the ip address of www host(www is the host name.)</li> </ul> <p>finally, type dig +trace www.google.com</p>"},{"location":"linux/admin/networking/#linux-dns-client-troubleshooting","title":"Linux DNS Client Troubleshooting","text":"<p>There are multiple potential points of failure during the DNS lookup process such as at the system performing the lookup, at the DNS cache, or on an external DNS server. </p> <p>Local Server Configuration</p> <p>it\u2019s important to understand the \u2018hosts\u2019 section of the /etc/nsswitch.conf file. <code>hosts: files dns myhostname</code></p> <p>Essentially this means that host name resolution will be performed in the order specified, left to right. First files will be checked, followed by DNS. As files are first these will be checked first, this references the local /etc/hosts file which contains static host name to IP address mappings. This file takes priority over any DNS resolution, any changes to the file will be placed straight into the DNS cache of that local server.</p> <p>If there is no entry in the hosts file DNS will be used next as per /etc/nsswitch.conf. The servers used for DNS resolution will be specified in the /etc/resolv.conf file</p> <p>For DNS resolution to succeed the DNS server will need to accept TCP and UDP traffic over port 53 from our server. A port scanner such as the nmap tool can be used to confirm if the DNS server is available on port 53</p> <pre><code>nmap -sU -p 53 &lt;dns server&gt;\ntcpdump -n host &lt;dns server&gt;\ndig google.com\n</code></pre>"},{"location":"linux/admin/networking/#website-down","title":"Website DOWN","text":""},{"location":"linux/admin/networking/#server-is-running","title":"Server is running?","text":"<pre><code>ping 1.2.3.4 \nssh 1.2.3.4\n</code></pre>"},{"location":"linux/admin/networking/#remote-port-opened","title":"remote port opened ?","text":"<pre><code>telnet 1.2.3.4 80\nnmap -p 80 1.2.3.4\nnc -vz 1.2.3.4 80\n</code></pre> <p>nmap states: - Open: target machine is listening for connections/packets on that port  - Filtered: A filtered nmap cannot determine whether the port is open because packet filtering prevents its probes from reaching the port. - Closed: ports have no application listening on them, though they could open up at any time. - unfiltered: Ports are classified as unfiltered when they are responsive to Nmap's probes, but Nmap cannot determine whether they are open or closed</p>"},{"location":"linux/admin/networking/#test-for-listening-ports","title":"Test for Listening Ports","text":"<pre><code>netstat -lnp | grep 80\n</code></pre> <p>Here the 0.0.0.0:80 tells us that the host is listening on all of its IPs for port 80 traffic.</p>"},{"location":"linux/admin/networking/#command-line-response-test","title":"Command line response test","text":"<p>curl has an advantage over raw telnet for web server troubleshooting in that it takes care of the HTTP protocol for us and makes things like testing authentication, posting data, using SSL</p> <pre><code>curl http://1.2.3.4\n</code></pre>"},{"location":"linux/admin/networking/#dns","title":"DNS","text":"<p>DNS resolution is the process of converting a domain name into its corresponding IP address. There are two types of DNS queries involved in this process: recursive and iterative queries.</p> <p>Recursive query: In a recursive query, the DNS resolver asks for the complete answer to a query from the DNS server. If the server has the answer, it responds with the required information. If not, the server takes responsibility for contacting other DNS servers to find the answer and then returns it to the resolver. Recursive queries put more responsibility on the DNS server to find the requested information.</p> <p>Iterative query: In an iterative query, the DNS resolver asks the DNS server for the best answer it has at the moment. If the server doesn't have the complete answer, it responds with a referral to another server that might have more information. The resolver then contacts that server with a new iterative query, repeating the process until it finds the complete answer. In iterative queries, the resolver takes on more responsibility for finding the requested information.</p> <p>DNS caching and TTL (Time To Live)</p> <p>To speed up the DNS resolution process, resolvers and servers cache the results of previous queries. When a resolver receives a query, it first checks its cache to see if the answer is already available. If it finds the cached information, it returns the answer without contacting other servers, saving time and reducing network traffic.</p> <p>Each DNS record has an associated Time To Live (TTL) value, which specifies how long the record should be stored in the cache. TTL is measured in seconds, and once the TTL expires, the cached information is removed to ensure that outdated information is not used.</p> <p>Negative caching</p> <p>Negative caching is the process of caching the non-existence of a DNS record. When a resolver receives a query for a non-existent domain or record, it caches this information as a negative response, preventing repeated queries for the same non-existent resource. This reduces the load on DNS servers and improves overall performance.</p> <p>DNS is essential for the smooth functioning of the internet. Some of its key benefits include:</p> <p>User-friendliness: Domain names are easier to remember and type than IP addresses, which are long strings of numbers. Scalability: DNS is a distributed and hierarchical system, allowing it to handle the ever-growing number of domain names and IP addresses on the internet. Flexibility: DNS allows websites to change their IP addresses without affecting users. When a website's IP address changes, the DNS records are updated, and users can continue accessing the site using the same domain name. Load balancing: DNS can distribute user requests across multiple servers, improving the performance and reliability of websites.</p> <p>Domain names: A domain name is a human-readable address used to access a website or other resources on the internet. It consists of a series of character strings separated by dots</p> <p>TLDs (Top-Level Domains): A top-level domain (TLD) is the rightmost part of a domain name, such as \".com\". TLDs are managed by various organizations and can be divided into two categories: generic TLDs (gTLDs), like .com, .org, or .net, and country-code TLDs (ccTLDs), which represent specific countries or territories, like .in for the India</p> <p>Subdomains: A subdomain is a subdivision of a domain name, allowing the creation of separate sections or areas within a website. Subdomains appear to the left of the main domain name, such as blog.example.com, where \"blog\" is the subdomain of example.com.</p> <p>Root servers: Root servers are the highest level of DNS servers and are responsible for directing queries to the appropriate TLD servers. There are 13 root server clusters worldwide, managed by various organizations, each having multiple servers for redundancy and reliability.</p> <p>TLD servers: TLD servers store information about domain names within their specific TLD(.com, .org..etc). When they receive a query, they direct it to the appropriate authoritative name server responsible for that domain.</p> <p>Authoritative name servers: These servers hold the actual DNS records for a domain, including its IP address and other information. They provide the final answer to DNS queries, allowing users to access the desired website or resource.</p> <p>A DNS resolver is any component (software or hardware) responsible for translating a human-friendly domain name (like example.com) into the IP address</p> <p>The DNS Lookup Process in Brief</p> <p>Before diving into the types of DNS resolvers, it helps to have a high-level overview of the DNS lookup process:</p> <p>You request a domain name (e.g., example.com) from your computer or device. Your computer\u2019s resolver (or stub resolver) sends the request to a DNS recursive resolver (often your ISP\u2019s or a public DNS like Google\u2019s 8.8.8.8). The recursive resolver checks if it already has the domain name\u2019s IP address in its cache. If so, it returns it immediately. If not, the recursive resolver queries the root DNS servers, then the TLD (Top-Level Domain) DNS servers, then the authoritative DNS server for the domain, following DNS hierarchy. Once the IP address is found, the resolver returns it to your computer. Your computer can then contact the web server at that IP.</p> <pre><code>User's Device (Stub Resolver)\n     |\n     v\nRecursive Resolver (Often ISP/ Public)\n     |\n     v\n  Root Server\n     |\n     v\n TLD Server (.com, .net, etc.)\n     |\n     v\n Authoritative Server (example.com)\n     |\n     v\n   IP Address\n</code></pre> <ol> <li>Stub Resolver</li> </ol> <p>A stub resolver is the minimal DNS client software running on your device that starts the DNS lookup process. It typically does not perform the full DNS query process by itself.</p> <p>how it works?</p> <ul> <li> <p>The stub resolver knows one or more DNS servers to send queries to. These DNS servers are often configured automatically (for example, via DHCP on your home router) or manually by users (e.g., configuring 8.8.8.8 for Google DNS).</p> </li> <li> <p>When your device needs to resolve a domain name, the stub resolver sends a request to the configured DNS server and waits for the response.</p> </li> <li> <p>The stub resolver takes the response (the IP address or an error) and hands it back to the application (like a web browser).</p> </li> <li> <p>Recursive Resolver</p> </li> </ul> <p>A recursive resolver is a DNS server that actively performs the DNS query process on behalf of the client. It hunts down the IP address by querying multiple DNS servers until it gets the final answer.</p> <ul> <li> <p>The recursive resolver receives a request from a stub resolver (or another forwarder).</p> </li> <li> <p>It first checks its local cache to see if the requested domain\u2019s IP address is stored there. If found, it returns the cached answer immediately.</p> </li> <li> <p>If the record is not cached, the resolver queries the root DNS servers to learn which TLD server (e.g., .com, .org) to query next.</p> </li> <li> <p>It then queries the relevant TLD server to find the authoritative DNS server for the specific domain.</p> </li> <li> <p>Finally, it queries the authoritative server to obtain the required DNS records (e.g., the A record for IPv4)</p> </li> <li> <p>The resolved IP is cached for future requests and returned to the stub resolver.</p> </li> </ul> <p>Public DNS Resolver: Google Public DNS (8.8.8.8), Cloudflare DNS (1.1.1.1), and OpenDNS (208.67.222.222) are common public recursive resolvers.</p> <ol> <li>Caching-Only Resolver</li> </ol> <p>A caching-only resolver is a type of DNS server whose primary function is to cache DNS query results and reuse them to speed up subsequent lookups. It does not host any DNS zones (i.e., it is not authoritative for any domain) and typically performs recursive lookups on behalf of clients.</p> <ul> <li> <p>Like a recursive resolver, a caching-only resolver forwards queries to other DNS servers if the record is not already in its cache.</p> </li> <li> <p>Once it obtains the result, it stores (caches) the DNS records for the duration specified by their TTL (Time to Live).</p> </li> <li> <p>Subsequent queries for the same domain within the TTL period are served faster from the cache, reducing the need for external lookups.</p> </li> <li> <p>Forwarder</p> </li> </ul> <p>A forwarder is a DNS server that forwards all queries (or queries that it cannot resolve locally) to another DNS server instead of performing the complete recursive resolution process itself.</p> <ul> <li> <p>A DNS server is configured to send queries to an upstream DNS server, often a well-known public DNS or an ISP DNS.</p> </li> <li> <p>The forwarder may still maintain a local cache to speed up DNS resolution for repeated queries.</p> </li> <li> <p>This setup is common in corporate networks to manage and log DNS queries centrally or apply custom policies (e.g., content filtering).</p> </li> <li> <p>Iterative (Non-Recursive) Resolver</p> </li> </ul> <p>Sometimes called a non-recursive resolver, an iterative resolver typically gives back partial results or referrals, instructing the client to continue the resolution process on its own.</p> <p>If a client asks this resolver for a record, the resolver either: Returns the answer if it is authoritative or has it cached, or Returns a referral with the address of another DNS server (for instance, the root or TLD server), prompting the client to \u201ctry there next.\u201d This type is less common for end-user devices; it is often used by authoritative DNS servers to direct queries up or down the DNS hierarchy.</p> <p>Finally, example </p> <ol> <li>Your Laptop (Stub Resolver) is set to use 8.8.8.8 (Google DNS).</li> <li>You type www.example.com into your browser.</li> <li>The stub resolver on your laptop sends the DNS query to 8.8.8.8 (a Public Recursive Resolver).</li> <li>Google DNS checks its cache:</li> <li>If www.example.com is cached, it returns the IP right away.</li> <li>If not, it queries the root server, then .com TLD server, then the example.com authoritative server in turn.</li> <li>Once found, the IP address is cached in Google\u2019s DNS servers and returned to your laptop\u2019s stub resolver.</li> <li>Your laptop connects to the returned IP address, and the website loads.</li> </ol> <p>Utility tools</p> Tool Purpose When to Use <code>dig</code> Detailed DNS query tool Primary debugging <code>nslookup</code> Simple DNS lookup Quick checks <code>host</code> Lightweight DNS lookup Fast validation <code>ping</code> Check resolution + reachability Basic connectivity <code>getent hosts</code> OS-level resolver check Check NSS resolution <code>resolvectl</code> systemd-resolved debugging Modern Ubuntu <code>tcpdump</code> Packet-level DNS tracing Deep analysis <code>ss</code> / <code>netstat</code> Check DNS port usage DNS service issues <code>systemctl status</code> Check DNS services Local resolver problems <code>journalctl</code> DNS service logs Service debugging <p>Everyday tools for troubleshooting DNS queries</p> Tool What to Check ping Name resolution success host CNAME or A record nslookup DNS server used dig Status, TTL, answer section dig @dns Compare DNS servers dig +trace Resolution chain dig -x Reverse DNS resolvectl Local resolver tcpdump Packet flow dig +tcp UDP blocking dig AAAA IPv6 issues"},{"location":"linux/admin/networking/#application-cannot-reach-mailgooglecom","title":"Application cannot reach mail.google.com","text":"<ol> <li>ping -&gt; Basic Resolution Test -&gt; did it resolve and any packet loss</li> </ol> <pre><code>\u279c  ~ ping -c2 mail.google.com\nPING mail.google.com (142.250.77.37): 56 data bytes\n64 bytes from 142.250.77.37: icmp_seq=0 ttl=119 time=18.928 ms\n64 bytes from 142.250.77.37: icmp_seq=1 ttl=119 time=21.889 ms\n\n--- mail.google.com ping statistics ---\n2 packets transmitted, 2 packets received, 0.0% packet loss\nround-trip min/avg/max/stddev = 18.928/20.409/21.889/1.481 ms\n\u279c  ~\n</code></pre> <p>Issues:</p> <p>If it says Temporary failure in name resolution \u2192 DNS issue.  If IP resolves but no reply \u2192 network issue(firewall blocking), not DNS</p> <ol> <li>host - Quick DNS Lookup</li> </ol> <pre><code>  ~ host mail.google.com\nmail.google.com has address 142.250.77.37\nmail.google.com has IPv6 address 2404:6800:4009:81c::2005\n\u279c  ~\n</code></pre> <p>Issues:</p> <p>If alias exists \u2192 follow CNAME chain. If no address \u2192 DNS misconfiguration.</p> <ol> <li>nslookup - Simple Resolver Query</li> </ol> <p>Which DNS server responded? What IP did it return? Is it authoritative?</p> <p>```nslookup mail.google.com Server:     192.168.1.1 Address:    192.168.1.1#53</p> <p>Non-authoritative answer: Name:   mail.google.com Address: 142.250.77.37</p> <p>\u279c  ~</p> <pre><code>\nIf wrong DNS server \u2192 resolver issue.\n\n4. dig - primary DNS debug tool\n\n</code></pre> <p>~ dig mail.google.com</p> <p>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; mail.google.com ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 37108 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1</p> <p>;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;mail.google.com.       IN  A</p> <p>;; ANSWER SECTION: mail.google.com.    18  IN  A   142.250.77.37</p> <p>;; Query time: 8 msec ;; SERVER: 192.168.1.1#53(192.168.1.1) ;; WHEN: Thu Feb 12 12:57:49 IST 2026 ;; MSG SIZE  rcvd: 60</p> <p>\u279c  ~</p> <pre><code>\nHEADER -&gt; status: NOERROR\n\nANSWER SECTION -&gt; \nRecord type -&gt; A\nTTL value -&gt; 18 seconds\ncorrect ip returned -&gt; 142.250.77.37\n\nQuery time -&gt; 8 msec\n\nif more than 200ms then DNS latency issue\n\nSERVER -&gt; confirms which DNS server responded(192.168.1.1)\n\n5. query specific DNS server\n\n</code></pre> <p>\u279c  ~ dig @8.8.8.8 mail.google.com</p> <p>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @8.8.8.8 mail.google.com ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 30080 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1</p> <p>;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 512 ;; QUESTION SECTION: ;mail.google.com.       IN  A</p> <p>;; ANSWER SECTION: mail.google.com.    81  IN  A   142.251.220.69</p> <p>;; Query time: 25 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: Thu Feb 12 13:04:30 IST 2026 ;; MSG SIZE  rcvd: 60</p> <pre><code>\n</code></pre> <p>\u279c  ~ dig @1.1.1.1 mail.google.com</p> <p>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @1.1.1.1 mail.google.com ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 11108 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1</p> <p>;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 1232 ;; QUESTION SECTION: ;mail.google.com.       IN  A</p> <p>;; ANSWER SECTION: mail.google.com.    245 IN  A   142.251.222.165</p> <p>;; Query time: 16 msec ;; SERVER: 1.1.1.1#53(1.1.1.1) ;; WHEN: Thu Feb 12 13:04:49 IST 2026 ;; MSG SIZE  rcvd: 60</p> <pre><code>\nIf public works but internal fails \u2192 internal DNS issue.\n\n6. trace full resolution path\n\n- Root servers response\n- TLD (.com)\n- Authoritative nameserver\n- Final record\n\n</code></pre> <p>~ dig +trace mail.google.com</p> <p>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; +trace mail.google.com ;; global options: +cmd .           499971  IN  NS  m.root-servers.net. .           499971  IN  NS  h.root-servers.net. .           499971  IN  NS  d.root-servers.net. .           499971  IN  NS  j.root-servers.net. .           499971  IN  NS  a.root-servers.net. .           499971  IN  NS  b.root-servers.net. .           499971  IN  NS  f.root-servers.net. .           499971  IN  NS  c.root-servers.net. .           499971  IN  NS  e.root-servers.net. .           499971  IN  NS  g.root-servers.net. .           499971  IN  NS  k.root-servers.net. .           499971  IN  NS  l.root-servers.net. .           499971  IN  NS  i.root-servers.net. .           499971  IN  RRSIG   NS 8 0 518400 20260224220000 20260211210000 21831 . jHotSqe/L+74ckVYvjjBAKrwjrovZbppJ4aFruufW6TdLrqGbx3MPRDx tvFWlbhK8gMEG8MI0jTyc+m/ZxTCkmLbTUIO7ZFL093fEGBGdvHSo1Xe UTb0E1R1QAGkw2+S5qqkaQuq/RMAU+LuTNxwWkXI33fEQqXQb1mkjmjo 4c2KfkDVnbJl6rpHKGJQ6zVjXvTkooQ/wUSGwmOVCKZx6i6FRUuLXrvR JNEEDx0vqxAckaDL7zUlLRMiz46MKsUGC/d1A5zwg7sqA/31QjPpPJfg ReRYz7AFG55jiiAyjXgxZ8k2hXwvbcNurc7od5uyUugTbMjuVuue+jJJ N7bf9g== ;; Received 525 bytes from 192.168.1.1#53(192.168.1.1) in 8 ms</p> <p>com.            172800  IN  NS  h.gtld-servers.net. com.            172800  IN  NS  m.gtld-servers.net. com.            172800  IN  NS  i.gtld-servers.net. com.            172800  IN  NS  b.gtld-servers.net. com.            172800  IN  NS  g.gtld-servers.net. com.            172800  IN  NS  c.gtld-servers.net. com.            172800  IN  NS  d.gtld-servers.net. com.            172800  IN  NS  e.gtld-servers.net. com.            172800  IN  NS  a.gtld-servers.net. com.            172800  IN  NS  f.gtld-servers.net. com.            172800  IN  NS  j.gtld-servers.net. com.            172800  IN  NS  k.gtld-servers.net. com.            172800  IN  NS  l.gtld-servers.net. com.            86400   IN  DS  19718 13 2 8ACBB0CD28F41250A80A491389424D341522D946B0DA0C0291F2D3D7 71D7805A com.            86400   IN  RRSIG   DS 8 1 86400 20260225050000 20260212040000 21831 . X/cQ1bCPaNoI5BWEG6MtuEwl1QsPr/oLjhFRuY/2lbRNzM7xl4CPdE8c R58+jbslIfnaqLgkhZ701BVzXibnMEkBMohiG5DkxiR+lh8XkeFCmZA+ cXqv3sMOur0kGu4hRWYVvbfeBfGH/FHtgA+9UGTYO/PN9lEt6YMNBbJ4 z+HhaMZJQp789bB7eoj09pX7vEKYDHLHh++zfKC96zwY7o+PPIwnKMLq jGxMaZQ5+7Am3GSTRPQkTjX/Pba91x0l0WtyIMbspcjpQVx6h7nxl/BM 9IyGldqlMhf3+vH+jVV32q+WkyVSdNE6EQjwfoCCozDRrw3G55cpi2nU zy53jQ== ;; Received 1178 bytes from 192.112.36.4#53(g.root-servers.net) in 120 ms</p> <p>google.com.     172800  IN  NS  ns2.google.com. google.com.     172800  IN  NS  ns1.google.com. google.com.     172800  IN  NS  ns3.google.com. google.com.     172800  IN  NS  ns4.google.com. CK0POJMG874LJREF7EFN8430QVIT8BSM.com. 900 IN NSEC3 1 1 0 - CK0Q3UDG8CEKKAE7RUKPGCT1DVSSH8LL  NS SOA RRSIG DNSKEY NSEC3PARAM CK0POJMG874LJREF7EFN8430QVIT8BSM.com. 900 IN RRSIG NSEC3 13 2 900 20260219002710 20260211231710 35511 com. Mvv0e2CAo+51hb57tq/ZXEzWjXkEfM8X3D6ADwGLSILhSJWvfQX1mLrG HfALHK8iWVGiXEQONeHDUytDqXVMIA== S84BOR4DK28HNHPLC218O483VOOOD5D8.com. 900 IN NSEC3 1 1 0 - S84BR9CIB2A20L3ETR1M2415ENPP99L8  NS DS RRSIG S84BOR4DK28HNHPLC218O483VOOOD5D8.com. 900 IN RRSIG NSEC3 13 2 900 20260216013314 20260209002314 35511 com. 458nY1ZPTiQMjwm578DuB+xnSPZBWY2vcyKJXEBgRZ8Aj0NHLrv6Vncp 7O5nLIWLxDEvc3ma9Acjso+RedbqTg== ;; Received 649 bytes from 192.42.93.30#53(g.gtld-servers.net) in 154 ms</p> <p>mail.google.com.    300 IN  A   142.250.70.37 ;; Received 60 bytes from 216.239.32.10#53(ns1.google.com) in 82 ms</p> <pre><code>\nIf it fails at:\nRoot \u2192 network issue\nTLD \u2192 domain misconfigured\nAuthoritative \u2192 zone issue\n\n7. reverse lookup\n\nPTR record exists?\nReverse DNS configured?\n\n</code></pre> <p>dig -x 142.250.77.37</p> <p>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; -x 142.250.77.37 ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 56031 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1</p> <p>;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;37.77.250.142.in-addr.arpa.    IN  PTR</p> <p>;; ANSWER SECTION: 37.77.250.142.in-addr.arpa. 68939 IN    PTR bom07s26-in-f5.1e100.net.</p> <p>;; Query time: 10 msec ;; SERVER: 192.168.1.1#53(192.168.1.1) ;; WHEN: Thu Feb 12 13:12:06 IST 2026 ;; MSG SIZE  rcvd: 93 \u279c  ~</p> <pre><code>\n8. check OS resolver\n\n</code></pre> <p>cat /etc/resolv.conf`</p> <pre><code>\nNameserver IP\nSearch domain\nMultiple nameservers?\n\n9. systemd-resolved Debug\n\nUbuntu\n\n```resolvectl status```\n\nCurrent DNS server\nDNSSEC status\nDomain routing\n\nFlush cache if ip is pointing to wrong DNS servers\n\n```sudo resolvectl flush-caches```\n\n10. packaet capture\n\nIs DNS query leaving?\nIs response coming back?\n\n</code></pre> <p>sudo tcpdump -i eth0 port 53</p> <pre><code>\nRequest sent but no reply \u2192 firewall or upstream DNS down\nNo request \u2192 local resolver issue\n\n11. Check DNS over TCP \n\nSome firewalls block UDP 53:\n\n</code></pre> <p>dig +tcp main.google.com</p> <pre><code>if TCP works but UDP doesn\u2019t \u2192 firewall blocking UDP.\n\nWhat is difference between A and CNAME?\n\nAn A record maps a hostname directly to an IPv4 address, while a CNAME maps a hostname to another hostname. CNAME adds an extra resolution step and is typically used for aliasing services like CDNs or load balancers. However, a hostname cannot have both A and CNAME records simultaneously.\n\n\n## Proxy\n\n### Forward proxy\n\nA forward proxy(proxy server)is a server that sits in front of one or more client machines and acts as an intermediary between the clients and the internet. When a client machine makes a request to a resource (like a web page or file) on the internet, the request is first sent to the proxy,  then forwards the request to the internet on behalf of the client machine and returns the response to the client machine.\n\nWe would have an demostration of forward proxy using EC2 instance. we would use squid for forward proxy as demo.. \n\n1. Create custom vpc where it has public and private subnets. \n\n![proxy_vpc](proxy_vpc.png)\n\n2. Create an EC2(Ubuntu image 24.02) in public subnet with public ip attached to it. allow your SG groups from your IP to ports 22, 443 and 3128(squid)\n\n3. login to the ec2 using public ip and configure squid. \n\n</code></pre> <p>sudo apt update sudo apt install -y squid sudo mv /etc/squid/squid.conf /etc/squid/squid.conf.original sudo vim /etc/squid/squid.conf</p> <p>http_port 3128</p>"},{"location":"linux/admin/networking/#your-client-ip-update-this","title":"Your client IP (update this!)","text":"<p>acl myip src /32"},{"location":"linux/admin/networking/#https-tunnel-support","title":"HTTPS tunnel support","text":"<p>acl SSL_ports port 443 acl CONNECT method CONNECT</p>"},{"location":"linux/admin/networking/#allow-connect-to-443-only-for-your-ip","title":"Allow CONNECT to 443 only for your IP","text":"<p>http_access allow myip CONNECT SSL_ports</p>"},{"location":"linux/admin/networking/#allow-normal-http-for-your-ip","title":"Allow normal HTTP for your IP","text":"<p>http_access allow myip</p>"},{"location":"linux/admin/networking/#deny-everything-else","title":"Deny everything else","text":"<p>http_access deny all</p> <p>sudo systemctl restart squid sudo tail -f /var/log/squid/access.log</p> <pre><code>\n\n**Testing**\n\n</code></pre> <p>\u279c  ~ curl -x http://13.221.194.201:3128 https://www.google.com -v</p> <p>or </p> <p>\u279c  ~ curl -X GET -x http://13.221.194.201:3128 https://www.google.com -v</p> <ul> <li>Trying 13.221.194.201:3128...</li> <li>Connected to 13.221.194.201 (13.221.194.201) port 3128</li> <li>CONNECT tunnel: HTTP/1.1 negotiated</li> <li>allocate connect buffer</li> <li>Establish HTTP proxy tunnel to www.google.com:443 <p>CONNECT www.google.com:443 HTTP/1.1 Host: www.google.com:443 User-Agent: curl/8.7.1 Proxy-Connection: Keep-Alive</p> <p>&lt; HTTP/1.1 200 Connection established &lt;</p> </li> <li>CONNECT phase completed</li> <li>CONNECT tunnel established, response 200</li> <li>ALPN: curl offers h2,http/1.1</li> <li>(304) (OUT), TLS handshake, Client hello (1):</li> <li>CAfile: /etc/ssl/cert.pem</li> <li>CApath: none</li> <li>(304) (IN), TLS handshake, Server hello (2):</li> <li>(304) (IN), TLS handshake, Unknown (8):</li> <li>(304) (IN), TLS handshake, Certificate (11):</li> <li>(304) (IN), TLS handshake, CERT verify (15):</li> <li>(304) (IN), TLS handshake, Finished (20):</li> <li>(304) (OUT), TLS handshake, Finished (20):</li> <li>SSL connection using TLSv1.3 / AEAD-CHACHA20-POLY1305-SHA256 / [blank] / UNDEF</li> <li>ALPN: server accepted h2</li> <li>Server certificate:</li> <li>subject: CN=www.google.com</li> <li>start date: Jan 19 08:39:05 2026 GMT</li> <li>expire date: Apr 13 08:39:04 2026 GMT</li> <li>subjectAltName: host \"www.google.com\" matched cert's \"www.google.com\"</li> <li>issuer: C=US; O=Google Trust Services; CN=WR2</li> <li>SSL certificate verify ok.</li> <li>using HTTP/2</li> <li>[HTTP/2] [1] OPENED stream for https://www.google.com/</li> <li>[HTTP/2] [1] [:method: GET]</li> <li>[HTTP/2] [1] [:scheme: https]</li> <li>[HTTP/2] [1] [:authority: www.google.com]</li> <li>[HTTP/2] [1] [:path: /]</li> <li>[HTTP/2] [1] [user-agent: curl/8.7.1]</li> <li>[HTTP/2] [1] [accept: /] <p>GET / HTTP/2 Host: www.google.com User-Agent: curl/8.7.1 Accept: / </p> </li> <li>Request completely sent off &lt; HTTP/2 200 &lt; date: Fri, 20 Feb 2026 04:00:43 GMT &lt; expires: -1 &lt; cache-control: private, max-age=0 &lt; content-type: text/html; charset=ISO-8859-1 &lt; content-security-policy-report-only: object-src 'none';base-uri 'self';script-src 'nonce-i2hPbsYVMJV0kT-IsNrJ0g' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/other-hp &lt; reporting-endpoints: default=\"//www.google.com/httpservice/retry/jserror?ei=a9yXaabCKI-g5NoPoIrd0AU&amp;cad=crash&amp;error=Page%20Crash&amp;jsel=1&amp;bver=2382&amp;dpf=I0qA2q1Zg-5SxZAegZEdGIYXEUJxOGrccG_kcqSQlrI\" &lt; accept-ch: Sec-CH-Prefers-Color-Scheme &lt; p3p: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\" &lt; server: gws &lt; x-xss-protection: 0 &lt; x-frame-options: SAMEORIGIN &lt; set-cookie: __Secure-STRP=AD6Dogt9i7XASweFBmNTQF9gAZDVVRRbxVWLT3r3TsnW5ahXd-425gpnwZiZ1GuLWZ8D7vkKxx4fIBXZrJ7D7Bz9QRWWgNEAplbB; expires=Fri, 20-Feb-2026 04:05:43 GMT; path=/; domain=.google.com; Secure; SameSite=strict &lt; set-cookie: AEC=AaJma5tco9TRnmsynZ31OC7HI9Z8LzeMac36UIFJnW5izuQgIJCptYpd4A; expires=Wed, 19-Aug-2026 04:00:43 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=lax &lt; set-cookie: NID=529=XrnU6xOgTWqktw_8A87XZI3pb-Uh6R6YoRkrZmIrl7buLFzJLhhHS5QhfybcMH7M-kyCe6WBqvGNAN_nYnEgFnJJnHysK9YunfBIrtzNwVgx74A_ryzZeQt3hctoWl_nyjMXuXlZCQYftP64d5O5XYrrSwr68_TTwMG0gDDD8nOs428svQZThabqD1HwHvGR2-CfI_hZhQhXJ43Xzu_6FO_bmqz9pnp5LJgmIA; expires=Sat, 22-Aug-2026 04:00:43 GMT; path=/; domain=.google.com; HttpOnly &lt; set-cookie: __Secure-BUCKET=CKEE; expires=Wed, 19-Aug-2026 04:00:43 GMT; path=/; domain=.google.com; Secure; HttpOnly &lt; alt-svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000 &lt; accept-ranges: none &lt; vary: Accept-Encoding &lt; .. .. ..</li> </ul> <pre><code>\n\nHTTP/1.1 200 Connection established (from Squid)\n\nBecause you requested an HTTPS site through an HTTP proxy, curl uses the CONNECT method:\n\n</code></pre> <p>Your Laptop (public IP: X.X.X.X) \u2192 CONNECT google.com:443         \u2193 AWS EC2 Proxy (public IP: 13.221.194.201) \u2192 Client: 200 Connection established</p> <pre><code>\nThis means: \u201cTunnel is created. Now you (client) talk TLS directly to google.com through me.\u201d\n\n\nAfter the tunnel is established, curl completes TLS with www.google.com and requests /, it responds with \n* Request completely sent off\n&lt; HTTP/2 200\n\n\n\nFull flow summary for above request \n\n1. TCP connect to proxy\n2. CONNECT request to proxy\n3. Proxy returns 200 (tunnel established)\n4. TLS handshake with Google\n5. Encrypted GET request\n6. Google returns 200\n7. Page delivered\n\n\n</code></pre> <p>sudo tail -f /var/log/squid/access.log</p> <p>timestamp       duration  client_ip  result/status  bytes  method  url  user  hierarchy/server_ip 1771560044.160    962 103.5.134.43 TCP_TUNNEL/200 23622 CONNECT www.google.com:443 - HIER_DIRECT/142.251.179.104 -</p> <pre><code>\nThere are manual ways to troubleshooting the request(telnet or nc), nc is cleaner way\n\nnc 13.221.194.201 3128\nGET http://example.com/ HTTP/1.1\nHost: example.com\n\nPRESS ENTER TWICE\n\nThe above one works only for HTTP not HTTPS...\n\nBecause HTTPS requires:\nCONNECT tunnel\nTLS handshake\nEncrypted GET\n\nYou cannot manually type TLS handshake in terminal, so curl would do it automatically. \n\n\nIf You Want to Manually Do HTTPS Properly..\n\n</code></pre> <p>openssl s_client -proxy 13.221.194.201:3128 -connect google.com:443 GET / HTTP/1.1 Host: google.com</p> <p>PRESS ENTER TWICE ```</p> <p>Now you\u2019ll see proper HTTPS response...</p>"},{"location":"linux/admin/security/","title":"security","text":""},{"location":"linux/admin/security/#linux-security-checklist","title":"Linux security checklist","text":"<ul> <li>Keep the system Updated with Latest Security Patches</li> <li>Keep Yourself updated with latest vulnerabilities through mailing lists, forums etc.</li> <li>Disable and stop unwanted services on the server</li> <li>Use SUDO to limit ROOT Access</li> <li>SSH security settings</li> <li>Check the integrity of critical files using checksum</li> <li>Tunnel all of your X-Window Sessions through SSH</li> <li>Use SeLinux If required.</li> <li>Only create required no of users</li> <li>Maintain a good firewall policy</li> <li>Configure SSL/TLS if you are using FTP</li> <li>check file permissions accross filesystems.</li> <li>Use tools like adeos for potential file state</li> <li>Ensure sticky bit on /tmp Directory</li> <li>check and lock users with blank passwords.</li> <li>Bootloader and BIOS security</li> <li>Give special attention to portmap related services</li> <li>Deploy your NFS shares with Kerberos Authentication.</li> <li>Enable remote Logging</li> <li>Disable root Logins by editing /etc/securetty</li> <li>Keep A good Pasword Policy</li> </ul>"},{"location":"linux/admin/security/#setup-self-signed-certificate-on-ec2","title":"Setup self signed certificate on EC2.","text":"<p>Create Ec2 instance with public ip installed with nginx server by default. Now, when you run public ip it would default to \"http\".. make SG to allow 80, 443 and 22.  we wil generate self signed certificate and load the nginx server so that only https would be re-directed. </p> <p>Install nginx server after logging to ec2 machine.</p> <pre><code>sudo yum update -y\nsudo yum install nginx -y\nsudo systemctl start nginx\nsudo systemctl enable nginx\n</code></pre> <p>curl http:// <p>Generate Self-Signed Certificate</p> <pre><code>sudo mkdir /etc/nginx/ssl\ncd /etc/nginx/ssl\nsudo openssl genrsa -out private.key 2048\nsudo openssl req -new -x509 -key private.key -out certificate.crt -days 365\n\nCommon Name (CN): &lt;public_ip&gt;\n.\n.\n.\n\n</code></pre> <p>This would have generated <code>private.key</code> and <code>certificate.crt</code> . Configure the nginx to load tls certs. </p> <pre><code>sudo vi /etc/nginx/conf.d/ssl.conf\n\nserver {\n    listen 443 ssl;\n    server_name &lt;public-ip&gt;;\n\n    ssl_certificate /etc/nginx/ssl/certificate.crt;\n    ssl_certificate_key /etc/nginx/ssl/private.key;\n\n    location / {\n        root /usr/share/nginx/html;\n        index index.html;\n    }\n}\n\nsudo nginx -t\nsudo systemctl restart nginx\n\n</code></pre> <p>https://, you can see that connection is not private.. click Advance and proceed as its self-signed not signed by CA so browser is complaining. <pre><code>    Client (Browser)\n        |\n        |  HTTPS Request\n        v\nEC2 + Nginx Server\n        |\n        |-- Sends Certificate\n        |\nSelf-Signed Certificate\n        |\n         Signed by Itself\n\n</code></pre> <p>Verify certificate from cli</p> <pre><code>openssl s_client -connect &lt;public_ip&gt;:443 \nVerify return code: 18 (self signed certificate)\n</code></pre>"},{"location":"linux/admin/security/#build-internal-pki","title":"Build Internal PKI","text":"<p>Build a secure internal architecture where:</p> <ul> <li>You create your own Root CA</li> <li>You create an Intermediate CA</li> <li>You sign server certificates</li> <li>You enable HTTPS</li> <li>You enable mTLS between services</li> <li>You troubleshoot certificate issues</li> </ul> <pre><code>                   Your Laptop\n                        |\n                        | HTTPS\n                        v\n                EC2-1 (Web Server)\n                        |\n                        | mTLS\n                        v\n                EC2-2 (Internal API)\n                        |\n                        |\n                Signed by Intermediate CA\n                        |\n                        |\n                   Root CA (Offline)\n</code></pre> <p>Setup</p> <p>Create 2 EC2 machines (t2.micro)</p> <p>EC2-1 \u2192 Public facing web server</p> <p>sg-1 Allow 22 (SSH) Allow 443 (HTTPS)</p> <p>sg-2 EC2-2 \u2192 Private internal API</p> <p>Allow 22 (SSH) Allow 8443 (Custom HTTPS) Allow inbound only from EC2-1 sg1</p> <pre><code>                    Internet\n                        |\n                        |\n                +----------------+\n                |  Internet GW   |\n                +----------------+\n                        |\n                 -------------------\n                 |                 |\n          Public Subnet       Private Subnet\n          (10.0.1.0/24)      (10.0.2.0/24)\n                 |                 |\n           +-------------+   +-------------+\n           |   EC2-1     |   |   EC2-2     |\n           | Web Server  |--&gt;| Internal API|\n           | Public IP   |   | No Public IP|\n           +-------------+   +-------------+\n                 |\n          Security Group 1\n          Allow:\n            22 (SSH)\n            443 (HTTPS)\n\n                              Security Group 2\n                              Allow:\n                              22 (SSH)\n                              8443 (HTTPS)\n                              Source = EC2-1 only\n</code></pre> <p>Create Enterprise-Style PKI</p> <pre><code>Root CA (offline)\n        |\nIntermediate CA\n        |\nServer Certificates\n</code></pre>"},{"location":"linux/admin/storage/","title":"storage","text":""},{"location":"linux/admin/storage/#logical-volume-manager","title":"logical volume manager","text":"<p>LVM allows for very flexible disk space management. It provides features like the ability to add disk space to a logical volume and its filesystem while that filesystem is mounted and active and it allows for the collection of multiple physical hard drives and partitions into a single volume group which can then be divided into logical volumes.</p> <p>The volume manager also allows reducing the amount of disk space allocated to a logical volume, but there are a couple requirements. First, the volume must be unmounted. Second, the filesystem itself must be reduced in size before the volume on which it resides can be reduced.</p>"},{"location":"linux/admin/storage/#create-volume","title":"create volume","text":"<pre><code>pvs\npvcreate /dev/hdd\nvgs\nvgextend /dev/MyVG01 /dev/hdd\nlvcreate -L +50G --name Stuff MyVG01\nmkfs -t ext4 /dev/MyVG01/Stuff\ne2label /dev/MyVG01/Stuff Stuff\nlvs\n</code></pre>"},{"location":"linux/admin/storage/#increase-volume","title":"increase volume","text":"<pre><code>pvs\nvgs\nlvs\nvgextend /dev/MyVG01 /dev/hdd\nlvextend -L +50G /dev/MyVG01/Stuff\nresize2fs /dev/MyVG01/Stuff\n</code></pre>"},{"location":"linux/admin/storage/#decrease-volume","title":"decrease volume","text":"<pre><code>df -h /testlvm1\numount /testlvm1\ne2fsck -f /dev/mapper/vg01-lv002\nresize2fs /dev/mapper/vg01-lv002 80G\nlvreduce -L 80G /dev/mapper/vg01-lv002\ne2fsck -f /dev/mapper/vg01-lv002\nmount /testlvm1\ndf -h /testlvm1\n</code></pre>"},{"location":"linux/admin/storage/#raid","title":"RAID","text":"<p>RAID stands for Redundant Array of Inexpensive/Independent Disks</p>"},{"location":"linux/admin/storage/#striped-andor-mirrored","title":"Striped and/or Mirrored","text":"<p>RAID 0</p> <p>RAID 0, data is written across the drives, or \u201cstriped\u201d. This means it can potentially be read from more than one drive concurrently. That can give us a real performance boost. Now we have two drives that could fail, taking out all our data. So, RAID 0 is only useful if we want a performance boost but don\u2019t care about long-term storage.</p> <p></p> <p>RAID 1</p> <p>We refer to RAID level 1 as \u201cmirrored\u201d because it is created with a pair of equal drives. Each time data is written to a RAID 1 device, it goes to both drives in the pair.</p> <p></p> <p>RAID10</p> <p>We can create a RAID 10 device with four disks: one pair of disks in RAID 0, mirroring another pair of disks in RAID 0</p> <p></p>"},{"location":"linux/admin/storage/#parity","title":"Parity","text":"<p>RAID5</p> <p>RAID 5 requires at least three equal-size drives to function. In practice, we can add several more, though rarely more than ten are used. RAID 5 sets aside one drive\u2019s worth of space for checksum parity data. It is not all kept on one drive, however; instead, the parity data is striped across all of the devices along with the filesystem data.</p> <p>This means we usually want to build our RAID out of a set of drives of identical size and speed. Adding a larger drive won\u2019t get us more space, as the RAID will just use the size of the smallest member. Similarly, the RAID\u2019s performance will be limited by its slowest member.</p> <p>RAID 5 can recover and rebuild with no data loss if one drive dies. If two or more drives crash, we\u2019ll have to restore the whole thing from backups.</p> <p>RAID6</p> <p>RAID 6 is similar to RAID 5 but sets aside two disks\u2019 worth for parity data. That means a RAID 6 can recover from two failed members.</p> <p>RAID 5 gives us more usable storage than mirroring does, but at the price of some performance. A quick way to estimate storage is the total amount of equal-sized drives, minus one drive. For example, if we have 6 drives of 1 terabyte, our RAID 5 will have 5 terabytes of usable space. That\u2019s 83%, compared to 50% of our drives were mirrored in RAID 1.</p> <p></p>"},{"location":"linux/admin/troubleshooting/","title":"booting issue","text":""},{"location":"linux/admin/troubleshooting/#kernel-panic-not-syncing-attempting-to-kill-init","title":"kernel panic: not syncing attempting to kill init","text":"<p>The issue is due to kernel image or the grub related, system wasn't able to locate the kernel or the label associated with it. </p> <p>Solution, attempt the linux machine into the sigle user mode, check the contents of the  grub.conf , which is tempravory, try modifying the file and bring up the system in runlevel 3. </p> <p>Once the machine is up, you can modify the grub.conf accordingly and boot the system.</p>"},{"location":"linux/admin/troubleshooting/#linux-booting-drops-into-grub","title":"linux booting drops into grub&gt;","text":"<p>grub wasn't able to find the second stage boot loader. try to load the second stage to boot the machine. </p> <pre><code>grub&gt; root(hd0,0)\ngrub&gt; kernel /vmlinux&lt;tab&gt; ro root=LABEL=/1\ngrub&gt; initrd /initrd&lt;tab&gt;\ngrub&gt; boot\n</code></pre> <p>Once the machine boots, you would check where is the second stage boot loader and will fix it.</p>"},{"location":"linux/admin/troubleshooting/#system-keeps-on-rebooting","title":"system keeps on rebooting","text":"<p>it could be a hardware related or the login related. </p> <p>Solution: Try to boot the system into single user mode, and if you find the error as no more process left in the runlevel, which means system is not able to start the process and you would required to boot in the rescue level. </p> <p>First, check the runlevel in /etc/inittab file, if runlevel is not correct, fix it</p> <p>secondly</p> <pre><code>Mount ISO or DVD\n: linux rescue\n&lt;skip&gt;\n\nchroot /mnt/sysimage\ncd etc\nls | grep inittab \nls | grep /etc/grub/grub.conf\nexit \nreboot\n\n</code></pre>"},{"location":"linux/admin/troubleshooting/#checking-filesystems-fsckext3-unable-to-resolve-label5-failed","title":"checking filesystems fsck.ext3: unable to resolve LABEL=/5  [ FAILED ]","text":"<p>Your file system file is corrupted, it can't find the parition or label to mount the directories. </p> <p>Solution: provide your single user password and fix the /etc/fstab</p> <pre><code>cat /etc/fstab\n&lt;check for correct entries&gt;\ne2label /dev/sda3\nmount -o remount,rw /etc/fstab\nvim /etc/fstab\n</code></pre>"},{"location":"linux/admin/troubleshooting/#verifying-dmi-pool-data","title":"Verifying DMI pool data","text":"<p>first stage boot loader is corrupted. </p> <p>Solution: </p> <pre><code>chroot /mnt/sysimage\ngrub-install /dev/sda\nctrl-d\n</code></pre>"},{"location":"linux/admin/troubleshooting/#you-lost-your-grub-password-how-would-you-recover","title":"You lost your GRUB password, how would you recover ?","text":"<p>Linux terminal, type <code>grub-md5-crypt &gt;&gt;/boot/grub/grub.conf</code> prefix it with password --md5 $jshhx.....</p> <p>insert ISO image or DVD, at the boot prompt, type  linux rescue </p> <pre><code>chroot /mnt/sysimage/\nvim /etc/grub/grub.conf\n\nDelete the line with any password entry, save and quit the file\n\nexit\n</code></pre>"},{"location":"linux/admin/troubleshooting/#root-unable-to-login","title":"root unable to login","text":"<p>1) password wrong</p> <p>2) bash not presented for root a/c   Symptom: when you type root login password, it will give you non-login shell. i.e password file corrupted. </p> <p>Solution: Go to single user mode, edit /etc/password(remove the passsord entry), save &amp; quit</p> <p>3) /etc/securetty file corrupted   try logging into the nonroot user and see if it works. only if root not abel to login, then the issue is with /etc/securetty. </p> <p>Solution: single usermode, verify settings in /etc/securetty</p>"},{"location":"linux/ansible/install/","title":"install","text":""},{"location":"linux/ansible/install/#local-setup","title":"local setup","text":"<p>We could do a local installation of the setup using vagrant + virtualbox.</p> <p>versions</p> <p>Vagrant: 2.4.1</p> <p>Oracle virtual box: 7.x</p>"},{"location":"linux/ansible/install/#configure-vagrant","title":"configure vagrant","text":"<pre><code>mkdir ansibledev\nvim Vagrantfile\n\n# start\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"centos/7\"\n  config.vm.network \"private_network\", ip: \"192.168.56.10\"\nend\n\n# save &amp; quit\n\nvagrant up\n</code></pre>"},{"location":"linux/ansible/install/#configure-ansible","title":"configure ansible","text":"<p>Now, you are required to make a modification to the newly created server from your physical machine for exeuting and working on with the playbooks</p> <pre><code># mkdir ansibledev\n# vim ansible.cfg\n[defaults]\ninventory = ./hosts\nremote_user = vagrant\nask_pass = false\nhost_key_checking = false\nprivate_key_file=/Users/sunilamperayani/vagrant/ansibledev/.vagrant/machines/default/virtualbox/private_key\n\n[privlege escalation]\nbecome= true\nbecome_method = sudo\nbecome_user = root\nbecome_ask_pass = false\n\n# Note: this is the same IP which you have created using oracle virtual box.\n\n# build ansible inventory!\n\n# vim hosts\n[test]\n192.168.56.10\n</code></pre>"},{"location":"linux/ansible/install/#test","title":"test","text":"<p>Test your connectivity using ping command</p> <pre><code>ansible -m ping all\n192.168.56.10 | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n</code></pre>"},{"location":"linux/ansible/overview/","title":"overview","text":"<p>You can read/practice <code>ansible</code> using the below repo. </p> <p>https://github.com/samperay/automation-with-ansible</p>"},{"location":"linux/scripting/faq/","title":"Interview Questions","text":"<p>Bash cheat sheet</p>"},{"location":"linux/scripting/faq/#list-files-in-directory","title":"list files in directory","text":"<pre><code>for item in /etc/*\ndo\n  if [ -f $item ]; then \n    echo ${item}\n  fi\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#sum-of-integers","title":"sum of integers","text":"<pre><code>set -x\na=$1\nb=$2\n\nfunction input_check(){\nif [ \"$#\" -ne 2 ]; then \n    echo 1\nelse\n    if [ \"$#\" -gt 2 ]; then \n        echo 1 \n    else \n        echo 0\n    fi\nfi\n}\n\ncheck_args=$(input_check $*)\nif [ $check_args -eq 1 ]; then \n    echo \"Usage: `basename $0` [arg1, arg2]\"\n    exit 1\nelse \n    sum=`expr $a + $b`\n    echo \"Result=$sum\"\nfi\n</code></pre>"},{"location":"linux/scripting/faq/#traverse-array-using-len","title":"traverse array using len","text":"<pre><code>array=(\"1\" \"2\")\necho ${#array[*]} // length of an array\n\n# fetch elements in an array\necho ${array[0]}\n\n# traverse an item in an array  \nfor item in ${array[@]}; do \n  echo $item \ndone\n\n# print the array index\nfor item in ${!array[*]}; do \n  echo $item \ndone\n\n# \"${array[@]}\" - returns each item as a separate word.\n# \"${array[*]}\" - returns all items in a word.\n\n# take each element in an array and print the results\nlen=${#array[@]}\nfor((i=0;i&lt;len;i++&gt;)); do \n  echo index:$i,item:${array[$i]}\ndone\n\n# iterate over index\nfor ((i=0;i&lt;$${#array[@]};i++)); do\n  echo ${array[i]}\ndone\n</code></pre> <pre><code>Fruits=('Apple' 'Banana' 'Orange')\necho \"${Fruits[0]}\"           # Element #0\necho \"${Fruits[-1]}\"          # Last element\necho \"${Fruits[@]}\"           # All elements, space-separated\necho \"${#Fruits[@]}\"          # Number of elements\necho \"${#Fruits}\"             # String length of the 1st element\necho \"${#Fruits[3]}\"          # String length of the Nth element\necho \"${Fruits[@]:3:2}\"       # Range (from position 3, length 2)\necho \"${!Fruits[@]}\"          # Keys of all elements, space-separated\n\nFruits=(\"${Fruits[@]}\" \"Watermelon\")    # Push\nFruits+=('Watermelon')                  # Also Push\nFruits=( \"${Fruits[@]/Ap*/}\" )          # Remove by regex match\nunset Fruits[2]                         # Remove one item\nFruits=(\"${Fruits[@]}\")                 # Duplicate\nFruits=(\"${Fruits[@]}\" \"${Veggies[@]}\") # Concatenate\nlines=(`cat \"logfile\"`)                 # Read from file\n\nfor i in \"${Fruits[@]}\"; do\n  echo \"$i\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#traversing-dicts","title":"traversing dicts","text":"<pre><code>declare -A sounds\nsounds[dog]=\"bark\"\nsounds[cow]=\"moo\"\nsounds[bird]=\"tweet\"\nsounds[wolf]=\"howl\"\n\necho \"${sounds[dog]}\" # Dog's sound\necho \"${sounds[@]}\"   # All values\necho \"${!sounds[@]}\"  # All keys\necho \"${#sounds[@]}\"  # Number of elements\nunset sounds[dog]     # Delete dog\n\n# iterate over values\n\nfor val in \"${sounds[@]}\"; do\n  echo \"$val\"\ndone\n\n# iterate over keys\n\nfor key in \"${!sounds[@]}\"; do\n  echo \"$key\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#diff-between-and","title":"diff between $@ and $*","text":"<p>$* and $@ when unquoted are identical and expand into the arguments.</p> <p>\"$*\" is a single word, comprising all the arguments to the shell, joined together with spaces. For example '1 2' 3 becomes \"1 2 3\".</p> <p>\"$@\" is identical to the arguments received by the shell, the resulting list of words completely match what was given to the shell. For example '1 2' 3 becomes \"1 2\" \"3\"</p> <p>In short, $@ when quoted (\"$@\") breaking up the arguments if there are spaces in them. But \"$*\" does not breaking the arguments. </p> <pre><code># ./specialvars.sh 1 2 \"3 4\"\nwithout quotes $*: 1\nwithout quotes $*: 2\nwithout quotes $*: 3\nwithout quotes $*: 4\n\nwithout quotes $@: 1\nwithout quotes $@: 2\nwithout quotes $@: 3\nwithout quotes $@: 4\n\nwith quotes \"$*\": 1 2 3 4\n\nwith quotes \"$@\": 1\nwith quotes \"$@\": 2\nwith quotes \"$@\": 3 4\n</code></pre>"},{"location":"linux/scripting/faq/#concat-two-str","title":"concat two str","text":"<pre><code>a=\"Sunil\"\nb=\"Kumar\"\n\necho $a $b\n</code></pre> <pre><code>array=(\"Sunil\" \"kumar\")\n\nfor i in ${array[@]}; do \n    strnew+=\"$i\"\ndone \n\necho $strnew\n</code></pre>"},{"location":"linux/scripting/faq/#function-defn-and-args","title":"function defn and args","text":"<pre><code>vim ./script Sunil\nfunction f1(){\n  echo \"Hello $1\"\n}\n\nf1 $1\n\n$./script Sunil\n</code></pre>"},{"location":"linux/scripting/faq/#func-return-value","title":"func return value","text":"<pre><code>function f1(){\n  echo \"Hello $1\"\n}\n\nretval=$(f1 $1)\necho $retval\n\n$./script Sunil \n</code></pre>"},{"location":"linux/scripting/faq/#file-types","title":"file types","text":"Operator Description Example -b file checks if file is a block special file; if yes, then the condition becomes true. [ -b $file ] is false. -c file checks if file is a character special file; if yes, then the condition becomes true. [ -c $file ] is false. -d file checks if file is a directory; if yes, then the condition becomes true. [ -d $file ] is not true. -f file checks if file is an ordinary file as opposed to a directory or special file; [ -f $file ] is true. -g file checks if file has its set group ID (SGID) bit set; if yes, then the condition becomes true. [ -g $file ] is false. -k file checks if file has its sticky bit set; if yes, then the condition becomes true. [ -k $file ] is false. -p file checks if file is a named pipe; if yes, then the condition becomes true. [ -p $file ] is false. -t file checks if file descriptor is open and associated with a terminal [ -t $file ] is false. -u file checks if file has its Set User ID (SUID) bit set [ -u $file ] is false. -r file checks if file is readable; if yes, then the condition becomes true. [ -r $file ] is true. -w file checks if file is writable; if yes, then the condition becomes true. [ -w $file ] is true. -x file checks if file is executable; if yes, then the condition becomes true. [ -x $file ] is true. -s file checks if file has size greater than 0; if yes, then condition becomes true. [ -s $file ] is true. -e file checks if file exists; is true even if file is a directory but exists. [ -e $file ] is true. <pre><code>test -f ${FILE} &amp;&amp; echo \"File Exists\" - Method 1 \n[ -f ${FILE} ] &amp;&amp; echo \"File Exists\" - Method 2 \n                  OR \n[[ -f ${FILE} ]] &amp;&amp; echo \"File Exists\"\n</code></pre>"},{"location":"linux/scripting/faq/#oddeven","title":"odd/even","text":"<pre><code># print even numbers\nfor i in {0..10..2}; do \n    echo \"${i}\"\ndone\n\n# sum of even numbers\nevensum=0\nfor i in {0..5..2}\ndo\n  evensum=$((evensum+i))\ndone\necho \"${evensum}\"\n\n# print odd numbers\nfor i in {1..10..2}; do \n    echo \"${i}\"\ndone\n\n# print sum of odd numbers\noddsum=0\nfor i in {1..5..2}\ndo\n  oddsum=$((oddsum+i))\ndone\necho \"${oddsum}\"\n</code></pre>"},{"location":"linux/scripting/faq/#reverse-string","title":"reverse string","text":"<pre><code>s=\"sunil\"\nfor((i=${#s};i&gt;=0;i--)); do \n    revstr=$revstr${s:$i:1}\ndone\n\n# oneliner\necho ${s}| rev \nrev &lt;&lt;&lt; ${s}\n</code></pre>"},{"location":"linux/scripting/faq/#for-and-while","title":"for and while","text":"<pre><code># for loop\nfor((i=1;i&lt;=10;i++)); do \n  echo $i\ndone\n\n# while loop\ni=0\nwhile [ $i -le 10 ];  do \n    echo $i \n    ((i++))\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#factorial","title":"factorial","text":"<pre><code>counter=5\nfactorial=1\nwhile [ $counter -gt 0 ]; do\n  factorial=$(($factorial * $counter))\n  counter=$(($counter - 1))\ndone\necho \"${factorial}\"\n</code></pre>"},{"location":"linux/scripting/faq/#reverse-of-num","title":"reverse of num","text":"<pre><code>n=123456\nrem=0\nrevnum=0\nwhile [ $n -gt 0 ]; do \n    rem=$(( $n % 10 ))\n    revnum=$(( $revnum * 10 + $rem ))\n    n=$(( $n / 10 ))\ndone\necho $revnum\n</code></pre>"},{"location":"linux/scripting/faq/#password-strength","title":"password strength","text":"<pre><code>code goes here\n</code></pre>"},{"location":"linux/scripting/faq/#line-count-in-file","title":"line count in file","text":"<pre><code>file=\"$1\"\nlet count=0\nwhile read line; do \n    ((count++))\ndone &lt;$file\n\necho \"Count:\" $count\n</code></pre>"},{"location":"linux/scripting/faq/#return-lines-from-func","title":"return lines from func","text":"<pre><code>function count() {\n  local file=\"$1\"\n  let count=0\n  while read line; do \n      ((count++))\n  done &lt;\"$file\"\n\n  echo $count \n}\n\necho \"Count:\" $(count \"$1\")\n</code></pre>"},{"location":"linux/scripting/faq/#discard-comments-in-file","title":"discard comments in file","text":"<pre><code>while read -r line\ndo\n  [[ $line = \\#* ]] &amp;&amp; continue\n  printf '%s\\n' \"$line\"\ndone &lt; ./passwd\n</code></pre>"},{"location":"linux/scripting/faq/#retain-the-last-50-lines-in-logfile","title":"retain the last 50 lines in logfile","text":"<pre><code>LOG_DIR=/var/log\nROOT_UID=0\nLINES=30\nE_WRONGARGS=85\nE_XCD=86\nE_NONROOT=87\n\nif [ \"$UID\" != \"$ROOT_UID\" ]\nthen\n  echo \"Must be root to run this script !\"\n  exit $E_NONROOT\nfi\n\n\ncase \"$1\" in \n  \"\") lines=50;;\n  *[0-9]*) echo \"Usage: `basename $0` #lines_to_cleanup\";\n           exit $E_WRONGARGS;;\n  *)      lines=$1;;\nesac\n\ncd /var/log || {\n  echo \"Cannot change to directory\" &gt;&amp;2\n  exit $E_XCD;\n  }\n\ntail -n $lines messages &gt; mesg.temp\nmv mesg.temp messages\n\necho \"Log files cleaned up\"\nexit 0\n</code></pre>"},{"location":"linux/scripting/faq/#random-num-generator-200-500","title":"random num generator 200-500.","text":"<pre><code>#!/bin/bash\nMIN=200\nMAX=500\nlet \"scope = $MAX - $MIN\" #300\nif [ \"$scope\" -le \"0\" ]; then\n  echo \"Error- MAX less than MIN\"\nfi\n\nfor i in {1..10}; do\n  let result=\"$RANDOM % $scope + $MIN\"\n  echo \"Random Number between Min/Max: $result\"\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#email-disk-alert","title":"email disk alert.","text":"<pre><code>#!/bin/bash \n\nWARNING=90\nCRITICAL=95\n\ndf -H | egrep -v '^/dev/loop|tmpfs|udev|Filesystem' | awk '{ print $5 \" \" $1}'| while read line;\ndo\n  echo $output\n  usep=$(echo $output | awk '{ print $1}' | cut -d'%' -f1  )\n  partition=$(echo $output | awk '{ print $2 }' )\n\n  if [ $usep -ge $WARNING ]; then \n    echo \"WARNING: Running out of space \\\"$partition - $usep\\\"\"\n  elif [ $usep -ge $CRITICAL ]; then \n    echo \"CRITICAL: Require Immediate attention on \\\"$partition - $usep\\\"\"\n    mail -s \"Alert:Critical: Disk running out of space\" &lt;youremailid&gt;\n  fi\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#fibonacci-series","title":"fibonacci series","text":"<pre><code>fib() {\n  return $(( $1 + $2 ))\n}\n\nF0=0\nF1=1\necho \"0: $F0\"\necho \"1: $F1\"\nfor count in `seq 2 20`; do\n fib $F0 $F1\n F2=$?\n if [ \"$F2\" -lt \"$F1\" ]; then\n  echo \"$count: $F2 (WRONG!)\"\n else\n  echo \"${count}: $F2\"\n fi\n F0=$F1\n F1=$F2\n sleep 0.1\ndone\n\nfib $F0 $F1\necho \"${count}: $?\"\n</code></pre> <pre><code>#!/bin/bash\nfunction fibonacci\n  {\n    echo $1 + $2 | bc | tr -d '\\\\\\n'\n  }\n\nF0=0\nF1=1\necho \"0: $F0, \"\necho \"1: $F1, \"\ncount=2\nwhile :\ndo\n  F2=`fibonacci $F0 $F1`\n  echo \"${count}: $F2,\"\n  ((count++))\n  F0=$F1\n  F1=$F2\n  sleep 0.1\ndone\nfibonacci $F0 $F1\n</code></pre>"},{"location":"linux/scripting/faq/#del-last-line-in-multiple-files","title":"del last line in multiple files","text":"<pre><code>for file in sub/*\n  do\n    if [ -f $file ]\n    then\n      vi -c '$d' -c 'wq' \"$file\"\n    fi\n  done\n</code></pre>"},{"location":"linux/scripting/faq/#replace-string-multiple-places-in-multiple-files","title":"replace string multiple places in multiple files","text":"<pre><code>for file in ./*\n  do\n    if [ -f $file ]\n    then\n      sed -i 's/2018/2019/g' \"$file\"\n    fi\n  done\n</code></pre>"},{"location":"linux/scripting/faq/#verify-string-not-null","title":"verify string not null","text":"<pre><code>str1=\"Not Null\"\nstr2=\" \"\nstr3=\"\"\nmessage=\"is not Null nor a space\"\n\nif [ ! -z \"$str1\" -a \"$str1\" != \" \" ]; then\n  echo \"str1 ${message}\"\nfi\n\nif [ ! -z \"$str2\" -a \"$str2\" != \" \" ]; then\n  echo \"str2 ${message}\"\nfi\n\nif [ ! -z \"$str3\" -a \"$str3\" != \" \" ]; then\n  echo \"str3 ${message}\"\nfi\n</code></pre>"},{"location":"linux/scripting/faq/#read-contents-in-a-file-line-by-line","title":"Read contents in a file line by line","text":"<pre><code>for h in $(&lt;hosts.txt)\ndo\n  echo $h\ndone\n</code></pre>"},{"location":"linux/scripting/faq/#bash-substitutions","title":"bash substitutions","text":"<p>${v} - Substitue the value of v. v1=1 echo \"substitute value of v1 '\\${v1}'\" echo \"sub value for v1:\"${v1} echo \"--\"</p> <p>${v:-val} - if v is null or unset, val is substituuted echo ${v2:-2} echo \"Value not set for v2:\" $v2 echo \"--\"</p> <p>${v:=val} - if v is null or unset, vs is set to val echo ${v3:=3} echo \"val is substituted:\" $v3 echo \"---\"</p> <p>${v:+val} - if v is set, val is substituted. v is unchanged v4=1234 echo \"val is substituted\" ${v4:+44} echo \"val is unchanged\" ${v4} echo \"---\"</p> <p>${v:?val} - if v is null or unset, val is printed to std err v5=5 echo \"no err printed as v5 is set\" ${v5:?5555} echo \"value unchanged\" ${v5}</p> <p>echo \"not setting value, yest printing results\" echo \"print value undefined\" ${v6:? \"Value unable to find\"} echo \"---\"</p>"},{"location":"linux/scripting/faq/#example-script","title":"$* &amp; $# example script","text":"<pre><code>#!/bin/bash\n\nfor i in $*; do \n    echo \"without quotes \\$*: $i\"\ndone\n\necho\n\nfor i in $@; do \n    echo \"without quotes \\$@: $i\"\ndone\n\necho\n\nfor i in \"$*\"; do\n    echo \"with quotes \"'\"$*\"'\": $i\"\ndone\n\necho\n\nfor i in \"$@\"; do\n    echo \"with quotes \"'\"$@\"'\": $i\"\ndone\n\n# ./specialvars.sh 1 2 \"3 4\"\n# without quotes $*: 1\n# without quotes $*: 2\n# without quotes $*: 3\n# without quotes $*: 4\n\n# without quotes $@: 1\n# without quotes $@: 2\n# without quotes $@: 3\n# without quotes $@: 4\n\n# with quotes \"$*\": 1 2 3 4\n\n# with quotes \"$@\": 1\n# with quotes \"$@\": 2\n# with quotes \"$@\": 3 4\n</code></pre>"},{"location":"linux/scripting/faq/#design-help-menu","title":"design help menu","text":"<pre><code>function help() {\n   echo \"Syntax: ./script &lt;arg&gt; [-h|-v]\"\n   echo \n   echo \"options:\"\n   echo \"-h  Print this Help.\"\n   echo \"-v  Print software version and exit.\"\n   echo\n}\n\nwhile getopts \":hv\" option\ndo\n  case ${option} in \n    \"h\") help ;;\n    \"v\") echo 12.10.10 ;;\n    \"*\") echo \"Error: Invalid Option\" || exit \n  esac\ndone\n</code></pre> <pre><code>[[ -z \"$1\" ]] &amp;&amp; grep \"^#:\" $0 | sed -e 's/#://' &amp;&amp; exit\n[[ \"$1\" == '-h' ]] &amp;&amp; grep \"^#:\" $0 | sed -e 's/#://' &amp;&amp; exit\n[[ \"$1\" == '-v' ]] &amp;&amp; echo \"12.10.10\" || echo \"Error: Invalid Option\" &amp;&amp; exit\n</code></pre>"},{"location":"linux/scripting/faq/#log","title":"log","text":"<pre><code>#!/bin/bash\n\nexec 3&gt;&amp;1 4&gt;&amp;2\ntrap 'exec 2&gt;&amp;4 1&gt;&amp;3' 0 1 2 3\nexec 1&gt;log.out 2&gt;&amp;1\n\nlog() {\n# write the log\n  local msg=\"$1\"\n  DATE=`date '+%b %e %H:%M:%S'`\n  echo INFO: $DATE $msg\n}\n\n\nlog \"Hello World\"\nlog \"Alice &amp;&amp; Bob wants to talk to each other in secure communication \"\n</code></pre>"},{"location":"linux/scripting/faq/#design-calculator","title":"design calculator","text":"<pre><code>#!/bin/bash \n\n# Simple Basic Calculator \n\n# check for arguments \nif [ $# -ne 2 ]; then \n   echo \"Usage: ./calculator.sh &lt;arg1&gt; &lt;arg2&gt;\"\n   exit 1\nfi\n\nfunction summing() {\n  result=`expr $1 + $2`\n  echo \"Sum:($1+$2)=\"$result\n}\n\n# Difference \nfunction difference(){\n  result=`expr $1 - $2`\n  echo \"Difference:($1-$2)=\"$result\n}\n\n# Multiplication \nfunction multiplication(){\n  result=`expr $1 \\* $2`\n  echo \"Multiplication:($1*$2)=\"$result\n}\n\n# Division \nfunction division(){\n  result=`expr $1 / $2`\n  echo \"Division:($1/$2)=\"$result\n}\n\nsumming $1 $2 \ndifference $1 $2\nmultiplication $1 $2 \ndivision $1 $2\n</code></pre>"},{"location":"linux/scripting/faq/#seq-numbers","title":"seq numbers","text":"<p>first method</p> <pre><code>i=1\nwhile [[ $i -le 3 ]]; do \n  echo $i\n  i=$((i+1))\ndone\n</code></pre> <p>second method</p> <pre><code>seq 1 3 | while read i ; do \n  echo $i\ndone\n</code></pre> <p>third method</p> <pre><code>for i in $(seq 1 3); do \n    echo $i\ndone\n</code></pre>"},{"location":"linux/scripting/hackerrank/","title":"Hackerrank","text":""},{"location":"linux/scripting/hackerrank/#cut","title":"cut","text":"<p>print the 3rd character from each line as a new line of output.</p> <pre><code>cut -c3\n</code></pre> <p>Display the 2nd and 7th character from each line of text.</p> <pre><code>cut -c2,7\n</code></pre> <p>Display a range of characters starting at the 2nd position of a string and ending at the 7th position (both positions included)</p> <pre><code>cut -c2-7\n</code></pre> <p>Given a tab delimited file with several columns (tsv format) print the first three fields.</p> <pre><code>cut -f1-3\n</code></pre> <p>Print the characters from thirteenth position to the end.</p> <pre><code>cut -c13-\n</code></pre> <p>Each Input sentence, identify and display its fourth word. Assume that the space (' ') is the only delimiter between words.</p> <pre><code>cut -d\" \" -f4\n</code></pre> <p>The output should contain N lines. For each input sentence, identify and display its first three words. Assume that the space (' ') is the only delimiter between words.</p> <pre><code>cut -d\" \" -f1-3\n</code></pre> <p>For each line in the input, print the fields from second fields to last field.</p> <pre><code>cut -f2-\n</code></pre>"},{"location":"linux/scripting/hackerrank/#head","title":"head","text":"<p>Output the first 20 lines of the given text file.</p> <pre><code>head -20\n</code></pre> <p>Output the first 20 characters of the text file</p> <pre><code>head -c20\n</code></pre> <p>Display the lines (from line number 12 to 22, both inclusive) for the input file.</p> <p>Hint: First display first 22 lines and then tail(22-12=10)</p> <pre><code>head -22|tail -11\n</code></pre>"},{"location":"linux/scripting/hackerrank/#paste","title":"paste","text":"<p>Replace the newlines in the input file with semicolons</p> <pre><code>paste -d\";\" -s\n</code></pre> <p>Restructure the file so that three consecutive rows are folded into one line and are separated by semicolons.</p> <pre><code>paste -d \";\" - - -\n</code></pre> <p>The delimiter between consecutive rows of data has been transformed from the newline to a tab. Previous solution: paste -s -d\"\\t\". The delimiter option is not necessary as tab is the delimiter of paste by default</p> <pre><code>paste -s\n</code></pre> <p>Restructure the file in such a way, that every group of three consecutive rows are folded into one, and separated by tab.</p> <pre><code>paste - - -\n</code></pre>"},{"location":"linux/scripting/hackerrank/#sort","title":"sort","text":"<p>Output the text file with the lines reordered in lexicographical order.</p> <pre><code>sort\n</code></pre> <p>Output the text file with the lines reordered in reverse lexicographical order.</p> <pre><code>sort -r\n</code></pre> <p>Output the text file with the lines reordered in numerically ascending order.</p> <pre><code>sort -n\n</code></pre> <p>The text file, with lines re-ordered in descending order (numerically)</p> <pre><code>sort -n -r\n</code></pre> <p>Rearrange the rows of the table in descending order of the values for the average temperature in January (i.e, the mean temperature value provided in the second column) in a tab seperated file.</p> <pre><code>sort -k2 -n -r -t$'\\t'\n</code></pre> <p>The data has been sorted in ascending order of the average monthly temperature in January (i.e, the second column) in a tsv file</p> <pre><code>sort -n -k2 -t$'\\t'\n</code></pre> <p>The data has been sorted in descending order of the average monthly temperature in January (i.e, the second column).</p> <pre><code>sort -k2 -n -r -t '|'\n</code></pre>"},{"location":"linux/scripting/hackerrank/#tail","title":"tail","text":"<p>Output the last 20 lines of the text file.</p> <pre><code>tail -20\n</code></pre> <p>Display the last 20 characters of an input file.</p> <pre><code>tail -c20\n</code></pre>"},{"location":"linux/scripting/hackerrank/#tr","title":"tr","text":"<p>Output the text with all parentheses () replaced with box brackets [].</p> <pre><code>tr \"()\" \"[]\"\n</code></pre> <p>In a given fragment of text, delete all the lowercase characters a - z</p> <pre><code>tr -d \"a-z\"\n</code></pre> <p>Replace all sequences of multiple spaces with just one space.</p> <pre><code>tr -s \" \"\n</code></pre>"},{"location":"linux/scripting/hackerrank/#uniq","title":"uniq","text":"<p>Given a text file, remove the consecutive repetitions of any line.</p> <pre><code>uniq\n</code></pre> <p>Given a text file, count the number of times each line repeats itself. Only consider consecutive repetitions. Display the space separated count and line, respectively. There shouldn't be any leading or trailing spaces. Please note that the uniq -c command by itself will generate the output in a different format than the one expected here.</p> <pre><code>uniq -c | cut -c7-\n</code></pre> <p>compare consecutive lines in a case insensitive manner. So, if a line X is followed by case variants, the output should count all of them as the same (but display only the form X in the second column).</p> <pre><code>uniq -i -c | cut -c7-\n</code></pre> <p>Given a text file, display only those lines which are not followed or preceded by identical replications.</p> <pre><code>uniq -u\n</code></pre>"},{"location":"linux/scripting/overview/","title":"Bash Overview","text":"<p>Simple script to git checkout </p> <pre><code>\n#!/bin/bash\nproject=\"$1\"\nbranch=\"$2\"\n\nproject_dir=\"$(basename ${project} .git)\"\n\nclone_project() {\n  if [ ! -d \"/home/bob/git/${project_dir}\" ]; then\n    cd /home/bob/git/\n    git clone ${project}\n    cd \"${project_dir}\"\n    git checkout \"${branch}\"\n  fi\n}\n\ngit_checkout() {\n  cd \"${project_dir}\"\n  git checkout \"${branch}\"\n\nfind_files() {\n  find . -type f | wc -l\n}\nclone_project\ngit_checkout\nfind_files\n\n</code></pre>"},{"location":"linux/scripting/overview/#redirecting-streams","title":"redirecting streams","text":"<p>file descriptor - A unique identifier that the operating system assigns to a file when it is opened.</p> <p>0 - stdin </p> <p>1 - stdout </p> <p>2 - stderr</p> <p>we have two conditions that are there to redirect streams</p> <ol> <li><code>&gt;</code> shell thinks you are redirecting output to <code>stdout</code> which is to a file by default(also written as <code>&gt;&amp;</code>).</li> <li><code>&amp;&gt;</code> output + stderr both are redirected to the <code>file descriptor</code> and to a location. </li> </ol> <p>example</p> <p><code>ls -z 2&gt;&amp;1 &gt; file1.txt</code> </p> <p>we are redirecting the <code>stderr</code>(i.e <code>2</code>) to the file descriptor i.e <code>&amp;1</code> which is <code>stdout</code> which by defaults to <code>file1.txt</code>. so we are writing output and error in the same file. </p> <p>the above can also be changed to below syntax which is very common. <code>ls -z &gt; file1.txt 2&gt;&amp;</code> <code>ls -z &gt; /dev/null 2&gt;&amp;1</code> these can be found in the scripts normally.. </p> <p>custom redirections</p>"},{"location":"linux/scripting/overview/#single-line-modifications","title":"single line modifications","text":"<pre><code>echo \"Suni lkumar@gmail.com\" &gt; email_file.txt\ncat email_file.txt \nSuni lkumar@gmail.com -&gt; observer there is missing 4th letter.\nexec 3&lt;&gt; email_file.txt # open the file with fd as 3\nread -n 4 &lt;&amp;3 # read 4 words by inputing fd 3 i.e email_file.txt\necho -n \".\" &gt;&amp;3 # output 4th letter to the fd 3 i.e email_file.txt\nexec 3&gt;&amp;- # close the fd 3\n\ncat email_file.txt \nSuni.lkumar@gmail.com -&gt; you can now see its updated\n</code></pre>"},{"location":"linux/scripting/overview/#multiline-modifications","title":"multiline modifications","text":"<p>this is for <code>heredocs</code> where you can create/execte multiple commands for the executions. </p> <pre><code>ssh root@servre1 &lt;&lt;EOF\nmkdir ~/heredocs\necho \"heredocs\" &gt; ~/heredocs/heredocs.txt\nEOF\n</code></pre>"},{"location":"linux/scripting/overview/#pipes","title":"pipes","text":"<ul> <li>names pipes: stdout to a file</li> </ul> <pre><code>sort &lt; abc.txt &gt; sorted_text.txt\n</code></pre> <p>sort will take the input and redirect it to stdout.. so we would proivde the input to sort and later we will output stored in the normal text file.</p> <ul> <li>Anomymous pipes: pass output from one place to another place</li> </ul> <p>command1 | command 2</p> <pre><code>cat filename.txt | grep -i \"o\" | sort\n</code></pre> <p>there is 1 issue with the pipe commands, if the command is invalid or failed, it would continue to process the next inputs. WE MUST AVOID THAT KIND..</p> <pre><code>MacBook-Pro:wiki sunilamperayani$ ls -z | echo \"helllo\"\nhelllo --&gt; you are able to execute this command despite the first command got failed.\nls: invalid option -- z\nusage: ls [-@ABCFGHILOPRSTUWabcdefghiklmnopqrstuvwxy1%,] [--color=when] [-D format] [file ...]\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ sort somefile.xtxt  | uniq &amp;&amp; echo \"hello\"\nsort: No such file or directory\nhello\nMacBook-Pro:wiki sunilamperayani$ echo $?\n0\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <p>despite getting failed return code was successful.</p>"},{"location":"linux/scripting/overview/#pipefail","title":"pipefail","text":"<p>immediately stop the execution of the command when one of the command fails in the chain of command.</p> <pre><code>MacBook-Pro:wiki sunilamperayani$ set -o pipefail\nMacBook-Pro:wiki sunilamperayani$ sort somefile.xtxt  | uniq &amp;&amp; echo \"hello\"\nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ echo $?\n2\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <p>we can also use <code>exit</code> codes to infact exit the scripts..</p> <pre><code># set-fail.sh \n#!/usr/bin/env bash\nset -o pipefail\nsort newfile.txt | uniq || exit 80\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ ./set-fail.sh \nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ echo $?\n80\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <pre><code># noclobber.sh\n#!/usr/bin/env bash\nset -o noclobber \necho \"line1\" &gt; file1.txt\necho \"line2\" &gt; file1.txt\n\nsort somefile.txt | uniq || exit 100\n</code></pre> <pre><code>MacBook-Pro:wiki sunilamperayani$ ./noclobber.sh \n./noclobber.sh: line 3: file1.txt: cannot overwrite existing file\n./noclobber.sh: line 4: file1.txt: cannot overwrite existing file\nsort: No such file or directory\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre> <pre><code># eval.sh\nMacBook-Pro:wiki sunilamperayani$ cat eval.sh \n#!/usr/bin/env bash\ncmd=\"ls -l\"\neval $cmd\nMacBook-Pro:wiki sunilamperayani$ \n</code></pre>"},{"location":"linux/scripting/overview/#arrays","title":"arrays","text":"<pre><code># bash shell &gt; 5.3-release\n\ndeclare -a servers\nservers=(\"server1\" \"coding\" \"structure tets\")\n\nnew_servers=(\"${servers[@]:0:1}\" \"server1.5\" ${servers[@]:1}]})\necho \"${new_servers[@]}\" # server1 server1.5 coding structure tets]}\n\nunset servers[1]\necho \"${servers[@]}\" # server1 structure tets\n\ndeclare -a array=(\"One\" \"Two\" \"Three\")\narray+=(\"Four\" \"Five\" \"Six\")\necho \"${array[@]}\" # One Two Three Four Five Six\n\ndeclare -a numbers=(5 1 3 2 4)\nprintf \"%s\\n\" \"${numbers[@]}\" | sort # 1 2 3 4 5\n\n# write an example of associative array key-value pair\n# instead of using index, we can use string as index\ndeclare -A fruits\nfruits=([apple]='red' [banana]='yellow' [cherry]='red')\necho \"${fruits[apple]}\" # red\n\n# add new key-value pair\nfruits[\"green\"]=\"pear\"\necho \"${fruits[green]}\" # pear\n\n# modify value\nfruits[\"red\"]=\"new apple\" \necho \"${fruits[@]}\"\n\n# delete key-value pair\nunset fruits[banana]\necho \"${fruits[@]}\" # red new apple pear\n\n# loop through key-value pair\nfor key in \"${!fruits[@]}\"; do\n    echo \"$key: ${fruits[$key]}\"\ndone\n</code></pre>"},{"location":"linux/scripting/overview/#variable-expansion","title":"variable expansion","text":"<pre><code>echo \"Hello ${name1:-unkown}\" # name is not defined so it will print unkown which is default value\n\nname=\"John Doe\"\necho \"Hello ${name:=unkown}\" # assign default values\n\n\necho \"Hello, ${name:0:4}\" # extract substring from 0 to 4\n\n# string replacement\necho \"${path/Downloads/Documents}\" # replace Downloads with Documents\n\n# string length\necho \"Length: ${#path}\" # print length of the string\n</code></pre>"},{"location":"linux/scripting/overview/#parameter-expansion","title":"parameter expansion","text":"<p>'#' matching <code>prefix</code> '%' matching <code>suffix</code></p> <pre><code># extract last part of the path\npath=\"/home/user/Downloads\"\necho \"Path: ${path##*/}\" # extract last part of the path\n\ngreeting=\"Hello World\"\necho \"${greeting#H}\" # matches from left to right\n#ello World\n\necho \"${greeting%d}\" # matches from right to left\n#Hello Worl\n\nmy_text_file=\"/home/my_username/text_file.txt\"\nmy_python_file=\"/usr/bin/app.py\"\n\necho \"${my_text_file##*/}\" # remove the last part of the path\n\necho \"${my_python_file##*/}\" # remove the last part of the path\n\necho \"${my_python_file%.*}\" # remove the last part of the path\n</code></pre>"},{"location":"linux/scripting/overview/#examples","title":"examples","text":""},{"location":"linux/scripting/overview/#bash-unit-testing","title":"bash unit testing","text":"<pre><code>#!/bin/bash \n\nhelp(){\n  scriptname=`basename $0`\n  echo \"Syntax: ${scriptname} &lt;filename&gt;\"\n  exit 1\n}\n\nvalidateOnlyOneArgument(){\n  if [ \"$#\" -ne 2 ]; then \n    echo \"PASSED: Atleast one argument passed\"\n  fi\n\n  return 0\n}\n\nvalidateFileOnly() {\n  [ -f ${filename} ] &amp;&amp; echo \"PASSED: fileonly\"\n  [ -d ${filename} ] &amp;&amp; echo \"Failed: directory\" &amp;&amp; exit 1\n\n  return 0\n}\n\nreadUnCommentLinesInFile() {\n  filename=\"$1\"\n  while read -r line; do \n    [[ \"$line\" = \\#* ]] &amp;&amp; continue \n    printf \"%s\\n\" \"$line\"\n  done&lt;${filename}\n}\n\ncleanLogFiles() {\n  local logfile=\"$1\"\n  local default_lines=50\n  tail -n ${default_lines} ${logfile} &gt; ${logfile}.tmp\n  mv ${logfile}.tmp ${logfile}\n  echo \"Info: housekeeping on ${logfile} completed\"\n\n  return 0\n}\n\nwhoExecutesThisScript() {\n  [ \"${UID}\" -ge 500 ] &amp;&amp; echo \"Info: non-root users can execute this script\"\n}\n\ncountNumberOfLinesInFile() {\n  local count=0\n  local filename=\"$1\"\n  while read -r line; do \n    ((count++))\n    echo $line &lt;$filename\n  done\n\n  return $count\n}\n\n# main function\nmain() {\n  filename=\"$1\"\n  logfilename=\"./app.log\"\n  whoExecutesThisScript\n  linecountinfile=countNumberOfLinesInFile \"$filename\"\n  echo \"lines in file: $?\"\n\n  # Unit tests for the script, only when successful, then continue\n\n  if validateOnlyOneArgument; then \n     validateFileOnly\n     cleanLogFiles $logfilename\n     readUnCommentLinesInFile $filename\n  fi\n}\n\n[ \"$#\" -ne \"1\" ] &amp;&amp; help \nmain \"$1\" | tee -a app.log\n</code></pre>"},{"location":"linux/scripting/overview/#good-practices","title":"Good practices","text":"<p>Bash reserved exit codes</p> <pre><code>0  success\n1  general error\n2  misuse of shell builtins\n126 cannot execute\n128  cannot execute\n130  script terminated by ctrl-c\n127  command not found\n</code></pre> <p><code>set -e</code>  exits on error <code>set -u</code> exits when a unset variable is used <code>set -o pipefail</code> catches errors in piped commands</p>"},{"location":"linux/scripting/overview/#set-e","title":"set -e","text":"<p>Incorrect bash script</p> <pre><code># safe.sh\n#!/usr/bin/env bash\n\nehco \"hello\"\nexit 0\n\n./safe.sh\necho $? #0 returns success code which is wrong\n</code></pre> <p>Correct Bash script</p> <pre><code># safe.sh\n#!/usr/bin/env bash\n\nset -e\n\nehco \"hello\"\nexit 0\n\n./safe.sh\necho $? #126\n</code></pre>"},{"location":"linux/scripting/overview/#set-u","title":"set -u","text":"<p>Incorrect bash script</p> <pre><code># safe1.sh\n\nname=\"Sunil\"\n# last_name=\"Kumar\"\n\necho \"My Full name is ${name} ${last_name}..!!\"\nexit 0 \n\n./safe1.sh\necho $? # 0 Incorrect as last_name is not executed\n</code></pre> <p>Corrected bash version</p> <pre><code>set -u\necho \"My Full name is ${name} ${last_name}..!!\"\nexit 0 \n\n./safe1.sh\necho $? # 1 \n</code></pre>"},{"location":"linux/scripting/overview/#set-o-pipefail","title":"set -o pipefail","text":"<pre><code># pipefail.sh\n\ncat non-existent-file.txt | sort | uniq \nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n0\n</code></pre> <p>In below script, if the <code>cat</code> has been failed, it should not execute the <code>echo</code> command.. but it does. so to fix it use <code>set -o pipefail</code></p> <pre><code># pipefail.sh\n\ncat non-existent-file.txt | sort | uniq &amp;&amp; echo \"this line should not be executed\";\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\n\"this line should not be executed\";\necho $? \n0\n</code></pre> <p>you won't get an <code>echo</code> statement if it fails when we set <code>set -o pipefail</code> but we still have an issue with the exit code.</p> <pre><code># pipefail.sh\nset -o pipefail\n\ncat non-existent-file.txt | sort | uniq &amp;&amp; echo \"this line should not be executed\";\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n0\n</code></pre> <pre><code># pipefail.sh\nset -e\nset -u\nset -o pipefail\n\nreadonly PIPE_ERROR=156\n\n# terminate function technique\nterminate()\n{\n  local -r msg=\"${1}\"\n  local -r code=\"${2:-160}\"\n  echo \"${msg}\" &gt;&amp;2\n  exit \"${code}\"\n}\n\ncat non-existent-file.txt | sort || { terminate \"error in piped command\" \"${PIPE_ERROR}\" }\n\nexit 0\n\n./pipefail.sh\ncat non-existent-file.txt | sort | uniq \ncat: non-existent-file.txt: no such file or directory\necho $? \n156\n</code></pre>"},{"location":"linux/scripting/overview/#no-op-command","title":"no-op command","text":"<p>dry run command in bash. its a shell-built in command which has no behaviour programmed.</p> <pre><code>#!/use/bin/env bash\n\nif [[ \"$1\" = \"start\" ]]; then\n  : #no-op command\nelse\n  echo \"invalid string\"\nfi\n</code></pre>"},{"location":"linux/scripting/overview/#logging","title":"logging","text":"<pre><code>#!/usr/bin/env bash\nlog() {\n  echo $(date -u +\"%Y-%m-%d%T%H:%M:%SZ\") \"${@}\"\n}\n\nlog \"hello world\"\n</code></pre>"},{"location":"linux/scripting/overview/#awk","title":"awk","text":"<ul> <li>domain specific language for streamling text operations.</li> <li>expect input as standard behaviour</li> <li>input can be passwd by keyboard, pipes or files</li> <li>' '(quotes) used in awk is to prevent the shell/terminal expansion. </li> </ul> <pre><code># awk expects input as the standard behaviour.\nawk -F\":\" '{print $1}' /etc/passwd \nawk -F\":\" '{print $1}' &lt; /etc/passwd \ncat /etc/passwd | awk '{print $1}'\n</code></pre>"},{"location":"linux/scripting/overview/#built-in-variables","title":"built-in variables","text":"<ul> <li>NR: total number of records (lines) processed so far across all input files</li> <li>NF: represents the number of fields (columns) in the current record (line)</li> <li>FILENAME: represents the name of the current file being processed.</li> </ul> <p>Program block = <code>BEGIN</code> Action block = <code>{print $1}</code></p> <pre><code>\nfile1.txt\n\napple\nbanana\ncherry\n\nawk '{ print NR, $0 }' file1.txt\n\n# output \n\n1 apple\n2 banana\n3 cherry\n\nNR=1 for first line\nNR=2 for second line\n</code></pre> <pre><code>file2.txt\n\napple banana cherry\ngrape mango\n\nawk '{ print NF, $0 }' file2.txt\n\n# output\n3 apple banana cherry\n2 grape mango\n\nThe first line has 3 fields, so NF=3.\nThe second line has 2 fields, so NF=2.\n</code></pre> <pre><code># file1.txt\napple\nbanana\n\n#file2.txt\ngrape\nmango\n\n\nawk '{ print FILENAME, NR, $0 }' file1.txt file2.txt\n\nfile1.txt 1 apple\nfile1.txt 2 banana\nfile2.txt 3 grape\nfile2.txt 4 mango\n\nFILENAME shows the file name being processed.\nNR continues counting records across both files.\n</code></pre> <pre><code>awk '{ print \"File:\", FILENAME, \"Record:\", NR, \"Fields:\", NF, \"Line:\", $0 }' file1.txt file2.txt\n\nFile: file1.txt Record: 1 Fields: 1 Line: apple\nFile: file1.txt Record: 2 Fields: 1 Line: banana\nFile: file2.txt Record: 3 Fields: 1 Line: grape\nFile: file2.txt Record: 4 Fields: 1 Line: mango\n</code></pre> <p>Field seperator</p> <pre><code>cat  /etc/passwd | awk -F \":\" '{print $1 $7}'\n</code></pre> <p>-v declare a vraible before executing the action block or a program.</p> <pre><code>awk -v var=\"Hello world\" 'BEGIN { print var }' \nHello world\n\nawk -F \":\" -v user=\"Users homedirectory: \" '{print user, $1}' /etc/passwd \n\n# output\n\nUsers homedirectory:  _notification_proxy\nUsers homedirectory:  _avphidbridge\nUsers homedirectory:  _biome\nUsers homedirectory:  _backgroundassets\nUsers homedirectory:  _mobilegestalthelper\nUsers homedirectory:  _audiomxd\nUsers homedirectory:  _terminusd\n</code></pre> <pre><code># uid&gt;100\nawk -F\":\" -v user=\"username:\" -v uid=\"100\" '$3&gt;=uid {print user,$1, $3}' /etc/passwd\n\n# 100&gt;uid&lt;300\nawk -F\":\" -v user=\"username:\" -v uid=\"100\" -v uidlow=\"300\" '$3&gt;=uid &amp;&amp; $3&lt;=uidlow {print user,$1, $3}' /etc/passwd\n</code></pre>"},{"location":"linux/scripting/overview/#using-awk-file","title":"using awk-file","text":"<p>you can use an awk program, but shell expansions like globbing, command substituitions, and  command like utilities are not available. </p> <pre><code># hello.awk\n\n#!/usr/bin/env awk -f \nBEGIN {\n  print \"hello\"\n}\nbash-3.2$ \n\n./hello.awk\nhello\n</code></pre> <pre><code>#!/usr/bin/env bash\n\nawk -v hello=\"Hello\" 'BEGIN { \n    print hello\n  }'\n\n</code></pre>"},{"location":"linux/scripting/overview/#sed","title":"sed","text":"<p><code>sed</code> takes the input parameter and would process the unprocessed text and the processed text to the stdout. it will take input from the keyboard, file, or the pipes. i.e similar to awk</p> <pre><code>sed 'p' # waits for the input from the keyboard\nt\nt # unprocessed text\nt # processed text i.e output\n</code></pre> <p>In order to supress automatic printing use <code>-n</code></p> <pre><code>sed -n '2p' filename.txt # print the second line supressing the automatic printing\n</code></pre> <p>delete(-d)</p> <pre><code>sed -n '2d' filename.txt # delete the 2nd line and print to standard output\nsed -n '2,5d' filename.txt # delete from 2nd line to 5th line and print to standard output\n</code></pre> <p>in-place (-i) </p> <pre><code>sed -i '2,5d' filename.txt # write to the filename.txt instead of the stdout\n</code></pre> <p>search</p> <p>search always ends with '//' follwed by 'p' print.  <pre><code>sed -n '/broot/p' /etc/passwd\n\n#\\b word boundary followed by string pattern.\nsed -n '/\\broot\\b/p' /etc/passwd # good practice to use '\\b'\n\n# -e indicated its an sed script for multiple options MANDATORY\nsed -n -e '/\\broot\\b/p' -e '/\\bsunil\\b/p' /etc/passwd \n\n# delete the root and write into the file\nsed -n -i -e '/\\broot\\b/d' -e '/\\bsunil\\b/p' /tmp/passwd\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/","title":"setup and install","text":""},{"location":"monitoring/elk/elasticsearch/01_install/#getting-started","title":"Getting started","text":"<p>Please download the elasticsearch and kibana from below official websites for your OS.</p> <p>Elasticsearch</p> <p>Kibana</p> <p>I would be using Mac for elasticsearch and hence would provide those links for this purpose. </p> <p>elasticsearch for Mac-Intel</p> <p>Kibana for Mac-Intel</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#local-installation-setup","title":"Local Installation Setup","text":"<p>Create a new directory and move those compressed files into the directory for extraction.</p> <pre><code>mkdir elastic-stack\ntar -zxvf elasticsearch-8.8.0-darwin-x86_64.tar.gz\ntar -zxvf kibana-8.8.0-darwin-x86_64.tar.gz\nmv elasticsearch-8.8.0 elasticsearch\nmv kibana-8.8.0 kibana\n</code></pre> <p>Open two terminals and move to respective directories for those two services. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#elasticsearch","title":"Elasticsearch","text":"<p>start the service, It provides the username, password and tokens as part of initial startup</p> <pre><code>cd elastic-stack/elasticsearch\nbin/elasticsearch [ Enter ]\n\n\u2705 Elasticsearch security features have been automatically configured!\n\u2705 Authentication is enabled and cluster connections are encrypted.\n\n\u2139\ufe0f  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):\n  Xl4mktLmPkO=22=_DwEl\n\n\u2139\ufe0f  HTTP CA certificate SHA-256 fingerprint:\n  5c47f6c2bd182fc83cd9487af5dc400c3fa75013995fa8c5cf5b63c930803cb1\n\n\u2139\ufe0f  Configure Kibana to use this cluster:\n\u2022 Run Kibana and click the configuration link in the terminal when Kibana starts.\n\u2022 Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):\n  eyJ2ZXIiOiI4LjguMCIsImFkciI6WyIxOTIuMTY4LjAuMTA2OjkyMDAiXSwiZmdyIjoiNWM0N2Y2YzJiZDE4MmZjODNjZDk0ODdhZjVkYzQwMGMzZmE3NTAxMzk5NWZhOGM1Y2Y1YjYzYzkzMDgwM2NiMSIsImtleSI6ImlTd1BjSWdCRmJwS05TRUtLSmwyOm9sVlc2aEd1VG9tYVJVUThCZ2Y1c3cifQ==\n\n\u2139\ufe0f  Configure other nodes to join this cluster:\n\u2022 On this node:\n  \u2043 Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.\n  \u2043 Uncomment the transport.host setting at the end of config/elasticsearch.yml.\n  \u2043 Restart Elasticsearch.\n\u2022 On other nodes:\n  \u2043 Start Elasticsearch with `bin/elasticsearch --enrollment-token &lt;token&gt;`, using the enrollment token that you generated.\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#kibana","title":"Kibana","text":"<p>Start the service, after which they are prompted using the url to navigate into browser. </p> <pre><code>xattr -d -r com.apple.quarantine kibana\ncd elastic-stack/kibana\nbin/kibana [ Enter ]\n\nhttp://localhost:5200/_  ..\n\n</code></pre> <p>Provide the username and password generated from starting the elasticsearch cluster and use your own default dashboard for setup. </p> <p>If you want to query, then <code>hover</code> to the kibana dashboard -&gt; <code>App -&gt;dev_settings-&gt;console</code></p>"},{"location":"monitoring/elk/elasticsearch/01_install/#search-query-using-console","title":"search query using console","text":"<p>All the queries in the elasticsearch would be using an REST API under the hood when after successful validation would respond with an JSON object.</p> <pre><code>GET /_cluster/health  # Cluster health status\nGET /_cat/indices?v # list indices\nGET /_cat/nodes?v # list nodes \n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#search-query-using-curl","title":"search query using curl","text":"<p>Since, its being an REST API call, you can use any of the client like <code>curl</code> or <code>postman</code> to query for the response. </p> <pre><code>curl -H \"Content-Type:application/json\" --cacert config/certs/http_ca.crt -u elastic:Xl4mktLmPkO=22=_DwEl -X GET https://localhost:9200/products/_search -d '{\"query\": {\"match_all\":{}}}'\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/01_install/#sharding","title":"Sharding","text":"<p>Data in Elasticsearch is organized into indices. Each index is made up of one or more shards. Each shard is an instance of a Lucene index, which you can think of as a self-contained search engine that indexes and handles queries for a subset of the data in an Elasticsearch cluster.</p> <p>Shard</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#replication","title":"Replication","text":"<p>Each index is divided into shards and each shard can have multiple copies, and these are called as <code>replication group</code> and must always be kept in sync when docs are added and deleted. The copied shard from the replicatio group is called <code>replica shard</code> and for better fault tolerance they are kept in the secondary node. </p> <p>Note: In critical production, it would always be good to have a 3 nodes and incase if there are failures, we would be able to search data from other backups. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#snaphosts","title":"Snaphosts","text":"<p>A snapshot is a backup of a running Elasticsearch cluster. </p> <p>A snapshot copies segments from an index\u2019s primary shards. When you start a snapshot, Elasticsearch immediately starts copying the segments of any available primary shards. If a shard is starting or relocating, Elasticsearch will wait for these processes to complete before copying the shard\u2019s segments. If one or more primary shards aren\u2019t available, the snapshot attempt fails</p> <p>To back up an index, a snapshot makes a copy of the index\u2019s segments and stores them in the snapshot repository.</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#adding-additional-nodes-to-the-elasticsearch","title":"Adding additional nodes to the Elasticsearch","text":"<p>copy the tar file of the <code>elasticsearch</code> and place in a new directory and extract. </p> <pre><code>mkdir elastic-stack/node2\ncp elasticsearch-8.8.0-darwin-x86_64.tar.gz elastic-stack/second-node\ntar -zxvf elasticsearch-8.8.0-darwin-x86_64.tar.gz \nmv elasticsearch-8.8.0 second-node\nvim elastic-stack/second-node/node2/config/elasticsearch.yml\n\n# modify the hostname, save &amp; quit\nnode.name: second-node\n</code></pre> <p>Now, you need to get the active token from already running(master) elasticsearch. </p> <pre><code>cd elastic-stack/elasticsearch/bin/\n./elasticsearch-create-enrollment-token --scope node\n</code></pre> <p>copy the token and go to second node. </p> <pre><code>cd elastic-stack/second-node/node2/\n./bin/elasticsearch --enrollement-token &lt;token&gt;\n</code></pre> <p>Once they are joined, you would need to go to kibana dashboard console and query for the <code>GET /_cluster/health</code> you would be seeing two nodes. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#starting-services","title":"Starting services","text":"<p>Open two terminals side by side and execute below</p> <pre><code>cd elastic-stack/elasticsearch/bin/\nbin/elasticsearch  [ Enter ]\n\ncd elastic-stack/kibana\nbin/kibana [ Enter ]\n</code></pre> <p>Copy paste kibana URL into browser http://localhost:5601/app/home#/</p>"},{"location":"monitoring/elk/elasticsearch/01_install/#console-login","title":"Console login","text":"<p>Login to the url kibana console login for elastic search http://localhost:5601/app/home#/.  Click on, Hover button -&gt; Dev tools to open console. </p>"},{"location":"monitoring/elk/elasticsearch/01_install/#cluster-health-check","title":"Cluster health check","text":"<pre><code>GET /_cluster/health\nGET /_cat/indices?v\nGET /_cat/shards?v\nGET /_cat/nodes?v\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/","title":"managing documents","text":""},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#create-index","title":"Create index","text":"<pre><code>PUT /products \n{\n  \"settings\": {\n    \"number_of_shards\": 2, \n    \"number_of_replicas\": 1\n  }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#delete-index","title":"Delete index","text":"<pre><code>DELETE /products\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#create-index-id","title":"Create index id","text":"<p>Creating some random document ID</p> <pre><code>POST /products/_doc\n{\n  \"name\": \"coffeemaker\",\n  \"price\": 64,\n  \"in_stock\": 10\n}\n\n</code></pre> <p>You can also create with index which you wish</p> <pre><code>POST /products/_doc/100\n{\n  \"name\": \"mobile charger\",\n  \"price\": 1999,\n  \"in_stock\": 0\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#get-index-by-id","title":"Get index by id","text":"<pre><code>GET /products/_doc/100\n\n# Get all the documents\nGET /products/_search\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#update-index-by-id","title":"Update index by id","text":"<pre><code>POST /products/_update/100\n{\n  \"doc\": {\n    \"name\": \"mobile earbuds\"\n  }\n}\n\nGET /products/_doc/100\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#add-new-fields","title":"Add new fields","text":"<pre><code>POST /products/_update/100\n{\n  \"doc\": {\n    \"tags\": [\"electornics\"]\n  }\n}\n\nGET /products/_doc/100\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#scripted-updates","title":"Scripted updates","text":"<pre><code>POST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#add-parameters-and-query","title":"Add parameters and query","text":"<pre><code>POST /products/_update/100\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock=params.quantity\",\n    \"params\": {\n      \"quantity\":1\n    }\n\n  }\n}\n\nGET /products/_doc/100\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#upsert","title":"Upsert","text":"<p>Incase the document exists, then run the query &amp; update else you would be creating a new document. </p> <pre><code>POST /products/_update/101\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock++\"},\n    \"upsert\":\n    {\n      \"name\": \"iphone 11\",\n      \"price\": 309999,\n      \"in_stock\": 8\n    }\n}\n\nGET /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#replace-document","title":"Replace document","text":"<pre><code>PUT /products/_doc/101\n{\n  \"name\": \"Google pixel\",\n  \"price\": 121111,\n  \"in_stock\": 3\n}\n\nGET /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#deleting-document","title":"Deleting document","text":"<pre><code>DELETE /products/_doc/101\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#routing","title":"Routing","text":"<p>When running a search request, Elasticsearch selects a node containing a copy of the index\u2019s data and forwards the search request to that node\u2019s shards. This process is known as search <code>**shard routing or routing**</code></p> <p>(How read and write happens in elastic search)[https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html]</p>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#update-by-query","title":"update by query","text":"<p>Updates documents that match the specified query. If no query is specified, performs an update on every document in the data stream or index without modifying the source, which is useful for picking up mapping changes.</p> <pre><code>POST products/_update_by_query\n{\n  \"script\": {\n    \"source\": \"ctx._source.in_stock--\"\n  },\n  \"query\": {\n      \"match_all\": {}\n    }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#delete-by-query","title":"delete by query","text":"<p>Deletes documents that match the specified query.</p> <pre><code>POST products/_delete_by_query\n{\n  \"query\": {\n      \"match_all\": {}\n    }\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/02_managing_docs/#bulk-api","title":"Bulk API","text":"<p>Performs multiple indexing or delete operations in a single API call. This reduces overhead and can greatly increase indexing speed.</p> <pre><code>POST _bulk\n{ \"index\" : { \"_index\" : \"products\", \"_id\" : \"200\" } }\n{ \"name\" : \"Mac Laptop\", \"price\": 102121,\"in_stock\":10 }\n{ \"create\" : { \"_index\" : \"products\", \"_id\" : \"201\" } }\n{ \"name\" : \"Laptop bags\", \"price\": 1032,\"in_stock\":29 }\n{ \"update\" : {\"_id\" : \"200\", \"_index\" : \"products\"} }\n{ \"doc\" : { \"name\" : \"Mac Laptop Premier\", \"price\": 102121,\"in_stock\":10} }\n{ \"delete\": { \"_index\" : \"products\", \"_id\" : \"201\" }}\n{ \"delete\": { \"_index\" : \"products\", \"_id\" : \"200\" }}\n</code></pre> <p>Incase you want to use the curl to pass the data, you need to create a new file <code>request</code> and update <code>_bulk</code> data into the file and pass it to curl. </p> <pre><code>curl -H \"Content-Type:application/json\" --cacert config/certs/http_ca.crt -u elastic:Xl4mktLmPkO=22=_DwEl -X POST https://localhost:9200/_bulk --data-binary \"@request\"; echo\n\n\n{\"took\":39,\"errors\":false,\"items\":[{\"index\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":13,\"_primary_term\":1,\"status\":201}},{\"create\":{\"_index\":\"products\",\"_id\":\"201\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":14,\"_primary_term\":1,\"status\":201}},{\"update\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":2,\"result\":\"updated\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":15,\"_primary_term\":1,\"status\":200}},{\"delete\":{\"_index\":\"products\",\"_id\":\"201\",\"_version\":2,\"result\":\"deleted\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":16,\"_primary_term\":1,\"status\":200}},{\"delete\":{\"_index\":\"products\",\"_id\":\"200\",\"_version\":3,\"result\":\"deleted\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":17,\"_primary_term\":1,\"status\":200}}]}\n\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/","title":"mapping and analysis","text":""},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#intro-to-analysis","title":"Intro to Analysis","text":"<p>standard analyzer or text analyzer</p> <p>When we index a text value, it goes through an analysis process. The purpose of it is to store the values in a data structure that is efficient for searching. When a text value is indexed, a so-called analyzer is used to process the text.</p> <p>An analyzer consists of three building blocks</p> <ul> <li>**character filters **</li> </ul> <p>A character filter receives the original text and may transform it by adding, removing, or changing characters. An analyzer may have zero or more character filters, and they are applied in the order in which they are specified.</p> <p>e.g get all text using html_strip</p> <p>-** a tokenizer **</p> <p>An analyzer must contain exactly one tokenizer, which is responsible for tokenizing the text. By \u201ctokenizing,\u201d I am referring to the process of splitting the text into tokens. As part of that process, characters may be removed, such as punctuation, exclamation marks, etc.</p> <p>e.g split a sentence into words by splitting the string whenever a whitespace is encountered. The input string is therefore tokenized into a number of tokens.</p> <ul> <li>token filters</li> </ul> <p>These receive the tokens that the tokenizer produced as input and they may add, remove, or modify tokens. As with character filters, an analyzer may contain zero or more token filters, and they are applied in the order in which they are specified.</p> <p>e.g token filter is probably the \u201clowercase\u201d filter, which lowercases all letters.</p> <p>Output: The result of analyzing text values is then stored in a searchable data structure.</p> <p>No character filter is used by default, so the text is passed on to the tokenizer as is. The tokenizer splits the text into tokens according to the Unicode Segmentation algorithm.  tokenizer breaks sentences into words by whitespace, hyphens, and such. In the process, it also throws away punctuation such as commas, periods, exclamation marks, etc.</p> <p>The tokens are then passed on to a token filter named \u201clowercase.\u201d it lowercases all letters for the tokens. </p> <p>This standard analyzer is used for all text fields unless configured otherwise. There are a couple of analyzers available besides the \u201cstandard\u201d analyzer, but that\u2019s the one you will typically use.</p> <p>From the elastic console, try to run the API query for the standard analyzer, you would understand.</p> <pre><code>POST /_analyze\n{\n    \"text\": \"walk into bar and don't.... DRINK\",\n    \"analyzer\": \"standard\"\n}\n</code></pre> <p>You can also write your custom analyzer, however it would behave same as standard analyzer outputs. </p> <pre><code>POST /_analyze\n{\n    \"text\": \"walk into bar and don't.... DRINK\",\n    \"char_filter\":[],\n    \"tokenizer\":\"standard\",\n    \"filter\":[\"lowercase\"]\n}\n</code></pre> <p>let\u2019s take a look at what actually happens with the result, being the tokens. Elasticsearch uses more than one data structure, is to ensure efficient data retrieval for different access patterns.</p> <p>e.g searching for a given term is handled differently than aggregating data.</p> <p>Actually these data structures are all handled by Apache Lucene and not Elasticsearch.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#inverted-indices","title":"Inverted indices","text":"<p>An inverted index is essentially a mapping between tokens that are emitted by the analyzer(a.k.a tokens) and which documents contain them.</p> <p>Anyway, let\u2019s take previous example and see how that would be stored within an inverted index.</p> <p>Document #1 -&gt; \"walk into bar and don't.... DRINK\" -&gt; [\"and\",\"bar\",\"don't\",\"DRINK\",\"into\",\"walk\"]  Document #2 -&gt; \"I walk into bar\" -&gt; [\"I\",\"bar\",\"into\",\"walk\"]</p> <p>As you can see, each unique term is placed in the index together with information about which documents contain the term, sorted alphbetically. Suppose that we perform a search for the term \"bar\", we can see that documents #1 and #2 contain the term.</p> <p>its called as inverted index because, the logical mapping is terms they contain to documents.  The inverted indices that are stored within Apache Lucene contain a bit more information, such as data that is used for relevance scoring.</p> <p>As you can imagine, we don\u2019t just want to get the documents back that contain a given term; we also want them to be ranked by how well they match.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#mapping","title":"mapping","text":"<p>Mapping defines the structure of documents and how they are indexed and stored.  This includes the fields of a document and their data types. As a simplification, you can think of it as the equivalent of a table schema in a relational database.</p> <p>In Elasticsearch, there are two basic approaches to mapping; explicit and dynamic mapping.  </p> <p>explicit mapping, we define fields and their data types ourselves, typically when creating an index. </p> <p>dymanic mapping, field mapping will automatically be created when elasticsearch encounters a new field. It will inspect the supplied field value to figure out how the field should be mapped, if you supply a string value, for instance, elasticsearch will use the \"text\" data type for the mapping.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#datatypes","title":"datatypes","text":"<p>there are many, few are discussed and frequently used. </p> <p>Object: JSON documents are hierarchical in nature, the document may contain inner objects which, in turn, may contain inner objects themselves, they are mapped using properties parameter. </p> <pre><code>PUT my-index-000001/_doc/1\n{ \n  \"region\": \"US\",\n  \"manager\": { \n    \"age\":     30,\n    \"name\": { \n      \"first\": \"John\",\n      \"last\":  \"Smith\"\n    }\n  }\n}\n</code></pre> <p>Internally, this document is indexed as a simple, flat list of key-value pairs</p> <pre><code>{\n  \"region\":             \"US\",\n  \"manager.age\":        30,\n  \"manager.name.first\": \"John\",\n  \"manager.name.last\":  \"Smith\"\n}\n</code></pre> <p>In a case where you have documents of same tokens, then it would make a flat array of internal documents and query. which would perform \"OR\" operation and provides wrong results.. hence we use the word nested </p> <pre><code>{\n  \"region\":             \"US\",\n  \"manager.age\":        [30,50,60]\n  \"manager.name.first\": [\"John\",\"Mat\",\"Suj\"],\n  \"manager.name.last\":  [\"Smith\",\"Hew\",\"Samuel\"]\n}\n\n</code></pre> <p>Keyword: exact matching of values, typically used for filtering, aggregating, sorting  e.g search articles only \"published\"</p> <p>text: use for full text searches.  e.g entire body of article</p> <p>References:  https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-data-types.html</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#coercion","title":"coercion","text":"<p>Data types are inspected when indexing docs, invalid values are rejected. so that only text fields are indexed. </p> <p>Note:  - Enabled by default - Always try using correct data types</p> <pre><code>PUT /coercion_test/_doc/1 {\n    \"price\": 7.4\n}\n</code></pre> <p>Creates and indexde as float as a new document. </p> <pre><code>PUT /coercion_test/_doc/1 {\n    \"price\": \"7.4\"\n}\n</code></pre> <p>Creates and indexed as string as a new document. </p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#understand-arrays","title":"understand arrays","text":"<p>Array values should be of same data type Array may contain nested arrays Arrays are flattened during indexing</p> <p>Note: Remember to use the nested data type for arrays of objects if you need to query the objects independently</p> <pre><code>POST /products/_doc {\n    \"tags\": [\"item1\",\"item2\"]\n}\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#adding-explict-mapping","title":"adding explict mapping","text":"<p>all field mapping defined in properties key, including nested objected and create index.</p> <pre><code>PUT /reviews {\n    \"mappings\": {\n        \"properties\" {\n            \"rating\": { \"type\": \"float\"},\n            \"content\": { \"type\": \"text\"},\n            \"product_id\": { \"type\": \"integer\"},\n            \"author\": {\n                \"properties\": {\n                    \"first_name\": {\"type\": \"text\"},\n                    \"last_name\": { \"type\", \"text\"},\n                    \"email\": {\"type\": \"keyword\"}\n\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>The above one is one method of creating an index, but there is second method of using i.e (dot-notation) which is easy to create index.</p> <pre><code>PUT /reviews {\n    \"mappings\": {\n        \"properties\" {\n            \"rating\": { \"type\": \"float\"},\n            \"content\": { \"type\": \"text\"},\n            \"product_id\": { \"type\": \"integer\"},\n            \"author.first_name\": {\"type\": \"text\"},\n            \"author.last_name\": { \"type\", \"text\"},\n            \"author.email\": {\"type\": \"keyword\"}\n\n\n\n        }\n    }\n}\n</code></pre> <p>create a new document</p> <pre><code>PUT /reviews/_doc/1 {\n    \"rating\": 4.3,\n    \"content\": \"Elastic search is good\",\n    \"product_id\": 123,\n    \"author\": {\n        \"first_name\": \"Sunil\",\n        \"last_name\": \"Kumar\",\n        \"email\": \"sun@gmail.com\"\n    }\n}\n</code></pre> <p>incase you don't provide email and submit, coercion while indexing would throw an error because its enabled by default.</p>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#retrive-mapping","title":"retrive mapping","text":"<pre><code>GET /reviews/_mapping\n\nGET /reviews/_mapping/field/content\n\nGET /reviews/_mapping/field/author.email\n</code></pre> <p>lets say you need to map a new value to already created index..</p> <pre><code>PUT /reviews/_mapping {\n    \"properties\": {\n        \"created_at\":  {\n            \"type\": \"date\"\n            }\n    }\n}\n\nGET /reviews/_mapping\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#how-date-works-in-elasticsearch","title":"how date works in elasticsearch","text":"<p>dates are referred in three ways and elastic search would convert those date and time formts into a long string. </p> <ul> <li>specially formatted strings</li> <li>milliseconds since the EPOCH(long)</li> <li>seconds since the EPOCH(integrer)</li> </ul> <p>supported formats </p> <ul> <li>date without time</li> <li>date with time</li> <li>milliseconds since the EPOCH(long) defaults to UTC</li> </ul> <pre><code>PUT /reviews/_doc/1 {\n    \"rating\": 4.3,\n    \"content\": \"Elastic search is good\",\n    \"product_id\": 123,\n    \"author\": {\n        \"first_name\": \"Sunil\",\n        \"last_name\": \"Kumar\",\n        \"email\": \"sun@gmail.com\",\n        \"birth\": \"2015-04-15\"  # only date\n        \"birth\": \"2015-04-15T15:00:00Z\" T seperated by date and time, followed by Z(UTC)\n        \"birth\": \"123223023823772\" # UNIX EPOCH timestamp\n    }\n}\n\nGET /reviews/_search {\n    \"query\" {\n        \"match_all\": {}\n    }\n}\n\n</code></pre>"},{"location":"monitoring/elk/elasticsearch/03_mapping_analysis/#mapping-parameters","title":"mapping parameters","text":"<ul> <li>format: customize date format, however recommended <code>date</code></li> <li>properties: used for object and nested </li> <li>coerce: index validation (enabled by default). you can disable during start but can be overide  when creating</li> <li>doc_values: used mainly for apache lucene, opposite for interved index.</li> <li>norms: normalizing factors used for revelance scores, i.e ranking. </li> <li>index: </li> <li>null_value:  cannont be index or searched</li> <li>copy_to: used to copy multiple field values into a group field(not tokes/terms), when search this field you won't get results because its not indexed.</li> </ul>"},{"location":"monitoring/elk/kibana/overview/","title":"overview","text":""},{"location":"monitoring/elk/kibana/overview/#kibana","title":"Kibana","text":""},{"location":"monitoring/elk/logstash/overview/","title":"overview","text":""},{"location":"monitoring/elk/logstash/overview/#logstash","title":"Logstash","text":""},{"location":"nginx/access_control/","title":"access control","text":""},{"location":"nginx/access_control/#whitelisting","title":"whitelisting","text":"<p>you can allow traffic reahing to your web server by configuring in the nginx server, so that only those requests are sent to the webserver. </p> <pre><code>vim /etc/nginx/conf.d/whilelist.conf    \nallow 192.168.56.101\ndeny all\n\nvim /etc/nginx/conf.g/nginx.conf \nserver {\n    server_name localhost.com\n\n    location /var/www/html/webroot\n    index admin.html\n    include /etc/nginx/conf.d/whilelist.conf  \n}\n\nnginx -t\nsystemctl restart nginx\n\ncurl -I localhost.com/admin \n</code></pre>"},{"location":"nginx/access_control/#limit-connections","title":"limit connections","text":"<p>let's say you have a network speed of 100Mpbs, so if there are 10 users who need to download, the person who has the highest bandwidth speed would choke up the server and others won't be able to download the file. Hence we need to make sure we set the limits for downloaing.. this can be done using the module <code>limit_rate</code></p> <pre><code>vim /etc/nginx/conf.g/nginx.conf \nserver {\n    server_name localhost.com;\n\n    location /download;\n    limit_rate 50k;\n\nnginx -t \nsystemctl restart nginx\n}\n</code></pre> <p>Now, in the above usecase you have set the limits for the speed, but if we have same ip connecting to same server multiple times, we have resources being unnecesary choked up by web server.. so we need to restrict the connections by using the module </p> <pre><code>vim /etc/nginx/conf.g/nginx.conf\n\n#global config section\n\nlimit_conn_zone $binary_remote_addr zone=addr:10m;\n\nwhere, \nbinary_remote_addr - remote client\nspeed at which the remote client uses 10m\n\nserver {\n    server_name localhost.com;\n\n    location /download\n    limit_rate 50k;\n    limit_conn addr 1;\n\nwhere, \n\nfor the download directior, the `addr` module can have only 1 conection.\n\nnginx -t \nsystemctl restart nginx\n}\n</code></pre>"},{"location":"nginx/access_control/#basic-auth","title":"basic auth","text":"<p>There are 3 types of auth. <code>basic</code>, <code>digest</code> and <code>NTML</code>. Client would send an <code>GET /admin</code> to the webserver, who would then sees that the page is configured to have an <code>basic</code> auth sent and would respond to the client with <code>401</code>, meaning authorization required by setting header <code>WWW-Authenticate: Basic retain=Family</code> in which you see a dialog prompt asking for <code>username</code> and <code>password</code>. Once you have entered credentials, the browser would <code>encode</code> the packet with the header response to the server, where it <code>decodes</code> and then retrives the page and then sends back response to the client. </p> <p>Configuration</p> <p>Install <code>apache2-utils</code></p> <p>sudo htpasswd -c /etc/nginx/.htpasswd user1 sudo htpasswd -c /etc/nginx/.htpasswd user2</p> <pre><code>vim /etc/nginx/conf.g/nginx.conf\n\nhttp {\n    server {\n        listen 192.168.1.23:8080;\n        root   /usr/share/nginx/html;\n\n        location /api {\n            api;\n            satisfy all;\n\n            deny  192.168.1.2;\n            allow 192.168.1.1/24;\n            allow 127.0.0.1;\n            deny  all;\n\n            auth_basic           \"Administrator\u2019s Area\";\n            auth_basic_user_file /etc/apache2/.htpasswd;\n        }\n    }\n}\n</code></pre>"},{"location":"nginx/access_control/#hashing","title":"hashing","text":"<p>Hashing is a technique for converting data into a fixed-size value, called a hash value or hash code. The hash value is typically used to index a data structure such as a hash table. Hashing functions are designed to be fast and efficient, and to produce a uniform distribution of hash values for a given set of input data.</p> <ul> <li>Data storage</li> <li>Data security</li> <li>Cryptography</li> </ul> <p>different hashing algorithms</p> <ul> <li>MD5 128-bit hash value</li> <li>SHA-1  160-bit hash value.</li> <li>SHA-2</li> </ul>"},{"location":"nginx/access_control/#digest-authentication","title":"digest authentication","text":"<p>Digest authentication is a method used for authenticating users in network communication protocols, particularly in the context of web servers and clients. It's an improvement over the more basic HTTP Basic Authentication, providing stronger security by avoiding sending passwords in plaintext over the network.</p> <p>how it works</p> <p>When a client (typically a web browser) sends a request to a server that requires authentication, the server responds with a special HTTP 401 Unauthorized status code, along with a challenge in the form of a nonce (a random string of characters), and possibly other parameters. </p> <p>Upon receiving the 401 response, the client knows it needs to provide authentication. It prompts the user for their username and password, just like with HTTP Basic Authentication.</p> <p>Instead of sending the password directly, the client computes a cryptographic hash function (often MD5 or SHA-1) of various pieces of information, including the username, password, and nonce received from the server. This hash is called a digest.</p> <p>The client sends this digest along with the username and other required information to the server in another request.</p> <p>The server, upon receiving the authentication information, recalculates the digest using the same algorithm and compares it with the one sent by the client. If they match, the server can authenticate the user.</p> <p>Digest authentication offers several advantages over Basic Authentication:</p> <p>Security: Passwords are not sent in plaintext over the network, making it less susceptible to eavesdropping attacks.</p> <p>Nonce: The use of a nonce makes replay attacks difficult, as each nonce is valid only for a single authentication attempt.</p> <p>Flexibility: Digest authentication can be integrated with existing authentication mechanisms and is compatible with proxies and caching.</p> <p>Note: nginx doesn't support dynamic authentic</p>"},{"location":"nginx/caching/","title":"caching","text":""},{"location":"nginx/caching/#overview","title":"Overview","text":""},{"location":"nginx/caching/#benefits","title":"Benefits","text":"<ul> <li>it reduces the overhead of server's resource.</li> <li>decreases the network bankwidth.</li> <li>pages are loaded much more faster.</li> </ul>"},{"location":"nginx/caching/#caching-subsystems","title":"caching subsystems","text":""},{"location":"nginx/caching/#caching-controlheaders","title":"caching control(headers)","text":"<p>these are used to speificy directives for caching mechanism, they are used to defined caching policies with various directives provided by the header. you would specify whether you need your web page to be cached or not to be cached by using these headers..</p> <p>use cases..</p> <ul> <li>do not store any kind of cache at all </li> <li>store the cache, but verify with webserver wheather file is modified</li> <li>store the cache for 24 hours</li> </ul> <p>varipus cache control headers</p> <ul> <li>Cache-Control: no-store</li> <li>Cache-Control: no-cache</li> <li>Cache-Control: no-store, must-revalidate</li> <li>Cache-Control: public</li> <li>Cache-Control: private</li> </ul> <p>Configure all your .png files don't need your browser to store cache.</p> <pre><code>server {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n\n    location ~ \\.(.png) {\n       root   /usr/share/nginx/html;\n       add_header Cache-Control no-store;    \n    }\n\n}\n\nsystemctl nginx restart\n</code></pre> <p>From your client cli, reqeust the .png, when you check the response, you would see the cache header as response.</p> <pre><code>curl -I http://your-web-server/myseed.png\n</code></pre>"},{"location":"nginx/caching/#if-modified-header","title":"if modified header","text":"<p>when you request for the webpage by client, the nginx would first give an GET request to the server and check's for the timestamp header (Last-Modified) and if there's no response it would be 302 message to the client saying there is no change in the webpage and hence it can serve from the cache server. </p> <p>incase if ther's change, it would serve a new request to cache and then to the client.</p>"},{"location":"nginx/caching/#cache-control-header","title":"cache control header","text":"<p>The amount of time the webpage needs to be in the cache.</p> <pre><code>location ~ \\.(.png) {\n    root   /usr/share/nginx/html;\n    expires 1h;    \n}\n</code></pre> <pre><code>curl -I http://your-web-server/myseed.png\n</code></pre>"},{"location":"nginx/caching/#cache-no-store-must-revalidate","title":"cache: no-store, must-revalidate","text":""},{"location":"nginx/caching/#cache-max-age-s-max-age","title":"cache: max-age, s-max-age","text":""},{"location":"nginx/caching/#cache-time","title":"cache time","text":""},{"location":"nginx/caching/#expires-headers","title":"expires headers","text":""},{"location":"nginx/caching/#keep-alive-connections","title":"keep-alive connections","text":""},{"location":"nginx/caching/#date-time-expires","title":"date-time expires","text":""},{"location":"nginx/cryptography/","title":"cryptography","text":""},{"location":"nginx/cryptography/#asymmetric-cryptography","title":"asymmetric cryptography","text":"<p>Asymmetric public-key encryption, often referred to as asymmetric cryptography or public-key cryptography, is a cryptographic system that uses pairs of keys: public keys and private keys. Unlike symmetric encryption, where the same key is used for both encryption and decryption, asymmetric encryption uses different keys for these operations.</p> <p>Each user generates a pair of keys: a public key and a private key, The public key is made available to anyone who wishes to send an encrypted message to the user. The private key is kept secret and known only to the user.</p> <p>Encryption</p> <ul> <li>When User A wants to send a secure message to User B, User A uses User B's public key to encrypt the message.</li> <li>The encryption process uses complex mathematical algorithms that are easy to perform in one direction (using the public key) but computationally difficult to reverse without the corresponding private key.</li> </ul> <p>Decryption</p> <ul> <li>User B receives the encrypted message and uses their private key to decrypt it.</li> <li>Since the private key is kept secret, only User B can decrypt the message.</li> </ul> <p>Because of its advantages that it offers, it can be used in variery of protocols like PGP, SSH, Bitcoin, TLS, S/MIME</p>"},{"location":"nginx/cryptography/#https","title":"https","text":""},{"location":"nginx/cryptography/#handshake","title":"handshake","text":"<ul> <li>When a user's browser initiates a connection to a website over HTTPS, a process called the SSL/TLS handshake begins.</li> <li>The browser requests a secure connection to the server, and the server responds by sending its SSL/TLS certificate to the browser.</li> <li>The certificate contains the server's public key and information about the website, including the domain name and the certificate authority (CA) that issued the certificate.</li> <li>The browser verifies the certificate to ensure it's valid and trusted. This verification involves checking the certificate's digital signature against a list of trusted CAs stored in the browser, ensuring the certificate hasn't expired, and confirming that the domain name matches the one the user is trying to connect to.</li> </ul>"},{"location":"nginx/cryptography/#key-exchange","title":"key exchange","text":"<ul> <li>After verifying the certificate, the browser generates a session key, which is a randomly generated symmetric encryption key.</li> <li>The browser encrypts the session key with the server's public key from the certificate and sends it to the server.</li> <li>The server decrypts the session key using its private key, establishing a secure connection.</li> </ul>"},{"location":"nginx/cryptography/#data-transfer","title":"data transfer","text":"<ul> <li>Once the secure connection is established, data transferred between the browser and the server is encrypted using symmetric encryption with the session key.</li> <li>This encryption ensures that even if an attacker intercepts the data, they won't be able to decipher it without the session key.</li> </ul>"},{"location":"nginx/cryptography/#install-certs","title":"install certs","text":"<pre><code>yum install certbot-nginx\ncertbot --nginx -d yourdomain.com \n\n# configure your configurations\n# will add ssl_certs in the nginx.conf..\n\nnginx -t \nsystemctl restart nginx\n</code></pre>"},{"location":"nginx/cryptography/#certs-revokcations","title":"certs revokcations","text":""},{"location":"nginx/cryptography/#crl","title":"CRL","text":"<p>Certificate Revocation List(CRL) is a method used by CAs to maintain a list of revoked digital certificates.</p> <p>working</p> <ul> <li>The CA periodically publishes a CRL, which contains the serial numbers of all certificates that have been - revoked before their expiration date.</li> <li>When a user wants to verify the validity of a certificate, they can check the CRL to see if the certificate's serial number is listed as revoked.</li> <li>CRLs are typically distributed and accessed via HTTP, LDAP, or other protocols.</li> </ul> <p>drawbacks</p> <ul> <li>CRLs can become large and cumbersome to manage, especially for CAs with a large number of certificates.</li> <li>There can be delays between when a certificate is revoked and when it appears on the CRL, leaving a window of vulnerability.</li> <li>Frequent downloads of large CRLs can lead to network congestion and performance issues.</li> </ul>"},{"location":"nginx/cryptography/#ocsp","title":"OCSP","text":"<p>Online Certificate Status Protocol(OCSP) provides a real-time method for checking the status of a digital certificate.</p> <p>working</p> <ul> <li>When a user wants to verify a certificate, their system sends a request to the CA's OCSP responder, providing  the certificate's serial number.</li> <li>The OCSP responder checks its records to see if the certificate is still valid or has been revoked.</li> <li>The responder then sends a response back to the user's system indicating the current status of the certificate (e.g., valid, revoked, unknown).</li> <li>OCSP provides real-time validation, reducing the window of vulnerability compared to CRLs.</li> <li>It can be more efficient than downloading and parsing large CRLs, especially for individual certificate checks.</li> </ul> <p>drawbacks</p> <ul> <li>OCSP requests can introduce additional latency into the certificate validation process, especially if the OCSP responder is slow to respond.</li> <li>OCSP relies on the availability and reliability of the CA's OCSP responder. If the responder is unavailable, validation may fail.</li> <li>OCSP requests can also leak information about which certificates a user is accessing, potentially compromising privacy.</li> </ul>"},{"location":"nginx/http_compression/","title":"http compression","text":""},{"location":"nginx/http_compression/#overview","title":"Overview","text":"<p>When the data is transferred from the server to client, if there is more data, then we would have more packets transferred after calculating the MTU size. </p> <p>So if there is compression from the server, then we would have less packats to reach to the client.  This is called as compressing.</p> <p>Client</p> <p>After the handshake, the client requests(GET /) from the server, in which the client would have an header set to the server i.e <code>Accept-Encoding: gzip, deflate, compress</code> </p> <p>Server</p> <p>Server already knows about the data header, so it knows that the client can decode so while it sends the data back to the client it would use a header <code>Content-Encoding: gzip, defalte, compress</code> . </p> <p>Once the data is received to client, it would use its <code>gzip, defalte, compress</code> decoder to decode the data.  There by, you would have less number of packets between server-client</p> <p>Note:</p> <ul> <li>When client has not set any header, which means it would accept all types of decoding mechanism</li> <li>although, the server sends data back to the client in compressed way, the <code>Context-Type: text/plain</code> always remains plain text.</li> </ul>"},{"location":"nginx/http_compression/#configure","title":"configure","text":"<pre><code>vim /etc/nginx/conf.d/nginx.conf\nhttp { \n    &lt;snip&gt;\n    gzip on;\n    gzip_types text/plain text/css test/xml text/javascript;\n    gzip_disable \"MSIE [1-6]\\.*;\n    gzip_comp_level 9;\n}\n\nservice nginx restart\n</code></pre> <p>testing</p> <pre><code>curl http://&lt;your-web-server&gt;/&lt;somefile&gt; &gt; c1.txt\ncurl -H \"Accept-Encoding:gzip\" http://&lt;your-web-server&gt;/&lt;somefile&gt; &gt; c2.txt\n\nls -l *.txt\n</code></pre> <p>Check their sizes, you would have found differences.</p>"},{"location":"nginx/http_compression/#referer","title":"referer","text":"<p>When there is the need for your context images not be copied due to copyrights or etc, you can configure the referer field so that imags don't get loaded. </p> <p>Usecase</p> <p>When you like someone's website or blog, you tend to copy and write on your own, so in that case you can restrict your nginx server as to not to load any websites/url's accessing from it..</p> <pre><code>vim /etc/nginx/conf.d/nginx\n\nserver {\n    location ~ \\.(jpe?g|png|gif)$ {\n        valid_referes none blocked servera.com *.serverb.com;\n        if ($invalid_referer) {\n            return 403;\n        }\n    }\n}\n\nsystem restart nginx\n</code></pre>"},{"location":"nginx/http_compression/#accept-language","title":"accept-language","text":"<p>Client can send the request in the preferred lanuguare by setting the header, so that we could get the resopinse in they way browser is set to.</p> <pre><code>curl -H \"Accept-Language: en\" &lt;your-webserver.com/index.html&gt;\ncurl -H \"Accept-Language: jp\" &lt;your-webserver.com/index.html&gt;\n</code></pre>"},{"location":"nginx/http_protocol/","title":"http protocol","text":""},{"location":"nginx/http_protocol/#protocols-introduction","title":"protocols introduction","text":"<p>system of rules that allow two or more entities of a communications system to transmit information.</p> <ul> <li>File Transfer Protocol (FTP)</li> <li>Domain Name System Protocol (DNS)</li> <li>Transmission Control Protocol (TCP)</li> <li>Secure File Transfer Protocol (SFTP)</li> <li>Hyper Text Transfer Protocol  (HTTP)</li> <li>Internet Protocol (IP)</li> </ul>"},{"location":"nginx/http_protocol/#http-protocol","title":"http protocol","text":"<p>HTTP is a TCP/IP-based communication protocol, that is used to deliver data (HTML files, image files, query results, etc.) on the World Wide Web. The default port used for HTTP is TCP 80, but other ports can be used as well based on the requirements.</p> <p></p>"},{"location":"nginx/http_protocol/#http-get","title":"HTTP GET","text":"<p>try to connect to the telnet port and check if the server is responding to the request</p> <pre><code>[root@centos ~]# telnet 192.168.56.10 80\nTrying 192.168.56.10...\nConnected to 192.168.56.10.\nEscape character is '^]'.\nGET /sample.html HTTP/1.1  -&gt; you have to type here\nHost: 192.168.56.10 -&gt; you have to type here [ press enter two times to get the response]\n\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:07:45 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n\n&lt;h1&gt; nginx tutorial &lt;/h1&gt;\n&lt;p&gt; line1 &lt;/p&gt;\n&lt;p&gt; line2 &lt;/p&gt;\n</code></pre>"},{"location":"nginx/http_protocol/#partial-get","title":"partial GET","text":"<p>The partial GET method is used to retrieve only specific content instead of everything</p> <pre><code># returns 20 bytes from the webserver\n\n[root@centos ~]# curl --header \"Range: bytes=0-20\" http://192.168.56.10/sample.html\n&lt;h1&gt; nginx tutorial &lt;[root@centos ~]#\n\n# returns complete data from the webserver as response\n\n[root@centos ~]# curl -I http://192.168.56.10/sample.html\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:12:52 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"nginx/http_protocol/#conditional-get","title":"conditional GET","text":"<p>The conditional GET method is used to fetch the information with a condition.</p> <pre><code>[root@centos ~]# curl --header \"If-Modified-Since: Tue, 26 Mar 2024 12:57:00 GMT\" http://192.168.56.10/sample.html\n[root@centos ~]#\n\n[root@centos ~]# curl -I --header \"If-Modified-Since: Tue, 26 Mar 2024 12:57:00 GMT\" http://192.168.56.10/sample.html\nHTTP/1.1 304 Not Modified\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:23:19 GMT\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\n[root@centos ~]#\n\n[root@centos ~]# curl --header \"If-Modified-Since: Tue, 26 Mar 2024 12:50:00 GMT\" http://192.168.56.10/sample.html\n&lt;h1&gt; nginx tutorial &lt;/h1&gt;\n&lt;p&gt; line1 &lt;/p&gt;\n&lt;p&gt; line2 &lt;/p&gt;\n[root@centos ~]#\n</code></pre>"},{"location":"nginx/http_protocol/#http-post","title":"HTTP POST","text":"<p>POST method is used to send some information which will be processed by the web-server in some way.</p> <pre><code>POST /login.php HTTP/1.1  \n   user=admin password=test123\n</code></pre>"},{"location":"nginx/http_protocol/#http-head","title":"HTTP HEAD","text":"<p>HEAD method is used to fetch only the HTTP headers as part of the response.</p> <p>HEAD method is identical to GET method, except that the server MUST NOT return a message-body in the response</p> <pre><code>[root@centos ~]# curl -I http://192.168.56.10/sample.html\nHTTP/1.1 200 OK\nServer: nginx/1.20.1\nDate: Tue, 26 Mar 2024 13:12:52 GMT\nContent-Type: text/html\nContent-Length: 57\nLast-Modified: Tue, 26 Mar 2024 12:57:00 GMT\nConnection: keep-alive\nETag: \"6602c61c-39\"\nAccept-Ranges: bytes\n</code></pre>"},{"location":"nginx/http_protocol/#http-trace","title":"HTTP TRACE","text":"<p>'TRACE' is a HTTP request method used for debugging which echo's back input back to the user.</p>"},{"location":"nginx/http_protocol/#http-options","title":"HTTP OPTIONS","text":"<p>OPTION  method is used to describe the communication option for the target resource. nginx doesn't support as security concern.</p> <pre><code>[root@centos ~]# curl -X \"OPTIONS\" http://192.168.56.10 -i\nHTTP/1.1 405 Not Allowed\nServer: nginx/1.20.1\nDate: Wed, 27 Mar 2024 02:31:35 GMT\nContent-Type: text/html\nContent-Length: 157\nConnection: keep-alive\n\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;405 Not Allowed&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;405 Not Allowed&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx/1.20.1&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n[root@centos ~]#\n</code></pre>"},{"location":"nginx/http_protocol/#conclusion","title":"conclusion","text":"<p>HTTP defines a set of request methods to indicate the desired action to be performed for a given resource</p> HTTP Method Description GET To retrieve data from the server. POST Send input data to the server. HEAD Exactly like GET, but server only responds with Headers. PUT Write documents to the server. DELETE Deletes resource from the server. OPTIONS Asks server on which methods it supports. TRACE ECHOS the Receive Request from the Web Server <p>HTTP Response Status Code</p> Status Code Description 100 Continue 101 Switching Protocols 200 OK 201 Created 202 Accepted 204 No Content 300 Multiple Choices 301 Moved Permanently 302 Found 304 Not Modified 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 500 Internal Server Error 501 Not Implemented 502 Bad Gateway 503 Service Unavailable"},{"location":"nginx/load_balancer/","title":"load balancer","text":""},{"location":"nginx/load_balancer/#overview","title":"Overview","text":"<p>LB: create set of multiple servers to distribute the traffic betweeen servers</p> <p>advantages of LB</p> <ul> <li>traffic distribution using multiple algorithms to backend servers</li> <li>health check of backend application</li> <li>supprts SSL/TLS terminations</li> </ul>"},{"location":"nginx/load_balancer/#implementation","title":"implementation","text":""},{"location":"nginx/load_balancer/#load-balancing","title":"load balancing","text":"<p>upstream</p> <p>upstream block can be used to specfiy the group of serves for which you want to load balanced traffic </p> <p>nginx server</p> <pre><code>[root@centos conf.d]# cat proxy.conf.backend\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://192.168.56.11;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host-Header $host;\n    }\n\n    location /admin {\n        proxy_pass http://192.168.56.12;\n        proxy_set_header X-Real-IP $remote_addr;\n      }\n}\n[root@centos conf.d]#\n</code></pre> <p>just hitting the nginx load balancer, it would be redirected to two of the backend servers.</p> <pre><code>[root@centos conf.d]# curl 192.168.56.11\nThis is application server backend\n[root@centos conf.d]# curl 192.168.56.12\nthis is backend server\n[root@centos conf.d]#\n[root@centos conf.d]# for i in `seq 1 7`\n&gt; do\n&gt; curl http://192.168.56.10\n&gt; done\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/load_balancer/#health-checks","title":"health checks","text":"<p>health checks are used to monitor the health of HTTP servers in upstream group i.e <code>backend</code>  if any server is not responding then nginx will stop sending the request to it. </p> <p>let's say one of the backend server has stopped so nginx would route to the healthy node. </p> <pre><code>[root@centos html]# systemctl stop nginx\n[root@centos html]# systemctl status nginx\n\u25cf nginx.service - nginx - high performance web server\n   Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled)\n   Active: inactive (dead) since Fri 2024-03-29 07:49:31 UTC; 5s ago\n     Docs: http://nginx.org/en/docs/\n  Process: 20660 ExecStop=/bin/sh -c /bin/kill -s TERM $(/bin/cat /var/run/nginx.pid) (code=exited, status=0/SUCCESS)\n  Process: 607 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS)\n Main PID: 614 (code=exited, status=0/SUCCESS)\n\nMar 27 12:36:31 centos systemd[1]: Starting nginx - hig...\nMar 27 12:36:32 centos systemd[1]: Can't open PID file ...\nMar 27 12:36:32 centos systemd[1]: Started nginx - high...\nMar 29 07:49:31 centos systemd[1]: Stopping nginx - hig...\nMar 29 07:49:31 centos systemd[1]: Stopped nginx - high...\nHint: Some lines were ellipsized, use -l to show in full.\n[root@centos html]#\n\n</code></pre> <p>load balancer server</p> <pre><code>[root@centos conf.d]# curl http://192.168.56.12\ncurl: (7) Failed connect to 192.168.56.12:80; Connection refused\n[root@centos conf.d]#\n</code></pre> <p>since one of the server is down, nginx load balancer would route the entry to healthy node itself.  this is as part of passive health checks</p> <pre><code>[root@centos conf.d]# for i in `seq 1 7`; do curl http://192.168.56.10; done\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\nThis is application server backend\n[root@centos conf.d]#\n</code></pre> <p>One the node is up, traffic is routed across.</p> <pre><code>[root@centos html]# systemctl start nginx\n[root@centos html]# systemctl status nginx\n\u25cf nginx.service - nginx - high performance web server\n   Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled; vendor preset: disabled)\n   Active: active (running) since Fri 2024-03-29 07:52:10 UTC; 13s ago\n     Docs: http://nginx.org/en/docs/\n  Process: 20660 ExecStop=/bin/sh -c /bin/kill -s TERM $(/bin/cat /var/run/nginx.pid) (code=exited, status=0/SUCCESS)\n  Process: 20673 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS)\n Main PID: 20674 (nginx)\n   CGroup: /system.slice/nginx.service\n           \u251c\u250020674 nginx: master process /usr/sbin/ngin...\n           \u2514\u250020675 nginx: worker process\n\nMar 29 07:52:10 centos systemd[1]: Starting nginx - hig...\nMar 29 07:52:10 centos systemd[1]: Can't open PID file ...\nMar 29 07:52:10 centos systemd[1]: Started nginx - high...\nHint: Some lines were ellipsized, use -l to show in full.\n[root@centos html]#\n</code></pre> <pre><code>[root@centos conf.d]# for i in `seq 1 7`; do curl http://192.168.56.10; done\nThis is application server backend\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\nThis is application server backend\nthis is backend server\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/load_balancer/#active-and-passive","title":"active and passive","text":"<p>active - only available in nginx plus, which is paid. In this check, the health check of an upstream server by sending special health check requests(GET /) to each server and check for the response. </p> <p>if there are any response of 2xx or 3xx then its considered and unhealthy and marked accordingly.</p> <p>passive - nginx monitors the communication between client and the upstream server.  if upstream server is not responding or rejecting connections, the passive health check with consider the serevr unhealthy and marked accordingly. </p>"},{"location":"nginx/load_balancer/#passive-healthcheck-parameters","title":"passive healthcheck parameters","text":"<p>you can configure passive checks in the nginx configs.. </p> <p>max_fails - set the number of failed attempts that must occur during the fail_timeout period for the server to be marked unavailable.</p> <p>fail_timeout - set the time during which number of failed attempts must happen for the server to be marked unavailable and also the time for which the server is marked unavailable.</p> <pre><code>[root@centos conf.d]# cat load-balancer.conf\nupstream backend {\n  server 192.168.56.11 max_fails=2 fail_timeout=30s;\n  server 192.168.56.12 max_fails=2 fail_timeout=30s;\n}\n\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://backend;\n }\n}\n[root@centos conf.d]#systemctl restart nginx\n</code></pre> <p>until the 30s for 2 attemps, nginx won't send any requests to that server.. you can configure it accordingly.  you can use the tool called 'ab' which is <code>httpd-tools</code> to send multiple requests to nginx. </p> <pre><code>ab -n 1000 localhost/ # send 1000 requests to the nginx proxy.\n</code></pre>"},{"location":"nginx/load_balancer/#server-weights","title":"server weights","text":"<p>allows us to customizd the request flow betwen nginx acting as LB to the upstream backends... you can send 80% of traffic to one server and 20% to another server. </p> <pre><code>upstream backend {\n  server 192.168.56.11;\n  server 192.168.56.12 weight=2;\n}\n</code></pre>"},{"location":"nginx/logging/","title":"logging","text":""},{"location":"nginx/logging/#access-logs","title":"access logs","text":"<p>what we could determine from the nginx access logs ..</p> <ul> <li>ip address of the requester</li> <li>date and time</li> <li>type of request (GET, POST, PUT ..etc)</li> <li>path to which request was asked</li> <li>response of the request </li> <li>browser name from which request was sent.</li> </ul> <p>logger file from the config goes below </p> <pre><code>log_format main \"\" ... \n</code></pre>"},{"location":"nginx/logging/#configure-custom-logs","title":"configure custom logs","text":"<p>vim /etc/nginx/conf.d/virualhost.conf</p> <pre><code>access_log /var/log/nginx/example.log main;\n</code></pre> <p>ls /var/log/nginx/example.log would have your log in the defined format.</p>"},{"location":"nginx/logging/#logging","title":"logging","text":"<p>logging levels</p> <p>no custom error logs can be configured</p> <p>emerg, alert, crit, error, warn, notice, info, debug </p> <pre><code>vim /etc/nginx/conf.d/nginx.conf\n\nerror_log /var/log/nginx.log emerg\nerror_log /var/log/example.log crit\nerror_log /var/log/domain_info.log info\n</code></pre>"},{"location":"nginx/nginx_overview/","title":"nginx overview","text":""},{"location":"nginx/nginx_overview/#nginx-architecture","title":"Nginx architecture","text":"<p>Configuration file: /etc/nginx/nginx.conf</p> <p>User: nginx</p> <p>Log: /var/log/nginx</p> <p>rpm: nginx-1.20.1-1.el7.ngx.x86_64.rpm</p> <p>OS: CentOS7</p> <pre><code>[root@centos ~]# ps -ef | grep nginx | grep -v grep\nroot       623     1  0 Mar26 ?        00:00:00 nginx: master process /usr/sbin/nginx\nnginx      627   623  0 Mar26 ?        00:00:00 nginx: worker process\n[root@centos ~]#\n</code></pre> <p>master process read and evaluate config file and maintain worker process. worker process can be 1 or more.</p> <p>worker process they do the actual processing.</p> <p><code>worker_processes</code> in the nginx config file, it would auto detect the number of CPU and then will launch the worker process accordingly. you have <code>error_log</code> and <code>pid</code> in the config file which is self explanatory. </p> <p>we can also include other modules instead of any modification in the main file.  <code>include /usr/share/nginx/modules/*.conf;</code>. To read more, please check the configuration file.</p> <p><code>worker_connections</code> sets max number of simultaneous connections that can be opened by a worker process. </p>"},{"location":"nginx/nginx_overview/#contexts","title":"contexts","text":"<p>nginx config file is divided across rage of contexts(sections) each context contains its own set of directives to control specific aspect of nginx ., i.e </p> <p>Any directive that exists entirely outside of the context is said to inhabit  the \"main\" context Main context is used to configure details that effect the entire application on a basic level - main - events - http - mail</p> <pre><code>cat /etc/nginx/nginx.conf\n&lt;snip&gt;\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main\n}\n&lt;snip&gt;\n</code></pre>"},{"location":"nginx/nginx_overview/#http","title":"http","text":"<p>contails all of the directives and other contexts necessary to define how to handle HTTP/HTTPS connections and associated parameters.</p> <pre><code>http {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n}\n</code></pre>"},{"location":"nginx/nginx_overview/#custom-config","title":"custom config","text":"<p>You can create your custom config file using below file. you can change the <code>location</code> to your desired one for youe pplciation. </p> <pre><code>[root@centos conf.d]# pwd\n/etc/nginx/conf.d\n[root@centos conf.d]# cat default.conf  | grep -v '#'\nserver {\n    listen       80;\n    server_name  localhost;\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm; \n    }\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n}\n</code></pre>"},{"location":"nginx/nginx_overview/#configure-multiple-websites-or-domains","title":"Configure multiple websites or domains","text":"<pre><code>[root@centos conf.d]# cat /etc/nginx/conf.d/dexter.conf\nserver {\n    listen       8080;\n    server_name  localhost;\n\n    access_log  /var/log/nginx/dexter.access.log  main;\n\n    location / {\n        root   /usr/share/nginx/html/dexter;\n        index  index.html;\n    }\n}\n[root@centos conf.d]# systemctl restart nginx\n[root@centos conf.d]# curl http://192.168.56.10:8080/index.html\nthis is an multiple site configured at nginx for domain dexter\n[root@centos conf.d]#\n\n\n[root@centos conf.d]# ls /var/log/nginx/dexter.access.log\n/var/log/nginx/dexter.access.log\n[root@centos conf.d]# cat /var/log/nginx/dexter.access.log\n192.168.56.1 - - [27/Mar/2024:10:45:26 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\" \"-\"\n192.168.56.10 - - [27/Mar/2024:10:45:40 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"curl/7.29.0\" \"-\"\n192.168.56.10 - - [27/Mar/2024:10:46:27 +0000] \"GET /index.html HTTP/1.1\" 200 63 \"-\" \"curl/7.29.0\" \"-\"\n[root@centos conf.d]#\n</code></pre>"},{"location":"nginx/nginx_overview/#nginx-cli","title":"nginx cli","text":"<pre><code>nginx -v # version\nnginx -V # more detail\nnginx -t # check syntax\nnginx -c new_nginx.conf # testing purpose\nnginx -h # help menu\n</code></pre>"},{"location":"nginx/nginx_overview/#modular-architecture","title":"modular architecture","text":"<p>referes to any system composed of seperate componets that can be connected together i.e its a collection of modules, we can also extend the functionality by adding 3rd party modules.</p>"},{"location":"nginx/nginx_overview/#static-modules","title":"static modules","text":"<ul> <li>Modules that are compiled into nginx server binary at compile time</li> <li>Single package, portable and works out of box</li> <li>Creates issue when one of the module has bug, hence difficult to troubleshoot</li> </ul>"},{"location":"nginx/nginx_overview/#dynamic-modules","title":"dynamic modules","text":"<ul> <li>Create/download dynamic module files.</li> <li>Reference the path of the module with the <code>load_module</code> directive.</li> </ul>"},{"location":"nginx/nginx_overview/#install-using-source-module","title":"install using source module","text":"<pre><code>http://nginx.org/en/download.html\n\nwget http://nginx.org/download/nginx-1.16.0.tar.gz\ntar -xzvf nginx-1.16.0.tar.gz\nyum -y install gcc make zlib-devel pcre-devel openssl-devel wget nano\n\n./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --pid-path=/var/run/nginx.pid --lock-path=/var/lock/subsys/nginx --user=nginx --group=nginx --with-http_mp4_module --add-module=../nginx-hello-world-module\n\nuseradd Nginx\nmkdir -p /var/lib/nginx/tmp/\nchown -R nginx.nginx /var/lib/nginx/tmp/\n\nSystemD file\n\nhttps://www.nginx.com/resources/wiki/start/topics/examples/systemd/\n</code></pre>"},{"location":"nginx/nginx_overview/#build-dynamic-module","title":"build dynamic module","text":"<pre><code>yum -y install git\ngit clone https://github.com/perusio/nginx-hello-world-module\n./configure --add-dynamic-module=../nginx-hello-world-module\n\nvim /etc/nginx/conf.d/nginx.conf\nload_module /etc/nginx/modules/something.so # it should be in global section\n\nserver {\n    listen 8080;\n\n    location = /test {\n        hello_world;\n    }\n}\n\ncurl -i http://example.com/test #this will be loaded from the dynamic module\n</code></pre> <p>Reference: https://github.com/perusio/nginx-hello-world-module</p>"},{"location":"nginx/nginx_overview/#build-static-module","title":"build static module","text":"<p>You don't have to use <code>load_module</code> directive, its same as the src compilation but you would add the static compile method <code>--add-module</code> as referenced here https://github.com/perusio/nginx-hello-world-module</p> <pre><code>./configure --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --pid-path=/var/run/nginx.pid --lock-path=/var/lock/subsys/nginx --user=nginx --group=nginx --with-http_mp4_module --add-module=../nginx-hello-world-module\n\nmake\nmake install\n\n\nserver {\nlisten 8080;\n\nlocation / {\n     hello_world;\n  }\n}\n\n\nsystemctl restart nginx\ncurl localhost:8080\n</code></pre>"},{"location":"nginx/nginx_overview/#web-application-firewallwaf","title":"web application firewall(waf)","text":"<p>A Web Application Firewall (WAF) is a security solution designed to protect web applications by monitoring, filtering, and potentially blocking HTTP traffic between a web application and the Internet. WAFs are deployed to provide an additional layer of defense, thereby protecting against data breaches, unauthorized access, and service disruption</p> <ul> <li>SQL Injection (SQLi)</li> <li>Cross-Site Scripting (XSS)</li> <li>Cross-Site Request Forgery (CSRF)</li> <li>File Inclusion</li> <li>Directory Traversal</li> <li>Brute Force Attacks</li> <li>Denial-of-Service (DoS) </li> <li>Distributed Denial-of-Service(DDoS)</li> </ul> <p>Signature-based Detection: WAFs use predefined signatures or patterns to identify known attacks and malicious traffic.</p> <p>Behavioral Analysis: Some advanced WAFs employ machine learning or heuristic algorithms to analyze traffic patterns and detect anomalies indicative of attack attempts.</p> <p>Request Inspection: WAFs inspect HTTP requests and responses, analyzing parameters, headers, payloads, and other attributes to identify suspicious activity.</p> <p>Traffic Filtering: WAFs can filter and block traffic based on predefined rules, such as IP addresses, user agents, request methods, or payloads.</p> <p>Protocol Validation: WAFs validate incoming requests against known HTTP standards and application-specific protocols to detect and block malformed or malicious requests.</p> <p>Logging and Reporting: WAFs provide logs and reports detailing detected threats, blocked requests, and other security events for analysis and investigation.</p>"},{"location":"nginx/reverse_proxy/","title":"reverse proxy","text":"<p>reverse proxy is type of proxy server which retrives resources onbehalf of the coient from one or more servers</p> <p></p> <p>use cases:</p> <ul> <li>it hides existance of the original backend servers</li> <li>can protect tthe backend server from webbased attacks, DOS etc </li> <li>can provide great caching functionality</li> <li>can optimize the content by compressing it </li> <li>can act as SSL terminating proxy</li> <li>request routing.. etc </li> </ul>"},{"location":"nginx/reverse_proxy/#reverse-proxy-setup","title":"Reverse Proxy Setup","text":"<p>proxy_pass</p> <p>directive forwards the request to the proxied servers specified along with the directive. </p> <p>1st - Nginx Reverse Proxy(192.168.56.10) 2nd - Application Server(192.168.56.11) 3rd - Authentication Server(192.168.56.12)</p> <p>BaseConfigurations for all 3 servers</p> <p>yum install -y wget net-tools wget https://nginx.org/packages/rhel/7/x86_64/RPMS/nginx-1.20.1-1.el7.ngx.x86_64.rpm yum -y install nginx-1.20.1-1.el7.ngx.x86_64.rpm systemctl start nginx systemctl enable nginx setenforce 0 reboot</p> <p>application server: cd /usr/share/nginx/html echo \"This is application server backend\" &gt; index.html</p> <p>auth server: mkdir /usr/share/nginx/html/admin echo \"This is auth server file under admin\" &gt; /usr/share/nginx/html/admin/index.html</p> <p>Proxy server:</p> <p>cd /etc/nginx/conf.d nano proxy.conf server {     listen       80;     server_name  localhost;</p> <pre><code>location / {\n    proxy_pass http://192.168.56.11;\n}\n\nlocation /admin {\n    proxy_pass http://192.168.56.12;\n  }\n</code></pre> <p>} nginx -t systemctl restart nginx</p> <p>You can now see if the request is made from the client, it would first get to the nginx server and then would reverse proxy the response from the application or the auth server to the client. These are being logged into the application or auth nginx logs..</p> <p></p>"},{"location":"nginx/reverse_proxy/#x-real-ip","title":"X-Real-IP","text":"<p>Problem statement</p> <p>original webservers would require an originating ip address to send response</p> <p>When the client sends a request to the web server through the reverse proxy, its always as usual that the backend web application would be only knowing the reverse proxy instead of original client server. sometime in the production environment the application servers would require to bind the originating ip address to send the respose. so in that case we need to use x-real-ip so that the reverse proxy would send the ip address of the client to the application server. </p> <p>Configuration</p> <p>Reverse Proxy Side</p> <pre><code>vim /etc/nginx/conf.d/proxy.conf\nproxy_set_header X-Real-IP $remote_addr;\n</code></pre> <p>Backend Server Side</p> <pre><code>vim /etc/nginx/nginx.conf\n\n# append $http_x_real_ip at the end of the line.\nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\" **\"$http_x_real_ip\"**';\n\nnginx -t\nsystemctl restart nginx                    \n</code></pre> <p>so now, it would log the remote(original) ip address to the application server along with the proxy.</p>"},{"location":"nginx/reverse_proxy/#proxy-host-header","title":"proxy host header","text":"<p>problem statement</p> <p>Host header that is received at the reverse proxy level is not forwarded to backend server. </p> <p>if there are multiple websites hosted on the application server and your client sends the GET request along with the headers, the nginx proxy won't send the headers to the multiple hosted web application and hence you won't be able to get the response. </p> <p>Reverse Proxy Level</p> <pre><code>[root@centos ~]# cat /etc/nginx/conf.d/proxy.conf\nserver {\n    listen       80;\n    server_name  localhost;\n\n    location / {\n        proxy_pass http://192.168.56.11;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host-Header $host;\n    }\n\n    location /admin {\n        proxy_pass http://192.168.56.12;\n        proxy_set_header X-Real-IP $remote_addr;\n      }\n}\n[root@centos ~]#systemctl restart nginx\n\n[root@centos ~]# curl localhost\nThis is application server backend\n[root@centos ~]#\n</code></pre> <p>Backend Server Level</p> <pre><code>yum -y install tcpdump\ntcpdump -A -vvvv -s 9999 -i eth1 port 80 &gt; /tmp/headers\n\ncat /tmp/headers\n\nGET / HTTP/1.0\nX-Real-IP: 127.0.0.1  \nHost-Header: localhost -&gt; headers are passed from reverse proxy to backend\nHost: 192.168.56.11\nConnection: close\nUser-Agent: curl/7.29.0\nAccept: */*\n</code></pre>"},{"location":"nginx/static_assets/","title":"static assets","text":"<p>when we make a request from client, it would query the nginx server and then contact the upstream server to get the response to the client. so when we want to load the index.html, all the static content has to be served from the upstream server(.png, .js, .css..etc) to get the response. </p> <p>Incase if we have configured static content to be served from the nginx reverse proxy, then only index.html would be loaded from the upstream server, there by providing better response time, performance on the upstream would be better.</p> <p>try to load the application and check for the nginx access logs, you can find many of the GET requests to upstream server to server the static content</p>"},{"location":"nginx/static_assets/#configure","title":"configure","text":"<pre><code>vim /etc/nginx/conf.d/proxy.conf\n\nserver {\n    server_name yourweb.in\n\n    location / {\n        proxy_pass http://192.168.56.10;\n        proxy_set_header Host $host;\n    }\n\n    # configue your static assets to load from the nginx reverse proxy\n\n    location ~* \\.(css|js|jp?g|JPG|png|PNG) {\n        root /var/www/assets;\n        try_files $uri $uri/;\n    }\n\n}\nngix -t\nsystemctl restart nginx\n</code></pre> <p>on your application server, </p> <pre><code>scp -r /var/www/html/js &lt;nginxsverer&gt;@:/var/www/assets/\nscp -r /var/www/html/css &lt;nginxsverer&gt;@:/var/www/assets/\nscp -r /var/www/html/master &lt;nginxsverer&gt;@:/var/www/assets/\n</code></pre> <p>open your application /var/log/nginx/access.log and try to load the webpage , you could see the minimization of the GET requests from the upstream server.</p>"},{"location":"nginx/webserver_overview/","title":"web server","text":""},{"location":"nginx/webserver_overview/#introduction","title":"introduction","text":"<p>webserver is a program that uses HTTP to serve the web pages to users in response to their request.  e.g Apache, nginx, IIS</p> <p>How web server works?</p> <p></p> <p>Client Request: When a user wants to access a web page or resource, they send a request from their web browser to the web server. This request includes the URL (Uniform Resource Locator) of the desired resource.</p> <p>Routing and Processing: The web server receives the request and determines which resource the client is asking for based on the URL provided. It then processes the request, which may involve executing scripts, querying databases, or accessing files.</p> <p>Resource Retrieval: If the requested resource is a static file (such as an HTML, CSS, or image file), the web server retrieves it directly from its file system and sends it back to the client. If the resource is dynamic (generated by a script or retrieved from a database), the web server executes the necessary code to generate the content.</p> <p>Response Generation: Once the requested resource has been processed or retrieved, the web server generates an HTTP (Hypertext Transfer Protocol) response. This response includes headers with metadata about the resource and a body containing the actual content.</p> <p>Sending Response: The web server sends the HTTP response back to the client over the internet.</p> <p>Client Rendering: The client's web browser receives the response and renders the content to display it to the user. This may involve parsing HTML, applying CSS styles, executing JavaScript, and rendering images.</p> <p>nginx is more than a webserver and it can accomplish like</p> <ul> <li>reverse proxy</li> <li>load balancing</li> <li>HTTP caching</li> </ul>"},{"location":"nginx/webserver_overview/#installation","title":"installation","text":"<p>CentOS 7/8</p> <pre><code>yum install epel-release -y\nyum install nginx -y\nsystemctl start nginx\nsystemctl enable nginx\nnginx -V\n</code></pre>"},{"location":"programming/clean_code/control_structure/","title":"control structures","text":"<p>Keep your control structures clean</p> <ul> <li>avoid deep nesting.</li> <li>use factory functions and polymorphism</li> <li>prefer positive checks(e.g IsValid, IsEmpty..etc)</li> <li>Utilize errors</li> </ul>"},{"location":"programming/clean_code/control_structure/#guards","title":"Guards","text":"<p>Use guards and fail fast</p> <pre><code>if !(email.includes('@')):\n    return  # fail fast instead of deep nesting \n\n# if there remaining code, then you would execute from here\n</code></pre>"},{"location":"programming/clean_code/control_structure/#embrace-errors","title":"Embrace errors","text":"<p>Throwing + handling errors can replace if statements and lead to more focussed functions. If something is an error, make it error insterad of using if-else statements to fix it.. you can use try-catch to log error.</p>"},{"location":"programming/clean_code/methods/","title":"function","text":""},{"location":"programming/clean_code/methods/#minimize-paramters","title":"minimize paramters","text":"<p>when you are writing the function, you need to minimize the number of parameters.</p> <p>None parameter -&gt; best possible option </p> <p>1 parameters -&gt; very good possible option i.e log(message), sqr(x) ..etc </p> <p>2 parameters -&gt; use with caution e.g point(2,3), login(email, password) ..etc</p> <p>3 parameters -&gt; avoid if possible</p> <p>greater than 3 paramters -&gt; always avoid. </p> <p>let's say if you don't want to use any of the paramters, then you would create an blueprint of the class object and write methods into it. then you can simply call the function, without any object.. BTW, clean code means ..</p> <ol> <li>should be readable and meaningful</li> <li>should reduce cognitive load</li> <li>shoud be concise to the point</li> <li>should avoid unintuitive names, complex nesting and big blocks.</li> <li>should follow common best practices and patterns. </li> <li>should be fun to write and maintain code. </li> </ol> <p>if you have too many values, then try to add key:value pair in the functions and then use it. </p> <p>example</p> <pre><code>@dataclass\nclass Compare\n    x:int\n    y:int\n\n    def small(self):\n        if self.x &lt; self.y: return self.x\n        else: return self.y\n\n    def big(self):\n        if self.x &gt; self.y: return self.x\n        else: return self.y\n\noperators=Compare(x=10,y=12)\n\nprint(\"small\", operators.small())\nprint(\"big\", operators.big())\n</code></pre> <p>Functions should be small and should do exactly one thing. </p>"},{"location":"programming/clean_code/methods/#abstractions","title":"abstractions","text":"<p>There are two levels i.e </p> <p>high level  - functions which we write and there is no room for interpretation.  e.g isEmail(email), userExists(username) etc</p> <p>low level - these are low level API or built-in but the intrepation must be added..  e.g username.split(\"@\") list_of_user.append(username) .. etc</p> <p>Do not mix levels of abstrations, we must split the the functions if they are doing multiple works. </p> <p>so when do we need to split the functions ?</p> <ul> <li>Extract the code that works on the same functionality (do-not-repeat)</li> <li>Extract the code that requires more interpretation than the surrounding code</li> <li>Split functions reasonable</li> </ul> <p>How would you make decision and don't split  - you are jusy renaming the function  - finding the new function will take longer than reading the extracted code  - can't produce a reasonable name for the extracted function. </p> <p>Pure function</p> <p>the same input always yeilds same output. it means they are predictable.  no side effects.</p> <pre><code>def genId(username):\n    return f\"id-{username}\"\n</code></pre> <p>A function that just not only changes input/output but changes the overall system/program state. </p> <pre><code>def create_user(username):\n    newuser = CreateUser(username)\n\n    # inpure function\n    startsession(newuser) # side effect is not bad, but can be avoided\n    return newuser\n</code></pre>"},{"location":"programming/clean_code/naming/","title":"naming","text":""},{"location":"programming/clean_code/naming/#overview","title":"Overview","text":"<p>Naming (variables, properties, functions, methods, classes) correctly and in an understandable way if an extremely important part of writing clean code.</p> <p>Names have one simple purpose: They should describe what's stored in a variable or property or what a function or method does. Or what kind of object will be created when instantiating a class.</p> <p>Variables and properties hold data - numbers, text (strings), boolean values, objects, lists, arrays, maps etc. Hence the name should imply which kind of data is being stored. Therefore, variables and properties should typically receive a noun as a name. e.g: user, product, customer, database, transaction etc. </p> <p>Alternatively, you could also use a short phrase with an adjective - typically for storing boolean values. e.g: isValid, didAuthenticate, isLoggedIn, emailExists</p> <p>Functions &amp; Methods  - Functions and methods can be called to then execute some code. That means that they perform tasks and operations. Therefore, functions and methods should typically receive a verb as a name. e.g: login(), createUser(), database.insert(), log() etc.</p> <p>Alternatively, functions and methods can also be used to primarily produce values - then, especially when producing booleans, you could also go for short phrases with adjectives. e.g: isValid(...), isEmail(...), isEmpty(...) etc.</p> <p>Classes - Classes are used to create objects (unless it's a static class). Hence the class name should describe the kind of object it will create. Even if it's a static class (i.e. it won't be instantiated), you will still use it as some kind of container for various pieces of data and/ or functionality - so you should then describe that container. </p> <p>Good class names - just like good variable and property names - are therefore nouns. e.g: User, Product,RootAdministrator, Transaction, Payment etc</p>"},{"location":"programming/clean_code/naming/#name-casing","title":"Name Casing","text":"<p>snake_case - everything is lowercase and vars seperated by underscore, that includes functions and vars e.g python</p> <p>cameCase - There is no space and everystart of new char starts with capital letter, that includes functions, methods, and vars e.g Java, JavaScript</p> <p>PascalCase - Used many prg, used mainly in Classes e.g Python, Java, Javascript</p> <p>Kebab-case - mainly used in HTML prog lang</p>"},{"location":"programming/clean_code/ooo/","title":"oop principles","text":""},{"location":"programming/clean_code/ooo/#diff-bw-obj-ds","title":"Diff b/w obj &amp; ds","text":"<p>Object</p> <ul> <li>private internals/properties, public API</li> <li>Contain your business logic </li> <li>Abstractions over conceretions</li> </ul> <p>data structures</p> <ul> <li>public internals/properties, no API</li> <li>stores and transports data</li> <li>concertions only</li> </ul>"},{"location":"programming/clean_code/ooo/#object-and-polymorphism","title":"Object and polymorphism","text":"<p>The ability of an object to take many forms.</p> <p>Classes should be small, and it should have Single Responsibility.</p>"},{"location":"programming/clean_code/ooo/#cohestion","title":"Cohestion","text":"<p>how much are your class methods using the class properties. Goal: not all the methods uses the propeties, but you can make sure that most of the properties will be used by the methods. i.e they are highly cohesive.</p>"},{"location":"programming/clean_code/ooo/#law-of-demeter","title":"law of demeter","text":"<p>principles of least knowledge: Don't depend on the internals of \"strangers\"(other objects that you don't know directly.)</p> <p>Code in a methods may only access direct internals(properris and methods) of: - the object it belongs to - objects that are stored in properties of that object - objects which are received as methods parameters - objects which are created in the method</p> <p>always tell what to do, instead of asking.</p>"},{"location":"programming/clean_code/ooo/#solid","title":"SOLID","text":"<p>work on the solid principles desing for clean code.</p>"},{"location":"programming/clean_code/overview/","title":"overview","text":""},{"location":"programming/clean_code/overview/#clean-code","title":"clean code","text":"<p>It's code which is readable and understandable. Clean code should be concise and to the point therefore, and you'll, for example, wanna avoid unintutive names, complex nestings or big code blocks.</p> <p>You, can follow common best practices and patterns and also a bunch of concepts and rules, it should be fun to write and to maintain code and with clean code, you ensure that maintaining can be fun because your code can be understood by others.</p> <p>As a developer, you are the author of your code. And you wanna write it such that it's fun and easy to read and understand your code. That should be your goal</p>"},{"location":"programming/clean_code/overview/#key-pain-points","title":"Key pain points","text":"<ul> <li>names - vars, functions, classes</li> <li>structure &amp; comments - code formating, good and bad comments </li> <li>fucntions - length, parameters</li> <li>conditions and error handling - dee nesting, missing error handling</li> <li>class objects and data structures - missing distinction, bloated classes. </li> </ul> <p>solutions:</p> <ul> <li>Rules and concepts</li> <li>pattern &amp; principles</li> <li>TTD</li> </ul> <p>References: https://github.com/academind/clean-code-course-code/tree/general-resources</p>"},{"location":"programming/clean_code/structure/","title":"structure","text":"<p>We would discuss more about the code structure, formating and comments in this section. </p>"},{"location":"programming/clean_code/structure/#comments","title":"comments","text":""},{"location":"programming/clean_code/structure/#bad-comments","title":"Bad comments","text":"<ul> <li>avoid redunant comments. i.e your vars name and code would almost mean same thing</li> <li>avoid dividers or blockers i.e naming globals etc in \"****\" for better reading etc, if this is big then create a new file and import it.</li> <li>misleading comments i.e method is being done something but comment is something</li> <li>comented code i.e delete code incase not required</li> </ul>"},{"location":"programming/clean_code/structure/#good-comments","title":"Good comments","text":"<ul> <li>legal information i.e disclaimer etc </li> <li>explanation which can't be replaced by good namming i.e regular expression </li> <li>warining i.e works only in dev env or etc </li> <li>todo nodes i.e add unfinished code to be written </li> <li>docstring i.e writing for the API would make sense. </li> </ul>"},{"location":"programming/clean_code/structure/#code-formatting","title":"code formatting","text":"<p>code formating improves readibilty and transports meaning. this would be lang specific. always use specific conventions and guide lines while writing code. </p>"},{"location":"programming/clean_code/structure/#vertical-formating","title":"vertical formating","text":"<ul> <li>veritical space between lines</li> </ul> <p>code should be readable like an essay - top to bottom without too many jumps.  If the code is too big in the file, then try considering breaking code into different multiple files and classes. this makes code short and readable. </p> <p>Different concepts(\"areas\") should be seperated by spacing. i.e imports and function should be having a one blank line.  </p> <ul> <li>grouping of code i.e all the related concepts should be kept the same. if we are writing to save code, all the code related to save should be one below the block.</li> </ul>"},{"location":"programming/clean_code/structure/#horizontal-formating","title":"horizontal formating","text":"<p>lines of code should be readable without scrollng. avoid very long \"sentences\"</p> <ul> <li>indentation i.e use it even if its not required.</li> <li>line width  i.e break long line of code into shorter. </li> <li>space between code </li> <li>use clear variable name rather than long name</li> </ul>"},{"location":"programming/go/basics/","title":"Data Types in Golang","text":""},{"location":"programming/go/basics/#1-basic-types","title":"1. Basic Types","text":"<p>These are the fundamental data types in Go.</p> Type Description Example bool Boolean values (<code>true</code> or <code>false</code>) <code>var isActive bool = true</code> string Sequence of characters <code>var name string = \"Golang\"</code> int Signed integer (platform-dependent size) <code>var age int = 25</code> uint Unsigned integer (platform-dependent size) <code>var count uint = 10</code> float32 32-bit floating-point number <code>var pi float32 = 3.14</code> float64 64-bit floating-point number <code>var pi float64 = 3.14159265359</code> complex64 Complex number with float32 real/imag parts <code>var c complex64 = 1 + 2i</code> complex128 Complex number with float64 real/imag parts <code>var c complex128 = 2 + 3i</code>"},{"location":"programming/go/basics/#2-integer-types","title":"2. Integer Types","text":"<p>Go provides multiple integer types of different sizes.</p> Type Size Description Example int8 8 bits Signed integer (-128 to 127) <code>var a int8 = 127</code> int16 16 bits Signed integer (-32,768 to 32,767) <code>var b int16 = 32000</code> int32 32 bits Signed integer (-2^31 to 2^31-1) <code>var c int32 = 2147483647</code> int64 64 bits Signed integer (-2^63 to 2^63-1) <code>var d int64 = 9223372036854775807</code> uint8 8 bits Unsigned integer (0 to 255) <code>var e uint8 = 255</code> uint16 16 bits Unsigned integer (0 to 65,535) <code>var f uint16 = 65535</code> uint32 32 bits Unsigned integer (0 to 2^32-1) <code>var g uint32 = 4294967295</code> uint64 64 bits Unsigned integer (0 to 2^64-1) <code>var h uint64 = 18446744073709551615</code>"},{"location":"programming/go/basics/#3-derived-types","title":"3. Derived Types","text":"<p>Derived or composite types are built using basic types.</p>"},{"location":"programming/go/basics/#a-array","title":"a. Array","text":"<ul> <li>Fixed-size collection of elements of the same type.</li> <li>Example: <code>var arr [3]int = [3]int{1, 2, 3}</code></li> </ul>"},{"location":"programming/go/basics/#b-slice","title":"b. Slice","text":"<ul> <li>Dynamic-sized, more flexible version of an array.</li> <li>Example: <code>var slice []int = []int{1, 2, 3}</code></li> </ul>"},{"location":"programming/go/basics/#c-map","title":"c. Map","text":"<ul> <li>Key-value pairs (similar to dictionaries in Python).</li> <li>Example: <code>var m map[string]int = map[string]int{\"one\": 1, \"two\": 2}</code></li> </ul>"},{"location":"programming/go/basics/#d-struct","title":"d. Struct","text":"<ul> <li>Collection of fields, used to define custom data types.</li> <li>Example:</li> </ul> <pre><code>type Person struct {\n    Name string\n    Age  int\n}\nvar p Person = Person{Name: \"John\", Age: 30}\n</code></pre>"},{"location":"programming/go/basics/#e-pointer","title":"e. Pointer","text":"<p>Stores the memory address of a variable. Example: var p *int = &amp;a</p>"},{"location":"programming/go/basics/#f-interface","title":"f. Interface","text":"<p>Defines a set of method signatures, allowing polymorphism.</p> <pre><code>type Shape interface {\n    Area() float64\n}\n</code></pre>"},{"location":"programming/go/basics/#operators","title":"Operators","text":"<p>Arthematic operators:  <code>+, -, *, / , %, ++, --</code></p> <p>Assignment operators: <code>=, +=, -=, *=, /=, %=, &amp;=, !=, ^=, &gt;&gt;=, &lt;&lt;=</code></p> <p>Comparision operators : <code>&lt;, &gt;, ==, !=, &lt;=, &gt;=</code></p> <p>Logical operators: <code>&amp;&amp;, || !</code></p> <p>Bitwise operators: <code>&amp;, |, ^, &lt;&lt;, &gt;&gt;</code></p>"},{"location":"programming/go/basics/#verb-formats","title":"verb formats","text":"Verb Description Example <code>%v</code> Default format (value representation) <code>fmt.Printf(\"%v\", 42)</code> <code>%+v</code> Adds field names for structs <code>fmt.Printf(\"%+v\", struct{Name string}{\"Go\"})</code> <code>%#v</code> Go syntax representation <code>fmt.Printf(\"%#v\", struct{Name string}{\"Go\"})</code> <code>%T</code> Type of the value <code>fmt.Printf(\"%T\", 42)</code> <code>%%</code> Literal percent sign <code>fmt.Printf(\"%%\")</code>"},{"location":"programming/go/basics/#boolean","title":"Boolean","text":"Verb Description Example <code>%t</code> Boolean (<code>true</code> or <code>false</code>) <code>fmt.Printf(\"%t\", true)</code>"},{"location":"programming/go/basics/#integer","title":"Integer","text":"Verb Description Example <code>%b</code> Binary representation <code>fmt.Printf(\"%b\", 42)</code> <code>%c</code> Character corresponding to the Unicode code point <code>fmt.Printf(\"%c\", 65)</code> <code>%d</code> Decimal representation <code>fmt.Printf(\"%d\", 42)</code> <code>%o</code> Octal representation <code>fmt.Printf(\"%o\", 42)</code> <code>%O</code> Octal with leading <code>0o</code> <code>fmt.Printf(\"%O\", 42)</code> <code>%q</code> Single-quoted character literal, escapes if necessary <code>fmt.Printf(\"%q\", 65)</code> <code>%x</code> Hexadecimal (lowercase) <code>fmt.Printf(\"%x\", 255)</code> <code>%X</code> Hexadecimal (uppercase) <code>fmt.Printf(\"%X\", 255)</code> <code>%U</code> Unicode format (e.g., <code>U+1234</code>) <code>fmt.Printf(\"%U\", '\u2713')</code>"},{"location":"programming/go/basics/#floating-point-and-complex","title":"Floating-Point and Complex","text":"Verb Description Example <code>%b</code> Exponent as a power of two <code>fmt.Printf(\"%b\", 3.14)</code> <code>%e</code> Scientific notation (lowercase <code>e</code>) <code>fmt.Printf(\"%e\", 3.14)</code> <code>%E</code> Scientific notation (uppercase <code>E</code>) <code>fmt.Printf(\"%E\", 3.14)</code> <code>%f</code> Decimal point, no exponent <code>fmt.Printf(\"%f\", 3.14)</code> <code>%F</code> Same as <code>%f</code> <code>fmt.Printf(\"%F\", 3.14)</code> <code>%g</code> Compact representation (<code>%e</code> or <code>%f</code>) <code>fmt.Printf(\"%g\", 3.14)</code> <code>%G</code> Compact representation (<code>%E</code> or <code>%F</code>) <code>fmt.Printf(\"%G\", 3.14)</code>"},{"location":"programming/go/basics/#string-and-slice","title":"String and Slice","text":"Verb Description Example <code>%s</code> String (plain text) <code>fmt.Printf(\"%s\", \"Go\")</code> <code>%q</code> Double-quoted string, escapes if necessary <code>fmt.Printf(\"%q\", \"Go\")</code> <code>%x</code> Hex dump of string (lowercase) <code>fmt.Printf(\"%x\", \"Go\")</code> <code>%X</code> Hex dump of string (uppercase) <code>fmt.Printf(\"%X\", \"Go\")</code>"},{"location":"programming/go/basics/#pointer","title":"Pointer","text":"Verb Description Example <code>%p</code> Pointer (base 16, with leading <code>0x</code>) <code>fmt.Printf(\"%p\", &amp;a)</code>"},{"location":"programming/go/basics/#width-and-precision","title":"Width and Precision","text":"Option Description Example <code>%5d</code> Minimum width of 5 (right-aligned) <code>fmt.Printf(\"%5d\", 42)</code> <code>%-5d</code> Minimum width of 5 (left-aligned) <code>fmt.Printf(\"%-5d\", 42)</code> <code>%.2f</code> Precision (2 decimal places) <code>fmt.Printf(\"%.2f\", 3.14159)</code> <code>%5.2f</code> Width 5, precision 2 <code>fmt.Printf(\"%5.2f\", 3.14159)</code>"},{"location":"programming/go/basics/#control-flow","title":"Control flow","text":""},{"location":"programming/go/basics/#if-and-elif","title":"**if and elif **","text":"<pre><code>func main() {\na := 10\nb := 10\n\nif a &gt; b {\n    fmt.Println(\"a is greater than b\")\n    } else if b &gt; a {\n        fmt.Println(\"b is greater than a\")\n        } else if a == b {\n            fmt.Println(\"a is equal to b\")\n        }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#for","title":"for","text":"<pre><code>\nfunc main() {\n    for i:=0 ; i&lt;=10; i++ {\n        fmt.Println(i)\n    }\n\n    // for loop with condition\n    j := 0\n    for j &lt;= 10 {\n        fmt.Println(j)\n        j++\n    }\n\n    // for loop with range\n    k := []int{1,2,3,4,5}\n    for index, value := range k {\n        fmt.Println(index, value)\n    }   \n\n\n    // for loop with map\n    l := map[string]string{\"a\": \"apple\", \"b\": \"banana\"}\n    for key, value := range l {\n        fmt.Println(key, value)\n    }\n\n    // for loop with string\n    m := \"Hello World\"\n    for index, value := range m {\n        fmt.Println(index, string(value))\n    }\n\n    // for loop with break\n    for i:=0; i&lt;=10; i++ {\n        if i == 5 {\n            break\n        }\n        fmt.Println(i)\n    }\n\n    // for loop with continue\n    for i:=0; i&lt;=10; i++ {\n        if i == 5 {\n            continue\n        }\n        fmt.Println(i)\n    }\n\n    // for loop with goto\n    i := 0\n    Loop:\n        fmt.Println(i)\n        i++\n        if i &lt;= 10 {\n            goto Loop\n        }\n\n    // for loop with nested loop\n    for i:=0; i&lt;=5; i++ {\n        for j:=0; j&lt;=5; j++ {\n            fmt.Println(i, j)\n        }\n    }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#switch","title":"switch","text":"<pre><code>func main() {\n    var num int\n    fmt.Printf(\"%T %v\\n\", num, num)\n    fmt.Printf(\"Enter a number from 1 to 3: \")\n    fmt.Scan(&amp;num)\n\n    switch num {\n        case 1:\n            fmt.Printf(\"You entered 1\")\n        case 2:\n            fmt.Printf(\"You entered 2\")\n        case 3:\n            fmt.Printf(\"You entered 3\")\n        case 4,5:\n            fmt.Printf(\"You entered either 4 or 5\")\n        default:\n            fmt.Printf(\"You entered an invalid number\")\n        }\n}\n</code></pre>"},{"location":"programming/go/basics/#arrays","title":"Arrays","text":"<pre><code>func main() {\n  var arr1 = [3]int{1,2,3}\n  arr2 := [5]int{4,5,6,7,8}\n\n  fmt.Println(arr1)\n  fmt.Println(arr2)\n}\n</code></pre>"},{"location":"programming/go/basics/#slices","title":"Slices","text":"<p>Like arrays, slices are also used to store multiple values of the same type in a single variable.</p> <p>However, unlike arrays, the length of a slice can grow and shrink as you see fit.</p> <pre><code>func main() {\n  myslice1 := []int{}\n  fmt.Println(len(myslice1)) // 0\n  fmt.Println(cap(myslice1)) // 0\n  fmt.Println(myslice1) // []\n\n  myslice2 := []string{\"Go\", \"Slices\", \"Are\", \"Powerful\"}\n  fmt.Println(len(myslice2)) // 4\n  fmt.Println(cap(myslice2)) // 4\n  fmt.Println(myslice2) // [Go Slices Are Powerful]\n  fmt.Println(myslice2[0]) // Go\n\n  myslice2 = append(myslice2, \"New\", \"append\")\n  fmt.Println(myslice2)\n\n  fmt.Println(len(myslice2)) // 6\n  fmt.Println(cap(myslice2)) // 8\n\n  // append one slice to another\n\n  myslice1 := []int{1,2,3}\n  myslice2 := []int{4,5,6}\n  myslice3 := append(myslice1, myslice2...)\n}\n</code></pre>"},{"location":"programming/go/basics/#maps","title":"Maps","text":"<p>Maps are used to store data values in key:value pairs.</p> <p>A map is an unordered and changeable collection that does not allow duplicates.</p> <p>Maps hold references to an underlying hash table.</p> <pre><code>func main() {\n    var a = map[string]string {\"name\":\"John\", \"age\":\"25\", \"job\":\"Engineer\", \"salary\":\"50000\"}\n\n\n\n    b:=make(map[string]string) // empty map\n\n    b[\"name\"]=\"John\"\n    b[\"age\"]=\"25\"\n    b[\"job\"]=\"Engineer\"\n    b[\"salary\"]=\"50000\"\n\n\n    b[\"name\"]=\"Doe\" // update value\n    b[\"color\"]=\"Red\" // add new key value pair\n\n    delete(b, \"color\") // delete key value pair\n\n    key, value := a[\"name\"] // check if key exists\n    fmt.Println(key, value)\n\n    _, value = a[\"name1\"] // check if value exists\n    fmt.Println(value)  \n    fmt.Println(\"a: \",a)\n    fmt.Println(\"b: \",b)\n\n    for k, v := range a {\n        fmt.Printf(\"%v : %v, \", k, v) //loop with no order\n      }\n\n    fmt.Println()\n    fmt.Println()\n    for _, element := range a {\n        fmt.Printf(\"%v : %v, \\n\", element, a[element]) // loop with the defined order\n    }\n\n}\n</code></pre>"},{"location":"programming/go/basics/#functions","title":"Functions","text":"<p>Information can be passed to functions as a parameter. Parameters act as variables inside the function.</p> <pre><code>func Add(a int, b int) int {\n    return a+b \n}\n\nfunc main() {\n    result := Add(1, 2)\n    fmt.Println(\"Sum of two numbers:\", result)\n}\n</code></pre> <p>If you need to omit the return result, you need to use '_'</p> <pre><code>\nfunc myFunction(x int, y string) (result int, txt1 string) {\n    result = x + x\n    txt1 = y + \" World!\"\n    return\n  }\n\nfunc main() {\n    _, b := myFunction(5, \"Hello\")\n  fmt.Println(b) // Hello World\n}\n</code></pre> <pre><code>func sumOfFib(n int ) int {\n    if n &lt; 0 {\n        return 0\n    }\n\n    a,b:=0,1\n    sum := a +b \n\n    for i:=3; i&lt;=n; i++ {\n        next:=a+b \n        sum+=next\n        a=b\n        b=next\n    }\n\n    return sum \n\n}\n\nfunc printFib(n int) {\n    if n &lt; 0 {\n        fmt.Println(\"invalid input\")\n        return\n    }\n\n    a,b := 0,1\n    for i:=1;i&lt;=n; i++ {\n        fmt.Printf(\"%d \", a)\n        a,b=b, a+b\n\n    }\n\n    fmt.Println()\n}\n\nfunc main() {\n    println(fact(5))\n    println(sumOfFib(6))\n    printFib(6)\n}\n</code></pre>"},{"location":"programming/go/basics/#pointers","title":"Pointers","text":""},{"location":"programming/go/basics/#struct-methods-and-interfaces","title":"Struct, Methods and Interfaces","text":"<p>A struct (short for structure) is used to create a collection of members of different data types, into a single variable.</p> <pre><code>\ntype Person struct {\n    name string\n    age int\n    job string\n    salary int\n  }\n\nfunc main() {\n    var pers1 Person\n\n\n  pers1.name = \"Hege\"\n  pers1.age = 45\n  pers1.job = \"Teacher\"\n  pers1.salary = 6000\n\n  // Access and print Pers1 info\n  fmt.Println(\"Name: \", pers1.name)\n  fmt.Println(\"Age: \", pers1.age)\n  fmt.Println(\"Job: \", pers1.job)\n  fmt.Println(\"Salary: \", pers1.salary)\n}\n</code></pre> <p>Pass struct as function</p> <pre><code>type Person struct {\n    name string\n    age int\n    job string\n    salary int\n  }\n\nfunc printPerson(pers1 Person) {\n      // Access and print Pers1 info\n      fmt.Println(\"Name: \", pers1.name)\n      fmt.Println(\"Age: \", pers1.age)\n      fmt.Println(\"Job: \", pers1.job)\n      fmt.Println(\"Salary: \", pers1.salary)\n}\n\n\nfunc main() {\n    var pers1 Person\n\n\n  pers1.name = \"Hege\"\n  pers1.age = 45\n  pers1.job = \"Teacher\"\n  pers1.salary = 6000\n\n  printPerson(pers1)\n\n}\n</code></pre>"},{"location":"programming/go/interviews/","title":"interviews faq","text":""},{"location":"programming/go/interviews/#coding-questions-1","title":"Coding Questions 1","text":""},{"location":"programming/go/overview/","title":"overview","text":"<p>Go (Golang) is a statically typed, compiled programming language developed by Google in 2007 and publicly released in 2009. It was designed to address challenges in software development, particularly for large-scale, distributed systems. It emphasizes simplicity, performance, and efficient concurrency.</p> <p>Go that make it different from other programming languages</p> <p>Simplicity and minimalism. Built-in support for concurrency (goroutines and channels). Statically typed with automatic garbage collection. Fast compilation and execution. Strong standard library for networking and web applications. Cross-platform support with built-in tools for building binaries.</p> <p>Key Features:</p> <p>Simplicity: Go has a minimalistic design with a straightforward syntax, making it easy to learn and use.</p> <p>Concurrency: Built-in support for concurrency using goroutines and channels.</p> <p>Performance: Compiled directly to machine code, Go offers performance comparable to C and C++.</p> <p>Garbage Collection: Automatic memory management for ease of development.</p> <p>Standard Library: A rich and robust standard library for handling networking, file I/O, and more.</p> <p>Cross-Platform: Easily compiles binaries for multiple platforms without external dependencies.</p> Aspect Golang Python Typing Statically typed Dynamically typed Compilation Compiled Interpreted Performance High (closer to C/C++) Moderate (slower than Go) Concurrency Native support via goroutines and channels Limited; requires libraries like <code>asyncio</code> or <code>threading</code> Ease of Learning Moderate (focus on simplicity) Easy (readable and beginner-friendly) Use Cases System-level programming, microservices, cloud applications Web development, data science, scripting, AI/ML Community Growing but smaller than Python Mature and vast Ecosystem Focused on performance and concurrency Broad, with extensive libraries Error Handling Explicit error handling (<code>if err != nil</code>) Exceptions and try-except blocks"},{"location":"programming/go/overview/#advantages-of-golang","title":"Advantages of Golang","text":"<ol> <li>Performance: Comparable to C/C++ due to compilation to native code.</li> <li>Concurrency: Efficient and lightweight goroutines enable high-performance concurrent programming.</li> <li>Simplicity: Minimalist syntax and design philosophy.</li> <li>Cross-Compilation: Build binaries for different platforms effortlessly.</li> <li>Standard Library: Comprehensive and robust, reducing the need for third-party dependencies.</li> <li>Scalability: Ideal for distributed systems and microservices.</li> </ol>"},{"location":"programming/go/overview/#disadvantages-of-golang","title":"Disadvantages of Golang","text":"<ol> <li>Verbose Error Handling: Requires explicit error checks, leading to repetitive code.</li> <li>Lack of Generics: (Resolved in Go 1.18, but earlier versions lacked this feature, leading to workarounds.)</li> <li>Limited Libraries: Smaller ecosystem compared to Python.</li> <li>No GUI Support: Not suitable for desktop application development.</li> <li>Minimalistic Design: Some developers find the lack of features (like inheritance or macros) restrictive.</li> </ol>"},{"location":"programming/go/overview/#advantages-of-python","title":"Advantages of Python","text":"<ol> <li>Ease of Use: Highly readable and beginner-friendly syntax.</li> <li>Rich Ecosystem: Extensive libraries for web development, data science, AI/ML, etc.</li> <li>Flexibility: Suitable for various domains, from scripting to AI.</li> <li>Community Support: Massive community and resources.</li> <li>Rapid Prototyping: Ideal for quickly building and testing applications.</li> </ol>"},{"location":"programming/go/overview/#disadvantages-of-python","title":"Disadvantages of Python","text":"<ol> <li>Performance: Slower than compiled languages like Go.</li> <li>Concurrency: Limited by the Global Interpreter Lock (GIL).</li> <li>Dynamic Typing: Can lead to runtime errors if not carefully managed.</li> <li>Deployment: Requires dependencies and runtime, unlike Go's single binary.</li> </ol>"},{"location":"programming/go/overview/#conclusion","title":"Conclusion","text":"<ul> <li>Choose Go: For performance-critical, scalable, and concurrent applications like microservices, system tools, and cloud-native apps.</li> <li>Choose Python: For rapid development, data science, AI/ML, web development, or scripting.</li> </ul>"},{"location":"programming/pandas/overview/","title":"Pandas Oneliners","text":"<p>A complete, example-driven quick reference using your attached dataset (<code>bios</code>, <code>results</code>). All examples are one-liners unless noted.</p>"},{"location":"programming/pandas/overview/#setup","title":"Setup","text":"<pre><code>import pandas as pd, numpy as np\nbios = pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"bios\")\nresults = pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"results\")\n</code></pre> <p>Columns:</p> <ul> <li>bios: athlete_id, name, born_date, born_city, born_region, born_country, NOC, height_cm, weight_kg, died_date</li> <li>results: year, type, discipline, event, as, athlete_id, noc, team, place, tied, medal</li> </ul>"},{"location":"programming/pandas/overview/#create-initialize-dataframes","title":"Create &amp; Initialize DataFrames","text":"<pre><code>pd.DataFrame({\"A\":[1,2], \"B\":[\"x\",\"y\"]})                           # from dict\npd.DataFrame([{\"A\":1,\"B\":\"x\"},{\"A\":2,\"B\":\"y\"}])                     # list of dicts\npd.DataFrame(np.arange(6).reshape(3,2), columns=[\"A\",\"B\"])          # from numpy\npd.date_range(\"2020-01-01\", periods=5, freq=\"D\").to_frame(\"date\")   # date range\npd.DataFrame(columns=[\"A\",\"B\"])                                     # empty schema\n</code></pre>"},{"location":"programming/pandas/overview/#io-read-write","title":"I/O (Read &amp; Write)","text":"<pre><code>pd.read_csv(\"file.csv\")                                            # read CSV\ndf.to_csv(\"out.csv\", index=False)                                  # write CSV\npd.read_excel(\"file.xlsx\", sheet_name=\"bios\")                      # read Excel\ndf.to_excel(\"out.xlsx\", index=False, sheet_name=\"S1\")              # write Excel\npd.read_parquet(\"file.parquet\")                                    # read Parquet\ndf.to_parquet(\"out.parquet\", index=False)                          # write Parquet\npd.read_json(\"file.json\")                                          # read JSON\ndf.to_json(\"out.json\", orient=\"records\", lines=False)              # write JSON\n</code></pre>"},{"location":"programming/pandas/overview/#inspect-explore","title":"Inspect &amp; Explore","text":"<pre><code>bios.head(3); bios.info(); bios.describe(include=\"all\")             # quick look\nbios.shape; bios.columns.tolist(); bios.dtypes                      # schema\nbios.nunique(); bios.isna().sum()                                   # counts\nbios[\"NOC\"].value_counts()                                          # freq\nresults.sample(5, random_state=0)                                   # random rows\n</code></pre>"},{"location":"programming/pandas/overview/#selecting-filtering-assigning","title":"Selecting, Filtering, Assigning","text":"<pre><code>bios[[\"name\",\"NOC\"]]                                               # select cols\nbios.loc[bios[\"born_country\"].eq(\"FRA\"), [\"name\",\"born_city\"]]     # filter eq\nbios.query(\"height_cm &gt; 190 and NOC == 'France'\")                  # query str\nbios.iloc[:5, :3]                                                  # by position\nbios.assign(bmi=lambda d: d[\"weight_kg\"] / (d[\"height_cm\"]/100)**2)# add col\nbios.drop(columns=[\"died_date\"])                                   # drop col\nresults.loc[results[\"medal\"].notna(), [\"year\",\"athlete_id\",\"medal\"]]# non-null\n</code></pre>"},{"location":"programming/pandas/overview/#missing-data","title":"Missing Data","text":"<pre><code>bios.fillna({\"height_cm\": bios[\"height_cm\"].median()})             # fill by stat\nbios[\"height_cm\"].fillna(0)                                        # fill scalar\nbios.dropna(subset=[\"height_cm\",\"weight_kg\"])                      # drop subset\nbios[\"born_date\"] = pd.to_datetime(bios[\"born_date\"], errors=\"coerce\")  # coerce\nbios.interpolate(numeric_only=True)                                # interpolate\n</code></pre>"},{"location":"programming/pandas/overview/#type-handling","title":"Type Handling","text":"<pre><code>bios[\"height_cm\"] = pd.to_numeric(bios[\"height_cm\"], errors=\"coerce\") # numeric\nbios[\"NOC\"] = bios[\"NOC\"].astype(\"category\")                           # category\nresults[\"year\"] = results[\"year\"].astype(\"int64\")                       # cast int\nresults[\"tied\"] = results[\"tied\"].astype(\"bool\")                        # cast bool\n</code></pre>"},{"location":"programming/pandas/overview/#datetime-features","title":"Datetime Features","text":"<pre><code>bios[\"born_date\"] = pd.to_datetime(bios[\"born_date\"], errors=\"coerce\")  # parse\nbios[\"born_year\"] = bios[\"born_date\"].dt.year                           # year\nbios[\"born_month\"] = bios[\"born_date\"].dt.month                         # month\nbios.set_index(\"born_date\").sort_index().last(\"5Y\")                     # recent 5y\n</code></pre>"},{"location":"programming/pandas/overview/#groupby-aggregations-olympics-examples","title":"GroupBy &amp; Aggregations (Olympics Examples)","text":"<pre><code>results.groupby(\"noc\")[\"medal\"].count().sort_values(ascending=False)     # medals by NOC (non-null)\nresults.groupby([\"year\",\"noc\"], as_index=False)[\"athlete_id\"].nunique()  # unique athletes per year/NOC\nbios.groupby(\"NOC\")[\"height_cm\"].agg([\"mean\",\"median\",\"count\"])          # height stats by NOC\nresults.groupby(\"discipline\").agg(events=(\"event\",\"nunique\"))            # events per discipline\n</code></pre>"},{"location":"programming/pandas/overview/#transform-window-ops","title":"Transform &amp; Window Ops","text":"<pre><code>bios[\"z_height\"] = (bios[\"height_cm\"] - bios[\"height_cm\"].mean())/bios[\"height_cm\"].std()    # z-score\nresults[\"rank_in_year\"] = results.groupby(\"year\")[\"place\"].rank(method=\"dense\", ascending=True) # rank\nresults.sort_values([\"year\",\"place\"]).groupby(\"year\")[\"place\"].rolling(3).mean().reset_index(level=0, drop=True) # rolling mean\n</code></pre>"},{"location":"programming/pandas/overview/#pivot-crosstab-reshape","title":"Pivot, Crosstab, Reshape","text":"<pre><code>pd.pivot_table(results, index=\"year\", columns=\"noc\", values=\"medal\", aggfunc=\"count\", fill_value=0) # medals heatmap\npd.crosstab(results[\"discipline\"], results[\"medal\"]).fillna(0)                                     # discipline x medal\nresults.melt(id_vars=[\"year\",\"noc\"], value_vars=[\"medal\",\"place\"], var_name=\"metric\", value_name=\"val\") # wide-&gt;long\n</code></pre>"},{"location":"programming/pandas/overview/#merge-join-bios-results","title":"Merge / Join (bios \u2194 results)","text":"<pre><code>res_bios = results.merge(bios[[\"athlete_id\",\"name\",\"NOC\",\"height_cm\",\"weight_kg\"]], on=\"athlete_id\", how=\"left\") # enrich\nbios_only = bios[~bios[\"athlete_id\"].isin(results[\"athlete_id\"])]                                                # anti-join style\n</code></pre>"},{"location":"programming/pandas/overview/#concatenate-append","title":"Concatenate &amp; Append","text":"<pre><code>pd.concat([results.query(\"year &lt; 2000\"), results.query(\"year &gt;= 2000\")], ignore_index=True) # stack rows\npd.concat([bios.set_index(\"athlete_id\"), bios.set_index(\"athlete_id\")], axis=1)             # concat cols\n</code></pre>"},{"location":"programming/pandas/overview/#sorting-ranking","title":"Sorting &amp; Ranking","text":"<pre><code>results.sort_values([\"year\",\"noc\",\"place\"], ascending=[True, True, True])                   # sort\nresults[\"noc_rank\"] = results.groupby(\"year\")[\"noc\"].rank(method=\"dense\")                   # rank within year\n</code></pre>"},{"location":"programming/pandas/overview/#string-ops","title":"String Ops","text":"<pre><code>bios[\"born_city\"].str.title()                                                                # title case\nresults[\"event\"].str.contains(\"Doubles\", na=False)                                           # contains\nresults[\"event_clean\"] = results[\"event\"].str.replace(r\"\\s+\\(Olympic\\)\", \"\", regex=True)  # regex replace\nresults[\"pair\"] = results[\"team\"].fillna(\"\").str.split(\",\").str[:2].str.join(\",\")            # first two teammates\n</code></pre>"},{"location":"programming/pandas/overview/#categorical-optimization","title":"Categorical Optimization","text":"<pre><code>results[\"discipline\"] = pd.Categorical(results[\"discipline\"])                                # category\nresults[\"medal\"] = pd.Categorical(results[\"medal\"], ordered=True, categories=[\"Bronze\",\"Silver\",\"Gold\"]) # ordered\n</code></pre>"},{"location":"programming/pandas/overview/#index-tricks-multiindex","title":"Index Tricks &amp; MultiIndex","text":"<pre><code>mi = results.set_index([\"year\",\"noc\"]).sort_index()                                          # multiindex\nmi.loc[(slice(2000,2012), \"USA\"), :]                                                         # slice by MI\nresults.reset_index(drop=True)                                                               # reset\n</code></pre>"},{"location":"programming/pandas/overview/#window-functions-rollingexpandingewm","title":"Window Functions (Rolling/Expanding/EWM)","text":"<pre><code>results.sort_values(\"year\").groupby(\"noc\")[\"place\"].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True) # rolling mean\nresults.groupby(\"noc\")[\"place\"].expanding().mean().reset_index(level=0, drop=True)                                   # expanding mean\nresults.sort_values(\"year\").groupby(\"noc\")[\"place\"].apply(lambda s: s.ewm(alpha=0.3).mean())                          # EWM\n</code></pre>"},{"location":"programming/pandas/overview/#conditional-logic-one-hot","title":"Conditional Logic &amp; One-Hot","text":"<pre><code>results[\"medal_flag\"] = np.where(results[\"medal\"].notna(), 1, 0)                           # binary flag\npd.get_dummies(results[\"medal\"], prefix=\"medal\")                                            # one-hot encode\n</code></pre>"},{"location":"programming/pandas/overview/#time-series-resampling-by-year","title":"Time-Series Resampling (by year)","text":"<pre><code>(results.assign(date=pd.to_datetime(results[\"year\"].astype(str) + \"-01-01\"))\n        .set_index(\"date\")\n        .resample(\"10Y\")[\"athlete_id\"].count())                                             # decadal counts\n</code></pre>"},{"location":"programming/pandas/overview/#deduplication-qc","title":"Deduplication &amp; QC","text":"<pre><code>results.drop_duplicates(subset=[\"year\",\"athlete_id\",\"event\"])                               # unique participations\nassert bios[\"athlete_id\"].is_unique                                                         # enforce unique key\n</code></pre>"},{"location":"programming/pandas/overview/#performance-tips","title":"Performance Tips","text":"<pre><code>pd.read_csv(\"big.csv\", chunksize=1_000_000)                                                 # chunked ingest\nresults[\"discipline\"] = results[\"discipline\"].astype(\"category\")                            # categorical\nresults.to_parquet(\"fast.parquet\", index=False)                                             # columnar storage\n</code></pre>"},{"location":"programming/pandas/overview/#handy-patterns-olympics-use-cases","title":"Handy Patterns (Olympics Use-Cases)","text":"<pre><code># Top 3 medal-heavy NOCs per discipline\n(results.dropna(subset=[\"medal\"])\n        .groupby([\"discipline\",\"noc\"]).size().reset_index(name=\"medals\")\n        .sort_values([\"discipline\",\"medals\"], ascending=[True,False])\n        .groupby(\"discipline\").head(3))\n\n# BMI distribution (needs height &amp; weight) for living athletes\n(bios.query(\"height_cm.notna() and weight_kg.notna() and died_date.isna()\")\n     .assign(bmi=lambda d: d[\"weight_kg\"]/((d[\"height_cm\"]/100)**2))\n     .groupby(\"NOC\")[\"bmi\"].describe())\n</code></pre>"},{"location":"programming/pandas/overview/#export-results","title":"Export Results","text":"<pre><code>summary = results.groupby([\"year\",\"noc\"], as_index=False)[\"athlete_id\"].nunique()\nsummary.to_excel(\"athletes_by_year_noc.xlsx\", index=False)                                  # Excel\nsummary.to_markdown(\"summary.md\", index=False)                                              # Markdown table\n</code></pre>"},{"location":"programming/pandas/overview/#end-to-end-one-liner-example","title":"End-to-End One-Liner Example","text":"<pre><code>(pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"results\")\n   .merge(pd.read_excel(\"olympics-data.xlsx\", sheet_name=\"bios\")[[\"athlete_id\",\"NOC\"]], on=\"athlete_id\", how=\"left\")\n   .dropna(subset=[\"medal\"])\n   .groupby([\"year\",\"NOC\"], as_index=False).size()\n   .sort_values([\"year\",\"size\"], ascending=[True,False])\n   .to_csv(\"medals_per_year_noc.csv\", index=False))\n</code></pre> <p>complete-pandas-tutorial</p>"},{"location":"programming/python/concepts/","title":"concepts","text":""},{"location":"programming/python/concepts/#property-decorator","title":"property decorator","text":"<p>The @property decorator in Python is used to define methods that are accessed like attributes, providing a way to implement computed properties or control access to class attributes. It allows you to define a method that can be accessed as if it were an attribute, without the need to explicitly call it as a method.</p> <pre><code>class Circle:\n    def __init__(self,radius):\n        self.radius = radius\n\n    @property\n    def diameter(self):\n        return self.radius\n\n    @diameter.setter\n    def diameter(self,value):\n        if value &gt; 0:\n             self.radius=value\n\n    @diameter.deleter\n    def diameter(self):\n        del self.radius\n\n    @property\n    def area(self):\n        return 3.14 * self.radius**2\n\nmycircle=Circle(2)\nmycircle.diameter=10  # you can use method as an attribute.\nprint(mycircle.diameter)\n\nprint(\"setter\")\n\nmycircle.diameter=0  \nprint(mycircle.diameter) # Output: 10, since the value is set to 0, it would output the previous value associated in the object.\n\nprint(\"deleter\")\ndel mycircle.diameter\nprint(mycircle.diameter)  # raise an exception as the circle object has deleted the 'radius' attribute\n</code></pre>"},{"location":"programming/python/concepts/#abstract-classes","title":"abstract classes","text":"<p>Abstract classes are those classes that can't be instantiated directly, it would only serve as a blue print. They are always designed to be <code>subclassed</code> and they would contain <code>abstractmethod</code> that must be implemented by their subclass. </p> <pre><code>from abc import ABC, abstractmethod\n\nclass Animal(ABC):\n    @abstractmethod\n    def make_sound(self):\n        pass\n</code></pre> <p>When you are trying to instantiated <code>animal=Animal()</code>, it would throw an error.</p> <p><code>Error: Can't instantiate abstract class Animal with abstract methods make_sound</code></p> <p>Now, you would have an subclass of <code>Animal</code> that uses an <code>abstractmethod</code> </p> <pre><code>class Dog(Animal):\n    def make_sound(self):\n        print(\"Woof\")\n\nclass Cat(Animal):\n    def make_sound(self):\n        print(\"Meow\")\n\ndog=Dog()\ndog.make_sound() # Output: Woof\n\ncat=Cat()\ncat.make_sound() # Output: Meow\n</code></pre>"},{"location":"programming/python/concepts/#first-class-functions","title":"first class functions","text":"<p>If a function can be assigned to a variable or passed as object/variable to other function, that function is called as <code>first class function</code> </p> <p>example-1</p> <pre><code>def square(x):\n    return x*x \n\ndef cube(x):\n    return x*x*x\n\n# Receives a function, executes and returns result. You will provide as an list here\ndef mymap(func,args):\n    result = []\n    for each_item in args:\n        result.append(func(each_item))\n\n    return result \n\nsqr=mymap(square,[1,2,3]) # Output: [1,4,9]\ncub=mymap(cube,[1,2,3]) # Output: [1,8,27]\n\n# provide a single value to get the result.\ndef print_result(x, func):\n    \"\"\"recieve a function and execute it and return result\"\"\"\n    return func(x)\n\ndo_square = square     # assigning square function to a variable\ndo_cube = cube         # assigning cube function to a variable\n\nres = print_result(5, do_square)   # passing function to another function\n</code></pre> <p>example-2</p> <p>We would use first-class function to create an html tag</p> <pre><code>def create_html(tag):\n    def wrap_text(text):\n        print(f\"&lt;{tag}&gt;{msg}/&lt;{tag}&gt;\")\n    return wrap_text\n</code></pre> <p><code>create_html</code> would return the fucntion(<code>wrap_text</code>), we would use that function to call our msg wrapped in the html msg. </p> <pre><code>print_h1=create_html(\"h1\")\nprint_h1(\"Heading1\") # Output: &lt;h1&gt;Heading1&lt;/h1&gt; \n</code></pre>"},{"location":"programming/python/concepts/#decorator","title":"decorator","text":"<p>A decorator is a function that takes another function as an argument and returns a modified version of the function. Decorators are often used to add functionality to functions, such as logging, timing, or error handling.</p> <pre><code>def log_function(func):\n  def wrapper(*args, **kwargs):\n    print(\"Calling function:\", func.__name__)\n    result = func(*args, **kwargs)\n    print(\"Function returned:\", result)\n    return result\n  return wrapper\n\n@log_function\ndef my_function(x, y):\n  return x + y\n\nprint(my_function(1, 2))\n</code></pre> <pre><code>def my_decorator(func):\n    def wrapper():\n        print(\"Something is happening before the function is called.\")\n        func()\n        print(\"Something is happening after the function is called.\")\n    return wrapper\n\n@my_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n</code></pre> <p>A generator is a function that produces a sequence of values, one at a time. Generators are created using the yield keyword. Generators are useful for a variety of tasks, such as filtering a sequence of values, transforming a sequence of values, or iterating over a large sequence of values without storing the entire sequence in memory.</p> <pre><code>def read_file(filename):\n  with open(filename, 'r') as f:\n    for line in f:\n      yield line\n\nfor line in read_file('my_file.txt'):\n  print(line)\n</code></pre> <p></p> <pre><code>def timeit(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(\"Time taken to execute function:\", end - start)\n        return result\n    return wrapper\n\n@timeit\ndef factorial(n):\n  print(\"Without using cache decorator\")\n  return n*factorial(n-1) if n else 1 \n\nprint(factorial(5))\n</code></pre>"},{"location":"programming/python/concepts/#closures","title":"closures","text":"<p>example-1</p> <p>Python closure is a nested function that allows us to access variables of the outer function even after the outer function is closed.</p> <pre><code>def outer_function():\nmessge=\"Hi\"\n\ndef inner_function():\n    print(messge) # you would be able to access the variable of outer function.\n\nreturn inner_function()\n\nhi=outer_function() # Output: Hi\n</code></pre> <p>example-2</p> <p>let's modify the code and we could store the <code>inner_function</code> as variable and then call return value</p> <pre><code>def outer_function(message):\n    messge=message\n\n    def inner_function():\n        print(messge) # you would be able to access the variable of outer function.\n\n    return inner_function\n\nprint_hi=outer_function(\"hi\")\nprint_hello=outer_function(\"hello\")\n\nprint(print_hi.__name__) # Output: inner_function\nprint(print_hello.__name__) # Output: inner_function\n\nprint_hi() # Output: hi\nprint_hello()  # Output: hello\n</code></pre> <p>example-3</p> <p>let's use closure to add and subtract example, additionally while doing so, we could log it in the file <code>example.log</code></p> <pre><code>import logging\nlogging.basicConfig(filename=\"example.log\",level=logging.DEBUG)\n\ndef logger(func):\n    def log_func(*args):\n        logging.info(f\"logging {func.__name__} with arguments {args}\")\n        print(func(*args))\n    return log_func\n\ndef add(x,y):\n    return x+y\n\ndef sub(x,y):\n    return x-y\n\nadd_logger=logger(add)\nsub_logger=logger(sub)\n\nprint(add_logger.__name__) # Output: log_func  \nprint(sub_logger.__name__) # Output: log_func\n\nadd_logger(4,5) # Output: 9\nsub_logger(20,10) # Output: 10\n</code></pre>"},{"location":"programming/python/concepts/#shallow-deep-copy","title":"shallow &amp; deep copy","text":"<p>A shallow copy creates a new object but references the same elements as the original object. In other words, it creates a new container object and fills it with references to the same elements as the original. The references point to the same memory addresses as the original elements. If any of the referenced elements are mutable, changes made to them will be reflected in both the original and the shallow copy.</p> <p>You can think of \"hardlink\" in linux prespective. </p> <pre><code>import copy\n\nlist1 = [1, 2, [3, 4]]\nlist2 = copy.copy(list1)\n\nlist2[0] = 5\nlist2[2].append(5)\n\nprint(list1)  # Output: [1, 2, [3, 4, 5]]\nprint(list2)  # Output: [5, 2, [3, 4, 5]]\n</code></pre>"},{"location":"programming/python/concepts/#usecase-of-shallow-copy","title":"Usecase of shallow copy","text":"<p>Cloning mutable objects: Shallow copy is often used when you want to create a new object that shares the internal state with the original object. This can be useful when working with mutable objects like lists or dictionaries, where you want to create a modified version while preserving the original.</p> <p>Performance optimization: Shallow copy can be more efficient in terms of time and memory when dealing with large objects or data structures. Instead of duplicating the entire object, a shallow copy simply creates references to the existing elements.</p> <p>Nested data structures: Shallow copy is appropriate when you have nested data structures, such as a list of lists or a dictionary of dictionaries. It allows you to create a new structure that maintains references to the original nested objects.</p> <p>Deep Copy:</p> <p>A deep copy creates a new object and recursively copies all the elements from the original object to the new object. It creates a completely independent copy with its own set of elements. Any changes made to the copied elements will not affect the original object.</p> <p>softlink from linux prespective. </p> <pre><code>import copy\n\nlist1 = [1, 2, [3, 4]]\nlist2 = copy.deepcopy(list1)\n\nlist2[0] = 5\nlist2[2].append(5)\n\nprint(list1)  # Output: [1, 2, [3, 4]]\nprint(list2)  # Output: [5, 2, [3, 4, 5]]\n</code></pre>"},{"location":"programming/python/concepts/#usecase-of-deep-copy","title":"usecase of deep copy","text":"<p>Creating independent copies: Deep copy is used when you want to create a completely independent copy of an object and its internal state. This is useful when you need to modify the copied object without affecting the original object.</p> <p>Avoiding unintended side effects: Deep copy ensures that any modifications made to the copied object or its nested elements do not impact the original object. This can be important when dealing with complex data structures or when multiple objects need to maintain their own separate state.</p> <p>Immutable objects: Deep copy is suitable for creating copies of immutable objects, such as strings or tuples, where modifying the object is not possible. Deep copy allows you to create a new object with the same value.</p>"},{"location":"programming/python/concepts/#iter-and-generators","title":"iter and generators","text":"<p>Iterator: An iterator is an object that allows sequential access to elements in a collection (e.g., lists, tuples, sets) without exposing the underlying structure.</p> <p>It operates on the principle of \"lazy evaluation,\" meaning it fetches the next element only when requested. This saves memory and improves performance when dealing with large collections.</p> <p>Iterators use two primary methods: iter: Returns the iterator object itself. next: Retrieves the next element from the collection. If there are no more elements, it raises the StopIteration exception.</p> <p>Iterators are typically used with a for loop or the built-in next() function.</p> <pre><code>my_list = [1, 2, 3, 4]\nmy_iterator = iter(my_list)\n\nprint(next(my_iterator))  # Output: 1\nprint(next(my_iterator))  # Output: 2\nprint(next(my_iterator))  # Output: 3\nprint(next(my_iterator))  # Output: 4\n# print(next(my_iterator)) # Raises StopIteration because no more elements.\n\n</code></pre> <p>Generator:</p> <p>A generator is a special type of iterator that is created using a function with one or more yield statements. It allows you to define an iterative algorithm by suspending the execution state and yielding values one at a time, instead of returning the entire collection at once.</p> <p>Generators are memory-efficient because they produce elements on-the-fly and don't store the entire collection in memory.</p> <p>They are usually implemented using a for loop or by calling the generator function directly.</p> <pre><code>def my_generator():\n    yield 1\n    yield 2\n    yield 3\n    yield 4\n\ngen = my_generator()\n\nprint(next(gen))  # Output: 1\nprint(next(gen))  # Output: 2\nprint(next(gen))  # Output: 3\nprint(next(gen))  # Output: 4\n# print(next(gen)) # Raises StopIteration because there are no more elements.\n</code></pre> <p>iterators are objects that provide sequential access to elements in a collection, whereas generators are a type of iterator that allows you to define a sequence using a function with yield statements, offering memory-efficient and lazy evaluation behavior.</p>"},{"location":"programming/python/concepts/#args-and-kwargs","title":"args and *kwargs","text":"<p>*args allows you to pass a variable number of non-keyword arguments to a function. It is used to handle scenarios where the exact number of arguments is not known in advance.</p> <p>**kwargs allows you to pass a variable number of keyword arguments. It is used when you want to handle named arguments in a function.</p> <pre><code>def my_function(*args, **kwargs):\n    print(\"Arguments:\", args)\n    print(\"Keyword arguments:\", kwargs)\n\nmy_function(1, 2, 3, name=\"John\", age=30)\n</code></pre>"},{"location":"programming/python/concepts/#python-manage-memory","title":"Python manage memory?","text":"<p>Python uses a private heap that stores all objects and data structures. The memory management is handled by Python's memory manager, which ensures that memory is allocated efficiently and that the interpreter doesn't run out of memory. Python also has an in-built garbage collector, which reclaims memory by deallocating objects that are no longer in use. The primary mechanism for garbage collection is reference counting, but Python also uses a cycle detector to deal with reference cycles.</p>"},{"location":"programming/python/concepts/#staticmethod-and-classmethod","title":"@staticmethod and @classmethod","text":"<p>@staticmethod: Defines a method that doesn\u2019t require access to the instance (self) or class (cls). It behaves like a plain function but belongs to the class\u2019s namespace.</p> <p>@classmethod: Defines a method that receives the class (cls) as its first argument instead of the instance (self). It can modify the class state that applies across all instances of the class.</p> <pre><code>class MyClass:\n    @staticmethod\n    def static_method():\n        print(\"This is a static method.\")\n\n    @classmethod\n    def class_method(cls):\n        print(f\"This is a class method of {cls.__name__}\")\n\nMyClass.static_method()\nMyClass.class_method()\n</code></pre>"},{"location":"programming/python/concepts/#is-and","title":"is and ==","text":"<p><code>is</code>: Checks whether two references point to the same object (identity) <code>==</code>: Checks whether the values of two objects are equal (equality).</p>"},{"location":"programming/python/oops/","title":"oops","text":""},{"location":"programming/python/oops/#oops","title":"OOPS","text":""},{"location":"programming/python/oops/#encapsulation-abstraction","title":"Encapsulation &amp; Abstraction","text":"<p>Bundling data and methods within a single unit. when you create a class, it means you are <code>implementing encapsulation</code></p> <p>encapsulation allows us to restrict accessing variables and methods directly and prevent accidental data modification by creating private data members and methods within a class. </p> <p>public : accessable anywhere form outside the class. </p> <p>private : Accessible within the class</p> <p>Protected : within the class and its sub-classes(inheritance)</p>"},{"location":"programming/python/oops/#getters-and-setters","title":"Getters and Setters","text":"<p>getter method to access data members and the setter methods to modify the data members</p> <p>private variables are not hidden fields like in other programming languages. The getters and setters methods are often used when:</p> <p>When we want to avoid direct access to private variables To add validation logic for setting a value</p> <pre><code>class Employee:\n\n    def __init__(self,name,salary,age):\n        # public members\n        self.name = name\n\n        # private members\n        self.__salary=salary\n        self.__age=age\n\n        # protected members\n        self._project=\"Oracle pvt ltd\"\n\n    # getters\n    def get_age(self):\n        print(self.__age)\n\n    # setters\n    def set_age(self,age):\n        # you can use the logic before changing \n        # the object\n        if age &gt; 130:\n            print(\"not sure anyone is alive.. please submit proof of living!\")\n        self.__age = age\n\n    def show(self):\n        print(f\"{self.name}'s age is {self.__age} drawing salry of {self.__salary}\")\n\n\nclass EmployeeDepartment(Employee):\n    def __init__(self,name):\n        Employee().__init__(self)\n        self.name = name\n\n    def show(self):\n        print(f\"{self.name} employee working in project {self._project}\")\n\n\nemp=Employee(\"employee\",100000,33)\nemp.show()\n\nemp.set_age(29) #Private members can be accessed by using public methods.\nemp.get_age()\nprint(emp._Employee__age) #Name mangling.\n\nprint(f\"Employee project details: {emp._project}\")\n\n\nsunil=Employee(\"Sunil\",100000,34)\nsunil.set_age(32)\n</code></pre>"},{"location":"programming/python/oops/#constructors","title":"Constructors","text":"<p>Special method used to create and initialize an object of a class. The primary use of a constructor is to declare and initialize data member/instance variables of a class.</p>"},{"location":"programming/python/oops/#constructor-types","title":"Constructor Types","text":"<ul> <li> <p>Default Constructor - It does not perform any task but initializes the objects</p> </li> <li> <p>Non-parametrized constructor - A constructor without any arguments is called a non-parameterized constructor. </p> </li> <li> <p>parametrized constructor - A constructor with defined parameters or arguments is called a parameterized constructor.</p> </li> </ul> <pre><code>class Employee:\n    # class variable \n    count=0\n    # Parameterized Constructor with default values\n    def __init__(self,name=\"Raghu\",age=12):\n        self.name = name\n        self.age=age\n        Employee.count+=1\n\n    # Non Parametrized Constructor.\n    # def __init__(self):\n    #    self.name=\"Raju\"\n    #    self.age=18\"\n\n    # when there is no __init__ method, its default constructor\n\n    def display(self):\n        print(f\"employee {self.name} is around {self.age} years old\")\n\n\nemp1=Employee(\"Ram\",18)\nemp1.display() # Output: employee Ram is around 18 years old\n\n# Object created with default values\nemp2=Employee()\nemp2.display() # Output: employee Raghu is around 12 years old\n\nprint(f\"Total number of employees: {Employee.count})\n</code></pre>"},{"location":"programming/python/oops/#constructor-chaining","title":"Constructor Chaining","text":"<p>Constructor chaining is the process of calling one constructor from another constructor. Constructor chaining is useful when you want to invoke multiple constructors, one after another, by initializing only one instance. constructor chaining is convenient when we are dealing with inheritance.</p> <pre><code>class Vehicle:\n    def __init__(self,engine):\n        self.engine = engine \n\nclass Car(Vehicle): \n    def __init__(self,engine, max_speed):\n        super().__init__(engine)\n        self.max_speed=max_speed\n\nclass ElectricCar(Car):\n    def __init__(self,engine,max_speed,km):\n        super().__init__(engine,max_speed)\n        self.km=km\n\nev = ElectricCar('1500cc', 240, 750)\nprint(f\"{ev.engine} is having max speed {ev.max_speed} travelling distance of {ev.km}\")\n</code></pre>"},{"location":"programming/python/oops/#polymorphism","title":"Polymorphism","text":"<p>Polymorphism in Python is the ability of an object to take many forms. Polymorphism is mainly used with inheritance. </p>"},{"location":"programming/python/oops/#polymorphism-with-inheritance","title":"Polymorphism With Inheritance","text":"<p>Using method overriding <code>polymorphism</code> allows us to defines methods in the child class that have the same name as the methods in the parent class. This <code>process of re-implementing the inherited method in the child class</code> is known as Method Overriding.</p> <pre><code>class Vehicle:\n    def __init__(self,name,color):\n        self.name = name\n        self.color = color\n\n    def show(self):\n        print(f\"{self.name} is having color of {self.color}\")\n\n\n    def max_speed(self):\n        print(\"Vehicle max speed is 150\")\n\n    def change_gear(self):\n        print('Vehicle change 6 gear')\n\nclass Car(Vehicle):\n\n    # due to polymorphism, max_speed() and change_gear() \n    # methods are overridden for the car object.\n\n    def max_speed(self):\n        print(\"car max speed is 250\")\n\n    def change_gear(self):\n        print('car change 7 gear')\n\n\n\ncar = Car('Benz', 'Red')\ncar.show() # this method isin't overridden.\ncar.max_speed()\n\nvehicle = Vehicle('Volvo', 'Yello')\nvehicle.show()\nvehicle.max_speed()\n</code></pre>"},{"location":"programming/python/oops/#polymorphism-class-methods","title":"Polymorphism class methods","text":"<pre><code>class StudentASection:\n    def students(self):\n        print(\"A section students\")\n\n    def class_teacher(self):\n        print(\"Mohan\")\n\nclass StudentBSection:\n    def students(self):\n        print(\"B section students\")\n\n    def class_teacher(self):\n        print(\"Mohana\")\n\na_section_student=StudentASection()\nb_section_student=StudentBSection()\n\n# you can create function and pass as objects\n#def some_function(obj):\n#    obj.call_methods\n\nfor students in (a_section_student,b_section_student):\n    students.students()\n    students.class_teacher()\n</code></pre> <p>You could also pass as a single object to the above <code>objects into the function</code></p> <pre><code>def get_student_details(obj):\n    obj.students()\n    obj.class_teacher()\n\nget_student_details(a_section_student)\nget_student_details(b_section_student)\n</code></pre>"},{"location":"programming/python/oops/#class-static-methods","title":"Class &amp; static methods","text":"<p><code>class and static methods</code> are special types of methods that have different behaviours and use cases when compared to <code>regular instances methods</code></p>"},{"location":"programming/python/oops/#regularinstance-method","title":"regular/instance method","text":"<p>these are bound to regular object instance. they can access and modify the object's state and can be called on an object instance. </p> <pre><code>class Person:\n    def __init__(self, name, age):\n        # instance variables\n        self.name = name\n        self.age=age \n\n    # instance methods\n    def get_name(self):\n        return self.name\n\n    # instance methods\n    def get_age(self):\n        return self.age\n\n    # instance methods\n    def set_name(self,newname):\n        self.name = newname\n\n    # instance methods\n    def set_age(self, newage):\n        self.age = newage\n\n\nperson1 = Person(\"Sunil\", 30)   \nprint(person1.get_name())\nprint(person1.get_age())\nperson1.set_name(\"Kumar\")\nprint(person1.get_name())\n</code></pre>"},{"location":"programming/python/oops/#class-methods","title":"class methods","text":"<p>class methods are bound to class itself and they can access only class variables. It can only allow to change the class variable state across all the class objects.</p> <p>Class methods are used when we are dealing with factory methods. factory methods are those which returns the class object for different purposes.</p> <p>They are always called using <code>ClassName.method_name()</code></p> <p></p> <pre><code>from datetime import date \n\nclass Student:\n    collage_name = \"ABC Collage\"\n    def __init__(self,name,age):\n        self.name = name\n        self.age = age \n\n    @classmethod\n    def calculate_age(cls,name,birthyear):\n        # calulate age and set it as age, then \n        # return a new object\n        return cls(name,date.today().year-birthyear)\n\n\n    def show(self):\n        print(f\"{self.name} age is: {self.age} studying in colleage: {Student.collage_name}\")\n\n\nsunil = Student(\"Sunil\",39)\nsunil.show()\n\nshiva=Student.calculate_age(\"Shiva\",1983) # invoke a new clas\nshiva.show()\n</code></pre> <p>Explanation of above code. </p> <ul> <li> <p>we created two objects, one using the constructor and the second using the <code>calculate_age()</code></p> </li> <li> <p>The constructor takes two arguments name and age. On the other hand, class method takes cls, name, and birth_year and <code>returns a class instance which nothing but a new object</code></p> </li> <li> <p>The <code>@classmethod</code> decorator is used for converting <code>calculate_age() method to a class method.</code></p> </li> <li> <p>The <code>calculate_age()</code> method takes Student class (cls) as a first parameter and <code>returns constructor by calling Student(name, date.today().year - birthYear), which is equivalent to Student(name, age).</code></p> </li> </ul>"},{"location":"programming/python/oops/#static-method","title":"static method","text":"<p>Any method we create in a class will automatically be created as an instance method. We must explicitly tell Python that it is a static method using the <code>@staticmethod</code></p> <pre><code>class Student:\n    @staticmethod\n    def student_greeting(greeting_msg):\n        print(f\"Hello {greeting_msg}\")\n\n\nStudent.student_greeting(\"Welcome Students !\")\n\nsunil=Student()\nsunil.student_greeting('Welcome Sunil !')\n</code></pre> <pre><code>class Employee:\n    def __init__(self,name,project_name):\n        self.name = name \n        self.project_name=project_name\n\n    # instance method\n    def work(self):\n        requirements = self.gather_requirements(self.project_name)\n        for task in requirements:\n            print(f\"Completed: {task}\")\n\n\n    @staticmethod\n    def gather_requirements(project_name):\n        if project_name == \"ABC\":\n            requirements = [\"task1\", \"task2\"]\n        else:\n            requirements = [\"task1\"]\n\n        return requirements\n\nemp = Employee(\"Sunil\",\"ABC\")\nemp.work()\n</code></pre> <p>output:</p> <pre><code>Completed: task1\nCompleted: task2\n</code></pre>"},{"location":"programming/python/oops/#access-class-variables-in-class-methods","title":"Access Class Variables in Class Methods","text":"<pre><code>@classmethod\ndef change_collage(cls, newcollname):\n    # change class variable\n    cls.collage_name=newcollname\n\nStudent.change_collage(\"New coll\")\nsunil.show()\n</code></pre>"},{"location":"programming/python/oops/#another-example-of-static-and-class-methods","title":"another example of static and class methods","text":""},{"location":"programming/python/oops/#classmethod","title":"classmethod","text":"<p>A class method is a method that takes the class itself as the first argument, rather than an instance of the class. It is useful when you want to perform operations that involve the class itself (for example, modifying class variables or calling other class methods). Class methods are defined using the @classmethod decorator, and the first argument is conventionally named cls.</p> <p>When to use @classmethod: When you need to work with the class as a whole (like creating factory methods, manipulating class-level data, or altering class attributes). When you want to create alternative constructors. When the logic is related to the class, not specific instances.</p> <pre><code>class Server:\n    server_count = 0  # Class variable to track how many servers exist\n\n    def __init__(self, name, ip):\n        self.name = name\n        self.ip = ip\n        Server.server_count += 1\n\n    @classmethod\n    def create_default_server(cls):\n        return cls(\"DefaultServer\", \"192.168.0.1\")\n\n# Example usage\nserver1 = Server(\"MainServer\", \"192.168.1.1\")\nserver2 = Server.create_default_server()  # Using class method to create an instance\n\nprint(server1.name)  # Output: MainServer\nprint(server2.name)  # Output: DefaultServer\nprint(Server.server_count)  # Output: 2\n\n</code></pre>"},{"location":"programming/python/oops/#staticmethod","title":"staticmethod","text":"<p>A static method does not receive any special first argument (neither self nor cls). It behaves like a regular function but belongs to the class's namespace. You use a static method when the method's logic neither depends on the instance nor the class, but you want to group it inside the class for logical reasons.</p> <p>When to use @staticmethod:</p> <p>When you need utility functions that are logically related to the class but don't need access to instance (self) or class (cls) data. When the method doesn't modify class or instance state.</p> <pre><code>class Server:\n\n    def __init__(self, name, ip):\n        self.name = name\n        self.ip = ip\n\n    @staticmethod\n    def is_valid_ip(ip):\n        parts = ip.split(\".\")\n        return len(parts) == 4 and all(0 &lt;= int(part) &lt; 256 for part in parts)\n\n# Example usage\nprint(Server.is_valid_ip(\"192.168.1.1\"))  # Output: True\nprint(Server.is_valid_ip(\"999.999.999.999\"))  # Output: False\n\n</code></pre> <p>Summary:</p> <p>Class Method (@classmethod): Use when you need to operate on the class itself. The method has access to the class via cls.</p> <p>Static Method (@staticmethod): Use when the logic doesn't need to access the instance (self) or class (cls). It is more like a utility function within the class.</p>"},{"location":"programming/python/qa/","title":"Q&A","text":""},{"location":"programming/python/qa/#strings","title":"Strings","text":"<p>my_string = \"In 2010, someone paid 10k Bitcoin for two pizzas.\"</p> <pre><code>print(my_string[-1]) # get the last character in the string.\nprint(my_string[7]) # return the comma character from the string.\nprint(my_string.index(\"B\")) # index of the B character in the string.\nprint(my_string.count('o')) # number of occurrences of the letter o in the string.\nprint(my_string.upper()) # convert all letters in the string to uppercase.\nprint(my_string.find('Bitcoin')) # index at which the substring Bitcoin starts.\nprint(my_string.index('Bitcoin')) # index at which the substring Bitcoin starts.\nprint(my_string.startswith('X')) # check of the string starts with the letter X\nprint(my_string.swapcase()) # convert all uppercase letters to lowercase and viceversa\nprint(my_string.replace(\" \", \"\")) # remove all spaces from the string\nprint(\"&amp;\".join(my_string)) # join the characters of the string using the &amp; symbol as a delimiter.\nprint(my_string.title()) # convert the first letter of each word in the string to uppercase.\nprint(my_string[::7]) # return every 7th character of the string, starting with the first character.\nprint(my_string[10::]) # return the string except the first 10 characters\nprint(my_string[:-4]) # return the string except the last 4 characters\nprint(my_string[-9::]) # return the last 9 characters of the string\nprint(my_string[:12]) # return the first 12 characters in the string\nmy_other_string = \"Poor guy!\"\nprint(my_string+my_other_string) # concatenate strings\n</code></pre>"},{"location":"programming/python/qa/#lists","title":"lists","text":"<pre><code># array[start:end:step_count]\n\na = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n#i= [ 0 ,  1 ,  2 ,  3 ,  4 ,  5 ,  6 ,  7 ]\n#r= [-8 , -7 , -6 , -5 , -4 , -3 , -2 , -1 ]\nprint('Middle two:  ', a[3:5])\nprint('All but ends:', a[1:7])\na[:]      # ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\na[:5]     # ['a', 'b', 'c', 'd', 'e']\na[:-1]    # ['a', 'b', 'c', 'd', 'e', 'f', 'g']\na[4:]     #                     ['e', 'f', 'g', 'h']\na[-3:]    #                          ['f', 'g', 'h']\na[2:5]    # ['c', 'd', 'e']\na[2:-1]   # ['c', 'd', 'e', 'f', 'g']\na[-3:-1]  # ['f', 'g']\na[::2]    # ['a','c', 'e', 'g'] # even numbers ::2 means \u201cSelect every second item \n          # starting at the beginning.\na[1::2]   # ['b','d', 'f', 'h'] # odd numbers \na[::-2]   # ['h', 'f', 'd', 'b'] # ::-2 means \u201cSelect every second item starting at the \n          # end and moving backward.\na[2::2]     # ['c', 'e', 'g']\na[-2::-2]   # ['g', 'e', 'c', 'a']\na[-2:2:-2]  # ['g', 'e']\na[2:2:-2]   # []\n</code></pre> <p>Given the code below, use the correct function on line 3 in order to find out the largest number in my_list.</p> <pre><code>my_list = [10, 10.5, 20, 30, 25.6, 19.25, 11.01, 29.99]\n\nprint(sorted(my_list)) # ascending sort\nprint(sorted(my_list), reverse=True) # descending sort\nprint(sorted(my_list)[-1]) # largest element\nprint(sorted(my_list)[0]) # smallest element\nprint(sum(my_list)) # sum of elements \nprint(set(my_list)) # remove duplicates\nprint(my_list.clear()) # delete all elemenents in list\n</code></pre> <ul> <li>add the elements of [30.01, 30.02, 30.03] to my_list and multiply the resulting list by 2.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 25.6, 19.25, 11.01, 29.99]\nadd = (my_list+[30.01, 30.02, 30.03])*2\nprint(add)\n</code></pre> <ul> <li>return the element 20 from my_list based on its index.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 'Python', 'Java', 'Ruby']\n\nelement = my_list[my_list.index(20)]\nor\nmy_list[2]\n\nprint(element)\n</code></pre> <ul> <li>return a slice made of [30, 'Python', 'Java'] from my_list based on negative indexes.</li> </ul> <pre><code>my_list = [10, 10.5, 20, 30, 'Python', 'Java', 'Ruby']\n\nmy_slice = my_list[-4:-2] # [30, 'Python', 'Java']\nmy_slice=my_list[-4:] # [30, 'Python', 'Java', 'Ruby']\nmy_slice=my_list[3:] # [30, 'Python', 'Java', 'Ruby']\nmy_slice=my_list[3:] # [10, 10.5, 20]\nmy_slice=my_list[:-4] # [10, 10.5, 20]\nmy_slice = my_list[0:5] # [10, 10.5, 20, 30, 'Python']\nmy_slice = my_list[0::3] # every 3rd element [10, 30, 'Ruby']\nmy_slice = my_list[-1::-4] # every 4th element from last element #['Ruby', 20]\nmy_slice = my_list[2:5] # consecutive element [20, 30, 'Python']\n</code></pre> <ul> <li>merge two lists</li> </ul> <pre><code>a = [3, 4, 6, 10, 11, 18]\nb = [1, 5, 7, 12, 13, 19, 21]\na.extend(b)\n\n# remove duplicate and merge two lists\nsorted(list(set(a+b)))\n\n# print common elemenents\nprint(set(a)&amp;set(b))\n</code></pre> <ul> <li>sort list by lengths</li> </ul> <pre><code>newlist=[\"sunil\",\"kua\",\"kumar\",\"ku\",\"kumaraswamy\",\"ramaswamy\",\"ramaswamykumaraswamy\"]\n\ndef sort_list_by_length(list):\n    list.sort(key=len)\n    return list\n\n# revese sort by length\ndef sort_list_by_length(list):\n    list.sort(key=len, reverse=True)\n    return list\n</code></pre>"},{"location":"programming/python/qa/#sets","title":"sets","text":"<p>my_set = {1, 4, 6, 5, 9, 0, 8, 3, 2, 7, 11}</p> <pre><code>my_set.add(19) # add element\nmy_set.remove(19) # delete element\n</code></pre> <pre><code>my_set1 = {1, 4, 6, 5, 9, 0, 8, 3, 2, 7, 11}\nmy_set2 = {12, 9, 4, 2, 0, 6}\ncommon = my_set1.intersection(my_set2)  # Common elements \njoin = my_set1.union(my_set2) # joining\n</code></pre> <p>find out the elements of my_set2 that are not members of my_set1.</p> <pre><code>diff = my_set2.difference(my_set1)\n</code></pre>"},{"location":"programming/python/qa/#tuples","title":"tuples","text":"<pre><code>my_tup = (\"Romania\", \"Poland\", \"Estonia\", \"Bulgaria\", \"Slovakia\", \"Slovenia\", \"Hungary\")\n\nnumber = my_tup.len(my_tup) # count of tuples\nindex = my_tup.index('Slovakia') # find out the index of Slovakia in my_tup.\nlast = max(my_tup) # find out the last element of my_tup in alphabetical order.\nnumber = my_tup.count('Estonia') # find out the number of occurrences of Estonia in my_tup.\nmy_slice = my_tup[2:] # return all the elements of my_tup, except the first two of them\nmy_slice = my_tup[-5::] # from negative index, return all the elements of my_tup, except the first two of them\n\nprint(number)\n</code></pre>"},{"location":"programming/python/qa/#ranges","title":"ranges","text":"<pre><code>print(list(range(10))) #[0,1,2..10]\nprint(list(range(0,10)))  #[0,1,2..10]\nprint(list(range(10,22,3))) # [10, 13, 16, 19] step function increase by 3\nmy_range = range(115, 125, 5) # return [115, 120] when converted to a list.\nmy_range = range(-75, -25,15 ) # return [-75, -60, -45, -30] \nmy_range = range(-25, 139, 30) # return [-25, 5, 35, 65, 95, 125]\nmy_range = range(-10,-9) # return [-10]\n</code></pre>"},{"location":"programming/python/qa/#dictionaries","title":"dictionaries","text":"<pre><code>crypto = {1: \"Bitcoin\", 2: \"Ethereum\", 3: \"Litecoin\", 4: \"Stellar\", 5: \"XRP\"}\n\ncrypto.pop(3) #  delete the key-value pair associated with key 3\ndel crypto[3] #  delete the key-value pair associated with key 3\nadd = sum(crypto) # sum of all the keys in the dictionary.\nval = crypto.values() # get a list of all the values in the dictionary.\nkey = min(crypto) #smallest key in the dictionary.\ncrypto.popitem() # arbitrary key-value pair from the dictionary.\n</code></pre>"},{"location":"programming/python/qa/#data-types","title":"data types","text":"<pre><code>value = 10\nconv = bin(value) # convert value to a binary representation\nconv = hex(value) # hexadecimal\nconv = int(value,2) # decimal\n</code></pre>"},{"location":"programming/python/qa/#conditions","title":"conditions","text":"<p>write code that prints out True! if x is a string and the first character in the string is T</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\nif type(x)==str and x[0] == 'T':\n    print(\"True!\")\n</code></pre> <p>write code that prints out True! if at least one of the following conditions occurs: - the string contains the character z - the string contains the character y at least twice</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif \"z\" in x or x.count(\"y\") &gt;= 2:\nprint(\"True!\")\n</code></pre> <p>write code that prints out True! if the index of the first occurrence of letter f is less than 10 and prints out False!</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif x.index('f') &lt; 10:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the last 3 characters of the string are all digits and prints out False!</p> <pre><code>x = \"The days of Python 2 are almost over. Python 3 is the king now.\"\n\nif x[-3:].isdigit():\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if x has at least 8 elements and the element positioned at index 6 is a floating-point number and prints out False! </p> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif len(x) &gt;= 8 and type(x[6]) is float:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the second string of the first list in x ends with the letter h and the first string of the second list in x also ends with the letter h, and prints out False! </p> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x[3][1].endswith(\"h\") and x[7][0].endswith(\"h\"):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if one of the following two conditions are satisfied and prints out False! otherwise.</p> <ul> <li>the third string of the first list in x ends with the letter h</li> <li>the second string of the second list in x also ends with the letter h</li> </ul> <pre><code>x = [115, 115.9, 116.01, [\"length\", \"width\", \"height\"], 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x[3][2].endswith('h') or x[7][1].endswith('h'):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the largest value among the first 3 elements of the list is less than or equal to the smallest value among the next 3 elements of the list. Otherwise, print out False!</p> <pre><code>x = [115, 115.9, 116.01, 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif max(x[:3]) &lt;= min(x[3:6]):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if 115 appears at least once inside the list or if it is the first element in the list. Otherwise, print out False!</p> <pre><code>x = [115, 115.9, 116.01, 109, 115, 119.5, [\"length\", \"width\", \"height\"]]\n\nif x.count(115) &gt;= 1 or x.index(115) == 0:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the value associated with key number 5 is Perl or the number of key-value pairs in the dictionary divided by 5 returns a remainder less than 2. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[5] == \"Perl\" or len(x) % 5 &lt; 2:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if 3 is a key in the dictionary and the smallest value (alphabetically) in the dictionary is C#. Otherwise, print out False!</p> <p>```python  x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}</p> <p>if 3 in x and sorted(x.values())[0] == \"C#\":     print(\"True!\") else:     print(\"False!\")  ```</p> <p>write code that prints out True! if the last character of the largest (alphabetically) value in the dictionary is n. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sorted(x.values())[-1][-1] == \"n\":\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the largest key in the dictionary divided by the second largest key in the dictionary returns a remainder equal to the smallest key in the dictionary. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sorted(x.keys())[-1] % sorted(x.keys())[-2] == sorted(x.keys())[0]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the sum of all the keys in the dictionary is less than the number of characters of the string obtained by concatenating the values associated with the first 5 keys in the dictionary. Otherwise, print out False!</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif sum(x) &lt; len(x[1] + x[2] + x[3] + x[4] + x[5]):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the 3rd element of the first range is less than 2, prints out False! if the 5th element of the first range is 5, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[0][2] &lt; 2:\n    print(\"True!\")\nelif x[0][4] == 5:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the 3rd element of the 3rd range is less than 6, prints out False! if the 1st element of the second range is 5, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[2][2] &lt; 6:\n    print(\"True!\")\nelif x[1][0] == 5:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the last element of the first range is greater than 3, prints out False! if the last element of the second range is less than 9, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif x[0][-1] &gt; 3:\n    print(\"True!\")\nelif x[1][-1] &lt; 9:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the length of the first range is greater than or equal to 5, prints out False! if the length of the second range is 4, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif len(x[0]) &gt;= 5:\n    print(\"True!\")\nelif len(x[1]) == 4:\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the sum of all the elements of the first range is greater than the sum of all the elements of the third range, prints out False! if the largest element of the second range is greater than the largest element of the third range, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif sum(x[0]) &gt; sum(x[2]):\n    print(\"True!\")\nelif max(x[1]) &gt; max(x[2]):\n    print(\"False!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the largest element of the first range minus the second element of the 3rd range is equal to the first element of the first range, prints out False! if the length of the first range minus the length of the 2nd range is equal to the first element of the 3rd range, prints out Maybe! if the sum of all the elements of the 3rd range divided by 2 returns a remainder of 0, and prints out None! otherwise.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif max(x[0]) - x[2][1] == x[0][0]:\n    print(\"True!\")\nelif len(x[0]) - len(x[1]) == x[2][0]:\n    print(\"False!\")\nelif sum(x[2]) % 2 == 0:\n    print(\"Maybe!\")\nelse:\n    print(\"None!\")\n</code></pre> <p>write code that prints out True! if the sum of the last 3 elements of the first range plus the sum of the last 3 elements of the 3rd range is equal to the sum of the last 3 elements of the 2nd range, and prints out False! if the length of the first range times 2 is less than the sum of all the elements of the 3rd range.</p> <pre><code>x = [list(range(5)), list(range(5,9)), list(range(1,10,3))]\n\nif sum(x[0][-3:]) + sum(x[2][-3:]) == sum(x[1][-3:]):\n    print(\"True!\")\nelif len(x[0]) * 2 &lt; sum(x[2]):\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the 2nd character of the value at key 1 is also present in the value at key 4, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[1][1] in x[4]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the second to last character of the value at key 3 is the first character of the value at key 5, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif x[3][-2] == x[5][0]:\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre> <p>write code that prints out True! if the number of characters of the smallest value in the dictionary is equal to the number of occurrences of letter a in the value at key 3, and prints out False! otherwise.</p> <pre><code>x = {1: \"Python\", 2: \"Java\", 3: \"Javascript\", 4: \"Ruby\", 5: \"Perl\", 6: \"C#\", 7: \"C++\"}\n\nif len(min(x.values())) == x[3].count(\"a\"):\n    print(\"True!\")\nelse:\n    print(\"False!\")\n</code></pre>"},{"location":"programming/python/qa/#loops","title":"loops","text":"<p>Write a for loop that iterates over the x list and prints out all the elements of the list in reversed order and multiplied by 10.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\nfor i in sorted(x,reverse=True):\n    print(i*10)\n</code></pre> <p>Write a for loop that iterates over the x list and prints out all the elements of the list divided by 2 and the string Great job! after the list is exhausted.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\nfor i in x:\n    print(i / 2)\nelse:\n    print(\"Great job!\")\n</code></pre> <p>Write a for loop that iterates over the x list and prints out the index of each element.</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor index,item in enumerate(x):\n    print(index)\n\nx = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor i in x:\n    print(x.index(i))\n</code></pre> <p>Write a while loop that prints out the value of x squared while x is less than or equal to 5. Be careful not to end up with an infinite loop!</p> <pre><code>x = 0\n\nwhile x &lt;= 5:\n    print(x ** 2)\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the value of x times 10 while x is less than or equal to 4 and then prints out Done! when x becomes larger than 4. Be careful not to end up with an infinite loop!</p> <pre><code>x = 0\n\nwhile x &lt;= 4:\n    print(x * 10)\n    x = x + 1\nelse:\n    print(\"Done!\")\n</code></pre> <p>Write a while loop that prints out the value of x plus 10 while x is less than or equal to 15 and the remainder of x divided by 5 is 0. Be careful not to end up with an infinite loop!</p> <pre><code>x = 10\n\nwhile x &lt;= 15 and x % 5 == 0:\n    print(x + 10)\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the absolute value of x while x is negative. Be careful not to end up with an infinite loop!</p> <pre><code>x = -7\n\nwhile x &lt; 0:\n    print(abs(x))\n    x = x + 1\n</code></pre> <p>Write a while loop that prints out the value of x times y while x is greater than or equal to 5 and less than 10, and prints out the result of x divided by y when x becomes 10. Be careful not to end up with an infinite loop!</p> <pre><code>x = 5\ny = 2\n\nwhile x &gt;= 5 and x &lt; 10:\n    print(x * y)\n    x = x + 1\nelse:\n    print(x / y)\n</code></pre> <p>Write code that will iterate over the x list and multiply by 10 only the elements that are greater than 20 and print them out to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [10, 12, 13, 14, 17, 19, 21, 22, 25]\n\nfor i in x:\n    if i &gt; 20:\n        print(i * 10)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6]\ny = [5, 10]\n\nfor i in x:\n    for j in y:\n        print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y that is less than 12, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if j &lt; 12:\n            print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x that is greater than 5 with each element of y that is less than 12, also printing the results to the screen.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if i&gt; 5 and j &lt; 12:\n            print(i * j)\n</code></pre> <p>Write code that will iterate over the x and y lists and multiply each element of x with each element of y that is less than or equal to 10, also printing the results to the screen. For y's elements that are greater than 10, multiply each element of x with y squared.</p> <p>Hint: use nesting!</p> <pre><code>x = [2, 4, 6, 8]\ny = [5, 10, 15, 20]\n\nfor i in x:\n    for j in y:\n        if j &lt;= 10:\n            print(i * j)\n        else:\n            print(i * j ** 2)\n</code></pre> <p>Write code that will print out each character in x doubled if that character is also inside y.</p> <p>Hint: use nesting!</p> <pre><code>x = \"cryptocurrency\"\ny = \"blockchain\"\n\nfor i in x:\n    if i in y:\n        print(i * 2)\n</code></pre> <p>Write code that will iterate over the range generated by range(9) and for each element that is between 3 and 7 inclusively print out the result of multiplying that element by the second element in the same range.</p> <p>Hint: use nesting!</p> <pre><code>my_range = range(9)\n\nprint(newrange)\nfor i in my_range:\n    if 3 &lt;= i &lt;= 7:\n        print(i * my_range[1])\n</code></pre> <p>Write code that will iterate over the range starting at 1, up to but not including 11, with a step of 2, and for each element that is between 3 and 8 inclusively print out the result of multiplying that element by the last element in the same range. For any other element of the range (outside [3-8]) print Outside!</p> <p>Hint: use nesting!</p> <pre><code>for i in range(1,11,2):\n    if 3 &lt;= i &lt;= 8:\n        print(i * range(1,11,2)[-1])\n    else:\n        print(\"Outside!\")\n</code></pre> <p>Write code that will iterate over the range starting at 5, up to but not including 25, with a step of 5, and for each element that is between 10 and 21 inclusively print out the result of multiplying that element by the second to last element of the same range. For any other element of the range (outside [10-21]) print Outside! Finally, after the entire range is exhausted print out The end!</p> <p>Hint: use nesting!</p> <pre><code>for i in range(5,25,5):\n    if 10 &lt;= i &lt;= 21:\n        print(i * range(5,25,5)[-2])\n    else:\n        print(\"Outside!\")\nelse:\n    print(\"The end!\")\n</code></pre> <p>Write a while loop that prints out the value of x times 11 while x is less than or equal to 11.  When x becomes equal to 10, print out x is 10! Be careful not to end up with an infinite loop!</p> <pre><code>x = 5\n\nwhile x &lt;= 11:\n    if x == 10:\n        print(\"x is 10!\")\n        x = x + 1\n    else:\n        print(x * 11)\n        x = x + 1\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 1 100 20 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n            break\n        print(i)\n    print(j)\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 10 20 2 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n        print(i)\n        break\n    print(j)\n</code></pre> <p>Insert a break statement where necessary in order to obtain the following result:</p> <p>1 1 100 10</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            break\n            print(i * j)\n        print(i)\n    print(j)\n</code></pre> <p>Insert a continue statement where necessary in order to obtain the following result:</p> <p>1 1 100 20 200 100</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            print(i * j)\n            continue\n        print(i)\n    print(j)\n</code></pre> <p>Insert a continue statement where necessary in order to obtain the following result:</p> <p>1 1 100 100</p> <pre><code>x = [1, 2]\ny = [10, 100]\n\nfor i in x:\n    for j in y:\n        if i % 2 == 0:\n            continue\n            print(i * j)\n        print(i)\n    print(j)\n</code></pre>"},{"location":"programming/python/qa/#exceptions","title":"exceptions","text":"<p>Add the necessary clause(s) to the code below so that in case the code under try raises no exceptions then the program prints out the result of the math operation and the string Clean! to the screen.</p> <pre><code>try:\n    print(25 % 5 ** 5 + 5)\nexcept:\n    print(\"Bug!\")\nelse:\n    print(\"Clean!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that no matter if the code under try raises any exceptions or not, then the program prints out the string Result! to the screen.</p> <pre><code>try:\n    print(25 % 0 ** 5 + 5)\nexcept:\n    print(\"Bug!\")\nfinally:\n    print(\"Result!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that in case the code under try raises the ZeroDivisionError exception then the program prints out the string Zero! to the screen; additionally, if the code under try raises the IndexError exception then the program prints out the string Index! to the screen.</p> <pre><code>x = [1, 9, 17, 32]\n\ntry:\n    print(x[3] % 3 ** 5 + x[4])\nexcept ZeroDivisionError:\n    print(\"Zero!\")\nexcept IndexError:\n    print(\"Index!\")\n</code></pre> <p>Add the necessary clause(s) to the code below so that in case the code under try raises no exceptions then the program prints out the result of the math operation and the string Clean! to the screen. If the code under try raises the ZeroDivisionError exception then the program prints Zero! to the screen. Ultimately, regardless of the result generated by the code under try, the program should print out Finish! to the screen.</p> <pre><code>try:\n    print(25 % 5 ** 5 + 5)\nexcept ZeroDivisionError:\n    print(\"Zero!\")\nelse:\n    print(\"Clean!\")\nfinally:\n</code></pre>"},{"location":"programming/python/qa/#functions","title":"functions","text":"<p>Implement a function called my_func() that takes a single parameter x and multiplies it with each element of range(5), also adding each multiplication result to a new (initially empty) list called my_new_list. Finally, the list should be printed out to the screen after the function is called.</p> <pre><code>def my_func(x):\n    my_new_list = []\n    for i in range(5):\n        my_new_list.append(i * x)\n    return my_new_list\n\nresult = my_func(2)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single parameter x (a tuple) and for each element of the tuple that is greater than 4 it raises that element to the power of 2, also adding it to a new (initially empty) list called my_new_list. Finally, the code returns the result when the function is called.</p> <pre><code>def my_func(x):\n    my_new_list = []\n    for i in x:\n        if i &gt; 4:\n            my_new_list.append(i ** 2)\n    return my_new_list\n\nresult = my_func((2, 3, 5, 6, 4, 8, 9))\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single parameter x (a dictionary) and multiplies the number of elements in the dictionary with the largest key in the dictionary, also returning the result when the function is called.</p> <pre><code>def my_func(x):\n    return len(x) * sorted(x.keys())[-1]\n\nresult = my_func({1: 3, 2: 3, 4: 5, 5: 9, 6: 8, 3: 7, 7: 0})\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single positional parameter x and a default parameter y which is equal to 10 and multiplies the two, also returning the result when the function is called.</p> <pre><code>def my_func(x, y = 10):\n    return x * y\n\nresult = my_func(5)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a single positional parameter x and two default parameters y and z which are equal to 100 and 200 respectively, and adds them together, also returning the result when the function is called</p> <pre><code>def my_func(x, y = 100, z = 200):\n    return x + y + z\n\nresult = my_func(50)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes two default parameters x (a list) and y (an integer), and returns the element in x positioned at index y, also printing the result to the screen when called.</p> <pre><code>def my_func(x: list, y:int):\n    return x[y]\n\nresult = my_func(list(range(2,25,2)), 4)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a positional parameter x and a variable-length tuple of parameters and returns the result of multiplying x with the second element in the tuple, also returning the result when the function is called.</p> <pre><code>def my_func(x, *args):\n    return x * args[1]\n\nresult = my_func(5, 10, 20, 30, 50)\nprint(result)\n</code></pre> <p>Implement a function called my_func() that takes a positional parameter x and a variable-length dictionary of (keyword) parameters and returns the result of multiplying x with the largest value in the dictionary, also returning the result when the function is called.</p> <pre><code>def my_func(x, **kwargs):\n    return x * sorted(kwargs.values())[-1]\n\nresult = my_func(10, val1 = 10, val2 = 15, val3 = 20, val4 = 25, val5 = 30)\nprint(result)\n</code></pre> <p>Write code that will import only the pi variable from the math module and then it will format it in order to have only 4 digits after the floating point. Of course, print out the result to the screen using the print() function.</p> <pre><code>from math import pi\n\nprint(\"%.4f\" % pi)\n</code></pre>"},{"location":"programming/python/qa/#files","title":"files","text":""},{"location":"programming/python/qa/#regular-expressions","title":"regular expressions","text":"<p>s = \"Bitcoin was born on Jan 3rd 2009 as an alternative to the failure of the current financial system. In 2017, the price of 1 BTC reached $20000, with a market cap of over $300B.\"</p> <pre><code>import re\n\nresult = re.match(\"Bitcoin\", s) # match 'Bitcoin' at start\nresult = re.match(r\"B.{6} .{3}\", s) # match 'Bitcoin' using dot syntax\nresult = re.match(\"Bitcoin\", s, re.I) # match 'Bitcoin' at start of str ignore case\nresult = re.search(r\"(\\d{4})\\s\", s) # match the year `2009`\nresult = re.search(r\"(\\d{4}),\", s) # search 2017 \nresult = re.search(r\"(.{3}\\s\\d\\w\\w\\s\\d{4})\\s\", s) # match the date Jan 3rd 2009\nresult = re.search(r\"([A-Z]{3})\", s) # match BTC in the string\nresult = re.search(r\"([0-9]\\s[A-Z]{3})\", s) # match 1 BTC in the string\nresult = re.search(r\"(\\$\\d{5}),\", s) # match $20000\nresult = re.search(r\"(\\$\\d{3}[A-Z])\\.\", s) # match $300B\nresult = re.search(r\"\\s(.{6} .{3} .{2})\\s\", s) # match market cap\nprint(result.group())\n</code></pre> <p>s = \"Bitcoin, Market Cap: $184,073,529,068, Price: $10,259.02, Volume 24h: $15,670,986,269, Circulating Supply: 17,942,600 BTC, Change 24h: 0.10%\"</p> <pre><code>import re\n\nresult = re.search(r\"\\$(\\d{3},[0-9]{3},\\d{3},[0-9]{3}),\", s) #  match 184,073,529,068\nresult = re.search(r\"\\$(\\d{1,3},\\d{1,3}\\.\\d{1,3}),\", s) # match 10,259.02\nresult = re.search(r\"\\s([0-9]{2},[0-9]{3},[0-9]{3}\\s.{3}),\", s) # match 17,942,600 BTC\nresult = re.search(r\"\\s(.{4}\\s\\d\\.\\d\\d%)\", s) # match 24h: 0.10%\n\n# match Volume 24h: $15,670,986,269\nresult = re.search(r\"\\.\\d\\d, (.{1,}:\\s\\$\\d{2,},\\d{2,},\\d{2,},\\d{2,}), \", s)\n\n# match Circulating Supply: 17,942,600 BTC \nresult = re.search(r\"(\\w+ \\w+: \\d{2}.+? [A-Z]{3}), \", s) \n\nresult = re.search(r\",([0-9]{3}\\.[0-9]{2},\\s.)\", s) # match 259.02, V \nresult = re.findall(r\"\\s(\\d{4})\", s) # match all the years\nresult = re.findall(r\"\\d{1,}\", s) # match all the numbers (3, 2009\nresult = re.findall(r\"\\s(\\w{3})\\s\", s) # match all the three-letter words\nresult = re.findall(r\"([A-Z]{1}.+?)\\s\", s) # match all the words starting with an uppercase letter\nresult = re.findall(r\"\\s(o.{1})\\s\", s) # match all the two-letter words starting with the letter o\nresult = re.findall(r\"\\w{8,}\", s) # match all the words that have at least 8 characters\n\n# match all the words starting with a or c and that have at least 3 letters\nresult = re.findall(r\"\\s([ac]\\w{2,})\\s\", s) \n\nresult = re.sub(r\"\\s\\d{4}\", \" XXXX\", s) # replace all the years in the string with XXXX\nprint(result.group(1))\n</code></pre> <p>s = \"Bitcoin was born on Jan 3rd 2009 as an alternative to the failure of the current financial system. In 2017, the price of 1 BTC reached $20000, with a market cap of over $300B. Bitcoin, Market Cap: $184,073,529,068, Price: $10,259.02, Volume 24h: $15,670,986,269, Circulating Supply: 17,942,600 BTC, Change 24h: 0.10%\"</p> <pre><code>import re\n\n# replace each floating-point number in the string (10,259.02 and 0.10) with a dot (.) \nresult = re.sub(r\"\\d{1,},*\\d*\\.\\d{1,}\", \".\", s)\n\n# replace all occurrences of BTC in the string with Bitcoin\nresult = re.sub(r\"[A-Z]{3}\", \"Bitcoin\", s)\n\n# replace all the digits less than or equal to 5 in the string with 8\nresult = re.sub(r\"[0-5]\", \"8\", s)\n\n# replace all the words starting with an uppercase letter or digits greater than or equal to 6 in the string with W\nresult = re.sub(r\"[A-Z]\\w{1,}|[6-9]\", \"W\", s)\n\nprint(result)\n</code></pre>"},{"location":"programming/python/qa/#classes","title":"classes","text":"<p>Write a class called ClassOne starting on line 1 containing:</p> <p>The init method with two parameters p1 and p2. Define the corresponding attributes inside the init method.</p> <p>A method called square that takes one parameter p3 and prints out the value of p3 squared.</p> <pre><code>class ClassOne:\n    def __init__(self,p1:int, p2:int):\n        self.p1=p1\n        self.p2=p2\n\n    def square(p3:int):\n        print(p3**2)\n\np = ClassOne(1, 2)\nprint(type(p))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to access the p1 attribute for the current instance of the class and print its value to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(p.p1)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to call the square() method for the current instance of the class using 10 as an argument and print the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\np.square(10)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to set the value of the p2 attribute to 5 for the current instance of the class, without using a function.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\np.p2 = 5\n\nprint(p.p2)\n</code></pre> <p>Considering the ClassOne class and the p object, write code on lines 11 and 12 in order to set the value of the p2 attribute to 50 for the current instance of the class using a function, and then get the new value of p2, again using a function, and print it out to the screen as well.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nsetattr(p, 'p2', 50)\nprint(getattr(p, 'p2'))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 in order to check if p2 is an attribute of p, using a function, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(hasattr(p, 'p2'))\n</code></pre> <p>Considering the ClassOne class and the p object, write code on line 11 to check if p is indeed an instance of the ClassOne class, using a function, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\np = ClassOne(1, 2)\n\nprint(isinstance(p, ClassOne))\n</code></pre> <p>Considering the ClassOne class, write code starting on line 9 to create a child class called ClassTwo that inherits from ClassOne and also has its own method called times10() that takes a single parameter x and prints out the result of multiplying x by 10.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        print(x * 10)\n\ny = ClassTwo(10, 20)\nprint(y.p1)\n</code></pre> <p>Considering the ClassOne and ClassTwo classes, where the latter is a child of the former, write code on line 15 in order to call the times10() method from the child class having x equal to 45, also printing the result to the screen</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        return x * 10\n\nobj = ClassTwo(15, 25)\n\nprint(obj.times10(45))\n</code></pre> <p>Considering the ClassOne and ClassTwo classes, write code on line 13 to verify that ClassTwo is indeed a child of ClassOne, also printing the result to the screen.</p> <pre><code>class ClassOne(object):\n    def __init__(self, p1, p2):\n        self.p1 = p1\n        self.p2 = p2\n\n    def square(self, p3):\n        print(p3 ** 2)\n\nclass ClassTwo(ClassOne):\n    def times10(self, x):\n        return x * 10\n\nprint(issubclass(ClassTwo, ClassOne))\n</code></pre>"},{"location":"programming/python/qa/#other-concepts","title":"other concepts","text":"<p>Write a list comprehension on line 1 that will iterate over range(1, 5) and return a list of its elements.</p> <pre><code>cph = [i for i in range(1, 5)]\nprint(cph)\n</code></pre> <p>Write a list comprehension on line 1 that will iterate over range(1, 15, 5) and return a list of its elements squared.</p> <pre><code>cph = [i**2 for i in range(1,15,5)]\nprint(cph)\n</code></pre> <p>Write a list comprehension on line 1 that will iterate over range(5, 25, 3) and return a list of its elements squared only for the elements that are less than or equal to 16.</p> <pre><code>cph = [i ** 2 for i in range(5, 25, 3) if i &lt;= 16]\n\nprint(cph)\n</code></pre> <p>Write a dictionary comprehension on line 1 that will iterate over range(9) and return a dictionary of key-value pairs where the value is equal to the key times 3.</p> <pre><code>cph = {x: x * 3 for x in range(9)}\n\nprint(cph)\n</code></pre> <p>Write a set comprehension on line 1 that will iterate over range(10, 19) and return a set of its elements divided by 2.5.</p> <pre><code>cph = {x / 2.5 for x in range(10, 19)}\n\nprint(cph)\n</code></pre> <p>Write a lambda function on line 1 that takes two parameters x and y and multiplies x with y.</p> <pre><code>lam = lambda x, y: x * y\n\nprint(lam(2, 5))\n</code></pre> <p>Write a lambda function on line 1 that takes a list list1 as a parameter, and multiplies each element of range(1, 5) with each element of list1 using a list comprehension.</p> <pre><code>lam = lambda list1: [x * y for x in range(1, 5) for y in list1]\n\nprint(lam([1, 2]))\n</code></pre> <p>Use the correct function from the itertools module on line 6, in between the parentheses of list(), in order to concatenate list1 and list2.</p> <pre><code>import itertools\n\nlist1 = [1, 2, 3]\nlist2 = [4, 5]\n\nresult = list(itertools.chain(list1, list2))\n\nprint(result)\n</code></pre> <p>Use the correct function from the itertools module with a for loop and a nested if/else block in order to return all the numbers starting at 20 and up to 31 with a step of 2. Be careful not to end up with an infinite loop!</p> <pre><code>import itertools\n\nfor i in itertools.count(20, 2):\n    if i &lt; 31:\n        print(i)\n    else:\n        break\n</code></pre> <p>Use the correct function from the itertools module on line 5, in between the parentheses of list(), in order to return the elements for which the lambda function given as an argument returns False.</p> <pre><code>import itertools\n\nlam = lambda x: x &lt; 5\n\nresult = list(itertools.filterfalse(lam, range(10)))\n\nprint(result)\n</code></pre>"},{"location":"programming/python/qa/#interview-essentials","title":"interview essentials","text":"<p>factorial</p> <pre><code>def fact(n):\n    if n==1: return 1 \n\n    result=n*fact(n-1)\n    return result\n\nprint(fact(5)) # 120\n</code></pre> <p>sum fibonacci series</p> <pre><code>def fib(n):\n    sum=0\n    a,b=0,1 \n    while a&lt;n:\n        sum+=a\n        print(a, end=\" \")\n        a,b=b,a+b\n    print()\n    print(\"sum of fibonacci numbers:\", sum)\n\nfib(90) # 0 1 1 2 3 5 8 13 21 34 55 89 \n\ndef fib(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return (fib(n-1) + fib(n-2))\n\nprint(fib(6)) # 8\n</code></pre> <p>list fibonacci numbers</p> <pre><code>def fib(n):\n    a,b=0,1\n    while a &lt; n:\n        print(a,end= ' ')\n        a, b = b, a+b\n        print(b)\n\nprint(fib(5)) # 0,1,1,2,3\n\n</code></pre> <p>string reverse</p> <pre><code>string1=\"Sunil\"\nprint(string1[::-1])\n\n# traditional method\n\nrevlist=[]\nn=len(string1)-1\ni=n\nwhile i&gt;=0 : \n    revlist.append(string1[i])\n    i-=1\nprint(\"\".join(revlist))\n\nrevstr=''\nfor char in string:\n    revstr = char + revstr\nprint(\"reverse of string: \", revstr)\n\n</code></pre> <p>reverse sentence</p> <pre><code>ss=\"This is sunil\"\nprint(\" \".join(ss.split()[::-1]))\n</code></pre> <p>Palindrome</p> <pre><code>string=\"mom\"\nprint(\"True\") if string == string[::-1] else print(\"False\")\n</code></pre> <p>word frequency</p> <pre><code>ss = \"\"\"Nory was a Catholic because her mother was a Catholic, \nand Nory's mother was a Catholic because her father was a Catholic, \nand her father was a Catholic because his mother was a Catholic, \nor had been.\"\"\"\n\nd={}\n\nfor eachword in ss.split():\n    d[eachword]=d.get(eachword,0)+1 \n\nprint(d)\n</code></pre> <p>digit frequency</p> <pre><code>L = [1,2,4,8,16,32,64,128,256,512,1024,32768,65536,4294967296]\n\n# {1: [1, 2, 4, 8], 2: [16, 32, 64], 3: [128, 256, 512], 4: [1024], 5: [32768, 65536], 10: [4294967296]}\n\nfrom collections import defaultdict \nd1=defaultdict(list)\n\nfor i in L:\n    d1[len(str(i))].append(i)\nprint(dict(d1))\n</code></pre> <p>list element frequency</p> <pre><code>l = [ 10, 20, 30, 40, 50, 50, 60,20,40, 40, 20,20]\n\nd={}\n\nfor eachitem in l:\n    d[eachitem]=d.get(eachitem,0)+1\n\nprint(d) # {10: 1, 20: 4, 30: 1, 40: 3, 50: 2, 60: 1}\n</code></pre> <p>anagams</p> <pre><code>def is_anagram(str1, str2):\n    \"\"\"a word, phrase, or name formed by rearranging the letters of another, such as cinema, formed from iceman.\"\"\"\n    if len(str1) != len(str2):\n        return False\n    else:\n        return sorted(str1) == sorted(str2)\n\nprint(is_anagram(\"sunil\",\"linus\")) # True\n</code></pre> <p>prime numbers</p> <pre><code>def is_prime(num):\n    if num &gt; 1:\n        for i in range(2, num):\n            if (num % i) == 0:\n                print(num, \"is not a prime number\")\n                print(i, \"times\", num // i, \"is\", num)\n                break\n        else:\n            print(num, \"is a prime number\")\n\n</code></pre> <p>max product in array</p> <pre><code>def max_product(arr):\n    max_product = 0\n    for i in range(len(arr)):\n        for j in range(i + 1, len(arr)):\n            if arr[i] * arr[j] &gt; max_product:\n                max_product = arr[i] * arr[j]\n    return max_product\n\nprint(max_product([5, 20, 2, 6])) # 120\n</code></pre> <p>revese digit</p> <pre><code>def reverse(x):\n    rev = 0\n    while x &gt; 0:\n        rev = rev * 10 + x % 10\n        x //= 10\n    return rev\nprint(reverse(1234))\n\ndef reverse(n):\n    revnum=''\n    for n in str(n):\n        revnum = n + revnum\n    return revnum\n\nprint(reverse(123456))\n</code></pre> <p>sum digit</p> <pre><code>num=123\nsum=0\n\nfor i in str(num):\n    sum+=int(i)\nprint(sum)\n</code></pre> <p>sum of even/odd numbers</p> <pre><code>print(\"even numbers sum:\",sum(list(range(0,50,2))))\nprint(\"odd numbers sum:\"sum(list(range(1,50,1))))\n\neven_nums=0\nfor i in list(range(0,50,2)):\n    even_nums+=i\nprint(even_nums)\n</code></pre> <p>Count vovels</p> <pre><code>from collections import Counter\n\ndef count_each_vowel():\n    s= \"I am going outside for lunch. will be back by another thirty minutes\"\n    vowels = \"aeiouAEIOU\"\n    vowel_counts = Counter(char for char in s if char in vowels)\n    print(vowel_counts)\n\ncount_each_vowel()\n</code></pre> <p>Reverse a String Without Using Slicing</p> <pre><code>def reverse_string(s):\n    result = ''\n    for char in s:\n        result = char + result\n    return result\n\nprint(reverse_string(\"DevOpsEngineer\"))  # reenignEsPOveD\n</code></pre> <p>Sort a Dictionary by Value</p> <pre><code>def sort_dict_by_value(d):\n    return dict(sorted(d.items(), key=lambda item: item[1]))\n\nprint(sort_dict_by_value({'a': 3, 'b': 1, 'c': 2}))  # {'b': 1, 'c': 2, 'a': 3}\n</code></pre> <p>Check for Anagrams</p> <pre><code>def are_anagrams(s1, s2):\n    return sorted(s1) == sorted(s2)\n\nprint(are_anagrams(\"listen\", \"silent\"))  # True\n</code></pre> <p>Find the Frequency of Each Element in a List</p> <pre><code>from collections import Counter\n\nlst = [1, 2, 2, 3, 4, 4, 4]\nprint(dict(Counter(lst)))  # {1:1, 2:2, 3:1, 4:3}\n</code></pre> <p>Read a File and Count Word Frequency</p> <pre><code>def count_words(filepath):\n    with open(filepath) as f:\n        words = f.read().split()\n    return dict(Counter(words))\n\n# count_words(\"test.txt\")\n</code></pre> <p>Find All Pairs in List That Sum to Target</p> <pre><code>def find_pairs(lst, target):\n    seen = set()\n    result = set()\n    for num in lst:\n        diff = target - num\n        if diff in seen:\n            result.add((min(num, diff), max(num, diff)))\n        seen.add(num)\n    return result\n\nprint(find_pairs([2, 4, 3, 5, 7], 7))  # {(3, 4), (2, 5)}\n</code></pre> <p>Rotate a List by k Elements</p> <pre><code>def rotate_list(lst, k):\n    k = k % len(lst)\n    return lst[-k:] + lst[:-k]\n\nprint(rotate_list([1,2,3,4,5], 2))  # [4,5,1,2,3]\n</code></pre> <p>Flatten a Nested List</p> <pre><code>def flatten(lst):\n    result = []\n    for item in lst:\n        if isinstance(item, list):\n            result.extend(flatten(item))\n        else:\n            result.append(item)\n    return result\n\nprint(flatten([1, [2, [3, 4], 5], 6]))  # [1,2,3,4,5,6]\n</code></pre> <p>Find Duplicate Characters in a String</p> <pre><code> def find_duplicates(s):\n    return [char for char, count in Counter(s).items() if count &gt; 1]\n\nprint(find_duplicates(\"programming\"))  # ['r', 'g', 'm']\n</code></pre> <p>Check Armstrong Number</p> <pre><code>def is_armstrong(n):\n    power = len(str(n))\n    return sum(int(d) ** power for d in str(n)) == n\n\nprint(is_armstrong(153))  # True\n</code></pre> <p>List All Prime Numbers in a Range</p> <pre><code>def primes_in_range(start, end):\n    return [n for n in range(start, end+1) if is_prime(n)]\n\nprint(primes_in_range(10, 30))\n</code></pre> <p>Capitalize First Letter of Each Word in a String</p> <pre><code>def capitalize_words(s):\n    return ' '.join(word.capitalize() for word in s.split())\n\nprint(capitalize_words(\"hello devops team\"))  # Hello Devops Team\n</code></pre> <p>Validate IPv4 Address</p> <pre><code>import re\n\ndef is_valid_ip(ip):\n    pattern = re.compile(r\"^(25[0-5]|2[0-4]\\d|1?\\d{1,2})(\\.(25[0-5]|2[0-4]\\d|1?\\d{1,2})){3}$\")\n    return bool(pattern.match(ip))\n\nprint(is_valid_ip(\"192.168.1.1\"))  # True\n</code></pre> <p>Merge Two Dictionaries</p> <pre><code>def merge_dicts(d1, d2):\n    return {**d1, **d2}\n\nprint(merge_dicts({'a': 1}, {'b': 2}))  # {'a':1, 'b':2}\n</code></pre> <p>Implement a Decorator to Measure Function Execution Time</p> <pre><code>import time\n\ndef timer(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f\"Executed in {time.time() - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef slow_function():\n    time.sleep(1)\n    return \"done\"\n\nprint(slow_function())\n</code></pre> <p>JSON Parsing and Writing</p> <pre><code>import json\n\ndata = {\"name\": \"DevOps\", \"role\": \"SRE\"}\n\n# Convert to JSON string\njson_string = json.dumps(data)\n\n# Convert back to dict\nparsed_data = json.loads(json_string)\n\nprint(json_string)     # {\"name\": \"DevOps\", \"role\": \"SRE\"}\nprint(parsed_data)     # {'name': 'DevOps', 'role': 'SRE'}\n</code></pre> <p>Check if a List is Sorted</p> <pre><code> def is_sorted(lst):\n    return lst == sorted(lst)\n\nprint(is_sorted([1, 2, 3, 4]))  # True\nprint(is_sorted([1, 3, 2]))    # False\n</code></pre> <p>Find the First Non-Repeating Character</p> <pre><code>from collections import Counter\n\ndef first_non_repeating(s):\n    counts = Counter(s)\n    for char in s:\n        if counts[char] == 1:\n            return char\n    return None\n\nprint(first_non_repeating(\"aabbccdef\"))  # d\n</code></pre> <p>Move All Zeros to the End of the List</p> <pre><code>def move_zeros(lst):\n    non_zeros = [x for x in lst if x != 0]\n    return non_zeros + [0] * (len(lst) - len(non_zeros))\n\nprint(move_zeros([0, 1, 0, 3, 12]))  # [1, 3, 12, 0, 0]\n</code></pre> <p>Count Occurrences of Each Word in a String</p> <pre><code>from collections import Counter\ndef word_frequency(s):\n    words = s.lower().split()\n    return dict(Counter(words))\n\n# {'this': 2, 'is': 2, 'a': 1, 'test.': 1, 'test': 1, 'simple.': 1}\nprint(word_frequency(\"This is a test. This test is simple.\"))\n</code></pre> <p>Check if Two Strings are Rotations of Each Other</p> <pre><code>def is_rotation(s1, s2):\n    return len(s1) == len(s2) and s2 in (s1 + s1)\n\nprint(is_rotation(\"abcd\", \"cdab\"))  # True\n</code></pre> <p>Find Second Largest Number in List</p> <pre><code>def second_largest(lst):\n    unique = list(set(lst))\n    unique.sort()\n    return unique[-2] if len(unique) &gt;= 2 else None\n\nprint(second_largest([4, 1, 3, 2, 5]))  # 4\n</code></pre> <p>Check Leap Year</p> <pre><code>def is_leap(year):\n    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\nprint(is_leap(2024))  # True\n</code></pre> <p>Extract Digits from a String</p> <pre><code>def extract_digits(s):\n    return [int(char) for char in s if char.isdigit()]\n\nprint(extract_digits(\"abc123xyz456\"))  # [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Find Common Elements in Two Lists</p> <pre><code>def common_elements(a, b):\n    return list(set(a) &amp; set(b))\n\nprint(common_elements([1, 2, 3], [2, 3, 4]))  # [2, 3]\n</code></pre> <p>Sum of All Digits in a String</p> <pre><code>def sum_of_digits(s):\n    return sum(int(char) for char in s if char.isdigit())\n\nprint(sum_of_digits(\"abc123xyz\"))  # 6\n</code></pre>"},{"location":"programming/python/qa/#builtins","title":"Builtins","text":""},{"location":"programming/python/qa/#string","title":"string","text":"<pre><code>capitalize()    Converts the first character to upper case\ncasefold()      Converts string into lower case\ncenter()        Returns a centered string\ncount()         Returns the number of times a specified value occurs in a string\nencode()        Returns an encoded version of the string\nendswith()      Returns true if the string ends with the specified value\nexpandtabs()    Sets the tab size of the string\nfind()          Searches the string for a specified value and returns the position of where it was found\nformat()        Formats specified values in a string\nformat_map()    Formats specified values in a string\nindex()         Searches the string for a specified value and returns the position of where it was found\nisalnum()       Returns True if all characters in the string are alphanumeric\nisalpha()       Returns True if all characters in the string are in the alphabet\nisascii()       Returns True if all characters in the string are ascii characters\nisdecimal()     Returns True if all characters in the string are decimals\nisdigit()       Returns True if all characters in the string are digits\nisidentifier()  Returns True if the string is an identifier\nislower()       Returns True if all characters in the string are lower case\nisnumeric()     Returns True if all characters in the string are numeric\nisprintable()   Returns True if all characters in the string are printable\nisspace()       Returns True if all characters in the string are whitespaces\nistitle()       Returns True if the string follows the rules of a title\nisupper()       Returns True if all characters in the string are upper case\njoin()          Converts the elements of an iterable into a string\nljust()         Returns a left justified version of the string\nlower()         Converts a string into lower case\nlstrip()        Returns a left trim version of the string\nmaketrans()     Returns a translation table to be used in translations\npartition()     Returns a tuple where the string is parted into three parts\nreplace()       Returns a string where a specified value is replaced with a specified value\nrfind()         Searches the string for a specified value and returns the last position of where it was found\nrindex()        Searches the string for a specified value and returns the last position of where it was found\nrjust()         Returns a right justified version of the string\nrpartition()    Returns a tuple where the string is parted into three parts\nrsplit()        Splits the string at the specified separator, and returns a list\nrstrip()        Returns a right trim version of the string\nsplit()         Splits the string at the specified separator, and returns a list\nsplitlines()    Splits the string at line breaks and returns a list\nstartswith()    Returns true if the string starts with the specified value\nstrip()         Returns a trimmed version of the string\nswapcase()      Swaps cases, lower case becomes upper case and vice versa\ntitle()         Converts the first character of each word to upper case\ntranslate()     Returns a translated string\nupper()         Converts a string into upper case\nzfill()         Fills the string with a specified number of 0 values at the beginning\n</code></pre>"},{"location":"programming/python/qa/#list","title":"list","text":"<pre><code>append()    Adds an element at the end of the list\nclear()     Removes all the elements from the list\ncopy()      Returns a copy of the list\ncount()     Returns the number of elements with the specified value\nextend()    Add the elements of a list (or any iterable), to the end of the current list\nindex()     Returns the index of the first element with the specified value\ninsert()    Adds an element at the specified position\npop()       Removes the element at the specified position\nremove()    Removes the first item with the specified value\nreverse()   Reverses the order of the list\nsort()      Sorts the list\n</code></pre>"},{"location":"programming/python/qa/#dictionary","title":"dictionary","text":"<pre><code>clear()     Removes all the elements from the dictionary\ncopy()      Returns a copy of the dictionary\nfromkeys()  Returns a dictionary with the specified keys and value\nget()       Returns the value of the specified key\nitems()     Returns a list containing a tuple for each key value pair\nkeys()      Returns a list containing the dictionary's keys\npop()       Removes the element with the specified key\npopitem()   Removes the last inserted key-value pair\nsetdefault() Returns the value of the specified key.If the key does not exist: insert the key, with the specified value\nupdate()    Updates the dictionary with the specified key-value pairs\nvalues()    Returns a list of all the values in the dictionary\n</code></pre>"},{"location":"programming/python/qa/#tuple","title":"tuple","text":"<pre><code>count()   Returns the number of times a specified value occurs in a tuple\nindex()   Searches the tuple for a specified value and returns the position of where it was found \n</code></pre>"},{"location":"programming/python/qa/#set","title":"set","text":"<pre><code>add()           Adds an element to the set\nclear()         Removes all the elements from the set\ncopy()          Returns a copy of the set\ndifference()    Returns a set containing the difference between two or more sets\ndifference_update() Removes the items in this set that are also included in another, specified set\ndiscard()       Remove the specified item\nintersection()  Returns a set, that is the intersection of two or more sets\nintersection_update()   Removes the items in this set that are not present in other, specified set(s)\nisdisjoint()    Returns whether two sets have a intersection or not\nissubset()      Returns whether another set contains this set or not\nissuperset()    Returns whether this set contains another set or not\npop()           Removes an element from the set\nremove()        Removes the specified element\nsymmetric_difference()  Returns a set with the symmetric differences of two sets\nsymmetric_difference_update()   inserts the symmetric differences from this set and another\nunion()         Return a set containing the union of sets\nupdate()        Update the set with another set, or any other iterable\n</code></pre>"},{"location":"programming/python/qa/#files_1","title":"files","text":"<pre><code>close()         Closes the file\ndetach()        Returns the separated raw stream from the buffer\nfileno()        Returns a number that represents the stream, from the operating system's perspective\nflush()         Flushes the internal buffer\nisatty()        Returns whether the file stream is interactive or not\nread()          Returns the file content\nreadable()      Returns whether the file stream can be read or not\nreadline()      Returns one line from the file\nreadlines()     Returns a list of lines from the file\nseek()          Change the file position\nseekable()      Returns whether the file allows us to change the file position\ntell()          Returns the current file position\ntruncate()      Resizes the file to a specified size\nwritable()      Returns whether the file can be written to or not\nwrite()         Writes the specified string to the file\nwritelines()    Writes a list of strings to the file\n</code></pre>"},{"location":"programming/python/qa/#other","title":"other","text":"<pre><code>abs()       Returns the absolute value of a number\nall()       Returns True if all items in an iterable object are true\nany()       Returns True if any item in an iterable object is true\nascii()     Returns a readable version of an object. Replaces none-ascii characters with escape character\nbin()       Returns the binary version of a number\nbool()      Returns the boolean value of the specified object\nbytearray() Returns an array of bytes\nbytes()     Returns a bytes object\ncallable()  Returns True if the specified object is callable, otherwise False\nchr()       Returns a character from the specified Unicode code.\nclassmethod()   Converts a method into a class method\ncompile()   Returns the specified source as an object, ready to be executed\ncomplex()   Returns a complex number\ndelattr()   Deletes the specified attribute (property or method) from the specified object\ndict()      Returns a dictionary (Array)\ndir()       Returns a list of the specified object's properties and methods\ndivmod()    Returns the quotient and the remainder when argument1 is divided by argument2\nenumerate() Takes a collection (e.g. a tuple) and returns it as an enumerate object\neval()      Evaluates and executes an expression\nexec()      Executes the specified code (or object)\nfilter()    Use a filter function to exclude items in an iterable object\nfloat()     Returns a floating point number\nformat()    Formats a specified value\nfrozenset() Returns a frozenset object\ngetattr()   Returns the value of the specified attribute (property or method)\nglobals()   Returns the current global symbol table as a dictionary\nhasattr()   Returns True if the specified object has the specified attribute (property/method)\nhash()      Returns the hash value of a specified object\nhelp()      Executes the built-in help system\nhex()       Converts a number into a hexadecimal value\nid()        Returns the id of an object\ninput()     Allowing user input\nint()       Returns an integer number\nisinstance()    Returns True if a specified object is an instance of a specified object\nissubclass()    Returns True if a specified class is a subclass of a specified object\niter()      Returns an iterator object\nlen()       Returns the length of an object\nlist()      Returns a list\nlocals()    Returns an updated dictionary of the current local symbol table\nmap()       Returns the specified iterator with the specified function applied to each item\nmax()       Returns the largest item in an iterable\nmemoryview()    Returns a memory view object\nmin()       Returns the smallest item in an iterable\nnext()      Returns the next item in an iterable\nobject()    Returns a new object\noct()       Converts a number into an octal\nopen()      Opens a file and returns a file object\nord()       Convert an integer representing the Unicode of the specified character\npow()       Returns the value of x to the power of y\nprint()     Prints to the standard output device\nproperty()  Gets, sets, deletes a property\nrange()     Returns a sequence of numbers, starting from 0 and increments by 1 (by default)\nrepr()      Returns a readable version of an object\nreversed()  Returns a reversed iterator\nround()     Rounds a numbers\nset()       Returns a new set object\nsetattr()   Sets an attribute (property/method) of an object\nslice()     Returns a slice object\nsorted()    Returns a sorted list\nstaticmethod()  Converts a method into a static method\nstr()       Returns a string object\nsum()       Sums the items of an iterator\nsuper()     Returns an object that represents the parent class\ntuple()     Returns a tuple\ntype()      Returns the type of an object\nvars()      Returns the __dict__ property of an object\nzip()       Returns an iterator, from two or more iterators\n</code></pre>"},{"location":"programming/python/qa/#counter","title":"counter","text":"<pre><code>Counter(elements)   Creates a Counter object from an iterable or a dictionary.\n.most_common([n])   Returns the n most common elements as a list of tuples. If n is omitted, returns all elements.\n.elements() Returns an iterator over elements, repeating each as per its count.\n.subtract(iterable or mapping)  Subtracts counts from another iterable or mapping.\n.update(iterable or mapping)    Adds counts from another iterable or mapping.\n.clear()    Removes all elements from the Counter.\n.copy() Returns a shallow copy of the Counter.\n.keys() Returns a list of unique elements (like a dictionary).\n.values()   Returns a list of counts corresponding to the elements.\n.items()    Returns a list of (element, count) pairs.\n.total()    Returns the sum of all counts (Python 3.10+).\n+Counter    Adds two Counters, summing up counts for common elements.\n-Counter    Subtracts counts, keeping only positive counts.\n&amp;Counter    Returns the intersection (min counts) of two Counters.\n</code></pre>"},{"location":"programming/python/qa/#todo","title":"TODO","text":""},{"location":"programming/python/qa/#practical-questions","title":"Practical Questions","text":"<ul> <li>what is the use of yield and why should we use them ?</li> <li>What is the use of map, filter and reduce ? can you provide some examples ?</li> <li>Why do we need to use lambda and how can we use them ?</li> <li>what is dictionary comprehension, provide me with an example ?</li> <li>Given a string, how would you remove the list of whitespaces from it and create a new string ?</li> <li>what's the difference between append, extend and concatenate ?</li> <li>find the first non-repeated character in a string.</li> <li>count the number of words in a string.</li> <li>find the maximum subarray sum in a given list of integers.</li> <li>check if a given string contains only digits.</li> <li>check if a given string is a valid email address.</li> <li>generate all possible permutations of a given string.</li> <li>convert a decimal number to binary.</li> <li>convert a binary number to decimal.</li> <li>check if a given number is a perfect square.</li> <li>check if a given number is an Armstrong number.</li> <li>find the longest common prefix among a list of strings.</li> <li>find the longest common suffix among a list of strings.</li> <li>find the first non-repeating character in a list.</li> <li>find the first repeating character in a list.</li> <li>remove all whitespace characters from a string.</li> <li>implement a Caesar cipher.</li> <li>find the number of occurrences of a substring in a given string.</li> <li>find the index of the first occurrence of a substring in a given string.</li> <li>count the number of lines in a file.</li> <li>count the number of words in a file.</li> <li>count the number of characters in a file.</li> <li>check if a given string is a valid IP address.</li> <li>check if a given string is a valid URL.</li> <li>find the most common element in a list.</li> </ul>"},{"location":"programming/python/qa/#data-structures","title":"Data Structures","text":""},{"location":"programming/python/qa/#single-linked-list","title":"Single linked list","text":""},{"location":"programming/python/qa/#double-linked-list","title":"Double linked list","text":""},{"location":"programming/python/qa/#stacks-queues","title":"Stacks &amp; Queues","text":""},{"location":"programming/python/qa/#trees","title":"Trees","text":""},{"location":"programming/python/qa/#hash-tables","title":"Hash tables","text":""},{"location":"programming/python/qa/#graphs","title":"Graphs","text":""},{"location":"programming/python/qa/#heaps","title":"Heaps","text":""},{"location":"programming/python/qa/#recursions","title":"Recursions","text":""},{"location":"programming/python/qa/#sorting","title":"Sorting","text":""},{"location":"programming/python/qa/#other-coding-exercises","title":"Other coding exercises","text":""},{"location":"programming/python/data_structures/hashing/","title":"hashing","text":""},{"location":"programming/python/data_structures/hashing/#overview","title":"Overview","text":"<p>Hashing is a technique used in computer science to map data of arbitrary size to fixed-size values, usually integers, which are used as keys to access or store data in a data structure called a hash table. Hashing allows for efficient data retrieval and storage by reducing the search space and providing constant-time average complexity for common operations like insertion, deletion, and retrieval.</p> <p>Deterministic hashing refers to the property of a hash function where the same input will always produce the same hash value. In other words, given a specific input, the hash function will consistently generate the exact same output hash code. This property is crucial for the reliability and predictability of hash-based data structures and algorithms.</p> <p>Python's dictionaries handle the details of hashing and collision resolution internally, making it easy to work with hashed data without needing to implement these aspects yourself.</p> <pre><code>\nstudent_scores = {\n    \"Alice\": 95,\n    \"Bob\": 88,\n    \"Charlie\": 92,\n    \"David\": 78\n}\n\nfor name, score in student_scores.items():\n    print(f\"{name}: {score}\")\n\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#collisions","title":"Collisions","text":"<p>Separate chaining is a collision resolution strategy used in hash-based data structures, such as hash tables, to handle situations where multiple keys hash to the same index. It involves creating a separate data structure, often a linked list, for each index in the hash table where collisions occur.</p>"},{"location":"programming/python/data_structures/hashing/#hash-table-initialization","title":"Hash Table Initialization","text":"<p>The hash table is divided into a fixed number of buckets (or slots), each with a unique index. Each bucket can hold multiple key-value pairs.</p>"},{"location":"programming/python/data_structures/hashing/#hashing","title":"Hashing","text":"<p>When a new key needs to be inserted into the hash table or when you want to retrieve a value associated with a key, a hash function is applied to the key to determine the index (bucket) where it should be stored or looked up.</p>"},{"location":"programming/python/data_structures/hashing/#collision-handling","title":"Collision Handling","text":"<ul> <li>If two or more keys hash to the same index (collision), separate chaining is used to handle the collision.</li> <li>At each index where a collision occurs, a separate data structure (typically a linked list) is maintained.</li> </ul>"},{"location":"programming/python/data_structures/hashing/#insertion","title":"Insertion","text":"<p>To insert a new key-value pair: - Hash the key to find the appropriate index (bucket). - Insert the key-value pair at the end of the linked list (or another chosen data structure) associated with that index.</p>"},{"location":"programming/python/data_structures/hashing/#retrieval","title":"Retrieval","text":"<p>To retrieve the value associated with a key: - Hash the key to find the appropriate index. - Traverse the linked list at that index to find the desired key and retrieve its corresponding value.</p>"},{"location":"programming/python/data_structures/hashing/#deletion","title":"Deletion","text":"<p>To delete a key-value pair: - Hash the key to find the appropriate index. - Search the linked list for the key and remove the corresponding pair if found.</p>"},{"location":"programming/python/data_structures/hashing/#constructor","title":"Constructor","text":"<pre><code>class HashTable:\n  def __init__(self, size=7):\n    self.data_map = [None]*size\n\n  def __hash(self,key):\n    my_hash=0\n    for letter in key:\n      my_hash=(my_hash+ord(letter)*23) % len(self.data_map)\n    return my_hash\n\nhash_table = HashTable()\nhash_table.print_hash_table()\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#print-table","title":"print table","text":"<pre><code>def print_hash_table(self):\n    for k,v in enumerate(self.data_map):\n        print(k,\": \", v)\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#set-item","title":"set item","text":"<pre><code>def set_item(self,key,value):\n    index = self.__hash(key)\n    if self.data_map[index] == None:\n        self.data_map[index]=[]\n    self.data_map[index].append([key,value])\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#get-item","title":"get item","text":"<pre><code>def get_item(self,key):\n    index = self.__hash(key)\n    if self.data_map[index] is not None:\n        for i in range(len(self.data_map[index])):\n        if self.data_map[index][i][0] == key:\n            return self.data_map[index][i][1]\n    return None\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#get-all-keys","title":"get all keys","text":"<pre><code>def keys(self):\n    all_keys=[]\n    for i in range(len(self.data_map)):\n        if self.data_map[i] is not None:\n        for j in range(len(self.data_map[i])):\n            all_keys.append(self.data_map[i][j][0])\n    return all_keys\n</code></pre>"},{"location":"programming/python/data_structures/hashing/#big-o","title":"Big-O","text":"<p>In the best scenerio, the item we are searching in the hash table is found at the index, it wouldbe O(1).  In wrost case even if the item is entirely hashed in the list, the max would be O(n), but however since we are hashing the key value pair are evenly distributed and have very less collisions, we would say its always the hash is  O(1)</p>"},{"location":"programming/python/data_structures/hashing/#interview-q","title":"Interview Q","text":"<p>provided the two lists, get the item in common. provide me the best solution in terms of bigO</p> <p>list1 = [2,3,4] list2 = [1,6,4]</p> <p>There are two approches for this. </p> <ol> <li>You would be iterating first item of list1 against all the elements in list2 and so on. Once you find you would be returning True or else False. Since there are multiple nested loops O(n^2)</li> </ol> <pre><code># code goes here\n</code></pre> <ol> <li>Create a dict for list1 and check if the key of dict in list2. Once you find you would return True or else False.  This would have O(2n) i.e removing constants, it would be O(n)</li> </ol> <pre><code># code goes here\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/","title":"linked list","text":""},{"location":"programming/python/data_structures/linked_list/#olinked-list-operations","title":"O(linked list operations)","text":"<p>There are few of the operations that which we do in the linked list, below snapshot describes you about the Big-O cases. </p> <p></p>"},{"location":"programming/python/data_structures/linked_list/#linked-list-under-the-hood","title":"linked list under the hood.","text":"<p>linked list appears to be as below, however in the memory they are scarattered in the different locations, but they are always connected with the pointer. There is head which is the start of the node and the tail which is the end of the node in the list. the connection elements between the nodes are the pointers which are connected</p> <p></p> <p>Under the hood, its nothing but a dictionary, which can be used as a variable to set and iterate the values.  </p> <p></p>"},{"location":"programming/python/data_structures/linked_list/#singly-linked-list-sll","title":"Singly linked list (sll)","text":"<p>Methods that are used in the linked list class. </p>"},{"location":"programming/python/data_structures/linked_list/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value\n        self.next = None\n\nclass LinkedList:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.head = new_node\n        self.tail = new_node\n        self.length = 1\n\nmy_linked_list = LinkedList(1)\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#print-sll","title":"print sll","text":"<pre><code>def print_list(self):\n    temp = self.head \n    while temp is not None:\n        print(temp.value,end=' ')\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#append-sll","title":"append sll","text":"<pre><code>def append(self,value):\n    new_node=Node(value)\n    if self.length==0:\n        self.head = new_node.head\n        self.tail = new_node.tail \n    else:\n        self.tail.next = new_node\n        self.tail = new_node\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#prepend-node-list","title":"prepend node list","text":"<pre><code>def prepend(self,value):\n    new_node=Node(value)\n\n    if self.length == 0:\n        self.head = new_node\n        self.tail = new_node\n    else:\n        new_node.next = self.head\n        self.head = new_node\n\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#get-node-at-particular-index","title":"get node at particular index","text":"<pre><code>def get(self,index):\n    if index &lt; 0 or index &gt;= self.length:\n        return None \n    temp = self.head \n    for _ in range(index):\n        temp = temp.next \n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#insert-node-at-particular-index","title":"insert node at particular index","text":"<pre><code>def insert(self,index,value):\n    if index&lt; 0 or index&gt;self.length:\n        return False \n    if index == 0:\n        return self.prepend(value)\n    if index==self.length:\n        return self.append(value)\n    new_node=Node(value)\n    temp = self.get(index-1) # get index from above function\n    new_node.next= temp.next \n    temp.next = new_node\n    self.length+=1\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#set-value-to-node-using-particular-index","title":"set value to node using particular index","text":"<pre><code>def set_value(self,index,value):\n    temp = self.get(index)\n    if temp:\n        temp.value=value\n        return True \n    return False\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-at-particule-index","title":"remove node at particule index","text":"<pre><code>def remove(self,index):\n    if index &lt; 0  or index&gt;=self.length:\n        return None \n    if index == 0:\n        return self.pop_first()\n    if index == self.length:\n        return self.pop()\n    pre = self.get(index-1)\n    temp = pre.next \n    pre.next = temp.next \n    temp.next = None\n    self.length-=1\n    return temp   \n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-list","title":"remove node list","text":"<pre><code>def pop(self):\n    # if node is empty\n    if self.length == 0:\n        return None\n    else:\n        # more than two nodes \n        temp=self.head \n        pre=self.head\n        while(temp.next):\n            pre=temp\n            temp=temp.next\n        self.tail = pre\n        self.tail.next=None\n        self.length-=1\n        # if node is 0 after decrementing\n        if self.length==0:\n            self.head = None \n            self.tail = None\n    return temp.value\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-first-node-in-list","title":"remove first node in list","text":"<pre><code>def pop_first(self):\n    if self.length == 0:\n        return None \n    else:\n        temp = self.head \n        self.head = self.head.next \n        temp.next = None \n        self.length-=1\n        if self.length==0:\n            self.tail = None\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#reverse-node-in-list","title":"reverse node in list","text":"<pre><code>def reverse(self):\n    # swap head and tail of the list\n    temp=self.head\n    self.head = self.tail\n    self.tail = temp \n\n    after = temp.next \n    before = None \n\n    for _ in range(self.length):\n        after = temp.next \n        temp.next = before \n        before = temp\n        temp = after\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#doubly-linked-listdll","title":"Doubly linked list(dll)","text":""},{"location":"programming/python/data_structures/linked_list/#constructor_1","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None\n        self.prev=None\n\nclass LinkedList:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.head = new_node\n        self.tail = new_node\n        self.length = 1\n\ndoubly_linked_list = DoublyLinkedList(\"10\")\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#print-dll","title":"print dll","text":"<pre><code>def print_list(self):\n    temp = self.head \n    while temp:\n        print(f\"{temp.value}\")\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#append-dll","title":"append dll","text":"<pre><code>def append(self,value):\n    new_node = Node(value)\n\n    if self.head is None:\n        self.head = new_node\n        self.tail = new_node\n    else:\n        self.tail.next = new_node\n        new_node.prev=self.tail\n        self.tail = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#pop-dll","title":"pop dll","text":"<pre><code>def pop(self):\n    if self.length == 0:\n        return None\n\n    temp = self.tail \n    if self.length==1:\n        self.head = None \n        self.tail = None\n    else:\n        self.tail = self.tail.prev\n        self.tail.next = None \n        temp.prev = None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#prepend-dll","title":"prepend dll","text":"<pre><code>def prepend(self,value):\n    new_node = Node(value)\n\n    if self.length == 0:\n        return None\n    else:\n        new_node.next = self.head \n        self.head.prev = new_node\n        self.head = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#pop-first-dll","title":"pop first dll","text":"<pre><code>def pop_first(self):\n    if self.length==0:\n        return None\n\n    temp = self.head \n    if self.length==1:\n        self.head = None\n        self.tail = None\n    else:\n        self.head = self.head.next\n        self.head.prev= None\n        temp.next = None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#get-node-using-index","title":"get node using index","text":"<pre><code>def get(self,index):\n    if index &lt; 0 or index &gt;=self.length:\n        return None\n    temp = self.head \n    if index &lt; self.length/2:\n        for _ in range(index):\n            temp = temp.next\n    else:\n        temp = self.tail\n        for _ in range(self.length-1, index,-1):\n            temp = temp.prev\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#set-value-using-index","title":"set value using index","text":"<pre><code>def set_value(self,index,value):\n    temp = self.get(index)\n    if temp:\n        temp.value = value\n        return True \n\n    return False\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#insert-node-at-particular-index_1","title":"insert node at particular index","text":"<pre><code>def insert(self,index,value):\n    if index &lt; 0 or index &gt;=self.length:\n        return False\n\n    if index==0:\n        return self.prepend(value)\n\n    if index == self.length:\n        return self.append(value)\n\n    new_node=Node(value)\n    before = self.get(index-1)\n    after = before.next\n    new_node.prev = before\n    new_node.next = after \n    before.next = new_node\n    after.prev = new_node\n    self.length+=1\n\n    return True\n</code></pre>"},{"location":"programming/python/data_structures/linked_list/#remove-node-at-particular-index","title":"remove node at particular index","text":"<pre><code>def remove(self,index):\n    if index &lt; 0 or index &gt;=self.length:\n        return None\n\n    if index==0:\n        return self.pop_first()\n\n    if index == self.length-1:\n        return self.pop()\n\n    temp = self.get(index)\n    temp.next.prev=temp.prev\n    temp.prev.next = temp.next\n    temp.next = None \n    temp.prev=None\n    self.length-=1\n\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/overview/","title":"overview","text":"<p>A data structure is a storage that is used to store and organize data. It is a way of arranging data on a computer so that it can be accessed and updated efficiently.</p>"},{"location":"programming/python/data_structures/overview/#classification-of-ds","title":"Classification of DS","text":"<p>Linear data structure data elements are arranged sequentially or linearly, where each element is attached to its previous and next adjacent elements</p> <p>Static: it has a fixed memory size. It is easier to access the elements in a static data structure. e.g arrary</p> <p>Dynamic: the size is not fixed. It can be randomly updated during the runtime which may be considered efficient concerning the memory (space) complexity of the code. e.g queue, stack </p> <p>Non-linear data structure: Data structures where data elements are not placed sequentially or linearly are called non-linear data structures. In a non-linear data structure, we can\u2019t traverse all the elements in a single run only. e.g trees and graphs</p>"},{"location":"programming/python/data_structures/overview/#big-o-complexities","title":"Big-O Complexities","text":"<p>Big-O notation is a way to describe the time and space complexity of a given algorithm. Big-O notation tells you the number of operations an algorithm will make. Big-O establishes a worst-case run time. </p>"},{"location":"programming/python/data_structures/overview/#time-complexity","title":"Time complexity","text":"<p>The amount of time take to execute the operations per second.</p>"},{"location":"programming/python/data_structures/overview/#space-complexity","title":"Space complexity","text":"<p>The amount of RAM or memory taken to execute. however, this would depend on the performance on the system. the higher the capacity of the machine, space complexity would be differing. </p> <p></p>"},{"location":"programming/python/data_structures/overview/#big-o-notations","title":"Big-O notations","text":""},{"location":"programming/python/data_structures/overview/#o1","title":"O(1)","text":"<p>The O(1) is also called constant time, it will always execute at the same time regardless of the input size.</p> <pre><code>x=10\nprint(x)\n\n# The input array could be 1 item or 1,000 items, \n# but this function would still just require one step.\n\nfor i in range(10):\n    print(i[0])\n</code></pre>"},{"location":"programming/python/data_structures/overview/#on","title":"O(n)","text":"<p>This function runs in O(n) time (or \"linear time\"), where n is the number of items in the array. If the array has 10 items, we have to print 10 times. If it has 1000 items, we have to print 1000 times.</p> <pre><code>for i in range(10):\n    print(i)\n</code></pre>"},{"location":"programming/python/data_structures/overview/#on2","title":"O(n^2)","text":"<pre><code>for i in range(10):\n    for j in range(10):\n        print(i,j)\n</code></pre> <p>Here we're nesting two loops. If our array has n items, our outer loop runs n times and our inner loop runs n times for each iteration of the outer loop, giving us n*n total prints. Thus this function runs in O(n2) time (or \"quadratic time\"). If the array has 10 items, we have to print 100 times. If it has 1000 items, we have to print 1000000 times.</p>"},{"location":"programming/python/data_structures/overview/#o2n","title":"O(2n)","text":"<pre><code>def fib(n):\n    if n&lt;=1: return n;\n    return fib(n-1)+fib(n-2)\n</code></pre> <p>An example of an O(2n) function is the recursive calculation of Fibonacci numbers. O(2n) denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2n) function is exponential - starting off very shallow, then rising meteorically.</p>"},{"location":"programming/python/data_structures/overview/#drop-less-significant-terms","title":"Drop less significant terms","text":"<pre><code>for i in range(10):\n    for j in range(10):\n        print(i,j)\n\nfor k in range(10):\n    print(k)\n</code></pre> <p>we would re-iterate i and j i.e n^2 along with 'k' would be 'n' times.  hence it would be O(n^2+n), in which we can easily drop the constant(n) as its insignificient.</p>"},{"location":"programming/python/data_structures/overview/#drop-constants","title":"Drop constants","text":"<pre><code>for i in range(10):\n    print(i)\n\nfor j in range(10):\n    print(j)\n</code></pre> <p>We would be looping 'i' n-times and 'j' n-times i.e O(n+n)=O(2n).</p> <p>Remember, for big O notation we're looking at what happens as n gets arbitrarily large. As n gets really big, adding 100 or dividing by 2 has a decreasingly significant effect, hence we would drop any constants. </p>"},{"location":"programming/python/data_structures/overview/#common-ds-alogs","title":"Common ds alogs","text":""},{"location":"programming/python/data_structures/overview/#references","title":"References","text":"<p>bigocheatsheet</p>"},{"location":"programming/python/data_structures/queues/","title":"queues","text":"<p>queue is a linear data structure that stores items in First In First Out (FIFO) manner. With a queue the least recently added item is removed first.</p> <p></p>"},{"location":"programming/python/data_structures/queues/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None \n\nclass Queue:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.first = new_node\n        self.last = new_node\n        self.length=1\n\nmy_queue=Queue(4)\n</code></pre>"},{"location":"programming/python/data_structures/queues/#print-queue","title":"print queue","text":"<pre><code>\ndef print_queue(self):\n    temp =self.first \n    while temp is not None:\n        print(temp.value)\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/queues/#enqueue","title":"enqueue","text":"<pre><code>def enqueue(self,value):\n    new_node=Node(value)\n    if self.first is None:\n        self.first=new_node\n        self.last=new_node\n    else:\n        self.last.next = new_node\n        self.last = new_node\n    self.length+=1\n</code></pre>"},{"location":"programming/python/data_structures/queues/#dequeue","title":"dequeue","text":"<pre><code>def dequeue(self):\n    if self.length==0:\n        return None \n\n    temp = self.first\n    if self.length==1:\n        self.first = None \n        self.last = None \n    else:\n        self.first = self.first.next \n        temp.next = None \n    self.length-=1\n    return temp \n</code></pre>"},{"location":"programming/python/data_structures/recursions/","title":"recursions","text":"<p>Recursion is a programming technique in which a function calls itself to solve a problem. Recursive functions break down complex problems into smaller, more manageable subproblems, and these subproblems are solved by invoking the same function recursively. </p> <p>examples using recursion</p> <pre><code># sum of factorial\ndef fact(n):\n    if n == 0: \n        return 1 \n    else:\n        return n*fact(n-1)\n</code></pre> <pre><code># sum of fibonacci numbers \ndef fib(n):\n    if n&lt;=1: \n        return n\n    else:\n        return fib(n-1)+fib(n-2)\n</code></pre> <pre><code># list of fib numbers\ndef fibonacci_list(n):\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n    else:\n        fib_list = fibonacci_list(n - 1)\n        fib_list.append(fib_list[-1] + fib_list[-2])\n        return fib_list\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#binary-search-treebst","title":"Binary Search Tree(BST)","text":""},{"location":"programming/python/data_structures/recursions/#contains","title":"contains","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\n    def __r_contains(self,current_node,value):\n      if current_node == None:\n        return False \n\n      if value == current_node.value:\n        return True\n\n      if value &lt; current_node.value:\n        return self.__r_contains(current_node.left,value)\n\n      if value&gt;current_node.value:\n        return self.__r_contains(current_node.right, value)\n\n    def r_contains(self,value):\n      return self.__r_contains(self.root,value)\n\n\n    def insert(self,value):\n        new_node = Node(value)\n        if self.root is None:\n            self.root=new_node\n            return True \n        temp = self.root\n        while(True):\n            # if the node of same value already exists\n            if new_node.value==temp.value: \n                return False\n\n            # if the node value is less then root value, add to left\n            if new_node.value &lt; temp.value:\n                if temp.left is None:\n                    temp.left = new_node\n                    return True\n                temp = temp.left\n            else:\n                # if the node value is greater then root value, add to right\n                if temp.right is None:\n                    temp.right = new_node\n                    return True\n                temp=temp.right\n\nmy_tree=BinarySearchTree()\nmy_tree.insert(2)\nmy_tree.insert(1)\nmy_tree.insert(3)\nprint(my_tree.r_contains(13))\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#insert","title":"insert","text":"<pre><code>\nclass Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\n    def __r_insert(self,current_node,value):\n\n        if current_node == None:\n            return Node(value)\n\n        if value &lt; current_node.value:\n            current_node.left = self.__r_insert(current_node.left,value)\n\n        if value &gt; current_node.value:\n            current_node.right = self.__r_insert(current_node.right,value)\n\n        return current_node\n\n    def r_insert(self,value):\n        if self.root==None:\n            self.root=Node(value)\n\n        self.__r_insert(self.root,value)\n\nmy_tree=BinarySearchTree()\nmy_tree.r_insert(2)\nmy_tree.r_insert(1)\nmy_tree.r_insert(3)\n\nprint(\"Root\", my_tree.root.value)\nprint(\"left:\", my_tree.root.left.value)\nprint(\"right:\", my_tree.root.right.value)\n</code></pre>"},{"location":"programming/python/data_structures/recursions/#delete","title":"delete","text":""},{"location":"programming/python/data_structures/sorting/","title":"sorting","text":"<p>Sorting refers to the process of arranging elements in a specific order, typically in ascending or descending order based on their values.</p>"},{"location":"programming/python/data_structures/sorting/#bubble","title":"bubble","text":"<p>Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.</p> <pre><code>def bubble_sort(my_list):\n    n = len(my_list)\n    for i in range(n):\n        # Last i elements are already in place, no need to compare them\n        for j in range(0, n-i-1):\n            if my_list[j] &gt; my_list[j+1]:\n                # Swap the elements if they are in the wrong order\n                my_list[j], my_list[j+1] = my_list[j+1], my_list[j]\n\n    return my_list\n\nprint(bubble_sort([3,5,1,4,2]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#selection","title":"selection","text":"<p>Selection Sort is a simple sorting algorithm that repeatedly selects the minimum (or maximum) element from the unsorted part of the array and places it at the beginning (or end) of the sorted part.</p> <pre><code>def selection_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        min_index = i\n        for j in range(i + 1, n):\n            if arr[j] &lt; arr[min_index]:\n                min_index = j\n\n        # Swap the minimum element with the first element in the unsorted part\n        arr[i], arr[min_index] = arr[min_index], arr[i]\n\nprint(selection_sort([64, 34, 25, 12, 22, 11, 90]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#insertion","title":"insertion","text":"<p>Insertion Sort is a simple sorting algorithm that builds the final sorted array one item at a time. It works by iteratively considering each element and inserting it into its correct position within the already sorted part of the array.</p> <pre><code>def insertion_sort(arr):\n    n = len(arr)\n    for i in range(1, n):\n        key = arr[i]  # Current element to be inserted\n        j = i - 1\n\n        # Move elements of arr[0..i-1], that are greater than key, to one position ahead of their current position\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\nprint(insertion_sort([64, 34, 25, 12, 22, 11, 90]))\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#merge","title":"merge","text":""},{"location":"programming/python/data_structures/sorting/#sorted-merge-list","title":"Sorted merge list","text":"<p>two merge list in the sorted order to make a single list</p> <pre><code>def merge_helper(list1,list2):\n    combined=[]\n    i=0\n    j=0\n    while i&lt;len(list1) and j&lt;len(list2):\n        if list1[i] &lt; list2[j]:\n            combined.append(list1[i])\n            i+=1\n        else:\n            combined.append(list2[j])\n            j+=1\n\n    while i &lt; len(list1):\n        combined.append(list1[i])\n        i+=1\n\n    while j &lt; len(list2):\n        combined.append(list2[j])\n        j+=1\n\n    return combined\n\ndef merge_sort(mylist):\n  if len(mylist) ==1:\n    return mylist\n  mid_index = int(len(mylist)/2) # find the mid of index\n  left = merge_sort(mylist[:mid_index]) # split list to left until 1 item in sorted list\n  right = merge_sort(mylist[mid_index:]) # split list to right until 1 item in sorted list\n\n  return merge_helper(left,right) # combine two sorted list to make one\n\nprint(merge_sort([3,1,4,2])) # [1, 2, 3, 4]\n</code></pre>"},{"location":"programming/python/data_structures/sorting/#big-o","title":"Big O","text":"<p>Space complexity: O(n) Time complexity: O(n log(n))</p>"},{"location":"programming/python/data_structures/sorting/#quick","title":"quick","text":"<p>Quick Sort is a widely used efficient sorting algorithm that follows the divide-and-conquer approach to sort an array or list of elements. It works by selecting a 'pivot' element from the array and partitioning the other elements into two subarrays: one containing elements less than the pivot and another containing elements greater than the pivot. The subarrays are then recursively sorted.</p> <pre><code>def swap(mylist,index1,index2):\n  mylist[index1],mylist[index2]=mylist[index2],mylist[index1]\n\n# return the index of the list\ndef pivot(mylist,pivot_index,end_index):\n  swap_index = pivot_index\n  for i in range(pivot_index+1,end_index+1):\n    if mylist[i]&lt;mylist[pivot_index]:\n      swap_index+=1\n      swap(mylist,swap_index,i)\n\n  swap(mylist,pivot_index,swap_index)\n  return swap_index\n\ndef quick_sort(mylist,left,right):\n  if left&lt;right:\n    pivot_index = pivot(mylist,left,right)\n    quick_sort(mylist,left,pivot_index-1)\n    quick_sort(mylist,pivot_index+1,right)\n  return mylist\n\nmylist = [4,6,1,7,3,2,5]\nprint(pivot(mylist,0,6))\n</code></pre>"},{"location":"programming/python/data_structures/stacks/","title":"stacks","text":"<p>A stack is a linear data structure that stores items in a Last-In/First-Out (LIFO) or First-In/Last-Out (FILO) manner. In stack, a new element is added at one end(push) and an element is removed from that end only(pop). </p> <p></p>"},{"location":"programming/python/data_structures/stacks/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.next = None \n\nclass Stack:\n    def __init__(self,value):\n        new_node = Node(value)\n        self.top = new_node \n        self.height=1\n\nmy_stack = Stack(4)\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#print-stack","title":"print stack","text":"<pre><code>\ndef print_stack(self):\n    temp = self.top \n    while temp is not None: \n        print(temp.value)\n        temp = temp.next\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#push","title":"push","text":"<pre><code>def push(self,value):\n    new_node = Node(value)\n    if self.height == 0:\n        self.top = new_node\n    else:\n        new_node.next =self.top \n        self.top = new_node\n    self.height+=1\n</code></pre>"},{"location":"programming/python/data_structures/stacks/#pop","title":"pop","text":"<pre><code>def pop_stack(self):\n    if self.height==0:\n        return None \n\n    temp = self.top \n    self.top = self.top.next \n    temp.next = None\n    self.height-=1\n    return temp\n</code></pre>"},{"location":"programming/python/data_structures/trees/","title":"trees","text":"<p>A tree consists of a root node, leaf nodes, and internal nodes. Each node is connected to its child via a reference, which is called an edge.</p> <p></p> <p>Root Node: The root node is the topmost node of a tree. It is always the first node created while creating the tree and we can access each element of the tree starting from the root node. e.g the node containing element 50 is the root node</p> <p>Parent Node: The parent of any node is the node that references the current node. e.g 50 is the parent of 20 and 45, and 20 is the parent of 11, 46, and 15. Similarly, 45 is the parent of 30 and 78.</p> <p>Child Node: Child nodes of a parent node are the nodes at which the parent node is pointing using the references. In the example above, 20 and 45 are children of 50. The nodes 11, 46, and 15 are children of 20 and 30 and 78 are children of 45.</p> <p>Edge: The reference through which a parent node is connected to a child node is called an edge. In the above example, each arrow that connects any two nodes is an edge.</p> <p>Leaf Node: These are those nodes in the tree that have no children. In the above example, 11, 46, 15, 30, and 78 are leaf nodes.</p> <p>Internal Nodes: Internal Nodes are the nodes that have at least one child. In the above example, 50, 20, and 45 are internal nodes.</p>"},{"location":"programming/python/data_structures/trees/#binary-search-treebst","title":"Binary Search Tree(BST)","text":"<p>If the value of the node is lesser than the root, it would be alligned to left or else right to the node of the tree. </p>"},{"location":"programming/python/data_structures/trees/#bst-big-o","title":"BST Big-O","text":"<p>When you want to access or search for particular node in the tree, you start from root if the value is lesser than the root, you would search from left of it, which means you would get rid of the right side of the tree. Similary vide versa for the value greater than the root node. </p> <p>The input number of nodes directs the output time resulting in an average time complexity of O(log(n)).</p> <p>Let's take the wrost case scenerio, the value of the node you are searching is greater, then you need to go to the end of the right of the tree, which is similar to the list. In such case, it would be O(n)</p> <p>Summary, </p> Operations stack BST Big-O lookup Not preferred Preferred O(log(n)) insert Preferred Not preferred O(n) or O(1) remove Not preferred Preferred O(log(n))"},{"location":"programming/python/data_structures/trees/#constructor","title":"Constructor","text":"<pre><code>class Node:\n    def __init__(self,value):\n        self.value = value \n        self.left = None \n        self.right=None\n\nclass BinarySearchTree:\n    def __init__(self):\n        self.root=None\n\nmy_tree=BinarySearchTree()\nprint(my_tree.root)\n</code></pre>"},{"location":"programming/python/data_structures/trees/#insert","title":"insert","text":"<pre><code>def insert(self,value):\n    new_node = Node(value)\n    if self.root is None:\n        self.root=new_node\n        return True \n    temp = self.root\n    while(True):\n        # if the node of same value already exists\n        if new_node.value==temp.value: \n            return False\n\n        # if the node value is less then root value, add to left\n        if new_node.value &lt; temp.value:\n            if temp.left is None:\n                temp.left = new_node\n                return True\n            temp = temp.left\n        else:\n            # if the node value is greater then root value, add to right\n            if temp.right is None:\n                temp.right = new_node\n                return True\n            temp=temp.right\n\nmy_tree=BinarySearchTree()\nmy_tree.insert(2)\nmy_tree.insert(1)\nmy_tree.insert(3)\nprint(my_tree.root.value) # root \nprint(my_tree.root.left.value) # left of root\nprint(my_tree.root.right.value) # right of root\n</code></pre>"},{"location":"programming/python/data_structures/trees/#search","title":"search","text":"<pre><code>def search(self,value):\n    if self.root == None:\n        return None\n    temp = self.root \n    while temp is not None:\n        if value &lt; temp.value:\n            temp = temp.left \n        elif value &gt; temp.value:\n            temp = temp.right\n        else:\n            return True \n    return False\n</code></pre>"},{"location":"programming/python/data_structures/trees/#tree-traversals","title":"Tree Traversals","text":""},{"location":"programming/python/data_structures/trees/#breadth-first-searchbfs","title":"Breadth-First Search(BFS)","text":"<p>Breadth-First Search (BFS) is a graph traversal algorithm used to explore all the nodes in a graph, starting from a specific source node and visiting its neighbors before moving on to their neighbors, and so on</p> <pre><code>def BFS(self):\n    current_node = self.root\n    queue=[]\n    results = []\n    queue.append(current_node)\n    while len(queue) &gt; 0:\n        current_node=queue.pop(0)\n        results.append(current_node.value)\n        if current_node.left is not None:\n            queue.append(current_node.left)\n        if current_node.right is not None:\n            queue.append(current_node.right)\n    return results\n</code></pre>"},{"location":"programming/python/data_structures/trees/#depth-first-searchdfs","title":"Depth-First Search(DFS)","text":"<p>DFS explores nodes in depth-first fashion, meaning it goes as deep as possible along a branch before backtracking to explore other branches.</p>"},{"location":"programming/python/data_structures/trees/#pre-order","title":"pre-order","text":"<p>First, append the root node and traverse all the way to the left and add the node value, and then back track until the root node and then move from right</p> <pre><code>def dfs_pre_order(self):\n    results = []\n\n    def traverse(current_node):\n        results.append(current_node.value)\n\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n    traverse(self.root)\n\n    return results\n</code></pre>"},{"location":"programming/python/data_structures/trees/#post-order","title":"post-order","text":"<p>First, move to the left of the root node and append the value. </p> <pre><code>\ndef dfs_post_order(self):\n    results = []\n\n    def traverse(current_node):\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n        results.append(current_node.value)\n\n    traverse(self.root)\n\n    return results \n</code></pre>"},{"location":"programming/python/data_structures/trees/#in-order","title":"in-order","text":"<pre><code>def dfs_in_order(self):\n    results = []\n\n    def traverse(current_node):\n        if current_node.left is not None:\n            traverse(current_node.left)\n\n        results.append(current_node.value)\n\n        if current_node.right is not None:\n            traverse(current_node.right)\n\n    traverse(self.root)\n\n    return results \n</code></pre>"},{"location":"programming/python/data_structures/trees/#output","title":"output","text":"<pre><code>\nmy_tree=BinarySearchTree()\nmy_tree.insert(47)\nmy_tree.insert(21)\nmy_tree.insert(76)\nmy_tree.insert(18)\nmy_tree.insert(27)\nmy_tree.insert(52)\n\nprint(\"root node = \", my_tree.root.value)\nprint(\"pre order = \",my_tree.dfs_pre_order())\nprint(\"post order = \",my_tree.dfs_post_order())\nprint(\"in order = \",my_tree.dfs_in_order())\n\nroot node =  47\npre order =  [47, 21, 18, 27, 76, 52]\npost order =  [18, 27, 21, 52, 76, 47]\nin order =  [18, 21, 27, 47, 52, 76]\n</code></pre>"},{"location":"programming/python/design_patterns/adapter/","title":"Adapter","text":"<p>The Adapter pattern is a structural design pattern that allows objects with incompatible interfaces to work together. It acts as a bridge between two incompatible interfaces, converting the interface of one class into another interface that the client expects</p> <pre><code>class PaymentGateway:\n    def process_payment(self, amount):\n        pass\n\nclass BankPaymentGateway:\n    def make_payment(self, amount):\n        print(f\"Making payment of ${amount} via Bank Payment Gateway\")\n\nclass PayPalPaymentGateway:\n    def send_payment(self, amount):\n        print(f\"Sending payment of ${amount} via PayPal Payment Gateway\")\n\nclass PaymentGatewayAdapter(PaymentGateway):\n    def __init__(self, payment_gateway):\n        self.payment_gateway = payment_gateway\n\n    def process_payment(self, amount):\n        if isinstance(self.payment_gateway, BankPaymentGateway):\n            self.payment_gateway.make_payment(amount)\n        elif isinstance(self.payment_gateway, PayPalPaymentGateway):\n            self.payment_gateway.send_payment(amount)\n\n# Usage\nbank_gateway = BankPaymentGateway()\npaypal_gateway = PayPalPaymentGateway()\n\nadapter1 = PaymentGatewayAdapter(bank_gateway)\nadapter2 = PaymentGatewayAdapter(paypal_gateway)\n\nadapter1.process_payment(100)  # Output: \"Making payment of $100 via Bank Payment Gateway\"\nadapter2.process_payment(200)  # Output: \"Sending payment of $200 via PayPal Payment Gateway\"\n\n</code></pre> <p>In this example, we have a common PaymentGateway interface that represents the desired interface for processing payments. The PaymentGateway class defines the process_payment() method.</p> <p>We also have two existing payment gateway classes: BankPaymentGateway and PayPalPaymentGateway. These classes have their own specific methods for making payments (make_payment() for the bank gateway and send_payment() for the PayPal gateway).</p> <p>The PaymentGatewayAdapter class acts as an adapter that implements the PaymentGateway interface and internally holds an instance of the specific payment gateway. It adapts the specific payment gateway's method calls to the process_payment() method of the PaymentGateway interface.</p> <p>In the usage section, we create instances of the specific payment gateways: bank_gateway and paypal_gateway. We then create adapter instances (adapter1 and adapter2) and pass the corresponding payment gateway instances to their constructors.</p> <p>When calling the process_payment() method on the adapter objects, they internally invoke the specific methods (make_payment() or send_payment()) of the respective payment gateway objects. The adapters bridge the gap between the common PaymentGateway interface and the specific payment gateway classes.</p>"},{"location":"programming/python/design_patterns/bridge/","title":"Bridge","text":"<p>The Bridge pattern is a structural design pattern that decouples an abstraction from its implementation, allowing them to vary independently. It provides a way to separate the interface and implementation of a class hierarchy, enabling them to evolve independently.</p> <pre><code>class Device:\n    def __init__(self):\n        self.state = False\n\n    def is_enabled(self):\n        return self.state\n\n    def enable(self):\n        self.state = True\n\n    def disable(self):\n        self.state = False\n\n\nclass RemoteControl:\n    def __init__(self, device):\n        self.device = device\n\n    def toggle_power(self):\n        if self.device.is_enabled():\n            self.device.disable()\n        else:\n            self.device.enable()\n\n    def volume_up(self):\n        pass\n\n    def volume_down(self):\n        pass\n\n\nclass TV(Device):\n    def __init__(self):\n        super().__init__()\n        self.volume = 50\n\n    def volume_up(self):\n        if self.volume &lt; 100:\n            self.volume += 10\n\n    def volume_down(self):\n        if self.volume &gt; 0:\n            self.volume -= 10\n\n    def get_volume(self):\n        return self.volume\n\n\nclass Radio(Device):\n    def __init__(self):\n        super().__init__()\n        self.volume = 30\n\n    def volume_up(self):\n        if self.volume &lt; 100:\n            self.volume += 5\n\n    def volume_down(self):\n        if self.volume &gt; 0:\n            self.volume -= 5\n\n    def get_volume(self):\n        return self.volume\n\n\n# Usage\ntv = TV()\nremote_control = RemoteControl(tv)\n\nremote_control.toggle_power()\nremote_control.volume_up()\nremote_control.volume_up()\nprint(tv.is_enabled())  # Output: True\nprint(tv.get_volume())  # Output: 70\n\nradio = Radio()\nremote_control = RemoteControl(radio)\n\nremote_control.toggle_power()\nremote_control.volume_down()\nprint(radio.is_enabled())  # Output: True\nprint(radio.get_volume())  # Output: 25\n</code></pre> <p>In this example, we have the Device class hierarchy, which represents different entertainment devices such as TVs and radios. Each device has its own implementation of enabling/disabling and adjusting volume.</p> <p>The RemoteControl class acts as the abstraction and holds a reference to a Device object. It provides methods for toggling power, increasing volume, and decreasing volume. These methods delegate the operations to the respective methods of the assigned device.</p> <p>The TV and Radio classes are concrete implementations of the Device class. They provide specific implementations for enabling/disabling and adjusting volume. In this example, they have additional methods get_volume() to retrieve the current volume.</p> <p>In the usage section, we create instances of the TV and Radio classes. Then, we create instances of the RemoteControl class, passing the respective device objects. We can then use the remote control to toggle power and adjust the volume, and we can retrieve the current state and volume of the devices.</p> <p>By using the Bridge pattern, we separate the abstraction (remote control) from its implementation (device). This allows us to independently extend and modify both the remote controls and the entertainment devices, and easily switch between different combinations of remote controls and devices.</p>"},{"location":"programming/python/design_patterns/builder-facet/","title":"Builder facet","text":"<p>Sometimes it would be difficuilt to build multiple attributes in the object and hence we would need to create a step-by-step process for builder creation object</p>"},{"location":"programming/python/design_patterns/builder-facet/#person-example","title":"Person Example","text":"<p>let's assume, you have a person class in which the object person needs to have two builders i.e  <code>personal</code> and <code>work</code></p> <pre><code>class Person:\n    def __init__(self):\n        print(f\"Creating person class interface\")\n\n        # personal\n\n        self.address = None\n        self.code = None\n        self.city = None\n\n        # professional\n        self.company = None\n        self.position = None\n        self.salary = None\n\n    def __str__(self):\n        return f\"address: {self.address} location in code {self.code} in city {self.city}\\n\" + \\\n                   f\"Employed at {self.company} as {self.position} with salary {self.salary}\"\n</code></pre> <p>We will create a two seperate builders one for the <code>personal</code> and another for <code>professional</code></p> <p>Seperate Builder Class</p> <pre><code>class PersonBuilder:\n    def __init__(self, person=None):\n        self.person = Person() if person is None else person\n\n    # function for personal builder\n    @property\n    def personal(self):\n        return PersonPersonalBuilder(self.person)\n\n    # function for work builder\n    @property\n    def work(self):\n        return PersonWorkBuilder(self.person)\n\n    def build(self):\n        return self.person\n</code></pre> <p>PersonalBuilder</p> <pre><code>class PersonPersonalBuilder(PersonBuilder):\n    def __init__(self, person):\n        super().__init__(person)\n\n    def location_at(self, address):\n        self.person.address = address\n        return self\n\n    def location_code(self, code):\n        self.person.code = code\n        return self\n\n    def location_city(self, city):\n        self.person.city = city\n        return self\n</code></pre> <p>Professional Work Builder</p> <pre><code>class PersonWorkBuilder(PersonBuilder):\n    def __init__(self, person):\n        super().__init__(person)\n\n    def work_company(self, company):\n        self.person.company = company\n        return self\n\n    def work_position(self, position):\n        self.person.position = position\n        return self\n\n    def work_salary(self, salary):\n        self.person.salary = salary\n        return self\n</code></pre> <p>Finally, start to build the person. </p> <pre><code>pb = PersonBuilder()\nperson1 = pb.personal.location_at(\"Bang\").location_city(\"Bang\").location_code(560026)\\\n            .work.work_company(\"ino\").work_position(\"engineer\").work_salary(\"232323\").build()\n\nprint(person1)\nperson2=PersonBuilder().build() # None object\nprint(person2)\n</code></pre> <p>Output</p> <pre><code>address: Bang location in code 560026 in city Bang\nEmployed at ino as engineer with salary 232323\naddress: None location in code None in city None\nEmployed at None as None with salary None\n</code></pre>"},{"location":"programming/python/design_patterns/builder/","title":"Builder","text":"<p>The Builder pattern is a well-known pattern in Python world. It\u2019s especially useful when you need to create an object with lots of possible configuration options.</p>"},{"location":"programming/python/design_patterns/builder/#example-1-violation","title":"Example - 1 [ Violation ]","text":"<pre><code>class Product:\n    def __init__(self):\n        self.name = None\n        self.price = None\n        self.quantity = None\n\n    def set_name(self, name):\n        self.name = name\n\n    def set_price(self, price):\n        self.price = price\n\n    def set_quantity(self, quantity):\n        self.quantity = quantity\n\n    def display(self):\n        print(f\"Product: {self.name}, Price: {self.price}, Quantity: {self.quantity}\")\n\n\n\nclass ProductBuilder:\n    def __init__(self):\n        self.product = Product()\n\n    def set_name(self, name):\n        self.product.set_name(name)\n\n    def set_price(self, price):\n        self.product.set_price(price)\n\n    def set_quantity(self, quantity):\n        self.product.set_quantity(quantity)\n\n    def build(self):\n        return self.product\n\n# the Product class exposes its setters publicly, \n# allowing the client to directly set the attributes \n# instead of going through the builder.\n\n# Usage\nbuilder = ProductBuilder()\nbuilder.set_name(\"Widget\")\nbuilder.set_price(9.99)\nbuilder.set_quantity(10)\nproduct = builder.build()\nproduct.display()  # Output: \"Product: Widget, Price: 9.99, Quantity: 10\"\n</code></pre> <p>To fix this violation and adhere to the Builder Pattern, we need to encapsulate the construction process within the builder class and make the attributes private.</p> <pre><code>class Product:\n    def __init__(self,name, price, quantity):\n        self.name=name\n        self.price=price\n        self.quantity=quantity\n\n    def display(self):\n        print(f\"Product: {self.name}, Price: {self.price}, Quantity: {self.quantity}\")\n\nclass ProductBuilder:\n    def __init__(self):\n        self.name=None\n        self.price=None \n        self.quantity=None\n\n    def set_name(self,name):\n        self.name=name\n        return self \n\n    def set_price(self,price):\n        self.price=price\n        return self \n\n    def set_quantity(self,quantity):\n        self.quantity=quantity\n        return self\n\n    def build(self):\n        return Product(self.name,self.price,self.quantity)\n\n\n#Usage \nbuilder=ProductBuilder()\nproduct=builder.set_name(\"Widget\").set_price(9.99).set_quantity(10).build()\nproduct.display() #Output: \"Product: Widget, Price: 9.99, Quantity: 10\"\n\n</code></pre>"},{"location":"programming/python/design_patterns/builder/#example-2","title":"Example - 2","text":"<p>https://refactoring.guru/design-patterns/builder/python/example</p>"},{"location":"programming/python/design_patterns/composite/","title":"Composite","text":"<p>The Composite pattern is a structural design pattern that allows you to treat individual objects and groups of objects uniformly. It composes objects into tree-like structures to represent part-whole hierarchies. The pattern enables clients to work with individual objects and groups of objects in a consistent manner.</p> <pre><code>from abc import ABC, abstractmethod\n\n# Component\nclass FileComponent(ABC):\n    @abstractmethod\n    def get_size(self):\n        pass\n\n# Leaf\nclass File(FileComponent):\n    def __init__(self, name, size):\n        self.name = name\n        self.size = size\n\n    def get_size(self):\n        return self.size\n\n# Composite\nclass Directory(FileComponent):\n    def __init__(self, name):\n        self.name = name\n        self.children = []\n\n    def add_child(self, child):\n        self.children.append(child)\n\n    def remove_child(self, child):\n        self.children.remove(child)\n\n    def get_size(self):\n        total_size = 0\n        for child in self.children:\n            total_size += child.get_size()\n        return total_size\n\n# Usage\nroot = Directory(\"Root\")\n\nfile1 = File(\"File1.txt\", 10)\nfile2 = File(\"File2.txt\", 15)\nfile3 = File(\"File3.txt\", 20)\n\nsubdirectory = Directory(\"Subdirectory\")\nfile4 = File(\"File4.txt\", 5)\n\nsubdirectory.add_child(file4)\n\nroot.add_child(file1)\nroot.add_child(file2)\nroot.add_child(file3)\nroot.add_child(subdirectory)\n\nprint(root.get_size())  # Output: 50\n</code></pre> <p>In this example, we have the FileComponent interface that defines the common operations for both individual files and directories. The File class represents a leaf component, which is an individual file with a specific size. The Directory class represents a composite component, which is a directory containing other files and subdirectories.</p> <p>The File class implements the get_size() method to return its own size.</p> <p>The Directory class implements the get_size() method, which recursively calculates the total size of all its children. It maintains a list of children and provides methods to add and remove them.</p> <p>In the usage section, we create instances of files and directories and organize them into a hierarchy. We add individual files and a subdirectory to the root directory. When we call the get_size() method on the root directory, it recursively calculates the total size of all its children, including the files in the subdirectory.</p> <p>The resulting output is the total size of all the files and subdirectories within the root directory: 50.</p> <p>By using the Composite pattern, we can treat individual files and directories uniformly as FileComponent objects. This allows us to work with complex tree-like structures in a unified manner, enabling easy traversal and manipulation of the hierarchy.</p>"},{"location":"programming/python/design_patterns/decorators/","title":"Decorator","text":""},{"location":"programming/python/design_patterns/decorators/#functional-decorators","title":"Functional Decorators","text":"<p>Function decorators in Python are a way to modify the behavior of a function without changing its source code. Decorators are implemented using the concept of higher-order functions, where a function takes another function as an argument and returns a modified version of it.</p> <pre><code>def uppercase_decorator(func):\n    def wrapper():\n        result = func()\n        return result.upper()\n    return wrapper\n\n@uppercase_decorator\ndef say_hello():\n    return \"Hello, World!\"\n\nprint(say_hello())  # Output: \"HELLO, WORLD!\"\n\n</code></pre> <p>In this example, we define a decorator function called <code>uppercase_decorator</code>. It takes a function (func) as an argument and returns a new function called wrapper. The wrapper function modifies the behavior of the original function by calling it and returning the result in uppercase.</p> <p>Function decorators are commonly used in Python for various purposes, such as logging, timing, authentication, and data validation. They provide a clean and reusable way to modify the functionality of functions without modifying their original code.</p>"},{"location":"programming/python/design_patterns/decorators/#classic-decorators","title":"Classic Decorators","text":"<pre><code>class TextComponent:\n    def render(self):\n        pass\n\nclass BaseTextComponent(TextComponent):\n    def __init__(self,text):\n        self.text = text \n\n    def render(self):\n        return self.text\n\n    def __str__(self):\n        return f'rendering the text {self.text}'\n\n\nclass BoldDecorator(TextComponent):\n    def __init__(self,text_component):\n        self.text_component=text_component\n\n    def render(self):\n        base_text=self.text_component.render()\n        return f\"&lt;b&gt;{base_text}&lt;/b&gt;\"\n\nclass ItalicDecorator(TextComponent):\n    def __init__(self,text_component):\n        self.text_component=text_component\n\n    def render(self):\n        base_text=self.text_component.render()\n        return f\"&lt;i&gt;{base_text}&lt;/i&gt;\"\n\ntext=\"Hello world\"\ncomponent = BaseTextComponent(text)\nbold_decorator=BoldDecorator(component)\nitalic_decorator=ItalicDecorator(component)\n\nprint(bold_decorator.render()) # Output: &lt;b&gt;Hello world&lt;/b&gt;\nprint(italic_decorator.render()) # Output: &lt;i&gt;Hello world&lt;/i&gt;\n</code></pre> <p>In this example, we have the TextComponent abstract class, which defines the common interface for rendering text components.</p> <p>The BaseTextComponent class is a concrete implementation of the TextComponent class. It represents the base text that can be rendered.</p> <p>The BoldDecorator and ItalicDecorator classes are concrete decorators that inherit from the TextComponent class. They wrap around existing text components and provide additional functionality. Each decorator adds its own formatting tags ( for bold,  for italic) around the base text. <p>In the usage section, we create an instance of the BaseTextComponent with the original text. We then wrap it with a BoldDecorator, followed by an ItalicDecorator. The decorators add the respective formatting tags around the base text.</p> <p>When calling the render() method on the italic_decorator, it invokes the render() method of the wrapped bold_decorator, which in turn invokes the render() method of the wrapped component. This way, the decorators stack up, adding the desired formatting to the text.</p> <p>The resulting output is the decorated text with both bold and italic formatting: \"Hello, World!\"</p> <p>By using the Decorator pattern, you can dynamically add or modify the behavior of objects at runtime by wrapping them with different decorators. This allows for flexible and extensible composition of objects with additional features or behaviors.</p>"},{"location":"programming/python/design_patterns/decorators/#dynamic-decorators","title":"Dynamic decorators","text":"<p>the flexibility of dynamic decorators, where the behavior of the decorator can be determined at runtime based on the provided options.</p> <pre><code>def dynamic_decorator(option):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            if option == 'uppercase':\n                result = func(*args, **kwargs)\n                return result.upper()\n            elif option == 'reverse':\n                result = func(*args, **kwargs)\n                return result[::-1]\n            else:\n                return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@dynamic_decorator('uppercase')\ndef say_hello():\n    return \"Hello, World!\"\n\nprint(say_hello())  # Output: \"HELLO, WORLD!\"\n\n@dynamic_decorator('reverse')\ndef say_hi():\n    return \"Hi, there!\"\n\nprint(say_hi())  # Output: \"!ereht ,iH\"\n\ndef do_nothing():\n    return \"Doing nothing.\"\n\nprint(do_nothing())  # Output: \"Doing nothing.\"\n</code></pre>"},{"location":"programming/python/design_patterns/factory-method/","title":"Factory Methods","text":"<p>Design pattern that provides an interface for creating objects, but allows <code>subclasess to decide which class to instantiate</code>. It provides loose coupling and flexibility in object creation. Factory Method pattern can be implemented using a base class or an abstract class and allowing subclasses to override a factory method</p> <pre><code>from abc import ABC, abstractmethod \n\n# Abstract class representing the product\nclass Animal(ABC):\n    @abstractmethod\n    def speak(self):\n        pass\n\n# Concrete class implementing the product\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof !\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow !\"\n\nclass Fox(Animal):\n    def speak(self):\n        return \"foxxx!\"\n\n# Creator class with factory method. \nclass AnimalFactory(ABC):\n    @abstractmethod\n    def create_animal(self)-&gt;Animal:\n        pass \n\nclass DogFactory(AnimalFactory):\n    def create_animal(self):\n        return Dog()\n\nclass CatFactory(AnimalFactory):\n    def create_animal(self):\n        return Cat()\n\nclass FoxFactory(AnimalFactory):\n    def create_animal(self):\n        return Fox()\n\n# Usage\n\ndog_factory=DogFactory()\ndog=dog_factory.create_animal()\nprint(dog.speak()) # Output: Woof !\n\nfox_factory=FoxFactory()\nfox=fox_factory.create_animal()\nprint(fox.speak()) # Output: Foxx !\n</code></pre>"},{"location":"programming/python/design_patterns/observer/","title":"Observer","text":"<p>The Observer Pattern is a behavioral design pattern used in software development to establish a one-to-many dependency between objects. In this pattern, one object (called the subject) maintains a list of dependent objects (observers) that are notified when the subject's state changes. This allows the observers to react and update themselves when the subject changes without the subject having to know about the observers specifically.</p> <pre><code># Define the Observer interface\nclass Observer:\n    def update(self, stock_symbol, stock_price):\n        pass\n\n# Define the Subject interface\nclass Subject:\n    def register_observer(self, observer):\n        pass\n\n    def remove_observer(self, observer):\n        pass\n\n    def notify_observers(self):\n        pass\n\n# Concrete implementation of the Subject\nclass StockMarket(Subject):\n    def __init__(self):\n        self.observers = []\n        self.stock_data = {}\n\n    def register_observer(self, observer):\n        self.observers.append(observer)\n\n    def remove_observer(self, observer):\n        self.observers.remove(observer)\n\n    def notify_observers(self):\n        for observer in self.observers:\n            observer.update(self.stock_data)\n\n    def set_stock_price(self, stock_symbol, stock_price):\n        self.stock_data[stock_symbol] = stock_price\n        self.notify_observers()\n\n# Concrete implementation of an Observer\nclass StockPriceDisplay(Observer):\n    def update(self, stock_data):\n        print(\"Stock Price Display:\")\n        for symbol, price in stock_data.items():\n            print(f\"{symbol}: {price}\")\n\n# Concrete implementation of another Observer\nclass StockAlert(Observer):\n    def update(self, stock_data):\n        for symbol, price in stock_data.items():\n            if price &gt; 200:\n                print(f\"Alert: {symbol} has crossed $100!\")\n\n# Main program\nif __name__ == \"__main__\":\n    stock_market = StockMarket()\n\n    price_display = StockPriceDisplay()\n    stock_alert = StockAlert()\n\n    stock_market.register_observer(price_display)\n    stock_market.register_observer(stock_alert)\n\n    # Simulate stock price changes\n    stock_market.set_stock_price(\"AAPL\", 150)\n    stock_market.set_stock_price(\"GOOGL\", 2500)\n    stock_market.set_stock_price(\"TSLA\", 800)\n</code></pre>"},{"location":"programming/python/design_patterns/prototype/","title":"Prototype","text":"<p>The Prototype design pattern is a creational design pattern that allows you to create new objects by cloning existing ones, rather than creating them from scratch.It promotes object reuse and reduces the need for subclassing</p> <pre><code>import copy\n\nclass Prototype:\n    def clone(self):\n        return copy.copy(self)\n\nclass Sheep(Prototype):\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def display(self):\n        print(f\"Sheep: {self.name}, Age: {self.age}\")\n\n# Usage\noriginal_sheep = Sheep(\"Shawn\", 2)\noriginal_sheep.display()  # Output: \"Sheep: Shawn, Age: 2\"\n\ncloned_sheep = original_sheep.clone()\ncloned_sheep.display()  # Output: \"Sheep: Shawn, Age: 2\"\n\ncloned_sheep.name = \"Dolly\"\ncloned_sheep.age = 3\n\ncloned_sheep.display()  # Output: \"Sheep: Dolly, Age: 3\"\n</code></pre> <p>In this example, we have a Prototype base class that defines the clone() method. The clone() method makes use of the copy.copy() function from the copy module to create a shallow copy of the object. This creates a new instance of the same class and copies the attributes of the original object to the clone.</p> <p>We can modify the cloned sheep's attributes independently. In the example, we change the cloned sheep's name to \"Dolly\" and age to 3. Calling the display() method on the cloned sheep shows the updated attribute values.</p>"},{"location":"programming/python/design_patterns/prototype/#prototype-using-factory-method","title":"Prototype using factory method","text":"<pre><code>import copy\n\nclass AnimalPrototype:\n    def clone(self):\n        return copy.copy(self)\n\n    def speak(self):\n        pass\n\nclass Dog(AnimalPrototype):\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat(AnimalPrototype):\n    def speak(self):\n        return \"Meow!\"\n\nclass AnimalFactory:\n    def __init__(self):\n        self.prototypes = {}\n\n    def register_prototype(self, animal_type, prototype):\n        self.prototypes[animal_type] = prototype\n\n    def create_animal(self, animal_type):\n        if animal_type in self.prototypes:\n            return self.prototypes[animal_type].clone()\n        else:\n            raise ValueError(\"Invalid animal type\")\n\n# Usage\nanimal_factory = AnimalFactory()\n\ndog_prototype = Dog()\nanimal_factory.register_prototype(\"Dog\", dog_prototype)\n\ncat_prototype = Cat()\nanimal_factory.register_prototype(\"Cat\", cat_prototype)\n\ndog = animal_factory.create_animal(\"Dog\")\nprint(dog.speak())  # Output: \"Woof!\"\n\ncat = animal_factory.create_animal(\"Cat\")\nprint(cat.speak())  # Output: \"Meow!\"\n\n</code></pre> <p>In this example, we have the AnimalPrototype base class that serves as the prototype for creating animal objects. It defines the clone() method to create a shallow copy of the object using copy.copy().</p> <p>The Dog and Cat classes inherit from AnimalPrototype and provide their own implementation of the speak() method.</p> <p>The AnimalFactory class acts as a factory that registers and creates animal objects based on their types. It maintains a dictionary of registered prototypes. The register_prototype() method allows new prototypes to be registered with their respective animal types, and the create_animal() method creates a new animal object based on the requested type by cloning the corresponding prototype.</p> <p>In the usage section, we create an instance of the AnimalFactory class. We then create instances of the Dog and Cat classes and register them as prototypes with their respective animal types using the register_prototype() method.</p> <p>Finally, we use the create_animal() method of the factory to create instances of animals by specifying their types. The factory retrieves the corresponding prototype, clones it, and returns the cloned object.</p> <p>By combining the Prototype and Factory Method patterns, we can create new animal objects by cloning existing prototypes, which promotes object reuse and avoids the need to create objects from scratch.</p>"},{"location":"programming/python/design_patterns/singleton/","title":"Singleton","text":"<p>The Singleton pattern is a creational design pattern that ensures that a class has only one instance and provides a global point of access to that instance.</p> <pre><code>class Singleton:\n    _instance = None\n\n    def __new__(cls):\n        if not cls._instance:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n\n# Usage\nsingleton1 = Singleton()\nsingleton2 = Singleton()\n\nprint(singleton1 is singleton2)  # Output: True\n\n</code></pre>"},{"location":"programming/python/design_patterns/singleton/#singleton-using-decorator","title":"Singleton using decorator","text":"<pre><code>def singleton(cls):\n    instances = {}\n\n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n\n    return wrapper\n\n@singleton\nclass Singleton:\n    def __init__(self, value):\n        self.value = value\n\n# Usage\nsingleton1 = Singleton(42)\nsingleton2 = Singleton(24)\n\nprint(singleton1 is singleton2)  # Output: True\nprint(singleton1.value)  # Output: 42\nprint(singleton2.value)  # Output: 42\n\n</code></pre> <p>In this example, we define a singleton decorator function. The decorator function takes a class as an argument and returns a wrapper function that manages the creation and storage of instances.</p> <p>The wrapper function checks if an instance of the class exists in the instances dictionary. If not, it creates a new instance of the class and stores it in the dictionary. Subsequent calls to the wrapper function return the existing instance.</p> <p>The @singleton decorator is then applied to the Singleton class. This means that whenever we create an instance of the Singleton class, it goes through the singleton decorator and returns the existing instance if it has already been created.</p>"},{"location":"programming/python/design_patterns/singleton/#another-example-for-decorator","title":"Another example for decorator","text":"<pre><code>def singleton(cls):\n    instances={}\n\n    def get_instance(*args,**kwargs):\n        if cls not in instances:\n            instances[cls]=cls(*args,**kwargs)\n        return instances[cls]\n\n    return get_instance\n\n\n@singleton\nclass Database:\n    def __init__(self):\n        print(\"loading from the database\")\n\nd1=Database()\nd2=Database()\n\nprint(d1 is d2) # Output: true\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/","title":"Solid principles","text":"<p>SOLID is an acronym that stands for five key design principles: </p> <ul> <li>Single responsibility principle</li> <li>Open-closed principle</li> <li>Liskov substitution principle</li> <li>Interface segregation principle</li> <li>Dependency inversion principle</li> </ul>"},{"location":"programming/python/design_patterns/solid_principles/#single-responsibility-principle-srp","title":"Single Responsibility Principle (SRP)","text":"<p>The Single Responsibility Principle (SRP) states that a class or module should have only one reason to change. In other words, each class or module should have a single responsibility or purpose.</p> <p>A class should be responsible for one specific task. A method should perform one specific action. A module should handle one specific area of functionality.</p> <p>Simplifies Maintenance: When a class has a single responsibility, it\u2019s easier to understand, debug, and modify. This reduces the likelihood of introducing bugs when making changes.</p> <p>Encourages Reusability: Classes that focus on one task are more reusable across different parts of the application or even in different projects.</p> <p>Makes Code Scalable: As your application grows, classes that follow SRP are easier to extend without becoming messy or bloated.</p> <p>Promotes High Cohesion: SRP ensures that each class or module is cohesive, meaning it focuses on a single task, which leads to more readable and maintainable code.</p> <p>Supports Low Coupling: By separating responsibilities, you reduce the dependency between classes, making your system more modular and flexible.</p> <pre><code>class Student:\n    def __init__(self, name, id_number):\n        self.name = name\n        self.id_number = id_number\n\n    def get_name(self):\n        return self.name\n\n    def get_id_number(self):\n        return self.id_number\n\n    def __str__(self):\n        return f\"name: {self.name} \\nid: {self.id_number}\\n\"\n\n    # Breaking SRP because, its saving to database and not associated with \n    Student class, so it should not be overloaded.\n    def register(self, filename):\n        with open(filename, \"w\") as fh:\n            fh.write(f\"name: {self.name} \\nid: {self.id_number}\\n\")\n</code></pre> <p>In order not to use the above rule, you would be modifying the rule as below and re-writing the function. </p> <pre><code>class StudentRegistration:\n    def save_to_file(self,student, filename):\n        with open(filename, \"w\") as fh:\n            fh.write(f\"name: {student.name} \\nid: {student.id_number}\\n\")\n</code></pre> <p>Create couple of students</p> <pre><code>student1=Student(\"Sunil\", \"1\")\nstudent2=Student(\"Shiva\", \"2\")\n</code></pre> <p>Save the student registration by using seperate class and save them.</p> <pre><code>register_student=StudentRegistration()\nfilename=r\"/Users/sunilamperayani/sandbox/design-patterns/students.db\"\nprint(\"saving the student to the database.\")\nregister_student.save_to_file(student1,filename)\n</code></pre> <p>Another example, </p> <pre><code>class FileReader:\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def read_file(self):\n        with open(self.file_path, 'r') as f:\n            return f.read()\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#open-closed-principle-ocp","title":"Open-Closed Principle (OCP)","text":"<p>The Open-Closed Principle (OCP) states that software entities (classes, modules, functions, etc.) should be open for extension but closed for modification. In other words, the behavior of a software entity should be easily extended without modifying its source code.</p> <p>Let's explain by example...</p> <p>You have a products, and you need to filter the products.</p>"},{"location":"programming/python/design_patterns/solid_principles/#without-ocp","title":"Without OCP","text":"<pre><code>class Product:\n    def __init__(self, name,size,color):\n        self.name = name \n        self.size = size\n        self.color = color\n\nclass ProductFilter:\n    def filter_by_size(self,product,size):\n        for p in products:\n            if p.size == size: yield p \n</code></pre> <p>Let's say you need to <code>filter_by_color</code> / <code>filter_by_color_and_size</code> / <code>filter_by_color_or_size</code>. In future, you have more parameters like \"weight, height, thickness etc\" .. you can't scale the class as such. </p> <p>Hence once you create a class you would need to close the class for modifications and open for extension.</p> <pre><code>apple = Product(\"Apple\", COLOR.GREEN, SIZE.SMALL)\ntree = Product(\"Tree\", COLOR.GREEN, SIZE.LARGE)\nhouse = Product(\"House\", COLOR.BLUE, SIZE.MEDIUM)\n\nproducts = [apple,tree,house]\n\npf = ProductFilter()\nprint('Green products (old):')\nfor p in pf.filter_by_color(products,COLOR.GREEN):\n    print(f' - {p.name} is green')\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#with-ocp","title":"With OCP","text":"<p>You create a new functionality for <code>specifications</code> which satisfies the criteria for filtering. We would create a two base classes which can be used to override logic in future cases. </p> <pre><code># Base class\nclass Specification:\n    def is_satisfied(self, item):\n        pass\n\n# Overide the base class and use your logic for \n# future application.\n\nclass ColorSpecification(Specification):\n    def __init__(self, color):\n        self.color = color\n\n    def is_satisfied(self, item):\n        return item.color == self.color\n\nclass SizeSpecification(Specification):\n    def __init__(self, size):\n        self.size = size\n\n    def is_satisfied(self, item):\n        return item.size == self.size\n\nclass HeightSpecifcation(Specification):\n    pass \n\nclass WeightSpecification(Specification):\n    pass\n\n</code></pre> <p>We will create filter specification and override from this section. </p> <pre><code># Base class\nclass Filter:\n    def filter(self, items, spec):\n        pass\n\n\n# Overrise the baseclass and use for your application/business logic.\n\nclass BetterFilter(Filter):\n    def filter(self, items, spec):\n        for item in items:\n            if spec.is_satisfied(item):\n                yield item\n</code></pre> <pre><code>apple = Product(\"Apple\", COLOR.GREEN, SIZE.SMALL)\ntree = Product(\"Tree\", COLOR.GREEN, SIZE.LARGE)\nhouse = Product(\"House\", COLOR.BLUE, SIZE.MEDIUM)\n\nproducts = [apple,tree,house]\n\nbf = BetterFilter()\n\nprint('Green products (new):')\ngreen = ColorSpecification(COLOR.GREEN)\nfor p in bf.filter(products,green):\n    print(f' - {p.name} is green')\n\nprint('Blue products (new):')\nblue = ColorSpecification(COLOR.BLUE)\nfor p in bf.filter(products,blue):\n    print(f' - {p.name} is blue')\n\nprint('Large products:')\nlarge = SizeSpecification(SIZE.LARGE)\nfor p in bf.filter(products, large):\n    print(f' - {p.name} is large')\n</code></pre> <p>Another example, </p> <pre><code>class Shape:\n    def area(self):\n        pass\n\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return 3.14 * self.radius**2\n\n\ndef calculate_total_area(shapes):\n    total_area = 0\n    for shape in shapes:\n        total_area += shape.area()\n    return total_area\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#liskov-substitution-principle-lsp","title":"Liskov substitution principle (LSP)","text":"<p>In this principle, if the program is using the base class, it should be able to work correctly with any derived class of that base class without needing to know the specific subclass. </p> <p>let's read using demo</p>"},{"location":"programming/python/design_patterns/solid_principles/#without-lsp","title":"Without LSP","text":"<p>we would calculate the area of <code>rectangle</code> and <code>square</code>. A rectable would have a <code>height</code> and <code>width</code> where as square would have all the sides as same so we would equate height=width and then calculate the result of the area. </p> <pre><code># Base class\nclass Rectangle:\n    def __init__(self, width, height)\n        self.width=width\n        self.height=height\n\n    def set_width(self,width):\n        self.width=width \n\n    def set_height(self,height):\n        self.height=height\n\n    def area(self):\n        return self.height * self.width\n\n# Derived/subclass\nclass Square(Rectangle):\n    # Override from the base class\n\n    # This breaks the LSP principle, you can see the output\n    def __init__(self,length):\n        super().__init__(length,length)\n\n    def set_width(self,width):\n        self.width=width \n        self.height=height\n\n    def set_height(self,height):\n        self.height=height\n        self.width = height\n\ndef print_area(rectangle):\n    rectangle.set_width(5)\n    rectangle.set_height(4)\n    print(\"Area\", rectangle.area())\n\nrectangle=Rectangle(5,4) # Area: 20\nsquare=Square(5) # Area: 16 \n</code></pre> <p>You might have seen the above square method, since its being overridden from the rectangle class, the value has been changed completely in violation of LSP. </p> <p>We would rectify now and see how we can use that prunciple. </p>"},{"location":"programming/python/design_patterns/solid_principles/#with-lsp","title":"With LSP","text":"<p>we would define the base class <code>shape</code> which can be overridden and calculates the area of the shape. </p> <pre><code>class Shape:\n    def area(self):\n        pass\n\nclass Rectangle(Shape):\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n\n    def area(self):\n        return self.width * self.height\n\nclass Square(Shape):\n    def __init__(self, length):\n        self.side_length = length\n\n    def area(self):\n        return self.side_length**2\n\ndef print_area(shape):\n    print(\"Area:\", shape.area())\n\nrectangle=Rectangle(5,3)\nsquare=Square(5)\n\nprint_area(rectangle) # Output: Area: 15\nprint_area(square) # Output: Area: 25\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#interface-segregation-principle-isp","title":"Interface Segregation Principle (ISP)","text":"<p>The Interface Segregation Principle (ISP) states that clients should not be forced to depend on interfaces they do not use.</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-1-violation","title":"Example - 1 [ Violation ]","text":"<pre><code>class Animal:\n    def move(self):\n        pass\n\n    def eat(self):\n        pass\n\n    def swim(self):\n        pass\n\n\nclass Fish(Animal):\n    def move(self):\n        print(\"Swimming\")\n\n    def eat(self):\n        print(\"Eating underwater\")\n\n    def swim(self):\n        print(\"Swimming\")\n\n\nclass Bird(Animal):\n    def move(self):\n        print(\"Flying\")\n\n    def eat(self):\n        print(\"Eating insects\")\n\n    def swim(self):\n        raise NotImplementedError()\n\n\nclass Client:\n    def __init__(self, animal):\n        self.animal = animal\n\n    def move_animal(self):\n        self.animal.move()\n\n    def feed_animal(self):\n        self.animal.eat()\n\n    def swim_animal(self):\n        self.animal.swim()\n\n\n# Usage\nfish = Fish()\nbird = Bird()\n\nfish_client = Client(fish)\nbird_client = Client(bird)\n\nfish_client.swim_animal() # Expected output: \"Swimming\"\nbird_client.swim_animal() # Raises NotImplementedError\n</code></pre> <p>In this example, we have an Animal interface that defines three methods: move, eat, and swim. The Fish and Bird classes implement the Animal interface. <code>The Fish class implements all three methods, while the Bird class implements only move and eat</code>.</p> <p>The Client class depends on the Animal interface, but it doesn't require all the methods defined in the interface. <code>The swim method is not relevant for birds, so it raises a NotImplementedError</code>.</p> <p>This violates the Interface Segregation Principle because the <code>Animal interface is too broad and forces clients to depend on methods they don't need</code>. In this case, the Bird class is forced to implement a method it doesn't need.</p> <p>In order to make this principle non-violating, we could create separate interfaces for <code>Swimmable, Flyable, and Eatable</code>, and let the clients depend on the specific interfaces they require.</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-2-non-violation","title":"Example - 2 [ Non-Violation ]","text":"<pre><code>from abc import ABC, abstractmethod\n\n\nclass Moveable(ABC):\n    @abstractmethod\n    def move(self):\n        pass\n\n\nclass Eatable(ABC):\n    @abstractmethod\n    def eat(self):\n        pass\n\n\nclass Swimmable(ABC):\n    @abstractmethod\n    def swim(self):\n        pass\n\n\nclass Fish(Moveable, Eatable, Swimmable):\n    def move(self):\n        print(\"Swimming\")\n\n    def eat(self):\n        print(\"Eating underwater\")\n\n    def swim(self):\n        print(\"Swimming\")\n\n\nclass Bird(Moveable, Eatable):\n    def move(self):\n        print(\"Flying\")\n\n    def eat(self):\n        print(\"Eating insects\")\n\n\nclass Client:\n    def __init__(self, moveable_animal, eatable_animal=None, swimmable_animal=None):\n        self.moveable_animal = moveable_animal\n        self.eatable_animal = eatable_animal\n        self.swimmable_animal = swimmable_animal\n\n    def move_animal(self):\n        self.moveable_animal.move()\n\n    def feed_animal(self):\n        if self.eatable_animal:\n            self.eatable_animal.eat()\n\n    def swim_animal(self):\n        if self.swimmable_animal:\n            self.swimmable_animal.swim()\n\n\n# Usage\nfish = Fish()\nbird = Bird()\n\nfish_client = Client(fish, fish, fish)\nbird_client = Client(bird, bird)\n\nfish_client.swim_animal()  # Expected output: \"Swimming\"\nbird_client.swim_animal()  # No output (bird cannot swim)\n\n</code></pre>"},{"location":"programming/python/design_patterns/solid_principles/#dependency-inversion-principle-dip","title":"Dependency Inversion Principle (DIP)","text":"<p>The Dependency Inversion Principle (DIP) is a principle in object-oriented design that states that high-level modules should not depend on low-level modules. Instead, both should depend on abstractions. It promotes decoupling and flexibility in the codebase</p>"},{"location":"programming/python/design_patterns/solid_principles/#example-1-violation_1","title":"Example - 1 [ Violation ]","text":"<pre><code>class PaymentProcessor:\n    def process_payment(self, amount):\n        print(f\"Processing payment of ${amount}\")\n\nclass PaymentService:\n    def __init__(self):\n        self.payment_processor = PaymentProcessor()\n\n    def perform_payment(self, amount):\n        self.payment_processor.process_payment(amount)\n\n# Usage\npayment_service = PaymentService()\npayment_service.perform_payment(100)  # Output: \"Processing payment of $100\"\n\n</code></pre> <p><code>PaymentService</code> class directory depends on the <code>PaymentProcessor</code>, creating an instance insite its consyructor there by, establishing a strong coupling between the two classes, violating <code>Dependency Inversion Principle</code></p>"},{"location":"programming/python/design_patterns/solid_principles/#example-2-fixing","title":"Example - 2 [ Fixing ]","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass PaymentProcessor(ABC):\n    @abstractmethod\n    def process_payment(self, amount):\n        pass\n\nclass CreditCardPaymentProcessor(PaymentProcessor):\n    def process_payment(self, amount):\n        print(f\"processing credit card payment {amount}\")\n\nclass PaypalPaymentProcessor(PaymentProcessor):\n    def process_payment(self, amount):\n        print(f\"processing paypal payment {amount}\")\n\nclass PaymentServices:\n    def __init__(self, payment_processor):\n        self.payment_processor=payment_processor\n\n    def perform_payment(self, amount):\n        self.payment_processor.process_payment(amount)\n\ncreditcard_processor=CreditCardPaymentProcessor()\npaypal_processor=PaypalPaymentProcessor()\n\npayment_service=PaymentServices(creditcard_processor)\npayment_service.perform_payment(100)\n\npayment_service=PaymentServices(paypal_processor)\npayment_service.perform_payment(200)\n</code></pre> <p>the <code>PaymentProcessor</code> abstract class, which serves as the abstraction that both the <code>PaymentService</code> and the concrete payment processors (<code>CreditCardPaymentProcessor and PayPalPaymentProcessor</code>) depend on</p> <p>By introducing the abstraction and decoupling the high-level and low-level modules, we adhere to the Dependency Inversion Principle, promoting flexibility, maintainability, and easier integration of new payment processors in the future</p>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/","title":"fastapi","text":""},{"location":"programming/python/webapp_framework/fastapi/fastapi/#install-fastapi","title":"Install FastAPI","text":"<pre><code>pip install \"fastapi[all]\"\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#hello-world-from-fastapi","title":"Hello World from FastAPI","text":"<pre><code>from fastapi import FastAPI\nimport uvicorn\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#path-parameters","title":"path parameters","text":"<p>You can declare path \"parameters\" or \"variables\" with the same syntax used by Python format strings path parameter item_id will be passed to your function as the argument item_id</p> <pre><code>@app.get(\"/items/{item_id}\")\nasync def read_item(item_id): # data convertion happens here\n    return {\"item_id\": item_id}\n</code></pre> <p>you can declare it with the data types</p> <pre><code>async def read_item(item_id: int ): # item_id is declared to be an int \n</code></pre> <p>All the data validation is performed under the hood by Pydantic, so you get all the benefits from it. You can use the same type declarations with str, float, bool and many other complex data types.</p> <p>Always remember, the order in which path parameters execute matters.  let's say you have two api calls, the one which is first defined will be returned. the path for /users/{user_id} would match also for /users/me, \"thinking\" that it's receiving a parameter user_id with a value of \"me\". </p> <p>If there is same two rest api end points, the one which is defined first would always be executed. </p> <pre><code>@app.get(\"/users/me\")\n#some code\n\n@app.get(\"/users/{user_id}\")\n#some code\n</code></pre>"},{"location":"programming/python/webapp_framework/fastapi/fastapi/#query-parameters","title":"query parameters","text":"<p>When you declare other function parameters that are not part of the path parameters, they are automatically interpreted as \"query\" parameters. The query is the set of key-value pairs that go after the ? in a URL, separated by &amp; characters.</p> <pre><code>fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n\n@app.get(\"/items/\")\nasync def read_item(skip: int = 0, limit: int = 10):\n    # Query params are: skip = 0, limit = 10\n    return fake_items_db[skip : skip + limit]\n</code></pre> <p>http://127.0.0.1:8000/items/?skip=0&amp;limit=10</p>"},{"location":"programming/python/webapp_framework/flask/flask/","title":"flask","text":""},{"location":"programming/python/webapp_framework/flask/flask/#flask-overview","title":"Flask Overview","text":""},{"location":"secretmgmt/vault/access_vault_token/","title":"access token","text":""},{"location":"secretmgmt/vault/access_vault_token/#vault-interfaces","title":"vault interfaces","text":"<p>when user is authenticated to vault in any form, vault would generate the token based on the policies that's being set and would return back the token for usage. We could use that token for any future authenticate again. </p> <p>tokens are the core method of authetication, most of the operations in vault require an existing token. </p> <ul> <li>The token auth method is responsible for creating and storing tokens</li> <li>The token auth method cannot be disabled</li> <li>Tokens can be used directly, or they can be used with another auth method</li> <li>Authenticating with an external identity (e.g. LDAP) dynamically generate tokens</li> <li>Tokens have one or more policies attached to control what the token is allowed to perform</li> </ul>"},{"location":"secretmgmt/vault/access_vault_token/#types-of-tokens","title":"types of tokens","text":"<p>service tokens are the default token type in Vault</p> <ul> <li>They are persisted to storage (heavy storage reads/writes)</li> <li>Can be renewed, revoked, and create child tokens</li> </ul> <p>batch tokens are encrypted binary large objects (blobs)</p> <ul> <li>Designed to be lightweight &amp; scalable</li> <li>They are NOT persisted to storage but they are not fully-featured</li> <li>Ideal for high-volume operations, such as encryption</li> <li>Can be used for DR Replication cluster promotion as well</li> </ul> <p></p> <p>Tokens carry information and metadata that determines how the token can be used, what type of token, when it expires, etc.</p> <ul> <li>Accessor</li> <li>Policies</li> <li>TTL</li> <li>Max TTL</li> <li>Number of Uses Left</li> <li>Orphaned Token</li> <li>Renewal Status</li> </ul> <pre><code>vault token lookup s.d1BCdhug8buTgAnSZhtPm8Hp\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#token-heirarchy","title":"token heirarchy","text":"<p>Each token has a time-to-live (TTL), except root token.</p> <p>Tokens are revoked once reached its TTL unless renewed</p> <ul> <li>Once a token reaches its max TTL, it gets revoked</li> <li>May be revoked early by manually revoking the token</li> <li>When a parent token is revoked, all of its children are revoked as well.</li> </ul>"},{"location":"secretmgmt/vault/access_vault_token/#controlling-token-life-cycle","title":"controlling token life cycle","text":""},{"location":"secretmgmt/vault/access_vault_token/#periodic-token-life-cycle","title":"Periodic token life cycle","text":"<ul> <li>Root or sudo users have the ability to generate periodic tokens</li> <li>Periodic tokens have a TTL, but no max TTL</li> <li>Periodic tokens may live for an infinite amount of time, so long as they are renewed within their TTL</li> </ul> <pre><code>vault token create -policy=training -period=24h\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#service-token-with-use-limit","title":"Service token with use limit","text":"<p>When you want to limit the number of requests coming to Vault from a particular token: - Limit the token's number of uses in addition to TTL and Max TTL - Use limit tokens expire at the end of their last use, regardless of their remaining TTLs - Use limit tokens expire at the end of their TTLs, regardless of remaining uses.</p> <pre><code>vault token create -policy=\"training\" -use-limit=2\nvault token lookup &lt;token&gt;\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#orphan-service-token","title":"Orphan service token","text":"<p>When the token hierarchy behavior is not desirable:</p> <ul> <li>Root or sudo users have the ability to generate orphan tokens</li> <li>Orphan tokens are not children of their parent; therefore, do not expire when their parent does</li> <li>Orphan tokens still expire when their own Max TTL is reached</li> </ul> <pre><code>vault token create -policy=\"training\" -orphan\nvault token lookup &lt;token&gt; \n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#set-token-types","title":"Set token types","text":"<pre><code>vault token create -policy=\"training\" -period=\"24h\"\n</code></pre> <p>configure the AppRole auth method to generate batch tokens</p> <pre><code>vault auth enable approle\nvault write auth/approle/role/training policies=\"training\" token_type=\"batch\" token_ttl=\"60s\"\n</code></pre> <p>configure the AppRole auth method to generate periodic tokens:</p> <pre><code>vault write auth/approle/role/jenkins policies=\"jenkins\" period=\"72h\"\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#managing-tokens-in-vault","title":"Managing Tokens in Vault","text":""},{"location":"secretmgmt/vault/access_vault_token/#cli","title":"CLI","text":"<pre><code>vault token create \u2013display_name=jenkins \u2013policy=training,certs \u2013ttl=24h \u2013explicit-max-ttl = 72h\n\nvault token create -ttl=5m -policy=training\nvault token lookup s.12VNpg4OA9tTdCd4V6ODuDRK\nvault token revoke s.12VNpg4OA9tTdCd4V6ODuDRK\nSuccess! Revoked token (if it existed)\n</code></pre> <p>Look up the capabilities of a token on a particular path</p> <pre><code>vault token capabilities s.dhtIk8VsE3Mj61PuGP3ZfFrg kv/data/apps/webapp\nvault token lookup s.dhtIk8VsE3Mj61PuGP3ZfFrg   # know ttl for this token\nvault token renew s.dhtIk8VsE3Mj61PuGP3ZfFrg\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#ui","title":"UI","text":"<p>skipping</p>"},{"location":"secretmgmt/vault/access_vault_token/#api","title":"API","text":"<p>parse the output for <code>.auth.client_token</code> thats the client token you need to read</p> <pre><code>curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/sunil | jq\n</code></pre> <p>store the token in a variable</p> <pre><code>token = $(curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/bryan |\njq -r \".auth.client_token\")\necho $token\n</code></pre> <p>Get the complete output and set into ENV variable</p> <pre><code>OUTPUT=$(curl --request POST --data @payload.json http://127.0.0.1:8200/v1/auth/userpass/login/sunil)\nVAULT_TOKEN=$(echo $OUTPUT | jq '.auth.client_token' -j)\necho $VAULT_TOKEN\n</code></pre> <p>Client token must be sent in the X-Vault-Token HTTP header and put into KV store</p> <pre><code>curl --header \"X-Vault-Token: s.dhtIk8VsE3Mj61PuGP3ZfFrg\" --request POST --data '{ \"apikey\": \"3230sc$832d\" }'https://vault.example.com:8200/v1/secret/apikey/splunk\n\n\ncurl --header \"X-Vault-Token: s.dhtIk8VsE3Mj61PuGP3ZfFrg\" --request GET https://vault.example.com:8200/v1/secret/data/apikey/splunk \n\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#root-token","title":"Root token","text":"<p>Root token is a superuser that has unlimited access to Vault</p> <ul> <li>It does NOT have a TTL \u2013 meaning it does not expire</li> <li>Attached to the root policy</li> <li>Note: Root tokens can create other root tokens that DO have a TTL</li> </ul> <p>Root tokens should NOT be used on a day-to-day basis</p> <ul> <li>In fact, rarely should a root token even exist</li> <li>Once you have used the root token, it should be revoked.</li> </ul> <p>Where Do Root Tokens Come From?</p> <p>Initial root token comes from Vault initialization</p> <ul> <li>Only method of authentication when first deploying Vault</li> <li>Used for initial configuration \u2013 such as auth methods or audit devices</li> <li>Once your new auth method is configured and tested, the root token should be revoked</li> </ul> <pre><code>vault token revoke s.dhtIk8VsE3Mj61PuGP3ZfFrg\n</code></pre> <p>Create a root token from an existing root token</p> <p>You can authenticate with a root token and run a vault token create.</p> <p>Now you have 2 root tokens</p> <pre><code>vault login s.lmmOCfNH1HZvvBwxnLErWrhK\nvault token create\n</code></pre> <p>Create a root token using unseal/recovery keys</p> <ul> <li>Helpful if you need to generate a root token in an emergency or a root token is needed for a particular task</li> <li>A quorum of unseal key holders can generate a new root token</li> <li>Enforces the \"no single person has complete access to Vault\"</li> </ul> <p>There are few steps to generate recovery root keys or unseal keys</p> <pre><code>vault operator generate-root -init # Generates the OTP\nvault operator generate-root # keep repeating until threshold is met and copy **Encoded Token**\nvault operator generate-root -otp=\"hM9q24nNiZfnYIiNvhnGo4UFc3\" -decode=\"G2NeKUZgXTsYYxILAC9ZFBguPw9ZXBovFAs\"\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#token-accessors","title":"token accessors","text":"<p>Every token has a token accessor that is used as a reference to the token</p> <p>Token accessors can be used to perform limited actions - Look up token properties - Look up the capabilities of a token - Renew the token - Revoke the token</p> <pre><code>vault login s.cbC7GJ6U6WJaDuDSgkyVcKDv\nvault token create -policy=training -ttl=30m\nvault token lookup -accessor gFq2UwnJ0jo87kESKwUcl1Ub\nvault token create -policy=training -ttl=30m # you can use token_accessor\nvault token revoke 2ogWa36gDH5wsO8VbuxroByx\nvault token renew -accessor gFq2UwnJ0jo87kESKwUcl1Ub\n</code></pre> <p>Cannot Use an Accessor to Perform Traditional Vault Actions</p> <pre><code>set VAULT_TOKEN=gFq2UwnJ0jo87kESKwUcl1Ub\nvault kv get secret/apps/training\nError making API request.\nURL: GET\nhttp://127.0.0.1:8200/v1/sys/internal/ui/mounts/secret/apps/training\nCode: 403. Errors:\n* permission denied\n</code></pre>"},{"location":"secretmgmt/vault/access_vault_token/#ttl-explanation","title":"TTL explanation","text":"<p>Every non-root token has a TTL, which is the period of validity (how long it's good for)</p> <p>TTL is based on the creation (or renewal) time: - Example: New token was created - valid for 30 minutes from now - Example: token was just renewed for 30 min = has a new 30m TTL</p> <p>When a token's TTL expires, the token is revoked and is no longer valid and cannot be used for authentication. - Renewal must take place before the TTL expires</p> <p>ttl examples</p> <p></p> <p></p> <p>Vault has a default TTL of 768 hours (which is 32 days), This can be changed in the Vault configuration file default_lease_ttl = 24h</p> <pre><code>vault token create \u2013policy=training \u2013ttl=60m\nvault write auth/approle/role/training-role token_ttl=1h token_max_ttl=24h\nvault token create \u2013policy=training\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/","title":"auth methods","text":""},{"location":"secretmgmt/vault/auth_methods/#introduction","title":"Introduction","text":"<ul> <li>vault components that perform authentication and manage identities.</li> <li>Responsible for assigning identity and policies to a user.</li> <li>Multiple authentication methods can be enabled depending on your use case.</li> <li>Auth methods can be differentiated by human vs. system methods.</li> <li>Once authenticated, vault will issue a client token used to make all subsequent vault requests (read/write).</li> <li>The fundamental goal of all auth methods is to obtain a token.</li> <li>Each token has an associated policy (or policies) and a TTL.</li> </ul> <p>fundamental goal of auth method is to get token, which are core method for auth within vault.</p> <p>token method is responsible for creating and storing token, which can't be disabled. Authenticating with external identity (LDAP, OIDC) will generate a token. if you are not supplying token for auth, you would get 403 error. </p> <p></p>"},{"location":"secretmgmt/vault/auth_methods/#working-with-auth-methods","title":"Working with auth methods","text":"<p>few auth methods which are valid and must be enabled before use..</p> <ul> <li>okta</li> <li>Github</li> <li>kubenetes</li> <li>kerberos</li> <li>username/password</li> <li>TLS certs</li> </ul> <p>The token auth method is enabled by default, and you cannot enable another nor disable the tokens auth method - New vault deployment will use a token for authentication - Only method of authentication for a new vault deployment is a root token. you can later change your auth once root token login. </p> <p>Auth methods can be enabled/disabled and configured using the UI, API, or CLI. You must provide a valid token to enable, disable, or modify auth methods in vault. The token must also have the proper privileges.</p> <p>Each auth method is enabled at a path. You can choose the path name when (and only when) you enable the auth method. If you do not provide a name, the auth method will be enabled at its default path</p> <p>auth methods</p>"},{"location":"secretmgmt/vault/auth_methods/#auth-using-cli","title":"auth using CLI","text":"<pre><code>vault auth enable approle or userpass\nvault auth disable approle or userpass\nvault auth list\n\nvault auth enable \u2013path=vault-course \u2013description=MyApps approle\n</code></pre> <p>vault auth -  Type of vault object you want to work with  enable  - Subcommand  \u2013path=vault-course - Customize the Path Name  \u2013description=MyApps - Add a description approle - Type of Auth Method</p> <p>After the auth method has been enabled, use the auth prefix to configure the auth method:</p> <pre><code>vault write auth/&lt;path name&gt;/&lt;option&gt; \n&lt;options&gt; = \"users\" for userpass or \"role\" for approle\n</code></pre> <p>auth methods using cli reference docs</p> <p>There are a few ways to authenticate to vault when using the CLI - Use the vault login command    - Authenticate using a token or another auth method    - Makes use of a token helper</p> <ul> <li>Use the vault_TOKEN Environment Variable</li> <li>Used if you already have a token</li> </ul> <pre><code>vault login  &lt;root token&gt; # uses token method\nvault login -method=userpass username=sunil # Once your username/password is correct, you would get the token. \n\n\u2013method=userpass - Type of Auth Method Used to Authenticate (not the enabled path)\n</code></pre> <p>Token Helper: Caches the token after authentication. Stores the token in a local file(.vault-token)so it can be referenced for subsequent requests.</p> <p></p> <p>HTTP API Response: </p> <p>Parsing the JSON Response to Obtain the vault Token</p> <pre><code>$ export vault_ADDR=\"https://vault.example.com:8200\"\n$ export vault_FORMAT=json\n$ OUTPUT=$(vault write auth/approle/login role_id=\"12345657\" secret_id=\"1nv84nd3821s\")\n$ vault_TOKEN=$(echo $OUTPUT | jq '.auth.client_token' -j)\n$ vault login $vault_TOKEN\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#auth-using-api","title":"auth using API","text":"<p>Create a new json object and save the file.</p> <pre><code>curl --header \"X-vault-Token: $vault_TOKEN\" --request POST --data '{\"type\": \"approle\"}' \\\nhttp://127.0.0.1:8200/v1/sys/auth/approle\n\n# The following command specifies that the tokens issued under the AppRole my-role should be associated with my-policy.\n\ncurl --header \"X-vault-Token: $vault_TOKEN\" --request POST --data '{\"policies\": [\"my-policy\"]}' \\\nhttp://127.0.0.1:8200/v1/auth/approle/role/my-role\n\n\n# fetches the RoleID of the role named my-role\n\ncurl --header \"X-vault-Token: $vault_TOKEN\" \\\nhttp://127.0.0.1:8200/v1/auth/approle/role/my-role/role-id | jq -r \".data\"\n\n</code></pre> <p>auth method using api reference docs</p> <p>Authentication requests to the vault HTTP API return a JSON response that include: - the token - the token accessor - information about attached policies</p> <p>It is up to the user to parse the response for the token and use that token for any subsequent requests  to vault.</p> <p>Autheticate to the vault server</p> <pre><code>curl --request POST --data @auth.json https://vault.example.com:8200/v1/auth/approle/login\n</code></pre> <p>If your role id and secret id are correct, then you would have below response where you have your token i.e  client_token. </p> <pre><code>{\n\"request_id\": \"0f874bea-16a6-c3da-8f20-1f2ef9cb5d22\",\n\"lease_id\": \"\",\n\"renewable\": false,\n\"lease_duration\": 0,\n\"data\": null,\n\"wrap_info\": null,\n\"warnings\": null,\n    \"auth\": {\n    \"client_token\": \"s.wjkffdrqM9QYTOYrUnUxXyX6\", -&gt; this is the user token\n    \"accessor\": \"Hbhmd3OfVTXnukBv7WxMrWld\",\n        \"policies\": [\n        \"admin\",\n        \"default\"\n        ],\n    }\n}\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#api-explorer","title":"API explorer","text":"<p>login to vault UI and to the right top corner, open an terminal and type \"API\". This will open a swagger which has all the API methods of the vault. You can try to send an request, if its authenticated correctly, it would response back.</p>"},{"location":"secretmgmt/vault/auth_methods/#vault-entities","title":"vault entities","text":"<ul> <li> <p>vault creates an entity and attaches an alias to it if a corresponding entity doesn't already exist.</p> </li> <li> <p>This is done using the Identity secrets engine, which manages internal identities that are recognized by vault</p> </li> <li> <p>An entity is a representation of a single person(userpass or LDAP) or system used(approle) to log into vault. Each has a unique value. Each entity is made up of zero or more aliases</p> </li> <li> <p>Alias is a combination of the auth method plus some identification. It is a mapping between an entity and auth method(s)</p> </li> </ul> <p>e.g let's say I am Sunil, I have to login to vault using to validate my creds to get the token. so I would be associated with different policies to get the different tokens, so I would always need to logout when requited to get secrets from different roles, which is cumbersome process... what if we have all policies grouped with single entity and provide entiry_id etc, so when I get creds it would use the entityy id to get the tokens for all the roles(Token inherits capabilities granted by both policies), so that I don't have to logout each and every time.</p> <p></p>"},{"location":"secretmgmt/vault/auth_methods/#vault-identity-groups","title":"vault identity groups","text":"<p>\u2022 A group can contain multiple entities as its members. \u2022 A group can also have subgroups. \u2022 Policies can be set on the group and the permissions will be granted to all members of the group.</p>"},{"location":"secretmgmt/vault/auth_methods/#vault-groups","title":"vault groups","text":"<p>Internal:</p> <p>Groups created in vault to group entities to propagate identical permissions, manually created</p> <p>Internal groups can be used to easily manage permissions for entities</p> <ul> <li>Frequently used when using vault Namespaces to propagate permissions down to child namespaces</li> <li>Helpful when you don't want to configure an identical auth method on every single namespace</li> </ul> <p>External:</p> <p>Groups which vault infers and creates based on group associations coming from auth methods, created manually or automatically.</p> <p>External groups are used to set permissions based on group membership from an external identity provider, such as LDAP, Okta, or OIDC provider. - Allows you to set up once in vault and continue manage permissions in the identity provider. - Note that the group name must match the group name in your identity provider</p>"},{"location":"secretmgmt/vault/auth_methods/#choosing-auth-methods","title":"Choosing auth methods","text":"<p>Many auth methods may satisfy the requirements, but often there's one that works \"the best\" for a situation e.g incase you are using a certain platform does not mean you need to use the related auth method e.g: Azure virtual machines can authenticate using the Azure auth method, but AppRole, Userpass, TLS, OIDC, etc. would still be a possibility. It's usually easy to eliminate auth methods based on the way they operate or integrate with applications</p> <p>Key words when choosing an auth method:</p> <p>Frequently Rotated - generally means a dynamic credential. - Meets the requirements: AWS, LDAP, Azure, GCP, K8s. - Does not meet the requirements: Userpass, TLS, AppRole.</p> <p>Remove Secrets from Process or Build Pipeline - generally means a dynamic or integrated credential. - Meets the requirements: AWS, Azure, GCP, K8s. - Does not meet the requirements: Userpass, LDAP.</p> <p>Use Existing User Credentials - Generally means you should integrate with an existing Identity Provider. - Meets the Requirement: OIDC, LDAP, Okta, GitHub. - Does not meet the requirements: Userpass, AWS, Azure, GCP.</p>"},{"location":"secretmgmt/vault/auth_methods/#human-based-auth","title":"Human based Auth","text":"<ul> <li>Integrates with an Existing Identity Provider</li> <li>Requires a Hands-On Approach to Use(Okta, Userpas, GitHub, JWT/OIDC, radius)</li> <li>Logging in via Prompt or Pop-up</li> <li>Often configured with the Platforms Integrated MFA</li> </ul>"},{"location":"secretmgmt/vault/auth_methods/#system-based-auth-methods","title":"System-Based Auth Methods","text":"<ul> <li>Uses non-human friendly methologies (not easy to remember) i.e tokens</li> <li>Usually Integrates with an Existing Platform(AWS, Azure, Oracle Cloud, kerberos, kubernetes, TLS certs)</li> <li>Vault validates credentials with the platform</li> </ul>"},{"location":"secretmgmt/vault/auth_methods/#lab","title":"lab","text":""},{"location":"secretmgmt/vault/auth_methods/#create-auth-method-of-userpass-and-get-the-user-token-from-vault","title":"Create auth method of userpass and get the user token from vault","text":"<pre><code>vault auth list\nvault auth enable userpass\nvault write auth/userpass/users/sunil password=sunil policies=sunil\nvault write auth/userpass/users/shiva password=shiva policies=shiva\nvault list auth/userpass/users\nvault read auth/userpass/users/sunil\nvault login -method=userpass username=sunil [Enter]\nPassword: sunil\n\n&lt;lists the token&gt;\n</code></pre>"},{"location":"secretmgmt/vault/auth_methods/#create-auth-method-of-approle-and-to-get-from-vault","title":"Create auth method of approle and to get from vault","text":"<pre><code>vault auth list\nvault auth enable approle\nvault write auth/approle/role/sunil policies=sunil token_ttl=20m\nvault list auth/approle/role/\nvault read auth/approle/role/sunil/role_id \nvault write -f auth/approle/role/sunil/secret-id \nvault write auth/approle/login role_id=\"\" secret_id=\"\"\n</code></pre>"},{"location":"secretmgmt/vault/policies/","title":"policies","text":""},{"location":"secretmgmt/vault/policies/#overview","title":"Overview","text":"<p>Vault policies provide operators a way to permit or deny access to certain paths or actions within Vault (RBAC).</p> <ul> <li>Gives us the ability to provide granular control over who gets access to secrets.</li> <li>Policies are written in declarative statements and can be written using JSON or HCL.</li> <li>When writing policies, always follow the principal of least privilege, in other words, give users/applications only the permissions they need.</li> <li>Policies are Deny by Default (implicit deny) - therefore you must explicitly grant to paths and related capabilities to Vault clients(No policy = no authorization).</li> <li>Policies support an explicit DENY that takes precedence over any other permission.</li> <li>Policies are attached to a token. A token can have multiple policies</li> </ul>"},{"location":"secretmgmt/vault/policies/#policy-types","title":"policy types","text":"<p>root policy is created by default \u2013 superuser with all permissions. - You cannot change nor delete this policy - Attached to all root tokens</p> <p>Note: The root policy does not contain any rules but can do anything within Vault. It should be used with extreme care.</p> <p>default policy is created by default \u2013 provides common permissions. - You can change this policy but it cannot be deleted - Attached to all non-root tokens by default (can be removed if needed)</p> <pre><code>vault policy list\nvalut policy read root\nvault policy read default\nvault policy write admin-policy /tmp/admin.hcl where, \n\npolicy - type of object you want to work with \nwrite - sub command\nadmin-policy - define the name of the policy you want to create\n/tmp/admin.hcl - location of the file containing pre-written policy.\n</code></pre>"},{"location":"secretmgmt/vault/policies/#manage-vault-policies","title":"Manage vault policies","text":""},{"location":"secretmgmt/vault/policies/#cli","title":"cli","text":"<p>Use the vault policy command has the below methods to perform management</p> <ul> <li>delete</li> <li>fmt</li> <li>list</li> <li>read</li> <li>write</li> </ul> <pre><code>vault policy list\nvault policy write webapp /tmp/webapp.hcl\nvault policy write packer /tmp/packer.hcl\n</code></pre>"},{"location":"secretmgmt/vault/policies/#http-api","title":"http API","text":"<pre><code>vim payload.json\n{\"policy\": \"path \\\"kv/apps/webapp\\\" { capabilities\u2026 \" }}\n\n# curl --header \"X-Vault-Token: s.bCEo8HFNIIR8wRGAzwXwkqUk\" --request PUT --data @payload.json \\\nhttp://127.0.0.1:8200/v1/sys/policy/webapp\n</code></pre>"},{"location":"secretmgmt/vault/policies/#permission","title":"permission","text":"<p>You know everything in vault is path based, hence you need to provide access or forbid access to these paths and operations.</p> <pre><code>path \"&lt;path&gt;\" {\n    capabilities = [ \"&lt;list of permissions&gt;\"]\n}\n\ne.g: \n\npath \"kv/data/apps/jenkns\" {\n    capabilities = [ \"read\", \"update\", \"delete\"]\n}\n</code></pre>"},{"location":"secretmgmt/vault/policies/#paths","title":"Paths","text":"<p>These are some of the standard paths that are defined in the vault.</p> <pre><code>sys/policy/vault-admin\nkv/apps/app01/web\nauth/ldap/group/developers\ndatabase/creds/prod-db\nsecrets/data/platform/aws/tools/ansible/app01\nsys/rekey\n</code></pre>"},{"location":"secretmgmt/vault/policies/#root-protected","title":"root-protected","text":"<ul> <li>Many paths in Vault require a root token or sudo capability to use</li> <li>These paths focus on important/critical paths for Vault or plugins</li> </ul> <p>e.g of root-protected paths:</p> <ul> <li>auth/token/create-orphan (create an orphan token)</li> <li>pki/root/sign-self-issued (sign a self-issued certificate)</li> <li>sys/rotate (rotate the encryption key)</li> <li>sys/seal (manually seal Vault)</li> <li>sys/step-down (force the leader to give up active status)</li> <li>sys/rotate (rotate the encryption key)</li> <li>sys/seal (manually seal Vault)</li> <li>sys/step-down (force the leader to give up active status)</li> </ul>"},{"location":"secretmgmt/vault/policies/#capabilities","title":"capabilities","text":"<p>Capabilities define what can we do? Capabilities are specified as a list of strings (yes, even if there's just one)</p> Capability HTTP Verb create POST/PUT read GET update POST/PUT delete DELETE list LIST Capability Description sudo Allows access to paths that are root-protected deny Disallows access regardless of any other defined capabilities <ul> <li>Create \u2013 create a new entry</li> <li>Read \u2013 read credentials, configurations, etc</li> <li>Update \u2013 overwrite the existing value of a secret or configuration</li> <li>Delete \u2013 delete something</li> <li>List \u2013 view what's there (doesn't allow you to read)</li> <li>Sudo \u2013 used for root-protected paths</li> <li>Deny \u2013 deny access \u2013 always takes presedence over any other capability</li> </ul> <p>example of deny polict in vault</p> <p></p>"},{"location":"secretmgmt/vault/policies/#customization","title":"customization","text":"<p>The glob (*) is a wildcard and can only be used at the end of a path, can be used to signify anything \"after\" a path or as part of a pattern.</p> <p>e.g</p> <p>secret/apps/application1/ - allows any path after application1 kv/platform/db- - would match kv/platform/db-2 but not kv/platform/db2</p> <p>i.e  secret - Path where the secrets engine is mounted apps/application1 - Path created on the secrets engine called secret * - Apply capabilities on anything AFTER application1</p> <pre><code>path \"secret/apps/application1/*\" {\ncapabilities = [\"read\"]\n} \n</code></pre> <p>Can I read from the following path? No, because the policy only permits read access for anything AFTER application1, not the path secret/apps/application1 itself.</p> <p>if you wanted to read, then change the policy...</p> <pre><code>path \"secret/apps/application1/*\" {\ncapabilities = [\"read\"]\n}\n\npath \"secret/apps/application1\" {\ncapabilities = [\"read\"]\n}\n</code></pre> <p>The plus (+) supports wildcard matching for a single directory in the path, can be used in multiple path segments (i.e., secret/+/+/db)</p> <p>e.g - secret/+/db - matches secret/db2/db or secret/app/db - kv/data/apps/+/webapp \u2013 matches the following: - kv/data/apps/dev/webapp - kv/data/apps/qa/webapp - kv/data/apps/prod/webapp</p> <p>secret/data/+/apps/webapp</p> <p>secret - Path where the secrets engineis mounted\\ data - Used for KV V2 Secrets Engine  + - Can be ANY Remaining path value  apps/webapp - Remaining path</p> <p>e.g combining + and * in policy. </p> <pre><code>path \"secret/+/+/webapp\" {\ncapabilities = [\"read\", \"list\"]\n}\n\npath \"secret/apps/+/team-*\" {\ncapabilities = [\"create\", \"read\"]\n}\n</code></pre>"},{"location":"secretmgmt/vault/policies/#acl","title":"acl","text":"<p>Use variable replacement in some policy strings with values available to the token, define policy paths containing double curly braces: {{}} <p>Creates a section of the key/value v2 secret engine to a specific user </p> <pre><code>path \"secret/data/{{identity.entity.id}}/*\" {\ncapabilities = [\"create\", \"update\", \"read\", \"delete\"]\n}\n\npath \"secret/metadata/{{identity.entity.id}}/*\" {\ncapabilities = [\"list\"]\n}\n</code></pre> <p></p> <p>Reference: https://developer.hashicorp.com/vault/tutorials/policies/policy-templating?in=vault%2Fpolicies</p> <p>How to identify the policies that are attached.</p> <pre><code>$ vault token create -policy=\"web-app\"\n\n# Authenticate with the newly generated token\n$ vault login &lt;token&gt;\n\n# Make sure that the token can read\n$ vault read secret/apikey/Google\n\n# This should fail\n$ vault write secret/apikey/Google key=\"ABCDE12345\"\n\n# Request a new AWS credentials\n$ vault read aws/creds/s3-readonly \n\n</code></pre>"},{"location":"secretmgmt/vault/policies/#admin-policies","title":"admin policies","text":"<ul> <li>Permissions for Vault backend functions live at the sys/ path</li> <li>Users/admins will need policies that define what they can do within Vault to administer Vault itself<ul> <li>Unsealing</li> <li>Changing policies</li> <li>Adding secret backends</li> <li>Configuring database configurations</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/secret_engine/","title":"secret engine","text":""},{"location":"secretmgmt/vault/secret_engine/#secrets","title":"secrets","text":""},{"location":"secretmgmt/vault/secret_engine/#static-secrets","title":"static secrets","text":"<p>what issues you get while using static secrets</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#dynamic-secrets","title":"dynamic secrets","text":""},{"location":"secretmgmt/vault/secret_engine/#example-for-application-using-vault","title":"example for application using vault","text":""},{"location":"secretmgmt/vault/secret_engine/#secret-engine","title":"secret engine","text":"<p>Secrets engines are components that can store, generate, or encrypt data - Many secrets engines can be enabled in Vault - You can even enable multiple instances of the same secrets engine - Secrets engines are plugins that extend the functionality of Vault</p> <p>Secrets engines are enabled and isolated at a path - All interactions with the secrets engine are done using the path - Path must be unique</p>"},{"location":"secretmgmt/vault/secret_engine/#secrets-as-a-service","title":"secrets-as-a-service","text":"<p>Use Vault to generate and manage the lifecycle of credentials on-demand - No more sharing credentials - Credentials get revoked automatically at the end of its lease - Audit trail can identify points of compromise - Use policies to control the access based on the client's role</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#enable-secret-engine","title":"enable secret engine","text":"<ul> <li>Cubbyhole and Identity are enabled by default (can\u2019t disable)</li> <li>Any other secrets engine must be enabled, enable using the CLI, API, or UI (most)</li> </ul> <p>Secrets engines are enabled and isolated at a path - All interactions with the secrets engine are done using the path - Path must be unique - Paths do not need to match the secrets engines name or type - Make them meaningful for you and your organization</p>"},{"location":"secretmgmt/vault/secret_engine/#responsibilities","title":"responsibilities","text":"<p>vault admin/securtiy team 1. Enable the Secrets Engine 2. Configure the connection to the backend platform (AWS, Database, etc.) 3. Create roles that define permissions to the backend platform 4. Create policies that grant permission to read from the secrets engine</p> <p>vault client(app/sercvices/users/machines)</p> <ol> <li>Read a set of credentials using token and associated policy</li> <li>Renew the lease before its expiration if needed (or permitted)</li> <li>Renew the token if needed (or permitted)</li> </ol> <p>How to enable vault secrets</p> <pre><code>vault secrets enable aws\nvault secrets tune -default-lease-ttl=72h pki/\n\nvault secrets list\nvault secrets list \u2013detailed\nvault secrets enable \u2013path=developers kv\nvault secrets enable \u2013description=\"my first kv\" kv\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#configure-secret-engine","title":"configure secret engine","text":"<p>configuring a secrets engine that will generate dynamic credentials. vault client must be authenticated before it can be requested for dynamic credentials.</p> <p>Step 1: Configure Vault with access to the platform</p> <p>Example 1: Vault to AWS </p> <p>The path is the default path for aws to configure <code>aws/config/root</code></p> <p>Provide credentials to a secrets engine that gives Vault permission to create, list, and delete credentials on the platform:</p> <pre><code>vault write aws/config/root access_key=AKIAIOSFODNN7EXAMPLE secret_key= wJalrXUtnFEMI/K7MDENGbPxRfiCYEXAMPLEKEY region=us-east-1\n</code></pre> <p>Example 2: vault to db</p> <p>default path for vault to configure database <code>database/config/prod-database</code></p> <pre><code>vault write database/config/prod-database \\\nplugin_name=mysql-aurora-database-plugin \\\nconnection_url=\"{{username}}:{{password}}@tcp(prod.cluster.us-east-1.rds.amazonaws.com:3306)/\" \\\nallowed_roles=\"app-integration, app-lambda\" \\\nusername=\"vault-admin\" \\\npassword=\"vneJ4908fkd3084Bmrk39fmslslf#e&amp;349\"\n</code></pre> <p>For other services, you need to refer to the documentation.</p> <p>Step 2: Configure Roles based on permissions needed</p> <p>Vault does not know what permissions, groups, and policies you want to attach to generated credentials. Each role maps to a set of permissions on the targeted platform.</p> <p></p> <pre><code>vault read aws/creds/data-consultant\n</code></pre> <p></p> <pre><code>vault read database/creds/oracle-reporting\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#kv-secret-engine","title":"kv secret engine","text":"<p>Key/Value secrets engine is used to store static secrets - There are two versions: v2 (kv-v2) is versioned but v1 (v1) is not. - Secrets are accessible via UI, CLI, and API \u2013 interactive or automated - Access to KV paths are enforced via policies (ACLs).</p> <p>like everything else in Vault, secrets written to the KV secrets engine are encrypted using 256-bit AES.</p> <p>Key/Value secrets engine can be enabled at different paths, each key/value secrets engine is isolated and unique - Secrets are stored as key-value pairs at a defined path \u2013 (e.g.,secret/applications/web01) - Writing a new secret will replace the old value (i.e v1 or v2). - Writing a new secret requires the create capability. - Updating/overwriting a secret to an existing path requires update capability.</p> <p>When you run Vault in \u2013dev server mode, Vault enables a KV v2 secrets engine at the secret/ path, by default</p>"},{"location":"secretmgmt/vault/secret_engine/#organize-kv-engine","title":"organize kv engine","text":"<p>Organize Data However It Makes Sense to Your Organization</p> <p></p> <p></p> <pre><code>vault secrets enable kv\nvault secrets enable \u2013path=training kv\nvault secrets list \u2013detailed\n\nvault secrets enable kv-v2\nvault secrets enable \u2013path=training \u2013version=2 kv\nvault secrets list \u2013detailed\n</code></pre> <p>how v2 is different from v1?</p> <p>Introduces two prefixes that must be accounted for when referencing secrets and/or metadata</p> <ul> <li>cloud/data \u2013 data is where the actual K/V data is stored</li> <li>cloud/metadata \u2013 the metadata prefix stores our metadata about a secret</li> </ul> <p>The data/ and metadata/ prefix is required for API and when writing Vault policies It does NOT change the way you interact with the KV store when using the CLI.</p>"},{"location":"secretmgmt/vault/secret_engine/#working-with-kv-engine","title":"working with kv engine","text":"<p>Use the vault kv command - put - write data to the KV - get - read data from the KV - delete - delete data from the KV - list - list data within the KV (paths)</p> <p>Only available for KV V2</p> <ul> <li>undelete - undelete version of secret</li> <li>destroy - permanently destroy data</li> <li>patch - add specific key in the KV</li> <li>rollback - recover old data in the KV</li> </ul> <pre><code>vault kv put kv/app/db pass=123 \nvault kv put kv/app/db pass=123 user=admin api=a8ee4b50cce124\nvault kv put kv/app/db @secrets.json\n\nvault kv get kv/app/db\nvault kv put kv/app/db api=39cms1204mfi2m\n\nvault kv rollback -version=1 kv/app/db\nvault kv patch kv/app/db user=bryan\nvault kv get kv/app/db\nvault kv get -format=json kv/app/db\n\nvault kv get \u2013version=3 kv/app/db\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#deleting-kv","title":"deleting kv","text":"<p>If the latest version of the secret has been deleted (KV V2), it will return the related metadata.</p> <p>You can read a previous version of a secret (if one exists) by adding the \u2013version=x flag to the request</p> <ul> <li>A delete on KV V1 is a delete \u2013 the data is destroyed</li> <li>You'd have to restore Vault/Consul to retrieve the old data</li> <li>A delete on KV V2 is a soft delete \u2013 data is not destroyed</li> <li>Data can be restored with a undelete/rollback action</li> <li>A destroy (only KV V2) is a permanent action \u2013 destroyed on disk</li> <li>Cannot be restored except for a Vault/Consul restore action</li> </ul> <pre><code>vault kv delete secret/app/database #latest version is deleted\nvault kv delete secret/app/database --version=2 # previous version deleted\nvault kv get secret/app/database\n</code></pre> <pre><code>vault kv destroy \u2013versions=3 secret/app/web\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#cubbyhole","title":"cubbyhole","text":"<p>cubbyhole secret engine is used to store the arbitary secrets enabled by default at cubbyhole/ path. Its lifeline is linked to the token used to write data. </p> <ul> <li>no concept of TTL or any refresh tokens</li> <li>even root token cannot be read if its not written by root. </li> </ul> <p>cubbyhole secrets engine cannot be disabled, moved, or enabled multiple times. </p> <ul> <li>each service will have its own cubbyhole. </li> <li>one token of cubbyhole cannot access another services cubbyhole. </li> <li>cubbyhole expires when token expires. </li> </ul> <pre><code>vault secrets list\nvault write cubbyhole/training certification=hcvop\nvault read cubbyhole/training\n\ncurl  --header \"X-Vault-Token: ...\" --request POST --data '{\"certification\":\"hcvop\"}' http://127.0.0.1:8200/v1/cubbyhole/training\n\ncurl --header \"X-Vault-Token: ...\" --request LIST http://127.0.0.1:8200/v1/cubbyhole/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#wrapping-response","title":"wrapping response","text":"<p>If one user has to send slack token to another, it can't be shared across any messager as it would be sent in the plain text, hence we use wrapping resonse where the token creates a cubbyhole and the other user can access using it. </p> <p>When requested, Vault can take the response it would have sent to an HTTP client and instead insert it into the cubbyhole of a single-use token, returning that single-use token instead. </p> <p>Logically speaking, the response is wrapped by the token, and retrieving it requires an unwrap operation against this token. Functionally speaking, the token provides authorization to use an encryption key from Vault's keyring to decrypt the data.</p> <p></p>"},{"location":"secretmgmt/vault/secret_engine/#benefits","title":"benefits","text":"<ul> <li>privacy</li> <li>malfeasance detection</li> <li>limitation of the lifetime secret exposure</li> </ul> <p>Reference: https://developer.hashicorp.com/vault/docs/concepts/response-wrapping</p> <pre><code>vault kv get -wrap-ttl=5m secrets/certification/hcvop\nvault token lookup &lt;wrap_token&gt;\n\nvault unwrap &lt;wrap-token&gt;\nOR\nvault VAULT_TOKEN=&lt;wrap-token&gt; vault unwrap\nOR\nvault login &lt;wrap-token&gt;\nvault unwrap\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#vault-transit-secret-engine","title":"vault transit secret engine","text":"<p>Transit secrets engine provides functions for encrypting/decrypting data,  Enables organizations to outsource/centralize encryption to Vault.</p> <ul> <li>Applications can send cleartext data to Vault for encryption</li> <li>Vault encrypts using the specified key and returns ciphertext to the app</li> <li>The application NEVER has access to the encryption key (stored in Vault)</li> <li>Decouples storage from encryption and access control</li> </ul> <p></p> <p>Note: Transit secrets engine DOES NOT STORE the encrypted data. It would encrypt and returns cipher teext back to application.</p> <ul> <li>Encryption keys are created and stored in Vault to process data</li> <li>Each application can have its own encryption key (or more!)</li> <li> <p>Apps must have permission to use the key for encryption/decryption operations, which is bound by the policy attached to its token.</p> </li> <li> <p>Keys can be easily rotated as often as needed</p> </li> <li>Keys are stored on keyring</li> <li>Can limit what version(s) of keys can be used for decryption</li> <li>You can create, rotate, delete, and export a key (need permissions)</li> <li>Easily rewrap ciphertext with a newer version of a key</li> </ul> <p></p> <p></p> <ul> <li>Vault also supports convergent encryption mode</li> <li>Means that every time you encrypt the same data, you'll get the same ciphertext back</li> <li>This enables you to have searchable ciphertext</li> </ul>"},{"location":"secretmgmt/vault/secret_engine/#working-with-vault-transit-secret-engine","title":"working with vault transit secret engine","text":"<p>Before you can use the Transit secrets engine to encrypt data, it must first be enabled we can use the default path of transit or enable on another path.</p> <p>The next step is to create one or many encryption keys used to encrypt/decrypt data</p> <p>Encrypt</p> <pre><code>vault secrets enable transit\nvault write -f transit/keys/vault_training\nvault write -f transit/keys/training_rsa type=\"rsa-4096\"\n</code></pre> <p>Pass the cleartext data to Vault \u2013 specifying the action and desired encryption key to use</p> <pre><code>vault write transit/encrypt/training plaintext=$(base64 &lt;&lt;&lt; \"Getting Started with HashiCorp Vault\")\n</code></pre> <p></p> <p>Decrypt</p> <pre><code>vault write transit/decrypt/training ciphertext=\"vault:v1:Fpyph6C7r5MUILiEiFhCoJBxelQbsGeEahal5LhDPSoN6HkTO\u2026\u2026\u2026\"\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#rotating-encryption-keys","title":"Rotating encryption keys","text":"<ul> <li>Transit allows for a simplified key rotation process</li> <li> <p>keys can be rotated manually or by an  automated process <li> <p>Vault maintains a versioned keyring</p> </li> <li>All versions of the encryption key are stored</li> <li>Vault admins can limit the minimum key version allowed to be used for decryption operations (older keys won't work)</li> <li>You can rewrap encrypted data (ciphertext) to use a newer version of the encryption key</li> <pre><code>vault write \u2013f transit/keys/training/rotate\nvault read transit/keys/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#key-configuration","title":"key configuration","text":"<ul> <li>We can limit what version of the key can be used to decrypt data</li> <li>Maybe we have old data that we have converted and don't want anybody to be able to decrypt it</li> <li>This is configured using the minimum key version configuration</li> <li>It can be configured for each encryption key (not key version)</li> </ul> <pre><code>vault write transit/keys/training/config min_decryption_version=4\nvault read transit/keys/training\n</code></pre>"},{"location":"secretmgmt/vault/secret_engine/#rewrapping-ciphertex","title":"Rewrapping Ciphertex","text":"<p>How can we upgrade our encrypted data to be encrypted by the latest version of the key?</p> <p>The data was never available in plaintext when rewrapping the data with the latest version of the key</p> <pre><code>vault write transit/rewrap/training ciphertext=\"vault:v1:Fpyph6C7r5MUILiEiFhCoJBxelQbsGeEahal5LhDPSoN6 HkTOhwn79DCwt0mct1ttLokqikAr0PAopzm2jQAKJg=2/QGPTMnzKPlw4cCPGTbkzE PlX5OyPkLIgX+erFWdUXKkKUIEbb6D2Gm5ZjTaola314LsVkbLF5G1RkBTACtskk=\"\n</code></pre>"},{"location":"secretmgmt/vault/vault_agent/","title":"agent","text":"<p>The Vault Agent is a client daemon that runs alongside an application to enable legacy applications to interact and consume secrets.</p> <p>Vault Agent provides several different features: - Automatic authentication including renewal - Secure delivery/storage of tokens (response wrapping) - Local caching of secrets - Templating</p>"},{"location":"secretmgmt/vault/vault_agent/#auto-auth","title":"auto-auth","text":"<ul> <li>The Vault Agent uses a pre-defined auth method to authenticate to Vault and obtain a token</li> <li>The token is stored in the \"sink\", which is essentially just a flat file on a local file system that contains  the Vault token</li> <li>The application can read this token and invoke the Vault API directly</li> <li>This strategy allows the Vault Agent to manage the token and guarantee a valid token is always available to the application</li> </ul> <p>Vault Agent supports many types of auth methods to authenticate and obtain a token. </p> <p>Auth methods are generally the methods you'd associate with \"machine-oriented\" auth methods - AliCloud - AppRole - AWS - Azure - Certificate - CloudFoundary - GCP - JWT - Kerberos - Kubernetes - LDAP</p> <p>Vault Agent Configuration File</p> <pre><code>auto_auth {\n    method \"approle\" {\n        mount_path = \"auth/approle\"\n        config = {\n        role_id_file_path = \"&lt;path-to-file&gt;\"\n        secret_id_file_path = \"&lt;path-to-file&gt;\"\n        }\n    }\n    sink \"file\" {\n        config = {\n        path = \"/etc/vault.d/token.txt\"\n        }\n    }\n}\n\nvault {\naddress = \"http://&lt;cluster_IP&gt;:8200\"\n}\n</code></pre> <p>file is the only supported method of storing the auto-auth token</p> <p>Common configuration parameters include: \u2022 type \u2013 what type of sink to use (again, only file is available) \u2022 path \u2013 location of the file \u2022 mode \u2013 change the file permissions of the sink file (default is 640) \u2022 wrap_ttl = retrieve the token using response wrapping</p>"},{"location":"secretmgmt/vault/vault_agent/#protecting-auto-auth","title":"protecting auto-auth","text":"<p>To help secure tokens when using Auth-Auth, you can have Vault response wrap the token when the Vault Agent authenticates \u2022 Response wrapped by the auth method \u2022 Response wrapped by the token sink</p> <p>The placement of the wrap_ttl in the Vault Agent configuration file determines where the response wrapping happens.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_agent/#vault-templating","title":"vault templating","text":""},{"location":"secretmgmt/vault/vault_agent/#consul-template","title":"Consul Template","text":"<p>A standalone application that renders data from Consul or Vault onto the target file system \u2022 https://github.com/hashicorp/consul-template \u2022 Despite its name, Consul Template does not require a Consul cluster to operate</p> <p>Consul Template retrieves secrets from Vault \u2022 Manages the acquisition and renewal lifecycle \u2022 Requires a valid Vault token to operate</p> <p>Consule template workflow</p> <p></p>"},{"location":"secretmgmt/vault/vault_agent/#vault-agent-templating","title":"vault agent templating","text":"<p>To further extend the functionality of the Vault Agent, a subset of the Consul-Template functionality is directly embedded into the Vault Agent. i.e No need to install the Consul-Template binary on the application server</p> <p>Vault secrets can be rendered to destination file(s) using the ConsulTemplate markup language, Uses the client token acquired by the auto-auth configuration.</p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_architecture/","title":"architecture","text":""},{"location":"secretmgmt/vault/vault_architecture/#vault-components","title":"vault components","text":""},{"location":"secretmgmt/vault/vault_architecture/#storage-backends","title":"storage backends","text":"<p>The storage stanza configures the storage backend, which represents the location for the durable storage of Vault's information. These are configured in the file and encryted in transit. </p> <p>There is only 1 storage backend/cluster</p> <p>https://developer.hashicorp.com/vault/docs/configuration/storage</p>"},{"location":"secretmgmt/vault/vault_architecture/#secrets-engine","title":"secrets engine","text":"<p>Secrets engines are components which store, generate, or encrypt data. Secrets engines are incredibly flexible, so it is easiest to think about them in terms of their function. Secrets engines are provided some set of data, they take some action on that data, and they return a result.</p> <p>Some secrets engines simply store and read data - like encrypted Redis/Memcached. Other secrets engines connect to other services and generate dynamic credentials on demand(GCP, AWS Secrets etc). Other secrets engines provide encryption as a service, totp generation, certificates, and much more.</p> <p>Secrets engines are enabled at a path in Vault. When a request comes to Vault, the router automatically routes anything with the route prefix to the secrets engine</p> <p>(secret engine life cycle)[https://developer.hashicorp.com/vault/docs/secrets#secrets-engines-lifecycle]</p> <p>https://developer.hashicorp.com/vault/docs/secrets</p>"},{"location":"secretmgmt/vault/vault_architecture/#auth-methods","title":"auth methods","text":"<p>Auth methods are the components in Vault that perform authentication and are responsible for assigning identity and a set of policies to a user. In all cases, Vault will enforce authentication as part of the request processing. In most cases, Vault will delegate the authentication administration and decision to the relevant configured external auth method (e.g., Amazon Web Services, GitHub, Google Cloud Platform, Kubernetes, Microsoft Azure, Okta ...).</p> <p>Once authenticated, vault will issue a client token used to make all subsequent vault requests.  - The main goal of all auth methods is to obtain a token. - each token has an associated policy(or policies) and a TTL(token validity)</p> <p>https://developer.hashicorp.com/vault/docs/auth</p>"},{"location":"secretmgmt/vault/vault_architecture/#audit-devices","title":"audit devices","text":"<p>Audit devices are the components in Vault that collectively keep a detailed log of all requests to Vault, and their responses. Because every operation with Vault is an API request/response, when using a single audit device, the audit log contains every interaction with the Vault API, including errors - except for a few paths which do not go via the audit system. </p> <p>Each line in the audit log is a JSON object and any sensitive information are hashed before logging.</p> <p>vault requires at least one audit device to write the log before completing the vault request. - if enabled.</p>"},{"location":"secretmgmt/vault/vault_architecture/#vault-architecture","title":"vault architecture","text":"<p>vault architecture document</p>"},{"location":"secretmgmt/vault/vault_architecture/#vault-path-structure","title":"vault path structure","text":"<ul> <li>Everything in vault is path-based</li> <li>path prefix tells vault which component a request should be routed</li> <li>secret engines, auth methods, audit devices are \"mounts\" at a specified path referred as mount</li> <li>system backend is default backend in vault which is mounted at the /sys endpoint</li> <li>vault components can be enabled at ANY path using --path flag or it does by default path</li> <li>vault has few system paths what are resrved and you cannot use it. <ul> <li>auth/  - endpoint for auth methods configs</li> <li>cubbyhole - endpoint for cubbyhole secrets engine</li> <li>identity/ - endpoint for config vault identity</li> <li>secret/  - endpoint used by key/vaulut v2 secrets engine if running in dev mode</li> <li>sys/ - system endpoint for config vault</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-data-protection","title":"vault data protection","text":"<p>How data is stored in vault...</p> <p>master key:  - used to decrypt the encryption key - created during vault init or rekey ops - never written to storage when using traditional unseal mechanism - written to core/master(storage backend) when using auto unseal</p> <p>encryption key: - used to encrypt/decrypt data written to storage backend. - encrypted by the master key. - stored alongside the data in a keyring on the storage backend. - can be easily rotated(manual ops).</p>"},{"location":"secretmgmt/vault/vault_architecture/#sealunseal","title":"seal/unseal","text":"<ul> <li>vault starts in a sealed state, meaning it knows where to access the data and how but can't decrypt it, which means there is no read or write etc.. i.e almost no ops are possible when vault in a sealed state.</li> <li>unsealing vault means that a node can reconstruct the master key in order to decrypt the encryption key and untilmately read the data, after unsealing the encryption key is stored in the memory, it can be unsealed using CLI/UI options</li> <li>incase you need to seal vault i.e \"throw away\" the encrytpion key, it requires another unseal to perform any further ops. When I need to seal vault ?</li> <li>key shars are exposed</li> <li>detecting of compromise or netwrom intrustions</li> <li>spyware/malware on the vault nodes</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#unseal-using-key-shards","title":"unseal using key shards","text":"<p>We know, by now that master key is required to encrypt \"encryption key\" to store the data in the backend.. so how do we protect the \"master key\". It is done by \"key shards\". In this key shards, vault expects alteast 3 keys to provide to unseal the master key. These key shards would be available to min of 5 members when we init the vault using diff encryption alogrithms. when you are unsealing you will provide equal number of employees to provide their key which is equal to threshold.</p> <p>As part of security, no single person should have all the key shards, and it should never be stored online.</p> <pre><code>vault status\nvault operator init\n&lt;vault status set to initialized&gt;\n&lt;displays 5 keys along with root token&gt;\n\nvault status\nvault operator unseal\n&lt;enter any of 3/5 keys from above until status of thershold is met&gt;\n\nvault status\n\nvault login &lt;root-token&gt; # you would authenticate the cluster\n\nvault secrets list\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#unsealing-using-auto-unseal","title":"unsealing using auto-unseal","text":"<p>When we have an vault mainteance, or restart of service, vault would seal itself. so we need a place so that it would unseal itself.. </p> <p>instead of key shards being with 5 ppl, you would have a key stored in the cloud services(KMS) to encrypt/decrypt the master key. encrypted master key is stored in the backend in core using KMS, so incase if vault restarts, it would read the encrypted master key using KMS then decrypt the master key which would then decrypt encryption key which will store in the memory to read/write secrets on the vault node.</p> <p>You can find in the config file <code>/etc/vault/vault.hcl</code></p> <p>Create a new KMS key and provide an endpoint for <code>kms_key_id</code>. That would be sufficient when vault restarts or so, it would use the KMS key to get it unsealed.. </p> <pre><code>seal \"awskms\" {\n    region = REGION\n    kms_key_id = \"KMSKEY\"\n}\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#unsealing-with-transit-auto-unseal","title":"unsealing with transit auto-unseal","text":"<p>We would be having an vaulut cluster running transit secret engine from which we would be unsealing the vault. In other words, we would use that one cluster dependent on another cluser for unsealing.</p> <p>This supports key rotation and also can be configured for HA.</p> <pre><code>seal \"transit\" {\n  address: \"https://vault.example.com:8200\",\n  token =\"x.Qft42..\", - acl token to use if enabled.\n  disable_renewal = \"false\" \n}\n\nkey_name = \"transit_key_name\", - transit key used for encryption/decryption\nmount_path = \"transit/\", - mount path to transit secret engine\nnamespace = \"ns1/\" - namespace path to transit secret engine.\n</code></pre> <p>Configure transite secret engine..</p> <p>host: 192.168.56.100</p> <pre><code>vault secrets enable transit\nvault write -f transit/keys/unseal-key\nvault list transit/keys\n\nvim policies.hcl\n\npath \"transit/encrypt/unseal-key\" {\n  capabilities = [\"update\"]\n}\n\npath \"transit/decrypt/unseal-key\" {\n  capabilities = [\"update\"]\n}\n\nvault policy write unseal policy.hcl\nvault policy list\nvault policy read unseal\nvault token create -policy=unseal - you get token from here\n</code></pre> <p>Now, go to your vault cluster which needs to be unsealed</p> <pre><code>vault status\nsudo vim /etc/vault/vault.hcl \n\nseal \"transit\" {\n  address: \"https://192.168.56.100:8200\",\n  token =\"use above token from transit engine\",\n  disable_renewal = \"true\",\n  key_name = \"unseal-key\",\n  mount_path = \"transit\"\n}\n\nsudo system restart vault\n\nvault status\nvault operator init\nvault status - vault would have unsealed\n\nvault login &lt;token&gt;\n\nvault secrets list\n\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#pros-and-cons-of-unsealing","title":"pros and cons of unsealing","text":"<p>pros:</p> <p>key shards: - simplest form of unsealing - works on any platform  - config options make it flexible</p> <p>auto unseal: - automatic unsealing - set and forget - integration benefits for running on same platform</p> <p>transit unseal: - automatic unsealing - set and forget - platform agnostic - useful when runing many vault clusters across clouds/data centres</p> <p>Cons:</p> <p>key shards:</p> <ul> <li>introduces risk of storing keys</li> <li>requires manual intervention</li> <li>require rotation manually</li> </ul> <p>auto unseal:</p> <ul> <li>regional requirements for cloud HSM</li> <li>cloud/vendor lockin</li> </ul> <p>transit unseal:</p> <ul> <li>requires centralized vault cluster</li> <li>since centralized vault cluster need highest level of uptime.</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-initialization","title":"vault initialization","text":"<ul> <li>initilization vault prepares the backend storage to receive data</li> <li>only need to initialize one time via single node</li> <li>vault init is when vault creates the master key and key shares</li> <li>options to defined threshold, key shares, receovery keys and encryption</li> <li>vault init is also where the intitial root token is generated and returned to user</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-configs","title":"vault configs","text":"<ul> <li>vault written in HCL or JSOn</li> <li>config file is specified(/etc/vault/vault.hcl) when starting vault using the<code>--config</code> flag</li> </ul> <pre><code>vault server -config &lt;localtion&gt;\n</code></pre> <p>vault config documentation</p> <p>example config file </p> <pre><code>storage \"consul\" {\n  address = \"127.0.0.1:8500\"\n  path    = \"vault/\"\n  token   = \"1a2b3c4d-1234-abdc-1234-1a2b3c4d5e6a\"\n}\nlistener \"tcp\" {\n address = \"0.0.0.0:8200\"\n cluster_address = \"0.0.0.0:8201\"\n tls_disable = 0\n tls_cert_file = \"/etc/vault.d/client.pem\"\n tls_key_file = \"/etc/vault.d/cert.key\"\n tls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n  region = \"us-east-1\"\n  kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n  endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\nreporting { #only for Vault 1.14 and up\n    license {\n        enabled = false\n   }\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = false\nlog_level = \"INFO\"\nlicense_path = \"/opt/vault/vault.hcl\"\ndisable_mlock=true\n</code></pre>"},{"location":"secretmgmt/vault/vault_architecture/#storage-backends_1","title":"storage backends","text":"<ul> <li>configures location for storage of vault data</li> <li>enterprice vault cluster use \"Hashicorp Consul\" or \"integrated storage\" </li> </ul> <p>There is only 1 storage backend / cluster</p>"},{"location":"secretmgmt/vault/vault_architecture/#audit-devices_1","title":"audit devices","text":"<ul> <li>keep detailed log of all autenticated req and resp to vault</li> <li>audit log is in JSON</li> <li>sensitive info is hashed </li> <li>log files shoud be protected as a user permission can still check the values of those secrets via /sts/audit-hash API and compare to the log file</li> </ul> <pre><code>vault audit enable file file_path=/var/log/vault_audit_log.log\n</code></pre> <ul> <li>Can and should have more than one audit device enabled</li> <li>if there are any sudit devices enabled, vault requires that it can write to the log before completing the client request</li> <li>if vault cannot write to a persistent log, it will stop responding to client requests - which mean its down !</li> </ul>"},{"location":"secretmgmt/vault/vault_architecture/#vault-interfaces","title":"vault interfaces","text":"<ul> <li>3 interfaces interact with vault: UI, CLI, HTTP</li> <li>CLI is just a wrapper and all CLI or UI underneath uses HTTP</li> <li>Auth is required to access any of the interfaces.</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/","title":"introduction","text":"<p>Manage Secrets and Protect Sensitive Data and provides a Single Source of Secrets for both Humans and Machines</p> <p>Secret</p> <ul> <li>Anything your organization deems sensitive:</li> <li>Usernames and passwords</li> <li>API keys</li> <li>Certificates</li> <li>Encryption Keys</li> </ul> <p>Lifecycle Management for Secrets</p> <ul> <li>Eliminates secret sprawl</li> <li>Securely store any secret</li> <li>Provide governance for access to secrets</li> </ul> <p>Vault has mainly 3 ways you can interact, CLI, UI and API</p> <p>As a human, you would authenticate to vault server with credentials(username/password | roleid | secretid| tls certs | cloud creds..etc), which gives you an generated token to carry out certain taks that you need for like (read/write/delete//list) to perform some actions(writing to path or reading from path) on the applications with certain time limit(TTL).</p> <p>When we want to retrive the data from path, we would proivde the token generated from auth and vault would validate the below tokens</p> <ol> <li>token provided is correct/valid</li> <li>token is not expired</li> <li>token has permission</li> </ol> <p>Once above are successful, you would be retrived the data from th path.</p>"},{"location":"secretmgmt/vault/vault_overview/#benefits","title":"Benefits","text":"<ul> <li>Store Long-Lived, Static Secrets</li> <li>Dynamically Generate Secrets, upon Request</li> <li>Fully-Featured API</li> <li>Identity-based Access Across different Clouds and Systems</li> <li>Provide Encryption as a Service(secret engine)</li> <li>Act as a Root or Intermediate Certificate Authority </li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#usecases","title":"Usecases","text":"<ul> <li> <p>Centralize The Storage Of Secrets  </p> <ul> <li>Chef </li> <li>Jenkins </li> <li>AWS secrets</li> <li>Azure key</li> </ul> </li> <li> <p>Migrate to Dynamically Generated Secrets </p> <ul> <li>short-lived</li> <li>follows principle of least priv </li> <li>auotmatically revoked</li> <li>each system can retrive unique creds</li> <li>prog retrived</li> <li>no human interactions</li> </ul> </li> <li> <p>Secure Data with a centralized workflow for Encryption Operations (secret engine)</p> <ul> <li>transit</li> <li>KMIP</li> <li>Key mgmt</li> <li>transform</li> </ul> </li> <li> <p>Automate the Generation of X.509 Certificates ( can work like PKI) you would provide an Certificate Request, in which valut would sign and provide the certtficate and key returned</p> </li> <li> <p>Migrate to IdentityBased Access</p> <ul> <li>Quickly Scale Up and Down</li> <li>Reduce/Eliminate Ticket-based Access</li> <li>Increase Time to Value</li> </ul> </li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#installations","title":"Installations","text":"<p>Choose your OS and download the vault server from the below link https://releases.hashicorp.com/vault/1.15.1/</p> <ul> <li>Install vault</li> <li>Create Configuration file </li> <li>Initialize vault </li> <li>Unseal vault</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#development-vault","title":"Development vault","text":"<p>features of development vault server</p> <ul> <li>Quickly run Vault without configuration</li> <li>Automatically initialized and unsealed</li> <li>Enables the UI \u2013 available at localhost</li> <li>Provides an Unseal Key</li> <li>AutomaBcally logs in as root</li> <li>Non-Persistent \u2013 Runs in memory</li> <li>Insecure \u2013 doesn\u2019t use TLS </li> <li>Sets the listener to 127.0.0.1:8200</li> <li>Mounts a K/V v2 Secret Engine</li> <li>Provides a root token</li> </ul> <p>benefits of development vault server</p> <ul> <li>POC</li> <li>New dev integrations</li> <li>testing new vault features</li> <li>experiment new features</li> </ul> <pre><code>vault server -dev\n</code></pre> <p>Open another terminal and set the env vars</p> <pre><code>\u279c  ~ export VAULT_ADDR='http://127.0.0.1:8200'\n\u279c  ~ vault status\nKey             Value\n---             -----\nSeal Type       shamir\nInitialized     true\nSealed          false\nTotal Shares    1\nThreshold       1\nVersion         1.15.1\nBuild Date      2023-10-20T19:16:11Z\nStorage Type    inmem\nCluster Name    vault-cluster-49bd9ee3\nCluster ID      e5a97669-5d1d-386f-5eb3-a72d5b72f744\nHA Enabled      false\n\n\u279c  ~ vault kv put secret/vault/sunil sunil=sunil\n</code></pre>"},{"location":"secretmgmt/vault/vault_overview/#production-vault","title":"Production vault","text":"<p>features of production vault server</p> <ul> <li>Deploy one or more persistent nodes via configuration file</li> <li>Use a storage backend that meets the requirements</li> <li>Multiple Vault nodes will be configured as a cluster</li> <li>Deploy close to your applications</li> <li>Most likely, you\u2019ll automate the provisioning of Vault</li> </ul> <p>To start Vault, run the vault <code>server \u2013config=&lt;file&gt;</code> command</p> <ul> <li> <p>In a production environment, you'll have a service manager executing and managing the Vault service (systemctl, Windows Service Manager, etc.)</p> </li> <li> <p>For Linux, you also need a systemd file to manage the service for Vault (and Consul if you're running Consul)</p> </li> </ul> <p>follow manual install process for vault incase you need to deploy locally or in cloud.</p> <ul> <li>Download Vault from HashiCorp</li> <li>Unpackage Vault to a Directory</li> <li>Set Path to Executable</li> <li>Add ConfiguraBon File &amp; Customize</li> <li>Create Systemd Service File</li> <li>Download Consul from HashiCorp</li> <li>Configure and Join Consul Cluster</li> <li>Launch Vault Service</li> </ul>"},{"location":"secretmgmt/vault/vault_overview/#consul-as-backend","title":"Consul as backend","text":"<p>You can use the two of the storage backends for using vault(storing key/value(KV) pairs) .. Consul is one of them and and then integrated storage..</p> <p>Consule storage uses spearte consul cluster to store key-value pairs for vault, which would store all its key-value pairs on that backend KV store.</p> <p>benefits of consul as backend</p> Provides Durable K/V Storage For Vault Supports High Availability Can Independently Scale Backend Distributed System Easy To Automate Built-in Snapshots For Data Retention Built-in Integration Between Consul/Vault HashiCorp Supported <p>They would mainly be 3-5 nodes for HA, but good news from consul is that you can independently scale the backend from vault. incase you want to increase the backend resouces(RAM, CPU ..etc) you never had to touch the vault nodes.</p> <ul> <li>Consul is deployed using multiple nodes(3-5) and configured as a cluster.</li> <li>Clusters are deployed in odd numbers (for voting members)</li> <li>All data is replicated among all nodes in the cluster, </li> <li>A leader election promotes a single Consul node as the leader</li> <li>The leader accepts new logs entries and replicates to all other nodes</li> <li>Consul cluster for Vault storage backend shouldn\u2019t be used for Consul functions in a production setting</li> </ul> <p>Note: Consul cluster for vault should not be used for other consul functions like (service discovery, service mesh and network automations etc because you would get resource constraints.. )</p>"},{"location":"secretmgmt/vault/vault_overview/#consul-deployment","title":"Consul deployment","text":"<p>Now, its always recommended that you have 3 vault nodes you would atleast have 5 consul nodes. Within the vault we would have consul agents listening on vault locally which join to consul clusters and it will communicate with consul backend clusters. </p> <p>example consul configs</p> <pre><code>storage \"consul\" {\n address = \"127.0.0.1:8500\"\n path = \"vault/\"\n token = \"1a2b3c4d-1234-abdc-1234-1a2b3c4d5e6a\"\n}\nlistener \"tcp\" {\naddress = \"0.0.0.0:8200\"\ncluster_address = \"0.0.0.0:8201\"\ntls_disable = 0\ntls_cert_file = \"/etc/vault.d/client.pem\"\ntls_key_file = \"/etc/vault.d/cert.key\"\ntls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n region = \"us-east-1\"\n kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = true\nlog_level = \"INFO\"\n</code></pre> <pre><code>{\n \"log_level\": \"INFO\",\n \"server\": true,\n \"key_file\": \"/etc/consul.d/cert.key\",\n \"cert_file\": \"/etc/consul.d/client.pem\",\n \"ca_file\": \"/etc/consul.d/chain.pem\",\n \"verify_incoming\": true,\n \"verify_outgoing\": true,\n \"verify_server_hostname\": true,\n \"ui\": true,\n \"encrypt\": \"xxxxxxxxxxxxxx\",\n \"leave_on_terminate\": true,\n \"data_dir\": \"/opt/consul/data\",\n \"datacenter\": \"us-east-1\",\n \"client_addr\": \"0.0.0.0\",\n \"bind_addr\": \"10.11.11.11\",\n \"advertise_addr\": \"10.11.11.11\",\n \"bootstrap_expect\": 5,\n \"retry_join\": [\"provider=aws tag_key=Environment-Name tag_value=consul-cluster region=us-east-1\"],\n \"enable_syslog\": true,\n \"acl\": {\n \"enabled\": true,\n \"default_policy\": \"deny\",\n \"down_policy\": \"extend-cache\",\n \"tokens\": {\n \"agent\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n }\n},\n \"performance\": {\n \"raft_multiplier\": 1\n }\n}\n</code></pre>"},{"location":"secretmgmt/vault/vault_overview/#local-storage-as-backend","title":"local storage as backend","text":"Vault Internal Storage Option Supports High Availability Leverages RaG Consensus Protocol Only need to troubleshoot Vault All Vault nodes have copy of Vault's Data Built-in Snapshots For Data Retention Eliminates Network Hop to Consul HashiCorp Supported"},{"location":"secretmgmt/vault/vault_overview/#local-deployment","title":"local deployment","text":"<ul> <li>Integrated Storage (aka Raft) allows Vault nodes to provide its own replicated storage across the Vault nodes within a cluster.</li> <li>Define a local path to store replicated data.</li> <li>All data is replicated among all nodes in the cluster.</li> <li>Eliminates the need to also run a Consul cluster and manage it.</li> </ul> <p>You would have 1 leader and rest other followers, but make sure to proivde the node_id uniquely so that you won't mess up the data. you would see the raft as storage</p> <pre><code>storage \"raft\" {\n path = \"/opt/vault/data\"\n node_id = \"node-a-us-east-1.example.com\"\n retry_join {\n auto_join = \"provider=aws region=us-east-1 tag_key=vault tag_value=us-east-1\"\n }\n}\nlistener \"tcp\" {\naddress = \"0.0.0.0:8200\"\ncluster_address = \"0.0.0.0:8201\"\ntls_disable = 0\ntls_cert_file = \"/etc/vault.d/client.pem\"\ntls_key_file = \"/etc/vault.d/cert.key\"\ntls_disable_client_certs = \"true\"\n}\nseal \"awskms\" {\n region = \"us-east-1\"\n kms_key_id = \"12345678-abcd-1234-abcd-123456789101\",\n endpoint = \"example.kms.us-east-1.vpce.amazonaws.com\"\n}\napi_addr = \"https://vault-us-east-1.example.com:8200\"\ncluster_addr = \" https://node-a-us-east-1.example.com:8201\"\ncluster_name = \"vault-prod-us-east-1\"\nui = true\nlog_level = \"INFO\"\n</code></pre> <p>Manually join the standby ndoes to the cluster</p> <pre><code>vault operator raft join https://active_node.example.com:8200\nvault operator raft list-peers\n</code></pre> <p>References:</p> <ul> <li>https://hashicorp.com/certification/vault-associate</li> <li>https://learn.hashicorp.com/tutorials/vault/associate-study</li> <li>https://vaultproject.io/docs</li> <li>https://vaultproject.io/api-docs</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/","title":"replication","text":""},{"location":"secretmgmt/vault/vault_replication/#replication","title":"replication","text":"<p>Organizations usually have infrastructure that spans multiple datacenters</p> <ul> <li>Vault needs to be highly-available for application access</li> <li>Needs to scale as organizations continue to add use cases and apps</li> <li>Common set of policies that are enforced globally</li> <li>Consistent set of secrets and configurations available to applications that need them regardless of data center</li> <li>Only available in Vault Enterprise</li> <li>Replication operates on a leader-follower model (primaries and secondaries)</li> <li>The primary cluster acts as the system of record and replicates most Vault data asynchronously</li> <li>All communication between primaries and secondaries is end-to-end encrypted with mutually-authenticated TLS sessions</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#performance-replication","title":"Performance Replication","text":""},{"location":"secretmgmt/vault/vault_replication/#dr-replication","title":"DR replication","text":"<ul> <li>Provides a warm-standby cluster where EVERYTHING is replicated to the DR secondary cluster(s)</li> <li>DR clusters DO NOT respond to clients unless they are promoted to a primary cluster</li> <li>Even as an admin or using a root token, most paths on a secondary cluster are disabled, meaning you can't do much of anything on a DR cluster</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#replication-comparisions","title":"replication comparisions","text":""},{"location":"secretmgmt/vault/vault_replication/#replication-architecture","title":"replication architecture","text":"<p>Real world examples</p> <p></p> <p></p>"},{"location":"secretmgmt/vault/vault_replication/#replication-networking","title":"replication networking","text":"<ul> <li>Communication between clusters must be permitted to allow replication, RPC forwarding, and cluster bootstrapping to work as expected.</li> <li>If using DNS, each cluster must be able to resolve the name of the other cluster</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#setup","title":"setup","text":""},{"location":"secretmgmt/vault/vault_replication/#activating-dr-replication","title":"Activating DR Replication","text":"<ul> <li>Replication is NOT enabled by default, so you must enable it on each cluster that will participate in the replica set</li> <li>Enables an internal root CA on the primary Vault cluster - creates a root certificate and client cert</li> <li>Vault creates a mutual TLS connection between the nodes using self-signed certificates and keys from the internal CA \u2013 NOT the same TLS configured for the listener</li> <li>If Vault sits behind a load balancer that is terminating TLS, it will break the mutual TLS between the nodes if inter-cluster traffic is forced through the load balancer</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#secondary-token","title":"Secondary Token","text":"<ul> <li>A secondary token is required to permit a secondary cluster to replicate from the primary cluster</li> <li>Due to its sensitivity, the secondary token is protected with response wrapping</li> <li>Multiple people should \u201chave eyes\u201d on the secondary token once it\u2019s been issued until it is submitted to the secondary cluster</li> <li>Once the token is successfully used, it is useless (single-use token)</li> <li>The secondary token includes information such as:</li> <li>The redirect address of the primary cluster</li> <li>The client certificate and CA certificate</li> </ul>"},{"location":"secretmgmt/vault/vault_replication/#configure-replication","title":"Configure replication","text":"<pre><code>primary$ vault write -f sys/replication/dr/primary/enable\nprimary$ vault write sys/replication/dr/primary/secondary-token id=&lt;id&gt; # provide meaningful name\nsecondary$ vault write sys/replication/dr/secondary/enable token=&lt;token&gt; #Provide token from primary cluster (command above)\n</code></pre>"},{"location":"softskills/leadership/","title":"leadership","text":"<p>Leadership Overview</p>"},{"location":"softskills/overview/","title":"Overview","text":"<p>Soft Skills</p>"},{"location":"sre/chaos_engineering/","title":"Chaos Engineering","text":""},{"location":"sre/chaos_engineering/#chaos-engineering","title":"Chaos engineering","text":"<p>instead of measuring, failing, and fixing, what if we simulate all these ?</p> <ul> <li>Create failures</li> <li>Find weakness</li> <li>fix proactively</li> </ul> <p>intentionally introduce system failure identify vulnerable before users raise issues</p> <p>Chaos engg - shows your system works under real world chaos</p> <p>Resiliebce testing</p> <ul> <li>kill a pod and verify failover works</li> <li>disconnect db to check fallback</li> <li>simulate latency to test timeout</li> </ul> <p>chaos engineering</p> <ul> <li>Introduce network paritions during peak traffic </li> <li>Memory pressure with database slowdown</li> <li>Multiple small failures cascading into big ones</li> </ul> <p>Safe Chaos experiments in stages</p> <p></p> <p></p> <p></p> <p>Chaos key insights</p> <p></p> <p>Chaos principles</p> <p></p> <p>Chaos maturity model </p> <p></p>"},{"location":"sre/chaos_engineering/#cost-of-realibility","title":"cost of realibility","text":"<p>Cost efficiency and realibility</p> <p>if SLO wants to move from 99.9 to 99.99%, the cost would be 100x more than current. find the below reasons  i.e if SLO is stricter, the more budget goes into reliability</p> <p></p> <p>Autoscaling strategies</p> <p></p> <p>SLO budget framework</p> <p></p> <p>why does SLO budget matters ?</p> <p></p>"},{"location":"sre/incident_management/","title":"Incident Management","text":""},{"location":"sre/incident_management/#incident-preparations","title":"Incident preparations","text":"<p>Why preparation matters </p> <p></p> <p>what involves preparation </p> <p></p> <p>preparation approach</p> <p></p> <p></p> <p></p> <p></p> <p>Mitigation steps for above playbook </p> <p></p> <p></p> <p>Simulation and training</p> <p>Regular practice involves incident response capabilities</p> <p></p> <p>Google's \"wheel of misfortune\"</p> <p></p>"},{"location":"sre/incident_management/#design-effective-alerts","title":"Design effective alerts","text":"<p>actionable critical alerts</p> <p></p> <p>![alert_example]./images/alert_example.png)</p> <p>system based is always on the user focus, where as cause based is always on the internal </p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>proper alert routing ensures the right people receive notification at the right time</p> <p></p> <p></p>"},{"location":"sre/incident_management/#incident-response-structure-and-response-level","title":"Incident response structure and response level","text":"<p>IMAG - Incident Management At Google - it would address the challenge</p> <ul> <li>Establish clear roles and responsibilities</li> <li>Porcess that scale with incident complexity</li> </ul> <p>key principles</p> <p></p> <p>Incident management common proactices</p> <p></p> <p></p> <p>traits under role</p> <p></p> <p>Incident severity levels</p> <p></p> <p>Incident classification</p> <p>P levels for incidents for immediate reponse</p> <p>Sev levels - Categorizes incident by severity</p> <p></p> <p>Incident reposnse tooling</p> <p></p>"},{"location":"sre/incident_management/#blameless-postmorterm-cluture","title":"Blameless postmorterm cluture","text":"<p>instead of asking who did ?, infact ask what caused the issue.. thats about blameless postmorterm. </p> <p></p> <p></p> <p></p> <p></p> <p>postmorterm templates can be found here https://github.com/dastergon/postmortem-templates</p>"},{"location":"sre/incident_management/#rca","title":"RCA","text":"<p>These are RCA misconceptions</p> <p></p> <p>Root cause Vs contributing factors</p> <p></p> <p>RCA 6 stages </p> <p></p> <p>Common patterns in root causes</p> <ul> <li>techincal patterns</li> </ul> <p></p> <ul> <li>process patterns</li> </ul> <p></p> <ul> <li>organization patters</li> </ul> <p></p>"},{"location":"sre/manage_complexity/","title":"Managing complexity","text":""},{"location":"sre/manage_complexity/#system-design-simplicity","title":"System design simplicity","text":"<p>Simplicity is a design goal</p> <p>One rule of managing complex systems is to ensure they're actually necessary.</p> <p>Complex systems are:</p> <ul> <li>Harder to understand </li> <li>Harder to maintain</li> <li>prone to undexpected failures</li> </ul> <p>Realibility cost complexity</p> <p></p> <p>Reason for increased complexties</p> <p></p> <p>Simplification strategies</p> <p></p> <p></p>"},{"location":"sre/manage_complexity/#managing-dependency","title":"Managing dependency","text":"<p>Dependency types:</p> <ul> <li>Direct deps - Componet A calls component B</li> <li>Indirect deps - Comp A relies on B through intermediary </li> <li>Runtime deps - External services, db and caches</li> <li>Build time deps - libs, framework, tools</li> </ul> <p>Blast Radius</p> <p>measures how widely a failure spead across systems, knowing a components blast radius guides realibility priorities</p> <p>Factors afftecting blast radius</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"sre/manage_complexity/#change-management","title":"Change Management","text":"<p>Common failures </p> <p></p> <p>SRE needs to balance between velocity and the stability for the change. </p> <p>small, frequent, validated code changes actually leads to :</p> <ul> <li>Better learning</li> <li>Greater system resilience</li> <li>Faster recovery </li> <li>Tighter feedback loops </li> </ul> <p></p> <p>Safe deployments strategies</p> <ul> <li>Blue green deployment</li> <li>Canary </li> <li>Feature flags</li> </ul> <p>Monitoring is crtutial during changes after deployments ..</p> <p>After change request </p> <p>Error rate - is it increasing after change ? latency - us system responding more slowly ? traffic - are users able to access the service ? saturation - are resources under pressure ? Deployment progress - is the chg expected as per plan</p> <p>Deployment verfiication process</p> <p>smoke tests - basic functionality checks integrration tests - validates component interactions performance tests - verified system performance canary test - real users test new version gradual traffic testing - increases traffic with monitoring</p>"},{"location":"sre/manage_complexity/#capacity-planning","title":"capacity planning","text":"<p>Key components of capacity planning</p> <p></p> <p>Benefits:</p> <ul> <li>Prevents outages </li> <li>reduce costs by right sizing infra</li> <li>supports business growth with adequate capacity</li> <li>improves user experiance by maintaining performance </li> <li>enables more predicatable and planning</li> </ul> <p>resource measurement </p> <p></p> <p>Forecating models</p> <p></p> <p></p> <p>threshold types</p> <p></p> <p></p>"},{"location":"sre/manage_complexity/#managing-operation-toil","title":"Managing operation toil","text":"<p>toil impact from enginner presepective</p> <p></p> <p>toil impact from Business prespective</p> <p></p> <p></p> <p></p> <p>Direct cost</p> <p></p> <p>Indirect costs</p> <p></p>"},{"location":"sre/observability/","title":"Observability","text":""},{"location":"sre/observability/#observability-overview","title":"Observability Overview","text":"<p>Metrics + logging + traces (what)  + (How)   + (where)</p> <ul> <li>Assume unknown failures</li> <li>Enable unexecpted questions</li> <li>Understand behaviours, not just health</li> </ul> <p></p> <p>Structured logging benefits</p> <ul> <li>Machine parsable for analysis</li> <li>Consistent fields across services</li> <li>Rich context for debugging</li> <li>Correleation with traces and metrics</li> </ul>"},{"location":"sre/observability/#data-source-and-visualization","title":"Data source and visualization","text":"<p>Grafana data source configuration </p> <p></p> <p>Laws of SRE Dashboard</p> <p></p> <p>level 1 - service health overview(is everything OK?) level 2 - service performance details (investigate trends and issues)</p> <p>Chart types</p> <p>Time series</p> <p>Best chart - line chart  use case - request rates, latency, error rates why - shows trends and spikes</p> <p>Status/health</p> <p>best chart - stat panel  use case - service health, SLO's  why - immediate visual stats</p> <p>Distribution data</p> <p>best chart - heatmap/histogram  use case - response time  why - reveals user experiance </p> <p>Comparative data</p> <p>best chart - bar chart use case - error rate by service  why - easy comparision </p> <p>Dashboard creation </p> <p></p>"},{"location":"sre/observability/#alert-design-and-implementation","title":"Alert design and implementation","text":"<p>Alerting principles</p> <p></p> <ul> <li>SLO based alerting </li> <li>Error budget alerting</li> <li>ALert routing strategy</li> <li>time based routing</li> </ul>"},{"location":"sre/observability/#performnce-monitoring","title":"performnce monitoring","text":"<p>Essential performance metrics</p> <p></p> <p>common bottlenecks</p> <p></p> <p>identify where the bottleneck</p> <p></p>"},{"location":"sre/observability/#advance-visualization","title":"Advance visualization","text":"<p>Despite single system, the dashboards needs to be different for different people. </p> <p></p> <p>Rules for dashboard </p> <p></p> <p></p>"},{"location":"sre/overview/","title":"Overview","text":""},{"location":"sre/overview/#fundamentals","title":"Fundamentals","text":"<p>SRE is considered a specific implementation of DevOps principles, focusing on reliability engineering and providing concrete practices.</p> <p></p> <ul> <li>Intersection of traditional IT and devops engineeer</li> <li>Brdiges design and runtime reality</li> <li>Ensures realibility and performance at any scale</li> <li>Embraces risk and anticipates failures</li> <li>realies on automation to build resilitant systems</li> </ul> <p></p> <p></p> <p></p> <p>Questions for SRE to ask.</p> <ul> <li>How can this application break ?</li> <li>What should we do when it breaks ?</li> <li>What does an acceptable level of service looks like ?</li> <li>How will we know if app is not working ?</li> <li>What actions should i need to take and what context do i need to respond effectively.</li> </ul> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Devops: cultural mindset and philosphy for delivering software systems faster and more collaboratively. It mainly about \"what\" and \"why\" </p> <p>SRE: Concentre implementation focussed on the realibility through engineering process. Its mainly on \"how\" on scalibility and realibility. It puts things in practice. </p>"},{"location":"sre/overview/#sre-team-models","title":"SRE Team models","text":""},{"location":"sre/overview/#manage-complextiesrisktoil","title":"Manage complexties/risk/toil","text":""},{"location":"sre/overview/#incident-management","title":"Incident management","text":""},{"location":"sre/overview/#release-enginerring","title":"Release Enginerring","text":""},{"location":"sre/overview/#observability-monitoring","title":"Observability &amp; Monitoring","text":""},{"location":"sre/overview/#advance-realibility","title":"Advance Realibility","text":""},{"location":"sre/release_engineering/","title":"Release Engineering","text":""},{"location":"sre/release_engineering/#production-readiness","title":"production readiness","text":"<p>real readiness meaning </p> <ul> <li>ready for real users</li> <li>withstand real load</li> <li>handles real problems</li> <li>avoids costly failures</li> </ul> <p></p> <p></p> <p></p> <p>Observability .. </p> <p>Metrics: is it slow ?</p> <p>logs: why is it slow ?</p> <p>traces: where is the slow ?</p> <p>Readiness Questions:</p> <ul> <li>would your service be healthy in 30 seconds ?</li> <li>Can you identigy problem in 5 minutes ?</li> <li>Can you wake up the right person automatically ?</li> </ul>"},{"location":"sre/release_engineering/#configuration-management","title":"Configuration Management","text":"<p>Overcome these pitfalls </p> <ul> <li>Env promotion</li> </ul> <p></p> <ul> <li>Create env specific configs</li> </ul> <p>env.dev.properties env.stage.properties env.prod.properties</p>"},{"location":"sre/release_engineering/#secure-software-releases","title":"Secure software releases","text":"<p>Security scanning</p> <ul> <li>Token auth </li> <li>Automated scanning</li> <li>SBOM tracking</li> <li>leaast privilege</li> <li>environment controls </li> <li>container scanning</li> </ul>"},{"location":"sre/release_engineering/#release-engineering-best-practices","title":"release engineering best practices","text":"<p>Deployment strategies</p> <p></p> <p>why do you need artifact ?</p> <p></p> <p>CICD plan</p> <p></p> <p>CICD best practices</p> <p></p> <p>Communication and learning</p> <p></p>"},{"location":"sre/sre_measure/","title":"SRE measurements","text":""},{"location":"sre/sre_measure/#sre-measurements","title":"SRE Measurements","text":"<p>monitoring tells you if you are healthy</p> <p> </p> <p>observability tells you why are you sick </p> <p></p> <p>There are 3 data types for measuring realibility</p> <p>Metrics</p> <p>whats happening along....</p> <p></p> <p>Traces</p> <p>where it went wrong .. </p> <p>logs</p> <p>what went wrong ...  Details the information for debug and diagonizes the cause for SLO viloations..</p> <p></p> <p>Golden rules</p> <p></p> <p></p> <p></p>"},{"location":"sre/sre_measure/#implementing-slis","title":"Implementing SLIs","text":"<p>Once you have identified whats SLI you need, you would choose for the queries to get metrics. </p> <p></p>"},{"location":"sre/sre_measure/#availability-sli","title":"Availability SLI","text":""},{"location":"sre/sre_measure/#availability-vs-allowed-downtime-cheat-sheet","title":"\ud83d\udcca Availability vs Allowed Downtime \u2014 Cheat Sheet","text":""},{"location":"sre/sre_measure/#uptime-targets","title":"\ud83d\udd22 Uptime Targets","text":"Availability Allowed Downtime per Day per Week per Month (30 days) per Year 99% (Two nines) 14m 24s 1h 40m 48s 7h 18m 3 days 15h 99.9% (Three nines) 1m 26s 10m 4s 43m 12s 8h 45m 99.99% (Four nines) 8.6 seconds 1 minute 4m 32s 52m 34s 99.999% (Five nines) 0.86 seconds 6 seconds 26 seconds 5m 15s"},{"location":"sre/sre_measure/#formula","title":"\ud83e\uddee Formula","text":"<p>Allowed downtime = (1 - Availability%) \u00d7 Total period Allowed downtime(99.99%) = (1-99.99%) i.e 0.0001 \u00d7 (30 days \u00d7 24 \u00d7 60 minutes) = 4.32 minutes</p>"},{"location":"sre/sre_measure/#latency-sli","title":"Latency SLI","text":""},{"location":"sre/sre_measure/#error-sli","title":"Error SLI","text":""},{"location":"sre/sre_measure/#throughput","title":"Throughput","text":""},{"location":"sre/sre_measure/#satuaration","title":"Satuaration","text":""},{"location":"sre/sre_measure/#slo-game","title":"SLO Game","text":""},{"location":"sre/sre_measure/#implementing-error-budgets","title":"Implementing Error budgets","text":"<p>its just a tradeoff for SLO</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"sre/sre_measure/#visualization-measurements","title":"visualization measurements","text":"<p>Effective SLO Dashboard</p> <ul> <li> <p>Focus on user fist</p> <ul> <li>Primary panels show user facing SLIs</li> <li>Clear visual indication of SLO compliance status</li> <li>User journey success rates</li> </ul> </li> <li> <p>Implement visual hierarchy</p> </li> </ul> <p></p> <ul> <li> <p>Use color strategically</p> <ul> <li>Green: comfortably meeting SLO</li> <li>Red: SLO violation</li> <li>Yellow: within SLO but trending toward threshold.</li> </ul> </li> <li> <p>Include error budget visuallizations</p> <ul> <li>Total error budget for the period</li> <li>Current consyumption percentage</li> <li>Burn rate</li> <li>Projected depletion date at current burn rate</li> </ul> </li> <li> <p>Include contextual information</p> <ul> <li>SLO targets clearly labeled</li> <li>time window of measurement</li> <li>links to incident response procedures</li> <li>service dependencies status</li> </ul> </li> </ul>"},{"location":"sre/sre_measure/#takeaways","title":"Takeaways","text":"<p>Monitoring Implementation:</p> <ul> <li>Metrics collection starts at the application level with instrumented code.</li> <li>Prometheus acts as the central metrics storage and query engine.</li> <li>Grafana provides visualization and dashboarding capabilities.=</li> <li>Both metrics collection and visualization require careful configuration for accuracy and relevance.</li> </ul> <p>Dashboard Best Practices:</p> <ul> <li>Prioritize user-experienced metrics when defining SLIs and SLOs.</li> <li>Choose appropriate visualization types based on the nature of the metric.</li> <li>Set meaningful thresholds aligned with SLO targets.</li> <li>Include both real-time and historical views for context and trend analysis.</li> </ul>"},{"location":"sysdesign/components/","title":"Compenents for system design architecting","text":""},{"location":"sysdesign/components/#load-balancing","title":"Load Balancing","text":""},{"location":"sysdesign/components/#working-principle","title":"Working principle","text":"<p>Load balancers work by distributing incoming network traffic across multiple servers or resources to ensure efficient utilization of computing resources and prevent overload. Here are the general steps that a load balancer follows to distribute traffic:</p> <p>The load balancer receives a request from a client or user. The load balancer evaluates the incoming request and determines which server or resource should handle the request. This is done based on a predefined load-balancing algorithm that takes into account factors such as server capacity, server response time, number of active connections, and geographic location.</p> <p>The load balancer forwards the incoming traffic to the selected server or resource. The server or resource processes the request and sends a response back to the load balancer. The load balancer receives the response from the server or resource and sends it to the client or user who made the request.</p> <p></p> <p>Typically a load balancer sits between the client and the server accepting incoming network and application traffic and distributing the traffic across multiple backend servers using various algorithms. By balancing application requests across multiple servers, a load balancer reduces the load on individual servers and prevents any one server from becoming a single point of failure, thus improving overall application availability and responsiveness.</p> <p>To utilize full scalability and redundancy, we can try to balance the load at each layer of the system. We can add LBs at three places:</p> <p>Between the user and the web server Between web servers and an internal platform layer, like application servers or cache servers Between internal platform layer and database.</p> <p></p>"},{"location":"sysdesign/components/#key-terminology-and-concepts","title":"Key terminology and concepts","text":"<p>Load Balancer: A device or software that distributes network traffic across multiple servers based on predefined rules or algorithms.</p> <p>Backend Servers: The servers that receive and process requests forwarded by the load balancer. Also referred to as the server pool or server farm.</p> <p>Load Balancing Algorithm: The method used by the load balancer to determine how to distribute incoming traffic among the backend servers.</p> <p>Health Checks: Periodic tests performed by the load balancer to determine the availability and performance of backend servers. Unhealthy servers are removed from the server pool until they recover.</p> <p>Session Persistence: A technique used to ensure that subsequent requests from the same client are directed to the same backend server, maintaining session state and providing a consistent user experience.</p> <p>SSL/TLS Termination: The process of decrypting SSL/TLS-encrypted traffic at the load balancer level, offloading the decryption burden from backend servers and allowing for centralized SSL/TLS management.</p>"},{"location":"sysdesign/components/#load-balancing-algorithms","title":"Load Balancing Algorithms","text":""},{"location":"sysdesign/components/#rr","title":"RR","text":"<p>It assigns a request to the first server, then moves to the second, third, and so on .. </p> <p>Pros:</p> <ul> <li>equal distribution among servers.</li> <li>easy implementation and understand</li> <li>works well when server has similar capabilities</li> </ul> <p>Cons:</p> <ul> <li>no load awareness: does not take into account the current load or capacity of each servers. all are treated equally regardless of their current state. </li> <li>no sesstion affinity: prob for staeful servers as request may reach to other server. </li> <li>performace issue with different capacities.</li> <li>Predictable Distribution Pattern: Round Robin is predictable in its request distribution pattern, which could potentially be exploited by attackers who can observe traffic patterns and might find vulnerabilities in specific servers by predicting which server will handle their requests.</li> </ul> <p>Good usecases</p> <p>Homogeneous Environments: Suitable for environments where all servers have similar capacity and performance. Stateless Applications: Works well for stateless applications where each request can be handled independently.</p>"},{"location":"sysdesign/components/#least-connections","title":"least connections","text":"<p>The Least Connections algorithm is a dynamic load balancing technique that assigns incoming requests to the server with the fewest active connections at the time of the request useful especially in environments where traffic patterns are unpredictable and request processing times vary.</p> <p>Pros:</p> <ul> <li>Load Awareness: Takes into account the current load on each server by considering the number of active connections, leading to better utilization of server resources.</li> <li>Dynamic Distribution: Adapts to changing traffic patterns and server loads, ensuring no single server becomes a bottleneck.</li> <li>Efficiency in Heterogeneous Environments: Performs well when servers have varying capacities and workloads, as it dynamically allocates requests to less busy servers.</li> </ul> <p>Cons:</p> <ul> <li>Higher Complexity: More complex to implement compared to simpler algorithms like Round Robin, as it requires real-time monitoring of active connections.</li> <li>State Maintenance: Requires the load balancer to maintain the state of active connections, which can increase overhead.</li> <li>Potential for Connection Spikes: In scenarios where connection duration is short, servers can experience rapid spikes in connection counts, leading to frequent rebalancing.</li> </ul> <p>Use Cases - Heterogeneous Environments: Suitable for environments where servers have different capacities and workloads, and the load needs to be dynamically distributed. - Variable Traffic Patterns: Works well for applications with unpredictable or highly variable traffic patterns, ensuring that no single server is overwhelmed. - Stateful Applications: Effective for applications where maintaining session state is important, as it helps distribute active sessions more evenly.</p>"},{"location":"sysdesign/components/#weighted-rr","title":"Weighted RR","text":"<p>It assigns weights to each server based on their capacity or performance, distributing incoming requests proportionally according to these weights. This ensures that more powerful servers handle a larger share of the load, while less powerful servers handle a smaller share.</p> <p>Pros</p> <ul> <li>Load Distribution According to Capacity: Servers with higher capacities handle more requests, leading to better utilization of resources.</li> <li>Flexibility: Easily adjustable to accommodate changes in server capacities or additions of new servers.</li> <li>Improved Performance: Helps in optimizing overall system performance by preventing overloading of less powerful servers.</li> </ul> <p>Cons</p> <ul> <li>Complexity in Weight Assignment: Determining appropriate weights for each server can be challenging and requires accurate performance metrics.</li> <li>Increased Overhead: Managing and updating weights can introduce additional overhead, especially in dynamic environments where server performance fluctuates.</li> <li>Not Ideal for Highly Variable Loads: In environments with highly variable load patterns, WRR may not always provide optimal load balancing as it doesn't consider real-time server load.</li> </ul> <p>Use Cases - Heterogeneous Server Environments: Ideal for environments where servers have different processing capabilities, ensuring efficient use of resources. - Scalable Web Applications: Suitable for web applications where different servers may have varying performance characteristics. - Database Clusters: Useful in database clusters where some nodes have higher processing power and can handle more queries.</p>"},{"location":"sysdesign/components/#weighted-least-connections","title":"weighted least connections","text":"<p>combines the principles of the Least Connections and Weighted Round Robin algorithms. It takes into account both the current load (number of active connections) on each server and the relative capacity of each server (weight)</p> <p>Pros Dynamic Load Balancing: Adjusts to the real-time load on each server, ensuring a more balanced distribution of requests. Capacity Awareness: Takes into account the relative capacity of each server, leading to better utilization of resources. Flexibility: Can handle environments with heterogeneous servers and variable load patterns effectively. Cons Complexity: More complex to implement compared to simpler algorithms like Round Robin and Least Connections. State Maintenance: Requires the load balancer to keep track of both active connections and server weights, increasing overhead. Weight Assignment: Determining appropriate weights for each server can be challenging and requires accurate performance metrics. Use Cases Heterogeneous Server Environments: Ideal for environments where servers have different processing capacities and workloads. High Traffic Web Applications: Suitable for web applications with variable traffic patterns, ensuring no single server becomes a bottleneck. Database Clusters: Useful in database clusters where nodes have varying performance capabilities and query loads.</p>"},{"location":"sysdesign/components/#ip-hash","title":"IP Hash","text":"<p>The load balancer uses a hash function to convert the client's IP address into a hash value, which is then used to determine which server should handle the request. This method ensures that requests from the same client IP address are consistently routed to the same server, providing session persistence.</p> <p>Pros Session Persistence: Ensures that requests from the same client IP address are consistently routed to the same server, which is beneficial for stateful applications. Simplicity: Easy to implement and does not require the load balancer to maintain the state of connections. Deterministic: Predictable and consistent routing based on the client's IP address. Cons Uneven Distribution: If client IP addresses are not evenly distributed, some servers may receive more requests than others, leading to an uneven load. Dynamic Changes: Adding or removing servers can disrupt the hash mapping, causing some clients to be routed to different servers. Limited Flexibility: Does not take into account the current load or capacity of servers, which can lead to inefficiencies. Use Cases Stateful Applications: Ideal for applications where maintaining session persistence is important, such as online shopping carts or user sessions. Geographically Distributed Clients: Useful when clients are distributed across different regions and consistent routing is required.</p>"},{"location":"sysdesign/components/#least-response-time","title":"least response time","text":"<p>TODO</p>"},{"location":"sysdesign/components/#dns-lb-ha","title":"DNS LB &amp; HA","text":""},{"location":"sysdesign/components/#rr-dns","title":"RR DNS","text":"<p>Round-robin DNS is a simple load balancing technique in which multiple IP addresses are associated with a single domain name. When a resolver queries the domain name, the DNS server responds with one of the available IP addresses, rotating through them in a round-robin fashion. This distributes the load among multiple servers or resources, improving the performance and availability of the website or service.</p> <p>However, round-robin DNS does not take into account the actual load on each server or the geographic location of the client, which can lead to uneven load distribution or increased latency in some cases.</p>"},{"location":"sysdesign/components/#geo-distributed-dns-servers","title":"Geo distributed DNS servers","text":"<p>By distributing DNS servers across different regions, they can provide faster and more reliable DNS resolution for users located closer to a server.</p> <p>Geographically distributed servers also offer increased redundancy, reducing the impact of server failures or network outages. If one server becomes unreachable, users can still access the service through other available servers in different locations.</p>"},{"location":"sysdesign/components/#anycast-routing","title":"Anycast routing","text":"<p>Anycast routing is a networking technique that allows multiple servers to share the same IP address. When a resolver sends a query to an anycast IP address, the network routes the query to the nearest server, based on factors like network latency and server availability.</p> <p>Anycast provides several benefits for DNS:</p> <p>Load balancing: Anycast distributes DNS queries among multiple servers, preventing any single server from becoming a bottleneck. Reduced latency: By directing users to the nearest server, anycast can significantly reduce the time it takes for DNS resolution. High availability: If a server fails or becomes unreachable, anycast automatically redirects queries to the next closest server, ensuring uninterrupted service.</p>"},{"location":"sysdesign/components/#cdn-dns","title":"CDN &amp;&amp; DNS","text":"<p>A Content Delivery Network (CDN) is a network of distributed servers that cache and deliver web content to users based on their geographic location. CDNs help improve the performance, reliability, and security of websites and web services by distributing the load among multiple servers and serving content from the server closest to the user.</p> <p>When a user requests content from a website using a CDN, the CDN's DNS server determines the best server to deliver the content based on the user's location and other factors. The DNS server then responds with the IP address of the chosen server, allowing the user to access the content quickly and efficiently.</p>"},{"location":"sysdesign/components/#uses-of-load-balancing","title":"Uses of load balancing","text":""},{"location":"sysdesign/components/#ha-fault-tolerance","title":"HA &amp; Fault tolerance:","text":"<p>A load balancer performs Health Checks. It acts as the heartbeat monitor for your cluster. It constantly pings your backend servers (\"Are you alive? Can you take a request?\"). If a server fails to answer or returns a 5xx error, the LB cuts it off instantly. It stops sending traffic to the corpse and reroutes it to the living.</p>"},{"location":"sysdesign/components/#horizontal-scalability","title":"Horizontal Scalability","text":"<p>The LB acts as the Unified Entry Point (Virtual IP). Clients only know the LB's address. When traffic spikes, you spin up more backend instances, register them with the LB, and boom, you have more capacity.</p>"},{"location":"sysdesign/components/#bluegreen-deployments","title":"Blue/Green deployments","text":"<p>Load balancers allow for Connection Draining and strategies like Blue-Green Deployment. You can signal the LB to stop sending new connections to a specific server while allowing existing connections to finish naturally, then take it offline for patching.</p>"},{"location":"sysdesign/components/#shield","title":"Shield","text":"<p>A Load Balancer acts as a Reverse Proxy. It terminates the connection. The client talks to the LB; the LB talks to the server. The internet never touches your backend. Furthermore, the LB can absorb DDoS attacks (Distributed Denial of Service) and filter malicious traffic before it even reaches your expensive application logic.</p>"},{"location":"sysdesign/components/#ssl-termination-the-offloader","title":"SSL Termination (The \"Offloader\")","text":"<p>Encryption is expensive. Handshaking SSL/TLS (decrypting HTTPS traffic) takes significant CPU power. You can offload this to the Load Balancer. This is called SSL Termination. The client speaks HTTPS to the Load Balancer. The Load Balancer decrypts it and speaks HTTP (or lighter encryption) to your backend servers inside your secure private network.</p>"},{"location":"sysdesign/components/#dns-load-balancing-and-ha","title":"DNS Load Balancing and HA","text":"<p>DNS load balancing and high availability techniques, such as round-robin DNS, geographically distributed servers, anycast routing, and Content Delivery Networks (CDNs), help distribute the load among multiple servers, reduce latency for end-users, and maintain uninterrupted service, even in the face of server failures or network outages.</p>"},{"location":"sysdesign/components/#lb-types","title":"LB types","text":""},{"location":"sysdesign/components/#hardware-load-balancing","title":"Hardware Load Balancing","text":"<p>They use specialized hardware components, such as Application-Specific Integrated Circuits (ASICs) or Field-Programmable Gate Arrays (FPGAs), to efficiently distribute network traffic</p> <p>Use case: A large e-commerce company uses a hardware load balancer to distribute incoming web traffic among multiple web servers, ensuring fast response times and a smooth shopping experience for customers.</p>"},{"location":"sysdesign/components/#software-load-balancing","title":"Software Load Balancing","text":"<p>Software load balancers are applications that run on general-purpose servers or virtual machines. They use software algorithms to distribute incoming traffic among multiple servers or resources.</p> <p>Use case: A startup with a growing user base deploys a software load balancer on a cloud-based virtual machine, distributing incoming requests among multiple application servers to handle increased traffic.</p>"},{"location":"sysdesign/components/#cloud-based-load-balancing","title":"Cloud-based Load Balancing","text":"<p>Cloud-based load balancers are provided as a service by cloud providers. They offer load balancing capabilities as part of their infrastructure, allowing users to easily distribute traffic among resources within the cloud environment.</p> <p>Use case: A mobile app developer uses a cloud-based load balancer provided by their cloud provider to distribute incoming API requests among multiple backend servers, ensuring smooth app performance and quick response times.</p>"},{"location":"sysdesign/components/#dns-load-balancing","title":"DNS Load Balancing","text":"<p>DNS (Domain Name System) load balancing relies on the DNS infrastructure to distribute incoming traffic among multiple servers or resources. It works by resolving a domain name to multiple IP addresses, effectively directing clients to different servers based on various policies.</p> <p>Use case: A content delivery network (CDN) uses DNS load balancing to direct users to the closest edge server based on their geographical location, ensuring faster content delivery and reduced latency.</p>"},{"location":"sysdesign/components/#global-server-load-balancing","title":"Global Server Load Balancing","text":"<p>GSLB is a technique used to distribute traffic across geographically dispersed data centers. It combines DNS load balancing with health checks and other advanced features to provide a more intelligent and efficient traffic distribution method.</p> <p>Use case: A multinational corporation uses GSLB to distribute incoming requests for its web applications among several data centers around the world, ensuring high availability and optimal performance for users in different regions.</p>"},{"location":"sysdesign/components/#hybrid-load-balancing","title":"Hybrid Load Balancing","text":"<p>Hybrid load balancing combines the features and capabilities of multiple load balancing techniques to achieve the best possible performance, scalability, and reliability. It typically involves a mix of hardware, software, and cloud-based solutions to provide the most effective and flexible load balancing strategy for a given scenario.</p> <p>Use case: A large-scale online streaming platform uses a hybrid load balancing strategy, combining hardware load balancers in their data centers for high-performance traffic distribution, cloud-based load balancers for scalable content delivery, and DNS load balancing for global traffic management. This approach ensures optimal performance, scalability, and reliability for their millions of users worldwide.</p>"},{"location":"sysdesign/components/#layer-4-load-balancing","title":"Layer 4 Load Balancing","text":"<p>Layer 4 load balancing, also known as transport layer load balancing, operates at the transport layer of the OSI model (the fourth layer). It distributes incoming traffic based on information from the TCP or UDP header, such as source and destination IP addresses and port numbers.</p> <p>Use case: An online gaming platform uses Layer 4 load balancing to distribute game server traffic based on IP addresses and port numbers, ensuring that players are evenly distributed among available game servers for smooth gameplay.</p>"},{"location":"sysdesign/components/#layer-7-load-balancing","title":"Layer 7 Load Balancing","text":"<p>Layer 7 load balancing, also known as application layer load balancing, operates at the application layer of the OSI model (the seventh layer). It takes into account application-specific information, such as HTTP headers, cookies, and URL paths, to make more informed decisions about how to distribute incoming traffic.</p> <p>Use case: A web application with multiple microservices uses Layer 7 load balancing to route incoming API requests based on the URL path, ensuring that each microservice receives only the requests it is responsible for handling.</p>"},{"location":"sysdesign/components/#stateless-and-stateful-lb","title":"Stateless and stateful LB","text":"<p>Stateless load balancers operate without maintaining any information about the clients' session or connection state. They make routing decisions based solely on the incoming request data, such as the client's IP address, request URL, or other headers.</p> <p>stateful load balancing preserves session information between requests. The load balancer assigns a client to a specific server and ensures that all subsequent requests from the same client are directed to that server.</p> <p>Stateful load balancing can be further categorized into two types:</p> <ul> <li>Source IP Affinity: This form of stateful load balancing assigns a client to a specific server based on the client's IP address.</li> <li>Session Affinity: In this type of stateful load balancing, the load balancer allocates a client to a specific server based on a session identifier, such as a cookie or URL parameter. This method ensures that requests from the same client consistently reach the same server, regardless of the client's IP address.</li> </ul> <p>Stateless load balancing is useful for applications capable of processing requests independently, while stateful load balancing is more appropriate for applications that depend on session data.</p>"},{"location":"sysdesign/components/#load-balancing-terminology","title":"load balancing terminology","text":""},{"location":"sysdesign/components/#availabilty-realibility","title":"Availabilty &amp; Realibility","text":"<p>availability is about whether a system is \"up,\" while reliability is about whether it \"works correctly\" once it\u2019s up</p> <p>A Car: If you have a car in your driveway ready to drive, it is available. However, if that car stalls every time you hit 60 mph, it is unreliable. A Website: A site that loads but gives you an error every time you click \"Checkout\" has high availability (it's online) but low reliability (it fails to perform its function).</p>"},{"location":"sysdesign/components/#upstream-and-downstream","title":"Upstream and Downstream","text":"<p>The exact meaning depends on the point of reference in the architecture e.g </p> <p><code>User requests \u2192 Load Balancer \u2192 App Server \u2192 Database</code></p> <p>Upstream = Traffic going OUT(moving away) from your system to another system/service.  Downstream = Traffic coming INTO(coming into) your system from another system/service.</p> <p>From the App Server\u2019s perspective:</p> <p>Downstream \u2192 User requests / Load Balancer (requests coming in) Upstream \u2192 Database (requests going out)</p> <p>From the Load Balancer perspective:</p> <p>Downstream -&gt; User requests upstream -&gt; App server(backend servers) </p>"},{"location":"sysdesign/components/#lb-ha-and-fault-tolerance","title":"LB HA and Fault Tolerance","text":"<p>To ensure high availability and fault tolerance, load balancers should be designed and deployed with redundancy in mind. </p> <p>Active-passive configuration: one load balancer (the active instance) handles all incoming traffic while the other (the passive instance) remains on standby. If the active load balancer fails, the passive instance takes over and starts processing requests.</p> <p>Active-active configuration: In this setup, multiple load balancer instances actively process incoming traffic simultaneously. Traffic is distributed among the instances using methods such as DNS load balancing or an additional load balancer layer</p> <p>Health checks and monitoring: Health checks are periodic tests performed by the load balancer to determine the availability and performance of backend servers. load balancers can automatically remove unhealthy servers from the server pool and avoid sending traffic to them</p> <p>Synchronization and State Sharing </p> <p>Centralized configuration management: Using a centralized configuration store (e.g., etcd, Consul, or ZooKeeper) to maintain and distribute configuration data among load balancer instances ensures that all instances are using the same settings and are aware of changes.</p> <p>State sharing and replication: In scenarios where load balancers must maintain session data or other state information, it is crucial to ensure that this data is synchronized and replicated across instances. This can be achieved through database replication, distributed caching systems (e.g., Redis or Memcached), or built-in state-sharing mechanisms provided by the load balancer software or hardware.</p>"},{"location":"sysdesign/components/#scalability-and-performance","title":"Scalability and Performance","text":"<p>Horizontal scaling: This involves adding more load balancer instances to distribute traffic among them. Horizontal scaling is particularly effective for active-active configurations, where each load balancer instance actively processes traffic.</p> <p>Vertical scaling: This involves increasing the resources (e.g., CPU, memory, and network capacity) of the existing load balancer instance(s) to handle increased traffic.</p> <p>Connection and request rate limits: Overloading a load balancer or backend servers can result in decreased performance or even service outages. Implementing rate limiting(such as IP addresses, client domains, or URL patterns) and connection limits at the load balancer level can help prevent overloading and ensure consistent performance.</p> <p>Caching and content optimization: Load balancers can cache static content, such as images, CSS, and JavaScript files, to reduce the load on backend servers and improve response times. Additionally, some load balancers support content optimization features like compression or minification, which can further improve performance and reduce bandwidth consumption.</p>"},{"location":"sysdesign/components/#lb-latency","title":"LB Latency","text":"<p>While the impact is typically minimal, it is important to consider the potential latency introduced by the load balancer and optimize its performance accordingly.</p> <p>Geographical distribution: Deploying load balancers and backend servers in geographically distributed locations can help reduce latency for users by ensuring that their requests are processed by a nearby instance.</p> <p>Connection reuse: Many load balancers support connection reuse or keep-alive connections, which reduce the overhead of establishing new connections between the load balancer and backend servers for each request</p> <p>Protocol optimizations: Some load balancers support protocol optimizations, such as HTTP/2 or QUIC, which can improve performance by reducing latency and increasing throughput.</p>"},{"location":"sysdesign/components/#challenges-of-lb","title":"Challenges of LB","text":"<p>Single Point of Failure: If not designed with redundancy and fault tolerance in mind, a load balancer can become a single point of failure in the system</p> <p>Configuration Complexity: Load balancers often come with a wide range of configuration options, including algorithms, timeouts, and health checks. Misconfigurations can lead to poor performance, uneven traffic distribution, or even service outages.</p> <p>Scalability Limitations: As traffic increases, the load balancer itself might become a performance bottleneck, especially if it is not configured to scale horizontally or vertically.</p> <p>Latency: Introducing a load balancer into the request-response path adds an additional network hop, which could lead to increased latency. While the impact is typically minimal, it is essential to consider the potential latency introduced by the load balancer and optimize its performance accordingly.</p> <p>Sticky Sessions Some applications rely on maintaining session state or user context between requests. In such cases, load balancers must be configured to use session persistence or \"sticky sessions\" to ensure subsequent requests from the same user are directed to the same backend server. However, this can lead to uneven load distribution and negate some of the benefits of load balancing.</p> <p>Cost: Deploying and managing load balancers, especially in high-traffic scenarios, can add to the overall cost of your infrastructure. This may include hardware or software licensing costs, as well as fees associated with managed load balancing services provided by cloud providers.</p> <p>Health Checks and Monitoring: Implementing effective health checks for backend servers is essential to ensure that the load balancer accurately directs traffic to healthy instances. Misconfigured or insufficient health checks can lead to the load balancer sending traffic to failed or underperforming servers, resulting in a poor user experience</p>"},{"location":"sysdesign/components/#api-gateway","title":"API Gateway","text":"<p>An API Gateway is a server-side architectural component in a software system that acts as an intermediary between clients (such as web browsers, mobile apps, or other services) and backend services, microservices, or APIs.</p> <p>Its main purpose is to provide a single entry point for external consumers to access the services and functionalities of the backend system. It receives client requests, forwards them to the appropriate microservice, and then returns the server\u2019s response to the client.</p> <p>The API gateway is responsible for tasks such as routing, authentication, and rate limiting. This enables microservices to focus on their individual tasks and improves the overall performance and scalability of the system.</p> <p></p>"},{"location":"sysdesign/components/#diff-between-api-gw-and-lb","title":"Diff between API G/W and LB","text":"Feature API Gateway Load Balancer Primary Purpose Manages, secures, and routes API requests Distributes incoming network traffic across multiple servers OSI Layer Typically Layer 7 (Application Layer) Layer 4 (Transport) and/or Layer 7 Protocol Handling HTTP, HTTPS, WebSocket, REST, gRPC TCP, UDP, HTTP, HTTPS Traffic Distribution Routes based on API path, headers, authentication, version Routes based on IP, port, protocol Authentication Built-in authentication (OAuth, JWT, API keys) No built-in authentication Rate Limiting Supports throttling and rate limiting Not typically supported Request Transformation Can modify headers, payloads, and responses Cannot modify request/response content Caching Supports response caching No caching SSL Termination Yes Yes Monitoring &amp; Analytics Detailed API-level metrics Basic traffic-level metrics Use Case Microservices architecture, external API exposure High availability, horizontal scaling Example AWS API Gateway, Kong, Apigee AWS ALB, NLB, HAProxy"},{"location":"sysdesign/components/#key-usages-of-api-gateways","title":"Key Usages of API Gateways","text":"<p>Request Routing: Directing incoming client requests to the appropriate backend service.</p> <p>Aggregation of Multiple Services: Combining responses from multiple backend services into a single response to the client.</p> <p>Security Enforcement: Implementing security measures such as authentication, authorization, and rate limiting.</p> <p>Load Balancing: Distributing incoming requests evenly across multiple instances of backend services to ensure no single service becomes a bottleneck</p> <p>Caching Responses: Storing frequently requested data to reduce latency and decrease the load on backend services.</p> <p>Protocol Translation: Converting requests and responses between different protocols used by clients and backend services.</p> <p>Monitoring and Logging: Tracking and recording request and response data for analysis, debugging, and performance monitoring.</p> <p>Transformation of Requests and Responses: Modifying the data format or structure of requests and responses to meet the needs of clients or services.</p> <p>API Versioning: Managing different versions of APIs to ensure backward compatibility and smooth transitions when updates are made.</p> <p>Rate Limiting and Throttling: Controlling the number of requests a client can make in a given time frame to protect backend services from being overwhelmed.</p> <p>API Monetization: Enabling businesses to monetize their APIs by controlling access, usage tiers, and billing.</p> <p>Service Discovery Integration: Facilitating dynamic discovery of backend services, especially in environments where services are frequently scaled up or down.</p> <p>Circuit Breaker Pattern Implementation: Preventing cascading failures by detecting when a backend service is failing and stopping requests to it temporarily.</p> <p>Content-Based Routing: Routing requests to different backend services based on the content of the request, such as headers, body, or query parameters.</p> <p>SSL Termination: Handling SSL/TLS encryption and decryption at the gateway level to offload this resource-intensive task from backend services.</p> <p>Policy Enforcement: Applying organizational policies consistently across all API traffic, such as data validation, request formatting, and access controls.</p> <p>Multi-Tenancy Support: Supporting multiple clients or tenants within a single API infrastructure while ensuring data isolation and customized configurations.</p> <p>A/B Testing and Canary Releases: Facilitating controlled testing of new features or services by directing a subset of traffic to different backend versions.</p> <p>Localization and Internationalization Support: Adapting responses based on the client's locale, such as language preferences or regional settings.</p> <p>Reducing Client Complexity: Simplifying the client-side logic by handling complex operations on the server side through the gateway.</p>"},{"location":"sysdesign/components/#advantages-of-using-api-gateway","title":"Advantages of using API Gateway","text":"<ul> <li>Improved performance - cache responses, rate limit requests, and optimize communication between clients and backend services</li> <li>Simplified system design - provides a single entry point for all API requests, making it easier to manage, monitor, and maintain APIs across multiple backend services.</li> <li>Enhanced security - can enforce authentication and authorization policies, helping protect backend services from unauthorized access or abuse</li> <li>Improved scalability - can distribute incoming requests among multiple instances of a microservice, enabling the system to scale more easily and handle a larger number of requests</li> <li>Better monitoring and visibility - can collect metrics and other data about the requests and responses, providing valuable insights into the performance and behavior of the system</li> <li>Simplified Client Integration - By providing a consistent and unified interface for clients to access multiple backend services, the API Gateway simplifies client-side development and reduces the need for clients to manage complex service interactions</li> <li>Protocol and Data Format Transformation - can convert requests and responses between different protocols (e.g., HTTP to gRPC) or data formats (e.g., JSON to XML), enabling greater flexibility in how clients and services communicate</li> <li>API Versioning and Backward Compatibility - can manage multiple versions of an API, allowing developers to introduce new features or make changes without breaking existing clients</li> <li>Enhanced Error Handling - can provide a consistent way to handle errors and generate error responses</li> <li>Load Balancing and Fault Tolerance - can distribute incoming traffic evenly among multiple instances of a backend service, improving performance and fault tolerance</li> </ul>"},{"location":"sysdesign/components/#disadvantages-of-using-api-gateway","title":"Disadvantages of using API Gateway","text":"<ul> <li>Additional Complexity - adds an extra layer of complexity to your architecture</li> <li>Single Point of Failure - If not configured correctly, the API Gateway could become a single point of failure in your system. If the gateway experiences an outage or performance issues, it can affect the entire system</li> <li>Latency - adds an extra hop in the request-response path, which could introduce some latency, especially if the gateway is responsible for performing complex tasks like request/response transformation or authentication</li> <li>Vendor Lock-in - managed API Gateway service provided by a specific cloud provider or vendor, you may become dependent on their infrastructure, pricing, and feature set. This could make it more challenging to migrate your APIs to a different provider</li> <li>Cost - Running an API Gateway, especially in high-traffic scenarios, can add to the overall cost of your infrastructure</li> <li>Maintenance Overhead - API Gateway requires monitoring, maintenance, and regular updates to ensure its security and reliability</li> <li>Configuration Complexity - API Gateways often come with a wide range of features and configuration options. Setting up and managing these configurations can be complex and time-consuming, especially when dealing with multiple environments or large-scale deployments.</li> </ul>"},{"location":"sysdesign/components/#network-essentials","title":"Network essentials","text":"<p>Differences Between HTTP and HTTPS</p>"},{"location":"sysdesign/components/#http-vs-https-comparison","title":"HTTP vs HTTPS Comparison","text":"Feature HTTP HTTPS Security No encryption; data is sent in plain text Encrypted using SSL/TLS protocols Port 80 443 Performance Slightly faster due to lack of encryption overhead Slightly slower due to encryption processes SEO Ranking Lower search engine ranking Higher search engine ranking Use Cases Non-sensitive data transmission Sensitive transactions (e.g., banking, e-commerce)"},{"location":"sysdesign/components/#tcp-vs-udp","title":"TCP vs UDP","text":"<p>these two of the main protocols used for transmitting data over the internet.</p> <p>TCP: TCP is a connection-oriented protocol that ensures reliable, ordered, and error-checked delivery of a stream of bytes between applications.</p> <p>Characteristics: - Reliability: TCP ensures that data is delivered accurately and in order, retransmitting lost or corrupted packets. - Connection-Oriented: Establishes a connection between sender and receiver before transmitting data. - Flow Control: Manages data transmission rate to prevent network congestion. - Congestion Control: Adjusts the transmission rate based on network traffic conditions. - Acknowledgements and Retransmissions: Uses acknowledgments to confirm receipt of data and retransmits if necessary.</p> <p>e.g: Loading a webpage: TCP is used to ensure all web content is loaded correctly and in the right order.</p> <p>UDP: UDP is a connectionless protocol that sends messages, called datagrams, without establishing a prior connection and without guaranteeing reliability or order.</p> <p>Characteristics: - Low Overhead: Does not establish a connection, leading to lower overhead and latency. - Unreliable Delivery: Does not guarantee message delivery, order, or error checking. - Speed: Faster than TCP due to its simplicity and lack of retransmission mechanisms. - No Congestion Control: Does not reduce transmission rates under network congestion. e.g: Streaming a live sports event: UDP is used for faster transmission, even if it means occasional pixelation or minor video artifacts.</p> <p>TCP vs UDP Comparison</p> Feature TCP UDP Reliability Reliable transmission, ensuring data is delivered accurately and in order Unreliable transmission; data may be lost or arrive out of order Connection Connection-oriented; establishes a connection before transmitting data Connectionless; sends data without establishing a connection Speed and Overhead Slower due to handshaking, acknowledgments, and congestion control Faster with minimal overhead, suitable for real-time applications Data Integrity High data integrity, suitable for applications like file transfers and web browsing Lower data integrity, acceptable for applications like streaming where perfect accuracy is less critical Use Case Suitability Used when data accuracy is more critical than speed Used when speed is more critical than accuracy"},{"location":"sysdesign/components/#http10-vs-http11-vs-http2-vs-http3","title":"HTTP/1.0 vs HTTP/1.1 vs HTTP/2 vs HTTP/3","text":"<p>Summary:</p> <p>HTTP has evolved to improve: - \u26a1 Performance (faster loading) - \ud83d\udd10 Security (stronger encryption) - \ud83d\udce1 Efficiency (better bandwidth usage) - \ud83c\udf0d Scalability (handling modern traffic)</p> <p>Each version builds upon the previous one to address web scalability challenges.</p> <p>Comparison:</p> Feature HTTP/1.0 HTTP/1.1 HTTP/2 HTTP/3 Release Year 1996 1997 2015 2020 Connection Model New TCP connection per request Persistent connections (Keep-Alive) Multiplexed streams over single TCP connection Multiplexed streams over QUIC (UDP) Protocol Format Text Text Binary Binary Multiplexing \u274c No \u274c No \u2705 Yes \u2705 Yes Header Compression \u274c No \u274c Limited \u2705 HPACK \u2705 QPACK Head-of-Line Blocking Yes Yes Yes (TCP-level) No Transport Protocol TCP TCP TCP UDP (QUIC) Security Optional Optional Usually HTTPS Mandatory TLS 1.3 Latency High Medium Low Very Low Packet Loss Handling Poor Poor Better Excellent Typical Use Case Static websites Dynamic web apps High-traffic apps Real-time applications"},{"location":"sysdesign/components/#http10-1996","title":"HTTP/1.0 (1996)","text":"<p>Characteristics: - One request = one TCP connection - Stateless - Basic headers - High latency</p> <p>Limitations: - Connection overhead - Slow page loads - Not suitable for modern resource-heavy websites</p> <p>Use cases: - Simple static websites - Early web applications</p>"},{"location":"sysdesign/components/#http11-1997","title":"HTTP/1.1 (1997)","text":"<p>Improvements Over 1.0: - Persistent connections (Keep-Alive) - Chunked transfer encoding - Host header (virtual hosting) - Better caching</p> <p>Benefits: - Reduced latency - Efficient resource usage - Enabled shared hosting</p> <p>Still Has: - Head-of-line blocking - Text-based inefficiencies</p> <p>Best Used For: - Dynamic websites - E-commerce platforms - APIs</p>"},{"location":"sysdesign/components/#http2-2015","title":"HTTP/2 (2015)","text":"<p>Major Enhancements: - Binary protocol - Multiplexing (multiple requests over one connection) - Header compression (HPACK) - Server Push</p> <p>Benefits: - Significant performance improvement - Faster page loads - Efficient bandwidth usage</p> <p>Limitation: - Still uses TCP \u2192 TCP-level head-of-line blocking</p> <p>Best Used For: - Social media platforms - Streaming services - Large web applications</p>"},{"location":"sysdesign/components/#http3-2020","title":"HTTP/3 (2020)","text":"<p>Built On: - QUIC protocol (UDP-based)</p> <p>Major Advantages: - Eliminates TCP head-of-line blocking - 0-RTT handshake (faster connection setup) - Better packet loss handling - Built-in TLS 1.3 (mandatory encryption)</p> <p>Benefits: - Lower latency - More resilient on unstable networks - Better mobile performance</p> <p>Best Used For - Video conferencing (Zoom, Teams) - Online gaming - Live streaming - Real-time apps</p> <p>Conclusion:</p> <p>The evolution from HTTP/1.0 to HTTP/3 represents, from simple request-response communication to to high-performance, encrypted, multiplexed, real-time web communication.</p>"},{"location":"sysdesign/components/#url-vs-uri-vs-urn","title":"URL vs. URI vs. URN","text":"<p>URL: Specifies both the identity and the location of a resource (How and Where). (https://www.example.com/path?query=term#section)</p> <p>URI: A more comprehensive term covering both URLs (identifying and locating) and URNs (just identifying). (https://www.example.com/path?query=term#section)</p> <p>URN: Focuses only on uniquely identifying a resource, not on where it is located or how to access it. (urn:isbn:0451450523)</p>"},{"location":"sysdesign/components/#proxy","title":"Proxy","text":"<p>forward proxy</p> <p>A forward proxy(proxy server)is a server that sits in front of one or more client machines and acts as an intermediary between the clients and the internet. When a client machine makes a request to a resource (like a web page or file) on the internet, the request is first sent to the proxy,  then forwards the request to the internet on behalf of the client machine and returns the response to the client machine. forward proxies are used to cache data, filter requests, log requests, or transform requests (by adding/removing headers, encrypting/decrypting, or compressing a resource). Proxies can combine the same data access requests into one request and then return the result to the user; this technique is called collapsed forwarding</p> <p></p> <p>Reverse Proxy</p> <p>A reverse proxy is a server that sits in front of one or more web servers and acts as an intermediary between the web servers and the Internet. When a client makes a request to a resource on the internet, the request is first sent to the reverse proxy. The reverse proxy then forwards the request to one of the web servers, which returns the response to the reverse proxy. The reverse proxy then returns the response to the client. Contrary to the forward proxy, which hides the client's identity, a reverse proxy hides the server's identity. this can be used for caching, load balancing, or routing requests to the appropriate servers</p> <p></p>"},{"location":"sysdesign/components/#forward-vs-reverse-proxy","title":"forward vs reverse proxy","text":""},{"location":"sysdesign/components/#uses-of-proxies","title":"Uses of Proxies","text":"<ul> <li> <p>Performance enhancement - cache frequently accessed content, reducing the need for repeated requests to the target server.</p> </li> <li> <p>Security enhancement - protective barrier between clients and target servers, protect internal networks from external threats and prevent unauthorized access to sensitive resources</p> </li> <li> <p>Anonymity and privacy - can mask the client's IP address and other identifying information, providing a level of anonymity and privacy when accessing the internet or other network resources</p> </li> <li> <p>Load balancing - Reverse proxy servers can distribute client requests across multiple target servers, preventing individual servers from becoming overburdened and ensuring high availability and performance</p> </li> <li> <p>Centralized control and monitoring - Proxy servers enable centralized control and monitoring of network traffic, facilitating easier administration and management of network resources. Administrators can implement policies, filters, and other configurations on the proxy server to manage traffic and optimize network performance.</p> </li> <li> <p>Content filtering and access control - can be configured to block or filter specific content types, websites, or services based on predetermined policies. This functionality is often used in educational and corporate environments to enforce acceptable use policies or comply with regulatory requirements</p> </li> <li> <p>Content adaptation and transformation - Proxy servers can modify and adapt content to suit specific client requirements, such as altering image formats, compressing data, or adjusting content for mobile or low-bandwidth devices. </p> </li> <li> <p>Logging and auditing - can log and record network traffic, providing a valuable source of information for auditing, troubleshooting, and monitoring purposes</p> </li> <li> <p>SSL termination - Reverse proxy servers can handle SSL/TLS encryption and decryption, offloading this task from the target servers</p> </li> <li> <p>Application-level gateway - can act as an application-level gateway, processing and forwarding application-specific requests and responses between clients and servers. This capability allows proxy servers to provide added functionality, such as authentication, content filtering, or protocol translation, at the application level.</p> </li> </ul>"},{"location":"sysdesign/components/#vpn-vs-proxy","title":"VPN Vs Proxy","text":"Feature VPN (Virtual Private Network) Proxy Server Definition Creates a secure encrypted tunnel between your device and the internet Acts as an intermediary between client and destination server Encryption Encrypts all traffic from your device Usually does NOT encrypt traffic (unless HTTPS proxy) Security Level High \u2013 protects against ISP tracking, hackers, MITM attacks Low to Moderate \u2013 mainly hides IP address IP Masking Yes \u2013 hides your real IP address Yes \u2013 hides your real IP address Scope of Protection Entire device (all applications) Specific application (e.g., browser) Performance Slightly slower due to encryption overhead Generally faster (no encryption overhead) Use Cases Secure remote work, public WiFi protection, accessing private networks Bypassing geo-restrictions, web scraping, basic anonymity Configuration Level OS-level configuration Application-level configuration Logging Depends on provider Depends on provider Protocol Examples OpenVPN, IPSec, WireGuard HTTP Proxy, SOCKS5 Cost Usually paid service Often free or low cost <p>When to Use VPN?</p> <ul> <li>Working remotely (secure corporate access)</li> <li>Using public Wi-Fi</li> <li>Protecting sensitive data</li> <li>Securing DevOps/admin access to private servers</li> </ul> <p>When to Use Proxy?</p> <ul> <li>Bypassing website blocks</li> <li>Web scraping</li> <li>Testing geo-location-based content</li> <li>Lightweight IP masking</li> </ul>"},{"location":"sysdesign/components/#distributed-fs","title":"Distributed FS","text":"<p>type of file system that manage the storage and retrieval of data across multiple servers and locations</p> <ul> <li>Cloud Storage Services: Services like Google Drive, Dropbox, and others use distributed file systems to store user files across many servers.</li> <li>Big Data Applications: Systems like Hadoop Distributed File System (HDFS) are specifically designed for storing and processing large datasets.</li> <li>Content Delivery Networks: Distributing content across different regions to improve access speed and reliability.</li> <li>High-Performance Computing: Where large datasets need to be accessed and processed concurrently by multiple systems.</li> </ul>"},{"location":"sysdesign/components/#arch-components-of-dfs","title":"Arch components of DFS","text":"<ul> <li> <p>Client Interface: - Provides a way for clients (users or applications) to access and manipulate files as if they were on a local file system.</p> </li> <li> <p>Metadata Servers: - Manage metadata about files, such as location information, directory structures, permissions, and file attributes.</p> </li> <li> <p>Data Nodes or Storage Nodes: - Store the actual file data</p> </li> <li> <p>Replication and Redundancy Mechanism: - Ensures data availability and durability by replicating files across multiple nodes.</p> </li> <li> <p>Load Balancer or Scheduler: - Distributes workload evenly across different nodes and manages resource allocation.</p> </li> <li> <p>Network Infrastructure: - Connects all components of the DFS and facilitates communication between them.</p> </li> <li> <p>Synchronization and Consistency Mechanisms: - Ensures that all copies of a file are kept consistent across the system.</p> </li> <li> <p>Fault Tolerance and Recovery Mechanisms: - Handles failures of nodes or network components without data loss or significant downtime.</p> </li> <li> <p>Security Features: - Protects data from unauthorized access and ensures secure communication across the network.</p> </li> </ul>"},{"location":"sysdesign/components/#key-components","title":"key components","text":""},{"location":"sysdesign/components/#replication","title":"Replication","text":"<p>Purpose: ensuring data availability and durability. By creating multiple copies of data across different nodes, DFS protects against data loss due to node failures</p> <p>Implementation: Files are often divided into blocks, and each block is replicated across multiple nodes. DFS usually allows configuring the replication factor, i.e., the number of replicas for each block. Intelligent placement of replicas across different nodes or racks to ensure high availability and fault tolerance.</p> <p>Challenges: Replication consumes network bandwidth, especially during the initial copying of data, Requires additional storage capacity for replicas.</p>"},{"location":"sysdesign/components/#scalability","title":"Scalability","text":"<p>Purpose: DFS can grow in capacity and performance as the amount of data or the number of users increases.</p> <p>Implementation: Horizontal Scaling, Load Distribution, Avoids single points of failure and bottlenecks, allowing for seamless scaling.</p> <p>Challenges: Scaling up involves efficiently managing metadata so that it doesn't become a bottleneck. Ensuring new nodes are effectively utilized and the load is evenly distributed</p>"},{"location":"sysdesign/components/#consistency","title":"Consistency","text":"<p>Purpose: ensuring that all clients see the same data at any given time, despite data replication and concurrent modifications.</p> <p>Implementation: use different consistency models()     strict consistency - where all nodes see the data at the same time      eventual consistency - where data updates will eventually propagate to all nodes but are not immediately visible</p> <p>Challenges: Strong consistency can impact system performance and latency. Ensuring data integrity in the presence of concurrent accesses and updates.</p>"},{"location":"sysdesign/components/#redunancy-replication","title":"Redunancy &amp; replication","text":""},{"location":"sysdesign/components/#redunancy","title":"Redunancy","text":"<p>Redundancy refers to the duplication of critical components or functions to increase the reliability, availability, and fault tolerance of a system. Redundancy = Duplicate system components</p> <p>Benefits:</p> <ul> <li>Improved reliability: can continue to function despite individual component failures, ensuring the availability of critical services and applications</li> <li>Enhanced fault tolerance: a system can better tolerate and recover from faults or failures, essential for maintaining high availability and minimizing downtime, particularly in mission-critical systems</li> <li>Increased availability:  services and applications remain available even during component failures or maintenance</li> <li>Simplified maintenance: maintenance and upgrades to be performed without disrupting system operation</li> <li>Disaster recovery: geographically distributed copies of data and resources, organizations can recover more quickly from disasters or catastrophic events that may affect a single location</li> </ul> <p>e.g(infra) - Two web servers in different Availability Zones - Load balancer distributes traffic - If one server fails \u2192 traffic shifts automatically</p> <p>Example: - AWS: EC2 instances in multiple AZs behind an ALB - IBM Cloud: VSI instances behind a Load Balancer</p>"},{"location":"sysdesign/components/#replication_1","title":"replication","text":"<p>Database replication is the process of copying and synchronizing data from one database to one or more additional databases. Replication = Duplicate data</p> <p>strategies:</p> <p>Synchronous: database replication where changes made to the primary database are immediately replicated to the replica databases before the write operation is considered complete. strong consistency between the primary and replica databases Asynchronous: the changes made to the primary database are queued and replicated to the replicas at a later time. Semi-synchronous: synchronous + asynchronous replication. changes made to the primary database are immediately replicated to at least one replica database, while other replicas may be updated asynchronously. the write operation on the primary is not considered complete until at least one replica database has confirmed that it has received and processed the changes. This ensures that there is some level of strong consistency between the primary and replica databases, while also providing improved performance compared to fully synchronous replication</p> <p></p> <p>Methods:</p> <ul> <li>Single-leader replication: one node handles all writes while one or more followers asynchronously or synchronously replicate its state.</li> <li>Multi-leader replication: multiple nodes can accept writes; they asynchronously propagate changes to each other, resolving conflicts via timestamps or application logic.</li> <li>Leaderless: No designated leader\u2014clients send reads/writes to any replica set and rely on read/write quorums to ensure consistency.</li> <li>Chain replication: Nodes are arranged in a fixed chain. Writes flow from head \u2192 \u2026 \u2192 tail; reads are served from the tail, so they see all preceding writes.</li> <li>Read-replica replication: A variation of single-leader (primary-backup) replication where the leader handles all writes and one or more replicas serve only read traffic. Replicas continuously pull or receive a stream of write updates from the leader but never accept writes themselves.</li> <li>Snapshot replication: Rather than continuously shipping every change, snapshot replication takes a full copy of the source dataset at a specific point in time and pushes that snapshot to one or more targets on a scheduled basis.</li> </ul> <p>Summary:</p> <ul> <li>Single-leader is the easiest option but is limited by having only one writer.</li> <li>Multi-leader boosts write locality at the cost of conflict resolution.</li> <li>Leaderless (quorum) removes single points but needs careful quorum tuning.</li> <li>Chain gives strong ordering and pipelining, yet ties latency to chain length.</li> <li>Read-replica replication is perfect when you need to scale out reads under a single-writer model, but be mindful of replica lag and write bottlenecks.</li> <li>Snapshot replication is a straightforward way to distribute a point-in-time copy of data on a schedule, best suited for static or slowly changing - datasets where resource cost and latency between snapshots are acceptable.</li> </ul>"},{"location":"sysdesign/components/#redundancy-vs-replication","title":"Redundancy vs Replication","text":"Feature Redundancy Replication Definition Duplication of critical components or systems to increase availability and eliminate single points of failure Copying data from one system to another to ensure data availability and consistency Primary Goal High Availability &amp; Fault Tolerance Data Consistency &amp; Data Availability Scope Infrastructure, hardware, network, power, systems Primarily data (databases, storage, files) Example Multiple servers behind a load balancer Primary database replicating to secondary database Data Synchronization Not always required Required (synchronous or asynchronous) Failure Handling If one component fails, another takes over If primary fails, replica can be promoted Complexity Infrastructure-level complexity Data consistency and conflict-handling complexity Cost Higher (duplicate infrastructure) Moderate to high (depends on replication strategy) Use Case Multi-AZ deployment in cloud Read replicas, disaster recovery database setup Performance Impact May improve load handling May improve read performance (read replicas)"},{"location":"sysdesign/components/#key-char-dfs","title":"Key Char - DFS","text":""},{"location":"sysdesign/components/#scalability_1","title":"Scalability","text":"<p>ability of a system to handle an increasing workload, either by adding more resources (scaling out) or by upgrading the capacity of existing resources.</p> <p>Horizontal scaling, also known as scaling out, involves adding more machines or nodes to a system to distribute the workload evenly. This approach allows the system to handle an increased number of requests without overloading individual nodes.</p> <p>Vertical scaling, or scaling up, refers to increasing the capacity of individual nodes within a system. This can be achieved by upgrading the hardware, such as adding more CPU, memory, or storage. Vertical scaling can help improve the performance of a system by allowing it to handle more workloads on a single node</p>"},{"location":"sysdesign/components/#availability","title":"Availability","text":"<p>Availability is a measure of how accessible and reliable a system is to its users. In distributed systems, high availability is crucial to ensure that the system remains operational even in the face of failures or increased demand</p> <p>Strategies for Achieving High Availability:</p> <ul> <li>High Availability through Redundancy and Replication: </li> </ul> <p>Duplicating critical components or entire systems, we can ensure that if one fails, the redundant system takes over seamlessly, avoiding any interruption in service. Replication involves creating multiple copies of data, ensuring that it is available even if one copy becomes inaccessible.</p> <ul> <li>Availability through Load Balancing: </li> </ul> <p>Load balancing involves distributing workloads across multiple servers, ensuring that no single server is overwhelmed. Through intelligent load-balancing algorithms, organizations can optimize resource utilization, prevent bottlenecks, and enhance high availability by evenly distributing traffic.</p> <p>Load balancing is particularly useful in web applications, where a large number of users access the system simultaneously. By distributing incoming requests across multiple servers, load balancers ensure that no single server becomes overloaded, leading to improved performance and availability.</p> <ul> <li>Availability through Distributed Data Storage: </li> </ul> <p>Storing data across multiple locations or data centers enhances high availability by reducing the risk of data loss or corruption. Distributed data storage systems replicate data across geographically diverse locations, ensuring that even if one site experiences an outage, data remains accessible from other locations.</p> <p>Distributed data storage is crucial for organizations that deal with large volumes of data and cannot afford to lose it. By replicating data across multiple sites, organizations can ensure that data is always available, even in the event of a catastrophic failure at one location.</p> <ul> <li>Availability and Consistency Models (Strong, Weak, Eventual): </li> </ul> <p>Consistency models define how a distributed system maintains a coherent and up-to-date view of its data across all replicas.  Different consistency models provide different trade-offs between availability, performance, and data correctness.</p> <ul> <li> <p>Strong consistency ensures that all replicas have the same data at all times, at the cost of reduced availability and performance. </p> </li> <li> <p>Weak consistency allows for temporary inconsistencies between replicas, with the advantage of improved availability and performance. </p> </li> <li> <p>Eventual consistency guarantees that all replicas will eventually converge to the same data, providing a balance between consistency, availability, and performance.</p> </li> <li> <p>Availability through Health Monitoring and Alerts: </p> </li> </ul> <p>Health monitoring involves continuously monitoring system performance, resource utilization, and various metrics to detect any anomalies or potential issues. Alerts are triggered when predefined thresholds are exceeded, allowing IT teams to take immediate action and prevent service disruptions</p> <ul> <li>Availability through Regular System Maintenance and Updates: </li> </ul> <p>Regular system maintenance and updates are crucial for achieving high availability. By keeping systems up to date with the latest patches, security enhancements, and bug fixes, organizations can mitigate the risk of failures and vulnerabilities that could compromise system availability.</p> <p>System maintenance involves tasks such as hardware inspections, software updates, and routine checks to ensure that all components are functioning correctly. By staying proactive and addressing any potential issues promptly, organizations can maintain high availability and minimize the impact of system failures.</p> <ul> <li>Availability through Geographic Distribution: </li> </ul> <p>Geographic distribution is a strategy that involves deploying system components across multiple locations or data centers. This ensures that even if one region or data center experiences an outage, users can still access the system from other geographically dispersed locations.</p> <p>Geographic distribution is particularly important for organizations with a global presence or those that rely heavily on cloud infrastructure. By strategically placing system components in different geographical areas, organizations can ensure that users from various locations can access the system without any interruptions, regardless of localized incidents or natural disasters.</p>"},{"location":"sysdesign/components/#latency-performance","title":"Latency &amp; performance","text":"<p>ability to handle large amounts of data and traffic</p> <ul> <li> <p>Data Locality: storing related data close together or near the nodes that access it most frequently, you can reduce the latency associated with data retrieval and improve overall performance</p> </li> <li> <p>Load Balancing:  distributing incoming network traffic or computational workload across multiple nodes or resources to ensure that no single node is overwhelmed</p> </li> <li> <p>Caching Strategies:  Caching is a technique used to store frequently accessed data or computed results temporarily, allowing the system to quickly retrieve the data from cache instead of recalculating or fetching it from the primary data source</p> </li> </ul>"},{"location":"sysdesign/components/#concurrancy-and-cordination","title":"Concurrancy and cordination","text":"<p>Concurrency control is the process of managing simultaneous access to shared resources or data in a distributed system. It ensures that multiple processes can work together efficiently while avoiding conflicts or inconsistencies.</p>"},{"location":"sysdesign/components/#concurrency-control","title":"Concurrency Control","text":"<p>Locking: Locks are used to restrict access to shared resources or data, ensuring that only one process can access them at a time.</p> <p>Optimistic concurrency control: This approach assumes that conflicts are rare and allows multiple processes to work simultaneously. Conflicts are detected and resolved later, usually through a validation and rollback mechanism.</p> <p>Transactional memory: This technique uses transactions to group together multiple operations that should be executed atomically, ensuring data consistency and isolation.</p>"},{"location":"sysdesign/components/#synchronization","title":"Synchronization","text":"<p>Synchronization is the process of coordinating the execution of multiple processes or threads in a distributed system to ensure correct operation</p> <p>Barriers: Barriers are used to synchronize the execution of multiple processes or threads, ensuring that they all reach a specific point before proceeding.</p> <p>Semaphores: Semaphores are signaling mechanisms that control access to shared resources and maintain synchronization among multiple processes or threads.</p> <p>Condition variables: Condition variables allow processes or threads to wait for specific conditions to be met before proceeding with their execution.</p> <p>Concurrency Control vs. Synchronization The main objective of concurrency control is to manage access to shared resources (like data or hardware resources) in an environment where multiple processes or threads are executing simultaneously.</p> <p>The purpose of synchronization is to coordinate the timing of multiple concurrent processes or threads. It's about managing the execution order and timing of processes to ensure correct operation.</p>"},{"location":"sysdesign/components/#coordination-services","title":"Coordination Services","text":"<p>Coordination services are specialized components or tools that help manage distributed systems' complexity by providing a set of abstractions and primitives for tasks like configuration management, service discovery, leader election, and distributed locking.</p>"},{"location":"sysdesign/components/#consistency-models","title":"Consistency Models","text":"<ul> <li> <p>Strong Consistency: After a write operation completes, any subsequent read operation will immediately see the new value.</p> </li> <li> <p>Eventual Consistency: Over time, all accesses to a particular data item will eventually return the last updated value. The time it takes to achieve consistency after a write is not guaranteed</p> </li> <li> <p>Causal Consistency:  Operations that are causally related are seen by all processes in the same order. Concurrent operations might be seen in a different order on different nodes.</p> </li> <li> <p>Read-Your-Writes Consistency: Guarantees that once a write operation completes, any subsequent reads (by the same client) will see that write or its effects.</p> </li> <li> <p>Session Consistency: A stronger version of read-your-writes consistency. It extends this guarantee to a session of interactions, ensuring consistency within the context of a single user session</p> </li> <li> <p>Sequential Consistency: Operations from all nodes or processes are seen in the same order. There is a global order of operations, but it doesn't have to be real-time.</p> </li> <li> <p>Monotonic Read Consistency: Ensures that if a read operation reads a value of a data item, any subsequent read operations will never see an older value.</p> </li> <li> <p>Linearizability (Strong Consistency): A stronger version of sequential consistency, it ensures that all operations are atomic and instantly visible to all nodes.</p> </li> </ul>"},{"location":"sysdesign/components/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Metrics Collection:</li> </ul> <p>Metrics are quantitative measurements that provide insights into the performance, health, and behavior of a distributed system. Collecting and analyzing metrics, such as latency, throughput, error rates, and resource utilization, can help identify performance bottlenecks, potential issues, and areas for improvement. (Prometheus, Graphite, or InfluxDB )</p> <ul> <li>Distributed Tracing:</li> </ul> <p>Distributed tracing is a technique for tracking and analyzing requests as they flow through a distributed system, allowing you to understand the end-to-end performance and identify issues in specific components or services (Jaeger, Zipkin, or OpenTelemetry)</p> <ul> <li>Logging: </li> </ul> <p>Logs are records of events or messages generated by components of a distributed system, providing a detailed view of system activity and helping identify issues or anomalies. Collecting, centralizing, and analyzing logs from all services and nodes in a distributed system can provide valuable insights into system behavior and help with debugging and troubleshooting. (Elasticsearch, Logstash, and Kibana (ELK Stack) or Graylog )</p> <ul> <li>Alerting and Anomaly Detection: </li> </ul> <p>Alerting and anomaly detection involve monitoring the distributed system for unusual behavior or performance issues and notifying the appropriate teams when such events occur. By setting up alerts based on predefined thresholds or detecting anomalies using machine learning algorithms, you can proactively identify issues and take corrective actions before they impact users or system performance (Grafana, PagerDuty, or Sensu )</p> <ul> <li>Visualization and Dashboards: - </li> </ul> <p>Visualizing metrics, traces, and logs in an easy-to-understand format can help you better comprehend the state of your distributed system and make data-driven decisions. Dashboards are an effective way to aggregate and display this information, providing a unified view of your system's performance and health. (Grafana, Kibana, or Datadog)</p>"},{"location":"sysdesign/components/#resilence-and-error-handling","title":"Resilence and error handling","text":"<p>Resilience and error handling help minimize the impact of failures and ensure that the system can recover gracefully from unexpected events.</p> <ul> <li> <p>Fault Tolerance: Fault tolerance is the ability of a system to continue functioning correctly in the presence of faults or failures.  </p> </li> <li> <p>Graceful Degradation: Graceful degradation refers to the ability of a system to continue providing limited functionality when certain components or services fail</p> </li> <li> <p>Retry and Backoff Strategies: transient failures like network issues, timeouts, or service unavailability are common. Implementing retry and backoff strategies can help improve resilience by automatically reattempting failed operations with an increasing delay between retries</p> </li> <li> <p>Error Handling and Reporting: </p> </li> </ul> <p>Proper error handling and reporting are crucial for understanding and addressing issues in distributed systems. By consistently logging errors, categorizing them, and generating alerts when necessary, you can quickly identify and diagnose problems in the system.</p> <ul> <li>Chaos Engineering: </li> </ul> <p>Chaos engineering is the practice of intentionally injecting failures into a distributed system to test its resilience and identify weaknesses. </p>"},{"location":"sysdesign/components/#fault-tolerance-vs-ha","title":"fault tolerance vs HA","text":"Aspect Fault Tolerance (FT) High Availability (HA) Definition System continues to operate with no interruption during failures System remains available with minimal downtime Downtime Zero or near-zero Seconds to minutes Failure Impact Users do not notice failures Users may notice a brief disruption Redundancy Model Active\u2013Active (parallel components) Active\u2013Passive or Active\u2013Active Failover Instant and automatic Automatic or manual, slightly delayed Recovery Time Immediate Short recovery time Complexity Very high Medium Cost Very high Moderate Scalability Harder to scale Easier to scale Common Technologies Hardware redundancy, lockstep systems Load balancers, replicas, health checks Typical Use Cases Flight control systems, stock trading, nuclear systems Web apps, APIs, databases, cloud platforms Cloud Example Dual-write synchronous systems across zones Multi-AZ load-balanced services Resume-Friendly Line Designed zero-downtime systems tolerant to component failure Designed highly available systems with rapid failover"},{"location":"sysdesign/components/#security","title":"Security","text":""},{"location":"sysdesign/components/#authentication","title":"Authentication","text":"<p>Authentication is the process of verifying the identity of users, services, or nodes before allowing them access to the system.</p> <p>types of auth</p> <p>Single-Factor Authentication (SFA): This is like showing just one ID card. It usually involves something you know, like a password or PIN.</p> <p>Two-Factor Authentication (2FA): This is like showing two forms of ID. For example, entering a password (something you know) and then entering a code sent to your phone </p> <p>Multi-Factor Authentication (MFA): This is like a high-security check where you need multiple proofs. It could be a combination of a password, a fingerprint, and a security token. (Passwords and PINs, Biometrics, Tokens and Cards, Behavioral Biometrics)</p> <p>OAuth vs JWT for Authentication:</p> Aspect OAuth 2.0 JWT (JSON Web Token) Type Authorization framework (often used for authentication) Token format used for authentication and authorization Purpose Delegates access to resources without sharing credentials Securely represents user identity and claims Primary Use Third-party access (e.g., login with Google/GitHub) Stateless authentication between client and server Who Defines Access Authorization Server Application issuing the token Token Format Can use opaque tokens or JWTs Always a JWT (JSON-based, signed) Authentication Role Indirect (OAuth enables authentication via flows like OIDC) Direct (JWT carries user identity info) State Management Can be stateful or stateless Fully stateless Scalability High (centralized auth server) Very high (no server-side session storage) Security Mechanism Access tokens, refresh tokens, scopes Signed (and optionally encrypted) tokens Token Revocation Easier (server can revoke tokens) Harder (tokens are valid until expiry) Common Flows Authorization Code, Client Credentials, Device Flow No flows; just token generation &amp; validation Typical Lifetime Short-lived access tokens Short-lived (recommended) Common Use Cases Social login, API access delegation Microservices, APIs, mobile &amp; SPA apps Example Login with Google to access an app API validates JWT in Authorization header Interview One-Liner OAuth answers \u201cCan this app access that resource?\u201d JWT answers \u201cWho is the user and what can they do?\u201d"},{"location":"sysdesign/components/#authorization","title":"Authorization","text":"<p>Authorization, determines what actions or resources the authenticated entity is allowed to access. Authorization is about having the right level of access to resources in a software system File System Permissions, Database Roles, Web Application Privileges.</p> <p>authentication Vs authorization</p> Aspect Authentication Authorization Definition The process of verifying who a user is The process of verifying what access a user has Focus Identity verification Access rights and privileges Example Entering a username and password Checking if a user can access a specific resource (file, API, database) How It Works Uses passwords, biometrics, OTPs, certificates, etc. Uses roles, policies, and permission rules Tools / Methods Login forms, OTP apps, biometric scanners Access Control Lists (ACL), Role-Based Access Control (RBAC) Order in Process Happens first Happens after successful authentication Key Concern Ensuring the user\u2019s identity is genuine Ensuring correct permission and access level Frequency Usually once per session Happens multiple times per resource request Dependence Can exist without authorization in some systems Requires authentication as a prerequisite"},{"location":"sysdesign/components/#data-encryption","title":"Data Encryption","text":"<p>Data encryption is the process of converting data into an encoded format that can only be decrypted and read by someone with the correct decryption key Techniques like symmetric and asymmetric encryption, as well as protocols such as TLS/SSL</p> <p>Symmetric Encryption vs Asymmetric Encryption</p> Aspect Symmetric Encryption Asymmetric Encryption Definition Uses the same key for encryption and decryption Uses a pair of keys (public &amp; private) Keys Used Single shared secret key Public key + Private key Speed Very fast Slower compared to symmetric Security Model Key must be shared securely Public key can be shared openly Key Distribution Difficult and risky Easy and secure Scalability Poor for large systems Better for large/distributed systems Encryption Strength Strong but depends on key secrecy Strong with mathematical complexity Typical Use Bulk data encryption Key exchange, authentication Common Algorithms AES, DES, 3DES, Blowfish RSA, ECC, DSA Data Size Handling Suitable for large data Not efficient for large data Computational Cost Low High Use in HTTPS Encrypts actual data Exchanges symmetric session key Example Disk encryption, database encryption SSL/TLS handshake, SSH Failure Impact Key compromise breaks security Private key compromise breaks security Interview One-Liner Fast but key sharing is the challenge Secure key exchange but computationally expensive <p>TLS Handshake</p> <p></p> <p>TLS uses asymmetric encryption only to exchange keys, then switches to symmetric encryption for data transfer.</p>"},{"location":"sysdesign/components/#intrusion-detection-and-prevention","title":"Intrusion Detection and Prevention","text":"<p>Intrusion detection and prevention systems (IDPS) are designed to monitor network traffic, detect malicious activities or policy violations, and take appropriate actions to mitigate potential threats. </p> <p>DDoS</p> <p>disrupt normal traffic of a targeted server, service, or network by overwhelming the target or its surrounding infrastructure with a flood of Internet traffic. effectiveness by utilizing multiple compromised computer systems as sources of attack traffic.</p> <p>How DDoS Attacks Work</p> <p>How It Happens: Hackers use a network of compromised computers and devices (botnets) to send a flood of internet traffic to a target, like a website or server.</p> <p>Goal: To overload the server's capacity to handle requests, causing slow service or complete shutdown.</p> <p>types:</p> <p>Volumetric Attacks: The most common form, these attacks flood the network with a substantial amount of traffic.</p> <p>Protocol Attacks: These target network layer or transport layer protocols to consume server resources or bandwidth.</p> <p>Application Layer Attacks: These are more sophisticated, targeting specific aspects of an application or server.</p> <p>Mitigations</p> <p>Network Redundancy: multiple pathways for network traffic</p> <p>DDoS Protection Services: large-scale network infrastructure capable of absorbing and diffusing attack traffic</p> <p>Firewalls and Anti-DDoS Software: advanced firewall systems and specific anti-DDoS software can help identify and block attack traffic</p> <p>Traffic Analysis: Continuously monitoring network traffic can help in identifying anomalies indicative of a DDoS attack.</p> <p>Responsive Plan: Having a response plan in place, including procedures for identifying, mitigating, and recovering from an attack, is crucial for minimizing damage</p> <p>Good Security Hygiene: Regularly updating security protocols and educating users about the risks of malware</p> <p>Scalable Infrastructure: Utilizing cloud services with the ability to scale rapidly can absorb and disperse high traffic loads during an attack</p>"},{"location":"sysdesign/components/#caching","title":"Caching","text":"<p>The cache is a high-speed storage layer that sits between the application and the original source of the data, such as a database, a file system, or a remote web service. When data is requested by the application, it is first checked in the cache. If the data is found in the cache, it is returned to the application. If the data is not found in the cache, it is retrieved from its original source, stored in the cache for future use, and returned to the application.</p> <p>Key terminology and concepts</p> <ol> <li> <p>Cache: A temporary storage location for data or computation results, typically designed for fast access and retrieval.</p> </li> <li> <p>Cache hit: When a requested data item or computation result is found in the cache.</p> </li> <li> <p>Cache miss: When a requested data item or computation result is not found in the cache and needs to be fetched from the original data source or recalculated.</p> </li> <li> <p>Cache eviction: The process of removing data from the cache, typically to make room for new data or based on a predefined cache eviction policy.</p> </li> <li> <p>Cache staleness: When the data in the cache is outdated compared to the original data source.</p> </li> </ol> <p>Advantages:</p> <ul> <li> <p>Reduced latency:  By serving data from the cache, which is typically faster to access than the original data source</p> </li> <li> <p>Improved system performance:  reducing the number of times data needs to be fetched from its original source, this results in a significant reduction in processing time, which leads to a more responsive application</p> </li> <li> <p>Reduced network load:  reduce network load by minimizing the amount of data that needs to be transmitted over the network</p> </li> <li> <p>Increased scalability: improve the scalability of an application by reducing the load on the original source.</p> </li> <li> <p>Better user experience: Faster response times and reduced latency can lead to a better user experience.</p> </li> </ul>"},{"location":"sysdesign/components/#types-of-caching","title":"Types of Caching","text":"<ul> <li> <p>In-memory caching: - In-memory caching is useful for frequently accessed data that can fit into the available memory. commonly used for caching API responses, session data, and web page fragments.(Memcached or Redis)</p> </li> <li> <p>Disk caching: - Disk caching is useful for data that is too large to fit in memory or for data that needs to persist between application restarts. commonly used for caching database queries and file system data.</p> </li> <li> <p>Database caching: - This type of caching is useful for data that is stored in a database and frequently accessed by multiple users</p> </li> <li> <p>Client-side caching: - This type of caching occurs on the client device, such as a web browser or mobile app. Client-side caching stores frequently accessed data, such as images, CSS, or JavaScript files, to reduce the need for repeated requests to the server. </p> </li> <li> <p>Server-side caching: - Server-side caching can be used to store frequently accessed data, precomputed results, or intermediate processing results to improve the performance of the server. Examples of server-side caching include full-page caching, fragment caching, and object caching</p> </li> <li> <p>CDN caching: - CDN caching stores data on a distributed network of servers, reducing the latency of accessing data from remote locations.</p> </li> <li> <p>DNS caching: - DNS caching improves the performance of the DNS system by reducing the number of requests made to DNS servers.</p> </li> </ul> <p></p>"},{"location":"sysdesign/components/#cache-replacement-policies","title":"Cache Replacement Policies","text":"<p>cache replacement policy to determine which items in the cache should be removed when the cache becomes full</p> <ul> <li> <p>Least Recently Used (LRU): - LRU is a cache replacement policy that removes the least recently used item from the cache when it becomes full</p> </li> <li> <p>Least Frequently Used (LFU): - removes the least frequently used item from the cache when it becomes full</p> </li> <li> <p>First In, First Out (FIFO): - removes the oldest item from the cache when it becomes full.</p> </li> <li> <p>Random Replacement: - removes a random item from the cache when it becomes full</p> </li> </ul>"},{"location":"sysdesign/components/#cache-invalidation","title":"Cache Invalidation","text":"<p>While caching can significantly improve performance, we must ensure that the data in the cache is still correct\u2014otherwise, we serve out-of-date (stale) information (you risk presenting users with wrong information or invalid results). This is where cache invalidation comes in.</p> <p>When the underlying data changes\u2014say a product\u2019s price updates in your database\u2014you must mark or remove the old (cached) data so users don\u2019t see stale information. This process is called \u201ccache invalidation.\u201d</p> <p>Large systems often have multiple caching layers. If any of these layers serve old data while others serve new data, users can encounter conflicting information.</p> <p>Cache invalidation strategies (e.g., time-to-live/TTL, manual triggers, event-based invalidation) are designed to minimize the performance cost of continuously \u201crefreshing\u201d the cache.</p> <ul> <li>Write-through cache</li> </ul> <p>Under this scheme, data is written into the cache and the corresponding database simultaneously. The cached data allows for fast retrieval and, since the same data gets written in the permanent storage, we will have complete data consistency between the cache and the storage. Since every write operation must be done twice before returning success to the client, this scheme has the disadvantage of higher latency for write operations.</p> <ul> <li>Write-around cache</li> </ul> <p>This technique is similar to write-through cache, but data is written directly to permanent storage, bypassing the cache. This can reduce the cache being flooded with write operations that will not subsequently be re-read, but has the disadvantage that a read request for recently written data will create a \u201ccache miss\u201d and must be read from slower back-end storage and experience higher latency.</p> <ul> <li>Write-back cache</li> </ul> <p>Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client. The write to the permanent storage is done based on certain conditions, for example, when the system needs some free space. This results in low-latency and high-throughput for write-intensive applications; however, this speed comes with the risk of data loss in case of a crash or other adverse event because the only copy of the written data is in the cache.</p> <ul> <li>Write-behind cache</li> </ul> <p>Under this scheme, data is written to cache alone, and completion is immediately confirmed to the client. but it is not immediately written to the permanent storage.</p> <p>In write-back caching, data is only written to the permanent storage when it is necessary for the cache to free up space, while in write-behind caching, data is written to the permanent storage at specified intervals.</p>"},{"location":"sysdesign/components/#cache-invalidations-methods","title":"Cache Invalidations Methods","text":"<p>Purge: removes cached content for a specific object, URL, or a set of URLs.(hard delete)</p> <p>Refresh: When a refresh request is received, the cached content is updated with the latest version from the origin server, ensuring that the content is up-to-date.</p> <p>Ban: The ban method invalidates cached content based on specific criteria, such as a URL pattern or header</p> <p>(TTL) expiration: This method involves setting a time-to-live value for cached content, after which the content is considered stale and must be refreshed</p>"},{"location":"sysdesign/components/#cache-read-strategies","title":"Cache Read Strategies","text":""},{"location":"sysdesign/components/#read-through-cache","title":"Read through cache","text":"<p>A read-through cache strategy is a caching mechanism where the cache itself is responsible for retrieving the data from the underlying data store when a cache miss occurs. This approach helps to maintain consistency between the cache and the data store, as the cache is always responsible for retrieving and updating the data. It also simplifies the application code since the application doesn't need to handle cache misses and data retrieval logic.</p>"},{"location":"sysdesign/components/#read-aside-cache","title":"Read aside cache","text":"<p>application is responsible for retrieving the data from the underlying data store when a cache miss occurs</p> <p></p>"},{"location":"sysdesign/components/#cdn","title":"CDN","text":"<p>A Content Delivery Network (CDN) is a distributed network of servers strategically located across various geographical locations to deliver web content, such as images, videos, and other static assets, more efficiently to users. The primary purpose of a CDN is to reduce latency and improve the overall performance of web applications by serving content from the server nearest to the user. CDNs can also help improve reliability, availability, and security of web applications.</p> <p>Key terminology and concepts</p> <ol> <li> <p>Point of Presence (PoP): PoPs are strategically placed close to end-users to minimize latency and improve content delivery performance.</p> </li> <li> <p>Edge Server: An edge server is a CDN server located at a PoP, responsible for caching and delivering content to end-users. These servers store cached copies of the content, reducing the need to fetch data from the origin server.</p> </li> <li> <p>Origin Server: The origin server is the primary server where the original content is stored(website's content, including HTML files, images, stylesheets, JavaScript, videos, and other digital assets). CDNs fetch content from the origin server and cache it on edge servers for faster delivery to end-users.</p> </li> <li> <p>Cache Warming: Cache warming is the process of preloading content into the edge server's cache before it is requested by users, ensuring that the content is available for fast delivery when it is needed.</p> </li> <li> <p>Time to Live (TTL) : TTL is a value that determines how long a piece of content should be stored in the cache before it is considered stale and needs to be refreshed from the origin server.</p> </li> <li> <p>Anycast: Anycast is a network routing technique used by CDNs to direct user requests to the nearest available edge server, based on the lowest latency or the shortest network path.</p> </li> <li> <p>Content Invalidation: Content invalidation is the process of removing or updating cached content when the original content on the origin server changes, ensuring that end-users receive the most up-to-date version of the content.</p> </li> <li> <p>Cache Purging: Cache purging is the process of forcibly removing content from the edge server's cache, usually triggered manually or automatically when specific conditions are met.</p> </li> </ol> <p>Benefits of using a CDN</p> <p>CDNs play a crucial role in enhancing the performance, reliability, and security of modern web applications. By serving content from geographically distributed edge servers, CDNs can provide users with a fast and seamless experience, while reducing load on origin servers and protecting against security threats. Here are the top benefits of using CDNs:</p> <ol> <li> <p>Reduced latency: By serving content from geographically distributed edge servers, CDNs reduce the time it takes for content to travel from the server to the user, resulting in faster page load times and improved user experience.</p> </li> <li> <p>Improved performance: CDNs can offload static content delivery from the origin server, freeing up resources for dynamic content generation and reducing server load. This can lead to improved overall performance for web applications.</p> </li> <li> <p>Enhanced reliability and availability: With multiple edge servers in different locations, CDNs can provide built-in redundancy and fault tolerance. If one server becomes unavailable, requests can be automatically rerouted to another server, ensuring continuous content delivery.</p> </li> <li> <p>Scalability: CDNs can handle sudden traffic spikes and large volumes of concurrent requests, making it easier to scale web applications to handle growing traffic demands.</p> </li> <li> <p>Security: Many CDNs offer additional security features, such as DDoS protection, Web Application Firewalls (WAF), and SSL/TLS termination at the edge, helping to safeguard web applications from various security threats.</p> </li> </ol>"},{"location":"sysdesign/components/#cdn-routing-and-request-handling","title":"CDN Routing and Request Handling","text":"<p>CDN routing is the process of directing user requests to the most suitable edge server. Routing decisions are typically based on factors such as network latency, server load, and the user's geographical location</p> <p>Anycast Routing: In anycast routing, multiple edge servers share a single IP address. When a user sends a request to that IP address, the network's routing system directs the request to the nearest edge server based on network latency or the number of hops.</p> <p>DNS-based Routing: With DNS-based routing, when a user requests content, the CDN's DNS server responds with the IP address of the most suitable edge server. This approach can take into account factors such as geographical proximity and server load to select the best edge server for handling the request.</p> <p>GeoIP-based Routing: In this approach, the user's geographical location is determined based on their IP address. The request is then directed to the nearest edge server in terms of geographical distance, which often corresponds to lower network latency.</p>"},{"location":"sysdesign/components/#caching-mechanisms","title":"Caching Mechanisms","text":"<p>Edge servers cache content to reduce latency and offload traffic from the origin server. Various caching mechanisms can be employed to determine what content is stored, when it is updated, and when it should be removed from the cache</p> <p>Time-to-Live (TTL): TTL is a value set by the origin server that determines how long a piece of content should be stored in the cache before it is considered stale and needs to be fetched again from the origin server.</p> <p>Cache Invalidation: Cache invalidation is the process of removing content from the cache before its TTL expires. This is typically done when content is updated or deleted on the origin server and needs to be reflected immediately in the CDN.</p> <p>Cache Control Headers: Cache control headers are used by the origin server to provide instructions to the CDN regarding caching behavior. These headers can dictate the cacheability of content, its TTL, and other caching-related settings.</p>"},{"location":"sysdesign/components/#cdn-network-topologies","title":"CDN Network Topologies","text":"<p>Flat Topology: In a flat topology, all edge servers in the CDN are directly connected to the origin server. </p> <p>Hierarchical Topology: In a hierarchical topology, edge servers are organized into multiple tiers, with each tier being responsible for serving content to the tier below it. This approach can improve scalability by distributing the load among multiple levels of servers and reducing the number of direct connections to the origin server.</p> <p>Mesh Topology: In a mesh topology, edge servers are interconnected, allowing them to share content and load with each other. This approach can enhance the redundancy and fault tolerance of the CDN, as well as improve content delivery performance by reducing the need to fetch content from the origin server.</p> <p>Hybrid Topology: A hybrid topology combines elements from various topologies to create an optimized CDN architecture tailored to specific needs. For example, a CDN could use a hierarchical structure for serving static content, while employing a mesh topology for dynamic content delivery.</p>"},{"location":"sysdesign/components/#push-cdn-vs-pull-cdn","title":"Push CDN vs Pull CDN","text":"Aspect Push CDN Pull CDN Definition Content is manually uploaded (\"pushed\") to the CDN server in advance. CDN automatically fetches (\"pulls\") content from the origin server when requested. How It Works You upload files to the CDN storage. CDN serves files directly from its edge locations. User requests content \u2192 CDN checks cache \u2192 If not available, CDN pulls from origin \u2192 Caches and serves it. Content Update Requires manual re-upload when content changes. Automatically updates when cache expires or is purged. Best For Static content (images, videos, downloads, software packages). Dynamic or frequently changing content (websites, APIs, blogs). Origin Server Load Very low, since CDN already has the content. Higher initially (on first request or cache miss). Setup Complexity Slightly more setup (need upload process or pipeline). Easier setup (just configure origin server). Storage Requirement Requires CDN storage space. No extra storage management required. Control Over Content Full control over what is uploaded. CDN decides what to cache based on requests. Cost Consideration Can be cheaper for large static assets with predictable traffic. May incur more origin bandwidth cost on cache misses. Example Use Case Hosting software downloads, media streaming files, large static assets. Hosting a website where content updates frequently."},{"location":"sysdesign/components/#advantages-and-disadvantages-of-push-cdn-vs-pull-cdn","title":"Advantages and Disadvantages of Push CDN vs Pull CDN","text":"CDN Type Advantages Disadvantages Push CDN \u2022 Reduced load on origin server\u2022 Faster delivery for large static files\u2022 Full control over uploaded content\u2022 Predictable performance\u2022 Good for high-traffic static assets \u2022 Manual upload required\u2022 Content updates need re-uploading\u2022 Extra storage management\u2022 Slightly more operational overhead\u2022 Not ideal for frequently changing content Pull CDN \u2022 Easy setup (just configure origin)\u2022 Automatically fetches updated content\u2022 No manual upload process\u2022 Good for dynamic or frequently updated websites\u2022 Lower operational effort \u2022 Initial request may cause latency (cache miss)\u2022 Higher load on origin during cache misses\u2022 Less control over what gets cached\u2022 Possible stale content if cache settings are incorrect\u2022 Bandwidth costs may increase with frequent misses <ul> <li>Push CDN \u2192 You upload content to CDN</li> <li>Pull CDN \u2192 CDN fetches content from your server when needed</li> </ul>"},{"location":"sysdesign/components/#quorum","title":"Quorum","text":"<p>In a distributed environment, a quorum is the minimum number of servers on which a distributed operation needs to be performed successfully before declaring the operation's overall success. quorum refers to the minimum number of machines that perform the same action (commit or abort) for a given transaction in order to decide the final operation for that transaction.</p> <p>Choosing quorum - more than half of the number of nodes(n/2+1) in the cluster:  where  is the total number of nodes in the cluster.</p> <p>every read will see at least one copy of the latest value written</p> <p>(N=3, W=1, R=3): fast write, slow read, not very durable (N=3, W=3, R=1): slow write, fast read, durable</p>"},{"location":"sysdesign/components/#data-partitioning","title":"Data partitioning","text":"<p>Data partitioning is a technique used in distributed systems and databases to divide a large dataset into smaller, more manageable parts, referred to as partitions(partitioned based on a certain criterion, such as data range, data size, or data type). Each partition is independent and contains a subset of the overall data. Each partition is then assigned to a separate processing node which can perform operations on its assigned data subset independently of the others(it allows processing to be distributed across multiple nodes, minimizing data transfer and reducing processing time).</p> <p>Partition: A partition is a smaller, more manageable part of a larger dataset, created as a result of data partitioning.</p> <p>Partition key: The partition key is a data attribute used to determine how data is distributed across partitions. An effective partition key should provide an even distribution of data and support efficient query patterns.</p> <p>Shard: A shard is a term often used interchangeably with a partition, particularly in the context of horizontal partitioning (will be discussed later).</p>"},{"location":"sysdesign/components/#horizontal-partitioning","title":"Horizontal Partitioning","text":"<p>aka sharding, dividing a database table into multiple partitions or shards, with each partition containing a subset of rows. Each shard is typically assigned to a different database server, which allows for parallel processing and faster query execution times. </p>"},{"location":"sysdesign/components/#vertical-partitioning","title":"Vertical Partitioning","text":"<p>Vertical data partitioning involves splitting a database table into multiple partitions or shards, with each partition containing a subset of columns. This technique can help optimize performance by reducing the amount of data that needs to be scanned, especially when certain columns are accessed more frequently than others.</p>"},{"location":"sysdesign/components/#hybrid-partitioning","title":"Hybrid Partitioning","text":"<p>This technique can help optimize performance by distributing the data evenly across multiple servers, while also minimizing the amount of data that needs to be scanned.</p>"},{"location":"sysdesign/components/#sharding-techniques","title":"Sharding techniques","text":"<ul> <li> <p>Range-based Sharding: - data is divided into shards based on a specific range of values for a given partitioning key. Each shard is responsible for a specific range, ensuring that the data is distributed in a predictable manner.</p> </li> <li> <p>Hash-based Sharding: - Hash-based sharding involves applying a consistent hash function to the partitioning key, which generates a hash value that determines the destination shard for each data entry. This method ensures an even distribution of data across shards and is particularly useful when the partitioning key has a large number of distinct values or is not easily divided into ranges.</p> </li> </ul> <p></p> <ul> <li>Directory-based Sharding: - Directory-based sharding uses a lookup table, often referred to as a directory, to map each data entry to a specific shard. This method offers greater flexibility, as shards can be added, removed, or reorganized without the need to rehash or reorganize the entire dataset. However, it introduces an additional layer of complexity, as the directory must be maintained and kept consistent.</li> </ul> <p></p> <ul> <li> <p>Geographical Sharding: - Geographical sharding involves partitioning data based on geographical locations, such as countries or regions. This method can help reduce latency and improve performance for users in specific locations by storing their data closer to them.</p> </li> <li> <p>Dynamic Sharding: - Dynamic sharding is an adaptive approach that automatically adjusts the number of shards based on the data\u2019s size and access patterns. This method can help optimize resource utilization and performance by creating shards as needed and merging or splitting them as the data grows or shrinks.</p> </li> <li> <p>Hybrid Sharding: The Best of Many Worlds: -  It might combine Geo-based with Directory-based sharding, or any other mix that suits a system's needs.</p> </li> </ul> <p></p>"},{"location":"sysdesign/components/#benefits-of-data-partitioning","title":"Benefits of Data Partitioning","text":"<p>Data partitioning offers a wide range of benefits that can significantly improve the performance, scalability, and resilience of data-driven systems. </p> <ul> <li> <p>Improved Query Performance: - When data is partitioned, queries can be targeted at specific partitions, enabling the system to retrieve only the necessary data</p> </li> <li> <p>Enhanced Scalability: - Partitioning data across multiple storage resources allows for greater system scalability.</p> </li> <li> <p>Load Balancing: - Data partitioning helps distribute the workload evenly across multiple storage nodes or servers</p> </li> <li> <p>Data Isolation: - Partitioning data can provide a level of data isolation, where the failure or corruption of one partition does not necessarily impact the other partitions. </p> </li> <li> <p>Parallel Processing: - Data partitioning enables parallel processing, where multiple partitions can be processed simultaneously by different processors or systems.</p> </li> <li> <p>Storage Efficiency: - Frequently accessed data can be stored on faster, more expensive storage resources, while less critical data can be stored on cheaper, slower storage resources.</p> </li> <li> <p>Simplified Data Management: - Data partitioning can make data management tasks, such as backup, archiving, and maintenance, more manageable and efficient</p> </li> <li> <p>Better Resource Utilization: - By aligning the data with the appropriate storage and processing resources, organizations can maximize the performance and efficiency of their data-driven systems.</p> </li> <li> <p>Improved Data Security: - By isolating sensitive data in separate partitions, organizations can implement stronger security measures for those partitions, minimizing the risk of unauthorized access or data breaches</p> </li> <li> <p>Faster Data Recovery: - By focusing on recovering specific partitions rather than the entire dataset, organizations can reduce downtime and restore critical data more quickly.</p> </li> </ul>"},{"location":"sysdesign/components/#common-problems-associated-with-data-partitioning","title":"Common Problems Associated with Data Partitioning","text":"<ul> <li> <p>Complexity: - added complexity can lead to increased development and maintenance efforts, as well as a steeper learning curve for team members.</p> </li> <li> <p>Data Skew: - data partitioning can result in uneven data distribution across partitions, known as data skew Data skew can result in reduced performance and resource utilization, negating the benefits of partitioning</p> </li> <li> <p>Partitioning Key Selection: - Choosing the appropriate partitioning key is crucial for achieving the desired benefits of data partitioning. An unsuitable partitioning key can lead to inefficient data distribution, performance bottlenecks, and increased management complexity.</p> </li> <li> <p>Cross-Partition Queries: - When queries need to access data across multiple partitions, performance can suffer, as the system must search through and aggregate data from several partitions. </p> </li> <li> <p>Data Migration: - Partitioning can sometimes require significant data migration efforts, especially when changing partitioning schemes or adding new partitions. This can be time-consuming and resource-intensive, potentially causing disruptions to normal system operation.</p> </li> <li> <p>Partition Maintenance: - As the data grows and evolves, organizations may need to reevaluate their partitioning strategies, which can involve repartitioning, merging, or splitting partitions. This can result in additional maintenance overhead and increased complexity</p> </li> <li> <p>Cost: - Implementing a data partitioning strategy may require additional hardware, software, or infrastructure, leading to increased costs. Furthermore, the added complexity of managing a partitioned system may result in higher operational expenses.</p> </li> </ul>"},{"location":"sysdesign/components/#databases","title":"Databases","text":""},{"location":"sysdesign/components/#index","title":"index","text":"<p>Database indexes are designed to improve the speed and efficiency of data retrieval operations</p> <p>Primary Index - The primary key is a column that uniquely identifies each row. the primary index improves read performance for identifying specific rows, with very little downside since every table usually needs a primary key.</p> <pre><code>SELECT * FROM Employees WHERE EmployeeID = 123 are very fast\n</code></pre> <p>Unique Index - A Unique Index ensures that all values in the indexed column are distinct. no two rows can have the same key value. A unique index functions like a regular index for lookup performance, but with the added rule that duplicate values are not allowed. Unique indexes speed up read queries just as non-unique indexes do, and in addition they guarantee data integrity by preventing duplicates.</p> <pre><code>SELECT * FROM Users WHERE Email = 'alice@example.com' are fast\n</code></pre> <p>Clustered Index</p> <p>Non-Clustered Index</p> <p>Composite Index</p> <p>Full-Text Index</p> <p>Hash Index</p>"},{"location":"sysdesign/framework/","title":"Architecting Framework","text":""},{"location":"sysdesign/framework/#q-a","title":"Q &amp; A","text":"<p>Step 1: Clarify Ambiguity. Never assume. Narrow down the exact scope.</p> <p>Step 2: Define Functional Requirements. Core features the system must have.</p> <p>Step 3: Define Non-Functional Requirements. Scale, latency, and availability baselines.</p> <p>Step 4: Capacity Estimation. Calculate Read/Write ratios, QPS, and storage needs.</p> <p>Step 5: Define System APIs. Establish REST/gRPC endpoints and payloads.</p> <p>Numbers Everyone Should Know (Latency)</p> <pre><code>Interviewers expect you to know these back-of-the-envelope numbers to justify your caching and database decisions:\n\nL1 cache reference: 0.5 ns\nMain memory read: 100 ns\nRead 1MB sequentially from memory: 250 us\nRead 1MB sequentially from network: 10 ms\nRead 1MB sequentially from SSD: 1 ms\nPacket roundtrip CA to Netherlands: 150 ms\n</code></pre> <p>Step 6: Data Model &amp; Schema. Map out core entities and relationships.</p> <p>Step 7: Database Selection. Justify SQL vs. NoSQL choices.</p> <p>Step 8: High-Level Diagram. Draw components from client to databases.</p> <p>Step 9: Component Deep Dive. Zoom in on the hardest logical challenge.</p> <p>Step 10: Partitioning &amp; Sharding. Split data to handle massive scale.</p> <p>Step 11: Caching Strategy. Implement Redis/Memcached to minimize latency.</p> <p>Step 12: Load Balancing. Eliminate single points of failure.</p> <p>Step 13: Asynchronous Processing. Use queues for heavy background tasks.</p> <p>Step 14: Trade-offs Analysis. Proactively point out the weaknesses in your own design.</p>"},{"location":"sysdesign/framework/#functional-requirements","title":"Functional Requirements","text":"<p>what a system should do. It should describe various system functions that must perform e.g - auth system: validate user and provide access - e-commerce website: allow user to browse products, add them to cart and purchase.  - report generation: collect data, process and generate</p>"},{"location":"sysdesign/framework/#non-functional-requirements","title":"Non-Functional Requirements","text":"<p>How the system should do it.</p> <ul> <li>Scalability: The system should handle growth in users or data.</li> <li>Performance: The system should process transactions within a specified time.</li> <li>Availability: The system should be up and running a defined percentage of time.</li> <li>Security: The system must protect sensitive data and resist unauthorized access.</li> </ul>"},{"location":"sysdesign/framework/#back-of-the-envelope-estimations","title":"Back-of-the-Envelope Estimations","text":"<p>Its a technique used to quickly approximate values and make rough calculations using simple arithmetic and basic assumptions. estimations is essential for several reasons</p> <ul> <li>Scalability: Highlights your understanding of how the system can grow or adapt</li> <li>proposed solutions: Estimation helps you ensure that your proposed architecture meets the requirements and can handle the expected load.</li> <li>Identify bottlenecks: Quick calculations help you identify potential performance bottlenecks and make necessary adjustments to your design.</li> <li>Demonstrate your thought process: Estimation showcases your ability to make informed decisions and trade-offs based on a set of assumptions and constraints</li> <li>Communicate effectively: Providing estimates helps you effectively communicate your design choices and their implications to the interviewer.</li> <li>Quick Decision Making: Reflects your ability to make swift estimations to guide your design decisions.</li> </ul>"},{"location":"sysdesign/framework/#estimation-techniques","title":"Estimation Techniques","text":"<ul> <li>Rule of thumb: based on experience and observation e.g user will generate 1 MB of data per day on a social media platform can serve as a starting point for capacity planning.</li> <li>Approximation: complex calculations by rounding numbers or using easier-to-compute values e.g For instance, assuming 1,000 users instead of 1,024 when estimating storage requirements</li> <li>Breakdown and aggregation: Breaking down a problem into smaller components and estimating each separately can make it easier to derive an overall estimate e.g For example, estimating the storage needs for user data, multimedia content, and metadata separately can help in determining the overall storage requirements</li> <li>Sanity check: estimate to ensure its plausibility and reasonableness.</li> </ul> <p>Types of estimations </p> <ul> <li>Load estimation: Predict the expected number of requests per second, data volume, or user traffic for the system.</li> <li>Storage estimation: Estimate the amount of storage required to handle the data generated by the system.</li> <li>Bandwidth estimation: Determine the network bandwidth needed to support the expected traffic and data transfer.</li> <li>Latency estimation: Predict the response time and latency of the system based on its architecture and components.</li> <li>Resource estimation: Estimate the number of servers, CPUs, or memory required to handle the load and maintain desired performance levels.</li> </ul>"},{"location":"sysdesign/app/airbnb/","title":"Airbnb","text":""},{"location":"sysdesign/app/airbnb/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Hotel </li> <li>Onboarding</li> <li>Updates</li> <li>bookings</li> <li>User</li> <li>search</li> <li>book</li> <li>check bookings</li> <li>Analytics</li> </ul>"},{"location":"sysdesign/app/airbnb/#non-functional-requirements","title":"Non functional requirements","text":"<ul> <li>Low latency</li> <li>HA</li> <li>high consistency</li> <li>scale </li> <li>500k hotels</li> <li>10M rooms</li> <li>10000 Rooms/hotel ( max 7500)</li> </ul>"},{"location":"sysdesign/app/amazon/","title":"Amazon","text":""},{"location":"sysdesign/app/amazon/#amazon-design-goes-here","title":"Amazon design goes here","text":""},{"location":"sysdesign/app/autonomouscars/","title":"Autonomous cars","text":"<p>In this case study, we are working on autonomous cars that are manufacturing vehicles, at present there are already 20K cars running, by end of year it would be arounf 200K. We are required to work on the telementry of the cars relability and display details about it.  i.e breakdown, malfunctions, accidents, and use the same data for better needs.</p>"},{"location":"sysdesign/app/autonomouscars/#requirements","title":"Requirements","text":"<p>what should system do ?</p> <ul> <li>Web based</li> <li>CURD Operations</li> <li>Manager alerts</li> <li>breakdowns</li> <li>malfunctions</li> <li>provide information to the admin team</li> <li>alert to near hospitals or mechanic shops</li> <li>road assistance</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#functional","title":"Functional","text":"<p>What system should do ?</p> <ol> <li>Web based </li> <li>Receive telementry from cars(localtion, speed, breakdown)</li> <li>Store telementry in persistent store</li> <li>Display dashboard summarizing the data</li> <li>Perform analysis on the data.</li> </ol>"},{"location":"sysdesign/app/autonomouscars/#non-functional","title":"Non Functional","text":"<p>What the systen should deal with ?</p> <p>So when we are designing the non-functional, we must first start witj what we already know of..</p> <ol> <li>Data intensive system </li> <li>Not a lot of users</li> <li>A lot of data</li> <li>Performance is important</li> </ol> <p>Questions to ask customers ?</p> <ol> <li> <p>How many expected concurrent users ? Sol: 10</p> </li> <li> <p>How many telementry msg received per second (load avg) Sol: 7000/sec</p> </li> <li> <p>What is the avg size of each msg sol: 1KB ( quite small)</p> </li> <li> <p>Is the msg schema-less ( any predefined data structures, etc ) sol: yes ( not predefined)</p> </li> <li> <p>Can we tolerate some msg loss ? sol: sort of.. </p> </li> <li> <p>What is desired SLA ? sol: Highest possible..</p> </li> </ol> <p>Data volume:</p> <p>1msg =1KB i.e 7000 msg = 7MB/sec i.e ~25GB/hr i.e ~605GB/day i.e ~221TB/year </p> <p>Retention period</p> <p>How long you want your rectods to be kept in the DB ?</p> <p>What happens to them after the retention period ?</p> <ul> <li>Deleted</li> <li>Move to archived data store</li> </ul> <p>Motivation:</p> <ul> <li>keeps db from expoloding</li> <li>improve query performance</li> </ul> <p>Two types of data:</p> <ul> <li> <p>Operational, near realtime (location, speed).. etc Retention period: 1 week</p> </li> <li> <p>Aggregated and ready for analysis(Business Intelligence) Retention period: Forever</p> </li> </ul>"},{"location":"sysdesign/app/autonomouscars/#mapping-components","title":"Mapping components","text":"<p>Component: - Receive telementry - Validate telementry - Store telementry - Query &amp; analyze telementry</p> <p>First component: Cars - Source of the data, since no control for us</p> <p>Second component: Telementry gateway - receives telementry data from cars. Since the load is very high, no validation, storing or query etc will be done.</p> <p>Third component: Telementry pipeline - It will receive the msg from the gateway, and put into the pipeline. hence no load on the system. i.e It will queue the telementry msg for processing</p> <p>Fourth component telementry processor - Validate and process any msg and store in the data base.</p> <p>Fifth component telementry viewer - Queries the database and displays real time data.(dashboards)</p> <p>Six component Data warehouse - store aggregated msg from databases</p> <p>Seventh component BI application: It is used only to report and analysis.</p> <p>Eighth component: Archive DB - Make sure you have huge database to just store the information. we don't need to query anything for operational purpose. incase you need to work, you need to put that data in the operational DB and work upon.</p>"},{"location":"sysdesign/app/autonomouscars/#telementry-gateway","title":"Telementry Gateway","text":"<ul> <li>Receives data from cars using TCP</li> <li>Pushes the data to pipeline</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - No</li> <li>Console - Yes</li> <li>Service - Yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#technology-stack","title":"Technology stack","text":"<p>Considerations: - Load ( 7000 msg/sec)  - Persormance is very good - Team's current knowledge and skill set - Environment (OS..etc)</p>"},{"location":"sysdesign/app/autonomouscars/#telementry-gateway-ques-for-customer","title":"Telementry gateway ques for customer","text":"<p>ask customer about environment and skill set..  Custome reply : python and javascript with good linux skills</p> <p>But we can't use python for this as its interpreeted lang and very slow Hence we go for nodejs which is great performance runs on linux and leverage javascript</p>"},{"location":"sysdesign/app/autonomouscars/#architecture","title":"Architecture","text":"<ul> <li>User/interface: No </li> <li>Business logic : No</li> <li>Data Access: No</li> <li>Data store: No</li> </ul> <p>Since telementry data uses none of the above, we don't need anything from above  We need Service interface and qquickly pushes into pipeline</p> <p>Redunancy for telementry gateway place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/app/autonomouscars/#telementry-pipeline","title":"Telementry pipeline","text":"<ul> <li>Gets msg from the gateway</li> <li>Queus the telementry for further processing</li> <li>Basically, queue for streaming high volume of data</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#telementry-pipeline-ques-for-customer","title":"Telementry pipeline ques for customer","text":"<ul> <li>is there any existing queue mechanism in company ?  No</li> <li>use 3rd party - we will go with Apache kafka.</li> </ul> <p>Apache Kafka </p> <p>Pros:</p> <ul> <li>Very popular</li> <li>Handle massive amount of data</li> <li>HA</li> </ul> <p>Cons:</p> <ul> <li>Complex to setup and configure</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#telementry-processor","title":"Telementry processor","text":"<ul> <li>Receives from the pipeline</li> <li>Process the msg(mainly validation)</li> <li>Stores msg in the database</li> </ul>"},{"location":"sysdesign/app/autonomouscars/#application-type_1","title":"Application Type","text":"<p>Web App &amp; Web API - No Mobile App - No Console - Yes Service - Yes Desktop App - No</p>"},{"location":"sysdesign/app/autonomouscars/#technology-stack_1","title":"Technology stack","text":"<p>Considerations:</p> <ul> <li>Processor: Solution: nodejs - already used, fast and great support for kafka</li> </ul> <p>Operational DB: - Data store: schema-less msg support - quick retrivals - no complex queries</p> <p>Solutiom: MangoDB </p> <p>Archive Db:</p> <ul> <li>support for huge amt of data</li> <li>not accessed frequently</li> <li>no need for fast retrival</li> <li>save costs</li> </ul> <p>Solution: Cloud storage (Azure)</p>"},{"location":"sysdesign/app/autonomouscars/#architecture_1","title":"Architecture","text":"<p>User/interface: No  Business logic : No Data Access: No Data store: No</p> <p>Redunancy for telementry processor place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/app/autonomouscars/#telementry-viewer","title":"Telementry viewer","text":"<ul> <li>Allow end uses to query data</li> <li>Displays real time data</li> </ul> <p>What it doesn't do - Analyze the data..</p>"},{"location":"sysdesign/app/autonomouscars/#application-type_2","title":"Application Type","text":"<p>Web App &amp; Web API - yes Mobile App - No Console - No Service - No Desktop App - No</p>"},{"location":"sysdesign/app/autonomouscars/#technology-stack_2","title":"Technology stack","text":"<p>Backend: Solution: NodeJS</p> <p>Frontend:  Solution: ReactJS</p>"},{"location":"sysdesign/app/autonomouscars/#architecture_2","title":"Architecture","text":"<p>We need to use classic 3-layered arch for this component.</p> <p>User/interface: Yes  Business logic : Yes Data Access: Yes Data store: No</p>"},{"location":"sysdesign/app/autonomouscars/#design-api-component","title":"Design API component","text":"<ul> <li>Get latest errors for all cars</li> </ul> <p>```GET /api/v1/telementry/erorrs  200 Ok or Empty list or dict</p> <pre><code>\n- Get latest telementry for specified car\n\n</code></pre> <p>GET /api/v1/telementry/{carid} 200 OK / 404 Not Found</p> <pre><code>- Get latest errors for specific car\n</code></pre> <p>GET /api/v1/telementry/error/{carid} 200 OK/ 404 Not Found ```</p> <p>Redunancy for telementry viewer place the gateway behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/app/autonomouscars/#bi-application","title":"BI Application","text":"<ul> <li>Analyze telementry data</li> <li>Display custom reports about data, trends, forcasts</li> </ul> <p>how many cars did break during the last month what is the total distance the cars drove  etc </p>"},{"location":"sysdesign/app/autonomouscars/#application-type_3","title":"Application Type","text":"<p>Doesn't matter  BI application is always based on the existing ones</p> <p>Figure our from the gartner </p> <p>Power BI - Microsoft  ableau </p> <p>Note: Designing BI is not part of arch job Inform the customer saying that we are not part of BI experts, he can bring anyone he wishes to and we would be working with him to assist him as we laid out the high level system specs</p>"},{"location":"sysdesign/app/facebook/","title":"Facebook","text":""},{"location":"sysdesign/app/facebook/#facebook-or-instangram-system-design-goes-here","title":"facebook or instangram system design goes here","text":""},{"location":"sysdesign/app/gmaps/","title":"Google Maps","text":""},{"location":"sysdesign/app/gmaps/#google-maps-system-design-goes-here","title":"google maps system design goes here","text":""},{"location":"sysdesign/app/grosscollections/","title":"Gross collections","text":"<p>In this case study, we are allowing customer to create shopping lists that get collected and delivered by grosscoll employees. It already available all oever world. Employees have dedicated tables and displaying the list</p> <p>Design the collection side of the system. Cutomer side is already developed,</p>"},{"location":"sysdesign/app/grosscollections/#system-requirements","title":"System Requirements","text":""},{"location":"sysdesign/app/grosscollections/#functional","title":"functional","text":"<p>What should system do ?</p> <ul> <li>Web based </li> <li>tablets receive the list to be called </li> <li>Employees can mark items as collected or unavailable.</li> <li>When collection is done, list must be transferred to payment engine</li> <li>offline support is a must</li> </ul>"},{"location":"sysdesign/app/grosscollections/#non-functional","title":"non-functional","text":"<p>What should system deal with ?</p> <ol> <li>is it data intrinsic ?</li> <li>Users ?</li> <li>How much of data ?</li> <li>Performance ?</li> </ol> <p>Questions to ask customers ?</p> <ol> <li> <p>How many concurrent users ? sol: 200</p> </li> <li> <p>How many list processed/day ? sol: 10000</p> </li> <li> <p>whats the avg size of the shopping list? sol: 500KB</p> </li> <li> <p>Do we need offline support ? sol: yes</p> </li> <li> <p>What's the SLA ? sol: highest possible</p> </li> <li> <p>Since we already know that customer shopping is already developed, then how do lists arrive to the system ? sol: Queue - which means we know that all the meg arrive in queue and we need to pull the msg and process the data. </p> </li> </ol> <p>Data volume</p> <p>1 list = 500KB 10000 list/per day =  ~5GB/per day monthly = ~150GB/month yearly = ~1800GB/year = ~2TB/year</p>"},{"location":"sysdesign/app/grosscollections/#map-the-components","title":"Map the components","text":"<p>Components</p> <ul> <li>Employee have tablets</li> <li>offline support</li> <li>retrive lists</li> <li>mark items</li> <li>export list to payment engine</li> </ul> <p></p> <p>we keep list receiver and service differently, because in future if there is any change in the database we don't have to work completely modifying the service..</p>"},{"location":"sysdesign/app/grosscollections/#list-receiver","title":"list receiver","text":"<ul> <li>receives the shopping list to handle queue</li> <li>stores the list in the db</li> </ul>"},{"location":"sysdesign/app/grosscollections/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - No</li> <li>Console - Yes</li> <li>Service - Yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/app/grosscollections/#technology-stack","title":"Technology stack","text":"<p>Should be able to connect to queue and nothing else</p> <p>we would ask for the customer about the skill expertise that they use as we only consume the queue.</p> <p>Solution from customer: Java. </p> <p>Java is a best solution for the queue solution and hence we go with it. </p> <p>Now, we need to use the database for storing the values as our data is relational we use relational DB also the expected volume of data is around 2TB/year which is a lot. We can use paritioning for this</p>"},{"location":"sysdesign/app/grosscollections/#architecture","title":"Architecture","text":"<ul> <li>Queue receiver: yes</li> <li>Business logic : yes</li> <li>Data Access: yes </li> <li>Data store: yes</li> </ul> <p>Redunancy</p> <p>We will need to use the kafka where we have consumer group and all the instances of list receiver would be grouped.</p>"},{"location":"sysdesign/app/grosscollections/#list-service","title":"list service","text":"<ul> <li>allow employees to query list</li> <li>marks items in list</li> <li>export payment type</li> </ul>"},{"location":"sysdesign/app/grosscollections/#application-type_1","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - yes</li> <li>Mobile App - no</li> <li>Console - no</li> <li>Service - no</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/app/grosscollections/#technology-stack_1","title":"Technology stack","text":"<p>You can use Java</p>"},{"location":"sysdesign/app/grosscollections/#architecture_1","title":"Architecture","text":"<ul> <li>User/interface: yes</li> <li>Business logic : yes</li> <li>Data Access: yes</li> <li>Data store: yes</li> </ul> <p>Any service that exposes, we need to use API</p>"},{"location":"sysdesign/app/grosscollections/#design-api-component","title":"Design API component","text":"<ul> <li>Get list to be processed(by location)</li> </ul> <pre><code>GET /api/v1/lists/next?location= ..\n\n200 OK / 400 Bad Request\n</code></pre> <ul> <li>mark item as collected/unavailable</li> </ul> <pre><code>PUT /api/v1/lists/{listId}/item/{itemId}\n\n200 OK / 404 Not Found\n</code></pre> <ul> <li>export payment list data</li> </ul> <pre><code>POST /api/v1/list/{listId}/export\n\n200 OK / 404 Not Found\n</code></pre> <p>Reduancy</p> <p>place the list service behind the Load balancer(3 instances) - scale across traffic and regions as the data will be high.</p>"},{"location":"sysdesign/app/grosscollections/#front-end","title":"front end","text":"<ul> <li>Displays shopping list</li> <li>marks items as unavailble / collected</li> <li>send list to payment system</li> <li>support offline mode</li> </ul>"},{"location":"sysdesign/app/grosscollections/#application-type_2","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - No</li> <li>Mobile App - yes</li> <li>Console - no</li> <li>Service - no</li> <li>Desktop App - yes</li> </ul>"},{"location":"sysdesign/app/grosscollections/#technology-stack_2","title":"Technology stack","text":"<p>Desktop, window based</p> <ul> <li>support all OS applications</li> <li>utilizes other app on the machine(db)</li> <li>requires setup, windows</li> </ul> <p>Web based </p> <ul> <li>limited functionality</li> <li>cannot use other apps</li> <li>fully compactible with other form factors</li> <li>no setup required</li> <li>cheaper hardware</li> </ul> <p>We don't need any redenacy for front end as there is no load. </p>"},{"location":"sysdesign/app/grosscollections/#exporter-list","title":"exporter list","text":"<ul> <li>used to send the shopping data to payment system</li> <li>it should be queue </li> </ul> <p>since its queue, we must ask question to customer</p> <ul> <li>do we already have a queue in the company ? soln: yes</li> </ul> <p>since there is already a list queue, we use the same for data queue to store or export data..</p> <ul> <li>do we need to use any third party ? soln: Nil</li> </ul>"},{"location":"sysdesign/app/grosscollections/#technical-diagram","title":"Technical diagram","text":""},{"location":"sysdesign/app/grosscollections/#physical-diagram","title":"physical diagram","text":""},{"location":"sysdesign/app/notifications/","title":"Notification service","text":""},{"location":"sysdesign/app/notifications/#notification-system-design-at-scale-goes-here","title":"notification system design at scale goes here","text":""},{"location":"sysdesign/app/papersource/","title":"Newsletter","text":"<p>In this we are designing case study for papersource, that sells paper supplies. i.e Printer paper, Envelopes.. etc. They also need a new HR system, which manages salaries, vacation, payments for the employees. </p>"},{"location":"sysdesign/app/papersource/#requirements","title":"Requirements","text":""},{"location":"sysdesign/app/papersource/#functional","title":"Functional","text":"<p>What system should do ?</p> <ul> <li>Web based</li> <li>CURD operations</li> <li>Manage salaries<ul> <li>Allow manager to ask for emplyess salary chage</li> <li>Allow HR manager to approve/reject request</li> </ul> </li> <li>Manage vacation days</li> <li>Use external payment systems</li> </ul>"},{"location":"sysdesign/app/papersource/#non-functional","title":"Non functional","text":"<p>what the system should deal with ?</p> <ol> <li>what type of system (classic, legacy, cloud)</li> <li>Not a lot of users</li> <li>Not a lot of data</li> <li>Interface to external system (i.e)</li> </ol> <p>Questions</p> <p>You must be getting these details from the customer/users</p> <ul> <li> <p>how many concurrent users - 10</p> </li> <li> <p>How many employees ? - 250</p> </li> <li> <p>What we know abt external payment system ?</p> <p>legacy hosted system in company farm received input files monthy once for processing payment informations.</p> <p>Data volume calculations</p> <p>1 Employess = 1MB </p> <p>Each employess has ~10scanned documents(contract, reviews, etc) i.e each employee has 51MB docs. As company expected to grow, 500 in 5 yrs, 51MB*500 = 25.5GB</p> </li> <li> <p>What about criticality of the system ?</p> <p>HR based system, not very critical</p> </li> </ul> <p>so finally, we have these below as non-functional requirments</p> <pre><code>10 Concurrent users\nManage 500 users\n25.5 GB of data volume forcast\nNot mission critical \nfile-based interface\n</code></pre>"},{"location":"sysdesign/app/papersource/#component-mapping","title":"Component mapping","text":"<p>Based on function requirements, it would be good practice to map single component to single service for managing and maintaining. </p> <pre><code>Entities: Employee, Vacation, Salary\nInterface to payment system\n</code></pre> <p></p>"},{"location":"sysdesign/app/papersource/#logging-services","title":"logging services","text":"<p>Questions to be asked for logging service.</p> <ul> <li>Are there any logging service used in the company at present ? Client: No</li> <li>Any 3rd party ? we can choose, ELK, but its quite complex and resources are needed to operate and maintain.     since, our application is not being used so much, we can develop our own logging mechanism</li> <li>Should we need to develop our own logging  ?  Architect: Yes</li> </ul> <p>How would you decide for developing own logging mechanism ?</p> <ul> <li> <p>Application type(what it does)</p> <ul> <li>read log records from queue</li> <li>validate the records</li> <li>store in the data store</li> </ul> </li> <li> <p>Alternatives</p> <ul> <li>Webapp app or webAPI - No</li> <li>Mobile app - No</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul> </li> </ul>"},{"location":"sysdesign/app/papersource/#technology-stack","title":"Technology stack","text":"<ul> <li>what should code do here ?<ul> <li>Access Queue API</li> <li>validate the data</li> <li>store the data</li> </ul> </li> </ul> <p>- Architecture</p> <ul> <li>3 layer architecture</li> <li>UI/Service interface: Not required for logging</li> <li>Business logic: validate the records (polling)</li> <li> <p>Data access: saves validated records into data store</p> </li> <li> <p>logging redunancy  if there are any crash for logging, you need to have another service that would detect. </p> </li> </ul> <p>always have 3 instances of logging service to replicate that helps you to log. (active-active)</p> <p>we must use method called is-alive which will check for the active instances and if it gets reponse it would not log, incase if that doesn't get any reposnse after waiting for some period of time, it would consider that the service is down and new leader would be elected. </p>"},{"location":"sysdesign/app/papersource/#view-service","title":"view service","text":"<p>Get request from end users browsers and return static content (html, css, js) which after that access service like employee, vacation, salary data.</p> <ul> <li>Applicatio type(what it does)</li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture </p> </li> </ul> <p>Since this is only service the static content between the user and the service, we would only have UI</p> <p>UI/Service interface: UI  Business/application logic: No  Data access: No</p> <ul> <li>view service redunancy we use load balancer to view service so it would have balance traffic and security as well.</li> </ul>"},{"location":"sysdesign/app/papersource/#employee-service","title":"Employee service","text":"<p>Allows end users to query employee's data and perform CURD ops, but it does't display as its <code>view</code> service.</p> <ul> <li>Applicatio type(what it does)</li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li>.NET Core </li> <li>Emloyee data(relational db) - Microsoft SQL - since the documents are around(~1MB) it would be good. since we also use .NET</li> <li> <p>Employee documents storage (relational DB, file system,object,cloud storage)</p> </li> <li> <p>Architecture</p> </li> <li> <p>Employee API : create an API for employee service</p> <ul> <li>get full emp details by ID <code>GET /api/v1/employee/{id}</code></li> <li>list of employees by parameters <code>GET /api/v1/employees?name=..&amp;birthdate=..</code></li> <li>add employee <code>POST /api/v1/employee</code></li> <li>update employee details <code>PUT /api/v1/employee/{id}</code></li> <li>remove employee(don't remove record, mark it as removed) <code>DELETE /api/v1/employee/{id}</code></li> </ul> </li> <li> <p>Document API: get list of docs for the employees</p> <ul> <li>Add document <code>POST /api/v1/employee/{id}/document</code></li> <li>Remove document <code>DELETE /api/v1/employee/{id}document/{docid}</code></li> <li>Get document <code>GET /api/v1/employee/{id}/document/{docid}</code></li> <li>Get documents by parameters <code>GET /api/v1/employees/{id}/documents</code></li> </ul> </li> <li> <p>employee service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/app/papersource/#salary-service","title":"salary service","text":"<ul> <li>Allow managers to ask for an employee's salary change</li> <li> <p>Allows HR to approve/reject the request</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     Salary API request</p> <ul> <li>Add salary request <code>POST /api/v1/salaryRequest/</code></li> <li>Remove salary request <code>DELETE /api/v1/salaryRequest{id}</code></li> <li>Get salary request <code>GET /api/v1/salaryRequests</code></li> <li>Approve salary request <code>POST /api/v1/salaryRequest/{id}/approval</code></li> <li>Reject salary request <code>POST /api/v1/salaryRequest/{id}/rejection</code></li> </ul> </li> <li> <p>salary service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/app/papersource/#vacation-service","title":"vacation service","text":"<ul> <li>Allows employees to manage their vacation days</li> <li> <p>Allows HR to set available vacation days for employees</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - yes</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - No</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     Vacation API request</p> <ul> <li>set available vacation days(HR) <code>PUT /api/v1/vacation/{empid}</code></li> <li>Get available vacation days(Employee) <code>GET /api/v1/vacation/{empid}</code></li> <li>Reduce vacation days(Employee) <code>POST /api/v1/vacation/{empid}/reduction</code></li> </ul> </li> <li> <p>vacation service redunancy     use load balancer for service redunancy</p> </li> </ul>"},{"location":"sysdesign/app/papersource/#payment-interface-service","title":"payment interface service","text":"<ul> <li>Queries the db once a month for salary data</li> <li> <p>passes payment data to the external payment system</p> </li> <li> <p>Applicatio type(what it does)</p> </li> <li>Webapp &amp; Web API - No</li> <li>Mobile app - No</li> <li>Console - No</li> <li>Service - Yes</li> <li> <p>Desktop App - No</p> </li> <li> <p>Technology stack(For)</p> </li> <li> <p>.NET Core </p> </li> <li> <p>Architecture     you need to use the timer as the application logic for triggering the job. </p> </li> <li> <p>Payment interface redunancy     use is-alive parameter between instance services of payment interface.. </p> </li> </ul>"},{"location":"sysdesign/app/papersource/#queue-tech-stack","title":"Queue - Tech stack","text":"<p>Choose as to which Queue service you need to use for your application. </p> <ul> <li> <p>RabbitMQ: General purpose message-broker engine, which is easy to setup and use</p> </li> <li> <p>Apache Kafka: Stream processing platform, which is more used in extensive data intrensic scenerios. </p> </li> </ul> <p>In our this case, since nothing is involved in the streaming platform, we can choose <code>RabbitMQ</code></p>"},{"location":"sysdesign/app/paymentprocessing/","title":"Payment processing","text":"<p>In this case study, we are studying about the payroll. Basically, payroll designs and builds payment processing system and what the system does is it receives files from various sources, mainly companies that want to pay their employees. </p> <p>The payroll system validates and processes the files and then sends instruction files to the banks in order to execute the actual payment. Now, one very important aspect of the payroll system is that it must be fully automatic, reliable and very quick. </p> <p>Note: there shouldn't be any human interaction while processing the file</p>"},{"location":"sysdesign/app/paymentprocessing/#system-requirements","title":"System Requirements","text":""},{"location":"sysdesign/app/paymentprocessing/#functional","title":"functional","text":"<p>What should system do ?</p> <ul> <li>receives files from various sources</li> <li>validates and processes the files</li> <li>work with various file formats</li> <li>perform various calculations on the file</li> <li>create bank payment file </li> <li>put payment file in a designated folder</li> <li>keep the log file for 7 years</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#non-functional","title":"non-functional","text":"<p>Questions to ask customers ?</p> <ol> <li> <p>How many files are we going to get each day? sol: 500 </p> </li> <li> <p>Any limitation on the latency of the process? sol: 1 min</p> </li> <li> <p>what is avg size of file ? sol: 1MB</p> </li> <li> <p>can we tolerate data loss ? sol: No</p> </li> </ol> <p>Data volume</p> <p>1 file = MB  500 file = 500MB  ~182GB/year ~1.3TB/7 years</p> <p>the space for the log file for 1 min procssing time</p> <p>~ 500KB log data ~ 500 file/day = 250MB log data/day ~ 91GB log data/year ~ 638GB log data/ 7 years</p>"},{"location":"sysdesign/app/paymentprocessing/#map-the-components","title":"Map the components","text":"<ul> <li>Passes payloads from logic unit to another.</li> <li>Balances load.</li> <li>Persists messages. (Durability!)</li> </ul> <p>Queue provides async, since we don't have any UI also the components are not waiting for any response, we would always use queue. </p> <p>So there are 2 queues to select </p> <ol> <li> <p>RabbitMQ  - This is Genral purpose and very easy to setup. this is not suitable for streaming services. </p> </li> <li> <p>Kafka - Great for streaming services, high load systems, but very complex to setup. </p> </li> </ol> <p>In this scenerio, we would use RabbitMQ as this is not a streaming servies also easy setup..</p>"},{"location":"sysdesign/app/paymentprocessing/#file-handler","title":"File handler","text":"<ul> <li>Pulls payment files from folders</li> <li>Put the files in the queue</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#application-type","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - no</li> <li>Mobile App - no</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#technology-stack","title":"Technology stack","text":"<ul> <li>Should be able to pull files from folders</li> <li>Should be able to connect to queue</li> </ul> <p>Would you want to ask the customer regarding the technology they know or they use or team using etc ..</p> <p>incase we would want to suggest, here is what it is ..</p> <ul> <li>performance</li> <li>community</li> <li>cross platform</li> <li>easy to learn </li> <li>evolving</li> <li>great threading support</li> </ul> <p>from above, we could either use java or .net core</p>"},{"location":"sysdesign/app/paymentprocessing/#architecture","title":"Architecture","text":"<p>3 layered arch</p> <ul> <li>service interface: yes</li> <li>Business logic : yes</li> <li>Data Access: yes </li> <li>Data store: yes</li> </ul> <p>File watcher, topic is selected by the file location, monitor the zip folder, put into the folder. </p>"},{"location":"sysdesign/app/paymentprocessing/#redunancy","title":"Redunancy","text":"<p>two of the file handler keeps shaking to check for liveness ..etc</p>"},{"location":"sysdesign/app/paymentprocessing/#file-formatter","title":"File Formatter","text":"<ul> <li>received files from its specific topic</li> <li>validates and formats the file to unified format</li> <li>puts the new file in quwue</li> <li>new formaters will be developed for new file</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#application-type_1","title":"Application Type","text":"<ul> <li>Web App &amp; Web API - no</li> <li>Mobile App - no</li> <li>Console - yes</li> <li>Service - yes</li> <li>Desktop App - No</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#technology-stack_1","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core. no need for any other specifics.</p>"},{"location":"sysdesign/app/paymentprocessing/#architecture_1","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#redunancy_1","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic, hence no extra work needed to work.</p>"},{"location":"sysdesign/app/paymentprocessing/#file-calculations","title":"File calculations","text":"<ul> <li>Receives file from queue</li> <li>performs some calculations</li> <li>puts new file in queue</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#technology-stack_2","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core.</p>"},{"location":"sysdesign/app/paymentprocessing/#architecture_2","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#redunancy_2","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic</p>"},{"location":"sysdesign/app/paymentprocessing/#file-exporter","title":"File exporter","text":"<ul> <li>receives file from queue</li> <li>put trhe file in bank's folder</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#technology-stack_3","title":"Technology stack","text":"<p>Its same as the previous one, java or .net core. no need for any other specifics.</p>"},{"location":"sysdesign/app/paymentprocessing/#architecture_3","title":"Architecture","text":"<p>2 layered arch</p> <ul> <li>Queue receiver: yes</li> <li>Business logic: yes, store file in queue</li> </ul>"},{"location":"sysdesign/app/paymentprocessing/#redunancy_3","title":"Redunancy","text":"<p>consure group itself would be availble to deal with traffic</p>"},{"location":"sysdesign/app/paymentprocessing/#logging-service","title":"logging service","text":"<ul> <li>write log of records</li> <li>allow easy visualizations and analytics</li> <li>Preferrably, based on existing platform. </li> </ul> <p>you can use elk component for log storage and visualizations. </p> <p></p> <p>You can use serilog to transport file to the elastic db. from queue, you can send the data using logstash. beats or .... would be used to send the data to the elastic. </p>"},{"location":"sysdesign/app/paymentprocessing/#arch-diagrams","title":"arch diagrams","text":""},{"location":"sysdesign/app/paymentprocessing/#logic","title":"logic","text":""},{"location":"sysdesign/app/paymentprocessing/#technical","title":"technical","text":""},{"location":"sysdesign/app/paymentprocessing/#physical","title":"physical","text":""},{"location":"sysdesign/app/twitter/","title":"Twitter","text":""},{"location":"sysdesign/app/twitter/#twitter-system-design-goes-here","title":"twitter system design goes here","text":""},{"location":"sysdesign/app/uber/","title":"Uber","text":""},{"location":"sysdesign/app/uber/#uber-system-design-goes-here","title":"uber system design goes here","text":""},{"location":"sysdesign/app/url_shortner/","title":"URL Shortner","text":""},{"location":"sysdesign/app/url_shortner/#functional","title":"Functional","text":"<ul> <li>Get the long url and shortern it</li> <li>On clicking, it must be re-directed to the long url</li> </ul>"},{"location":"sysdesign/app/url_shortner/#non-functional","title":"Non functional","text":"<ul> <li>very low latency</li> <li>HA </li> </ul> <p>Questions</p> <ul> <li>What's the traffic generated for the long urls </li> <li>what kind of the length should I need to keep for shorturl</li> <li>what's the duration of the shorturl ?</li> </ul> <p>let's say you have 'x' requests for a year, then you would have traffic around </p> <p>x606024365 days = y  based on the value of 'y' you would then be required to choose the length of the short url which contains  A-z, a-z, 0-9 which are 62 characters.. </p> <p>i.e 62^5 or 62^6 would provide around 3.5billion combinations..</p> <p></p> <p>since you have LB, the request can be moved to any of the services.. which ideally means our short url can be duplicated with the same alpha numbers.. i.e ideally you could have 2 same short urls for the long ones which is incorrect..</p> <p>so we can use an <code>redis</code> so that it would assign a unique way of assigning the requests.. but what if the redis goes down, it would forget the tokens that are created and will start from afresh which defeats our purpose.  </p> <p>so we are using <code>service token</code> in sitting on the mysql.. we could be assigning some range for every url_shortner_node so that it would assign token starting from their defaults, solving our problems..</p>"},{"location":"sysdesign/app/url_shortner/#logging","title":"logging","text":"<p>Implement the logger service for analytics.. once we get response from the user, we would can add additional fields to the reponse like, country orgin, platform, ip address etc which can be used for tracking purpose.. </p> <p>we can send the log to write into kafka async where it would miss few of the requests, but since this is not payment related it should be okay gradually .. we still should avoid kafka as it is IO bound ops and we could lose the latency as we stated in the non-functional requirement. Instead we could use a local logger and aggregate all the logs and write at once to kafka, which improves IO but if the service is gone, we would lose all the logs at once. it would be better to check with the customer on this design requirement. </p>"},{"location":"sysdesign/app/whatsapp/","title":"Whatsapp","text":""},{"location":"sysdesign/app/whatsapp/#whatsapp-system-design-goes-here","title":"whatsapp system design goes here","text":""},{"location":"sysdesign/app/youtube/","title":"Youtube","text":""},{"location":"sysdesign/app/youtube/#youtube-or-netflix-system-design-goes-here","title":"youtube or netflix system design goes here","text":""},{"location":"sysdesign/app/zoom/","title":"Zoom","text":""},{"location":"sysdesign/app/zoom/#zoom-system-design-goes-here","title":"zoom system design goes here","text":""},{"location":"vcontroller/git/faq/","title":"Interview Questions","text":""},{"location":"vcontroller/git/overview/","title":"Git Arch","text":"<p>Working directory - current files that are stored, it's also called as untracked files</p> <p>Staging area - files that you wish to commit(to create snapshot of the files).</p> <p>Git directory - after commit is fired, files which are in staging area will move to git repository.</p>"},{"location":"vcontroller/git/overview/#checkout","title":"Checkout","text":"<p>you would use the commit id and move your HEAD to that particular commit and then you would branch from there. </p> <pre><code>git commit &lt;commit_id&gt;\ngit checkout -b &lt;new_branch&gt;\n</code></pre> <p>Incase you don't need to branch out, this method is not suitable</p> <p>second way,</p> <pre><code>git log --oneline\ngit checkout &lt;commit-id&gt; -- filename.txt\ngit status\ngit log --oneline\ngit commit -m 'new commit id'\n</code></pre>"},{"location":"vcontroller/git/overview/#branching","title":"branching","text":"<p>fast-forward </p> <p>Developers create a feature branch, work on it, and when it's ready to be integrated into the main development branch, they perform a fast-forward merge if the conditions are met. This keeps the commit history clean and straightforward.</p> <p>Default merging startergy would be <code>ff</code></p> <pre><code>git branch -b feature/feature1\ngit checkout main\ngit merge feature/feature1\n</code></pre>"},{"location":"vcontroller/git/overview/#diff","title":"diff","text":"<p>git diff command is used to display the differences between two sets of changes, such as comparing files between branches.</p> <pre><code># differences between the master branch and a feature\ngit diff master..feat1 \n\n#differences for file1.txt and file2.js between the master and feat1 branch\ngit diff master..feat1 file1.txt file2.js \n\n#Difference for a Single File\ngit diff master:app.js feat1:app.js\n\n#Differences for Staged Changes\ngit diff --staged\n</code></pre>"},{"location":"vcontroller/git/overview/#stash-unstash","title":"stash &amp; unstash","text":"<p>Let's assume you created file and commited in the main branch. Now, lets say you would checout a new branch and modify the files without committing. Once you switch back to main branch you would get the changes made in another branch to main which is not recommended. The solution would be to stash changes to the branch before switching.</p> <pre><code>git stash list\ngit stash / git stash save\n</code></pre> <p>Once you are back to your branch(or any branch you would want to apply changes on), you can unstash and start working from where you left off. </p> <p><code>pop</code> it would apply changes to the current branch you are in, and removes from the stash. <code>apply</code> applies the changes to the current directory and would not remove from the stash.</p> <pre><code># removes from the stash\ngit stash pop\n\nor \n\n# would not remove, but can be applied later to any branch.\n\ngit stash apply\n</code></pre> <p>you can clear or drop stashes</p> <pre><code>git stash drop stash@{1}\ngit stash clear\n</code></pre>"},{"location":"vcontroller/git/overview/#detached-head","title":"detached HEAD","text":"<p>When we commit any file in the branch, the HEAD always points to the branch. when we checkout and workon, we still have our HEAD being pointed at the branch we work upon.</p> <p>When we checkout a particular commit, HEAD points at that commit rather than the branch, then we call it as a <code>detached HEAD</code></p> <pre><code>git log --oneline\ngit commit &lt;commit-id&gt;\ngit status \n</code></pre> <p>It would be very essential sometimes that you need to take up a particular commit and then work upon. In that case, you can checkout particular commit, where youe HEAD would point to the branch you took from.</p> <pre><code>git checkout &lt;commit-id&gt;\n&lt;modify your files&gt;\ngit add .\ngit commit -m 'your new commits'\ngit status\n</code></pre> <p>You can select the previous commit using HEAD</p> <pre><code>git checkout HEAD~1\ngit checkout HEAD~2\ngit switch - # it would take you back where you were there\n</code></pre>"},{"location":"vcontroller/git/overview/#discard-changes","title":"discard changes","text":"<p>you have made changes to the file, but don't want to keep those. You can revert back the file to whatever it looked like when you last committed, <code>reverting back to HEAD</code></p> <pre><code>1. git checkout HEAD file1.txt\n2. git checkout -- file1.txt file2.txt\n3. git restore file1.txt file2.txt\n</code></pre> <p>Let's say you have commited the secrets file(i.e staged area), and now you want to restore it.</p> <pre><code>git status\ngit restore --staged secrets.txt\ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#git-reset","title":"git reset","text":"<p>soft</p> <p>let's say you have made a couple of bad commits on the master branch, but you actually made them on a seperate branch instead, then you use <code>git reset &lt;commitid&gt;</code>. here, in this case you would actually lose the commit id's but your changes in the file remains the same. Hence you would need to take a new branch and commit those files. Once commited then come back to <code>master/main</code> branch. you won't see those commits as well as the changes made to that file.</p> <p>simple words, won't effect working directory or staging area, only commitid would be changed.</p> <pre><code>git log --oneline\ngit reset commitid\ngit log --oneline - bad commits to the file not seen, head is relaed to your commitid\ngit status - it says modified, but your changes still exists\ngit checkout -b do_your_commits\ngit add . \ngit commit 'removed changed made to files from the branch'\ngit checkout master\ngit status\n</code></pre> <p>hard</p> <p>If you want your changes and your commitid to be reverted from <code>working directory and stage</code> then you would be performing the <code>hard</code> reset. </p> <pre><code>git log --oneline\ngit status \ngit reset --hard commitid - all your changes are lost along with commitid\ngit status\n</code></pre> <p>Note: Make sure always that you need to be careful in hard reset as you would lose changes made in working directory.</p>"},{"location":"vcontroller/git/overview/#git-revert","title":"git revert","text":"<p>Creates a new commit which reverses/undo the changes from a commit. </p> <pre><code>git add .\ngit commit -m 'bad commits'\ngit log --oneline\ngit revert commitid - this will prompt an editor for changes and on saving you would \"revert bad commit\", changes to the file are lost\ngit status\ngit log --oneline - your revert entry will be added so that collaborators would know that you have reverted changes on the code.\n</code></pre>"},{"location":"vcontroller/git/overview/#git-originmain","title":"git origin/main","text":"<p>remote tracking branch</p> <p>At the time you last committed with this remote repo. </p> <p>let's say you have cloned the repo, when you search for the branches you would see there are <code>remote/origin</code>  which is nothing but the pointer for the remote repo of main branch. when you add files and commit, then you would get message like your <code>origin/main</code> is ahead by 1 commit.</p> <p>origin/master - references the state of the master branch on the remote repo named origi</p> <pre><code>remote branch -r\ngit add newfile.txt\ngit commit 'your remote branch ahead'\n</code></pre> <p>In case, you wanted to know what exactly your changes were in the remote repo, then you would switch to <code>origin/master</code>. It would message as your <code>HEAD</code> has been detached, no need to panic. incase you want to make some more new changes, then take out a new branch from it and then work on. </p> <pre><code>git switch -c origin/master\n&lt;detached head&gt;....\n\ngit checkout -b &lt;new_branch&gt; - incase you need a new branch\n\ngit switch -c master\n</code></pre>"},{"location":"vcontroller/git/overview/#git-fetch-and-pull","title":"git fetch and pull","text":"<p>git fetch </p> <ul> <li>Allows changes from the remote repository to the local repository.</li> <li>Updates the remote-tracking branches with new changes</li> <li>Does not merge changes onto your current HEAD branch</li> <li>Safe to do anytime. </li> </ul> <p>git pull </p> <ul> <li>Allows changes from the remote repository to local repository to working directory.</li> <li>Updates the current branch with new changes, mergung them</li> <li>Can result in merge conflicts</li> <li>Not recommended if you have un-committed chanegs</li> </ul> <p>git pull = git fetch + git merge</p>"},{"location":"vcontroller/git/overview/#merge-pr-with-conflicts","title":"merge PR with conflicts","text":"<p>Switch to the branch in question. Merge in master/main and resolve conflicts</p> <pre><code>git fetch \ngit switch my-new-feature\ngit merge master\nfix conflicts\n</code></pre> <p>Switch to master, marge the feature branch(with no conflicts now), push changes to github. </p> <pre><code>git switch master\nget merge my-new-feature\ngit push origin master\n</code></pre> <p>Now, your PR would be without conflicts</p> <ul> <li>what is rebase and explain</li> <li>Explain about the branchinig startergy</li> </ul>"},{"location":"vcontroller/git/overview/#clone-and-fork","title":"clone and fork","text":"<p>Cloning is about creating a local copy for working on a project, while forking is about creating a separate copy, often used in the context of open-source collaboration, with the potential to contribute changes back to the original project. The choice between cloning and forking depends on your specific needs and the collaborative context of the project you're working on.</p> <p>Let's say you forked, which creates a copy from the original in your account, but when the changes happens to the original repo, your changes would be out of sync. Hence you would need to configure to allow remote repo to get changes incase the original repo changes. </p> <pre><code>git remote -v \ngit remote add upstream main # Configure to the original repo for incoming changes.\ngit remote -v \n</code></pre> <pre><code>git pull upstream main \ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#rebase","title":"rebase","text":"<ul> <li>It can be used as an alternative to merge</li> <li>It can be used as a clean up tool for your commits. </li> </ul> <p>Let's say when you are working on the feature branch, there are few of the bug fixes and they would have commited to master branch. So now you need to merge the chages from main branch to your feature branch, resulting in a merge commit. </p> <p>When the above keeps happening for a quite long time, your branch would have all the merge commits from main and your commits description on the feature would not be so much visible. Hence in this case, we would use rebase all the feature branch commits would be available at the tip of master branch, so no merge commits</p> <p>Merge (creates extra commit)</p> <pre><code>main:      A---B---C\n               \\\nfeature:        D---E\n\nAfter merge:\n\nmain:      A---B---C--------M\n               \\          /\nfeature:        D---E------\n</code></pre> <p>Rebase (rewrites history)</p> <pre><code>main:      A---B---C\n               \\\nfeature:        D---E\n\nAfter rebase:\n\nmain:      A---B---C\n                    \\\nfeature:             D'---E'\n</code></pre> <p>When, you have a conflict in the master branch, you would fix the conflict and add the files to the branch. </p> <pre><code>git switch feat\ngit merge master\n\n&lt;RESOLVE AUTO CONFLICTS&gt;\n\ngit commit -am 'fixed merge conflicts'\ngit status\n</code></pre>"},{"location":"vcontroller/git/overview/#git-commits","title":"git commits","text":"<p>In above section we told that we could use <code>rebase</code> as a clean up tool, here we will learn more about this. i.e rewrite, delete, rename, or even re-order commits(before sharing to others)</p> <pre><code>git log --oneline\ngit rebase -i HEAD~6\n\n# after modifying file, save and quit, you would be opened by another edit, save a new commit msg.\ngit status \ngit log --oneline\n</code></pre> <p>Some of them you must try out..</p> <pre><code># p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n</code></pre>"},{"location":"vcontroller/git/overview/#tag","title":"tag","text":"<p>Two types of tagging</p> <ol> <li>lightweight tag: they are just a name/label that points to a prticular commit</li> <li>annotated tags: store extra info including the authors anme, email, date, tag msg.. etc</li> </ol> <pre><code>git tag\ngit tag -l \"*beta*\"\ngit tag -l \"v17*\"\ngit diff v17.0.0..v17.0.1\n</code></pre> <p>lightweight</p> <pre><code>git commit -am 'added patch version'\ngit tag v18.0.1\n\ngit commit -am 'added readme.md'\ngit tag v10.0.2\n\ngit diff v18.0.1..v10.0.2\n</code></pre> <p>annotated</p> <pre><code>git tag -a v18.0.3 # it would open an editor to provide msg.\n</code></pre> <p>you can also use tag from taking previous commit id</p> <pre><code>git tag &lt;tagname&gt; &lt;commitid&gt;\ngit tag &lt;tagname&gt; &lt;commitid&gt; -f # force tag for commit incase it already exists\ngit tag -d &lt;tagname&gt;\n</code></pre> <p>Note: when you update the remote repo using tags, it won't push all the tags. instead you need to explicitly tell to push it</p> <pre><code>git push origin &lt;tagname&gt;\n\ngit push origin --tags # push all the tags\n</code></pre>"},{"location":"vcontroller/git/overview/#behind-the-git","title":"behind the git","text":"<p>when you do <code>git init</code> you would be getting an <code>.git</code> directory and it holds all the version files for the repo. we would look few of the main files that helps in understanding the git better. </p> <pre><code>ls .git/\nCOMMIT_EDITMSG config         hooks          info           objects        refs\nHEAD           description    index          logs           packed-refs\n\n\u279c  react git:(main) ls .git/refs\nheads   remotes tags\n\u279c  react git:(main) ls .git/refs/heads\nmain\n\u279c  react git:(main) ls .git/refs/remotes\norigin\n\u279c  react git:(main) ls .git/refs/remotes/origin\nHEAD\n\u279c  react git:(main) ls .git/objects\n0e   19   41   52   54   6e   8f   a8   dc   info pack\n</code></pre> <p>refs -  Contains one file per branch in a repository. Each file is named after a branch and contains the hash of the commit at the tip of the branch(last commit). </p> <p>HEAD - text file that keeps track of where HEAD points. during the DETACHED HEAD it contains a hash instead of branch. </p> <p>objects - contains all the repo files. This is where git stores the backups of files, commits in a repo etc  The files are all compressed and encrypted..</p> <p>There are 4 types of objects</p> <ol> <li>commit </li> <li>tree</li> <li>blob </li> <li>annotated tag</li> </ol> <p>When ever we write commits, its SHA-1 that encrypts and stores in a database that has key-value pairs.  you need anything to hash out and check, you can use the below.</p> <p>Encrypt</p> <pre><code>echo 'hello' | git hash-object --stdin \necho 'hello' | git hash-object --stdin -w\nce013625030ba8dba906f756967f9e9ca394464a\n\nls .git/objects # your hash object stored in this directory, which is encrypted.\n</code></pre> <p>Decrypt</p> <pre><code>\u279c git:(main) git cat-file -p ce013625030ba8dba906f756967f9e9ca394464a\nhello\n</code></pre> <p>git blobs</p> <p>git uses to store the contents of files in a given repository.Blobs don't even include the filenames of each file or any other data. It looks like a commit hash, but its only blob hash.</p> <p>trees</p> <p>Tress are git objects used to store the contents of a directory. each tree contains pointers that can refer to blobs and to other trees. each entry in a tree contains SHA-1 hash of a blob or tree, as well as the mode,type, and filename</p> <pre><code>git cat-file -p main^{tree}\n040000 tree e4cacd8e23c9de749e53b4d06e5cf76fd10bf22d    .circleci\n040000 tree d0b0e04eb6108e5cd4c4a2c87a7c68f80772bbb1    .codesandbox\n100644 blob 07552cfff88bafaf4d207e6255394bc6d6215302    .editorconfig\n100644 blob 7d79ef692311259a6986aaa9160b1f6e7e795180    .eslintignore\n100644 blob 9d88915811935871123b2b450e972de4251ae109    .eslintrc.js\n100644 blob 176a458f94e0ea5272ce67c36bf30b6be9caf623    .gitattributes\n040000 tree 855f8e70e7e3e1bc69f0f4771e4591dceee09e54    .github\n100644 blob 6ec345e172e5e034cf68ef9c4a9c34fd8043da95    .gitignore\n100644 blob e661c3707d5de330ad0b939af9623f894a0bc0d8    .mailmap\n100644 blob e329619ca22426dece9974cfc626442201c19afa    .nvmrc\n100644 blob 6f69f7f891d672bc7b6696c3c33aaa760956e715    .prettierignore\n100644 blob 4f7ef193130c9019539a08bfb5738ba7af968c83    .prettierrc.js\n100644 blob 0967ef424bce6791893e9a57bb952f80fd536e93    .watchmanconfig\n100644 blob 146796383fbeaa19371045f4559d8d32817bf939    AUTHORS\n100644 blob 6040a0b246cb3402a626d3624f1ae6fe939adbbb    CHANGELOG-canary.md\n100644 blob 8f6df415e32a3706fe120094a22f8253fcd900fa    CHANGELOG.md\n100644 blob 08b500a221857ec3f451338e80b4a9ab1173a1af    CODE_OF_CONDUCT.md\n100644 blob 589af800fdc36dd1658ea6e96ad88a60572ca523    CONTRIBUTING.md\n100644 blob b93be90515ccd0b9daedaa589e42bf5929693f1f    LICENSE\n100644 blob a8d33198d23c47de66eca1caccdeeea7d9e78661    README.md\n100644 blob ef97ee7e3afc3716226dfb5d33a34e951c142690    ReactVersions.js\n100644 blob 655dfeaec0e67a9c448bf08a5f32d1f73aaa9611    SECURITY.md\n100644 blob d4d1e3213c574c85ed774d85c73567c06d534129    babel.config.js\n100644 blob e29426afda7a956b0cebbc24f374cc2b3276044b    dangerfile.js\n040000 tree 16d7de144f85053e52b5df9c2fa2741113e03e0e    fixtures\n100644 blob 76443cdd50285039de8c4e1ff755722c402bc03c    netlify.toml\n100644 blob d45f2f57c4b5bfa9507f134f6eea6adf88782464    package.json\n040000 tree 11c2952c7a7bd2fb611231701ba90a7766e1c09d    packages\n040000 tree a4a83cf4096449c73f4d552f4e8c9bbf15be22ad    scripts\n100644 blob 30b017680e30f3699c43aec49bfcf31e6f1a4dab    yarn.lock\n</code></pre> <p>commits</p> <p>Commit objects combine a tree object along with information about the context that led to the current tree. commits store a reference to parent commit, authors, the committer and commit msg.</p>"},{"location":"vcontroller/git/overview/#reflogs","title":"reflogs","text":"<p>Git keeps a record of when the tips of branches and other references were updated in the repo. you can view and update these ref logs using the <code>git reflog</code>. It would be helpful incase you have messed up commits or while you rebase you need to know what changes in the past you made etc .. you can always use <code>reflogs</code> to go that commit and work on.</p> <p>Note: reflogs keep track only local activity. they are not shared with collaborators. older entries are removed after 90 days.</p> <pre><code>git reflog show HEAD\ngit reflog\n</code></pre> <p>differences between <code>log</code> and <code>reflog</code> is that, log gives the commits history where as reflog provides the switch between the different branches along with all commit ids. </p>"},{"location":"vcontroller/git/overview/#alias","title":"alias","text":"<p>you can find the config file in <code>~/.git/.gitconfig</code>, you can define your alias as below</p> <p>vim ~/.git/.gitconfig</p> <pre><code>[alias]\n    l = log\n    st = status\n</code></pre> <p>Now, you can use </p> <pre><code>git l \ngit st\n</code></pre> <p>you can also use the CLI utility that helps you to achive the same(below)..</p> <pre><code>git config --global alias.l log \ngit config --global alias.st status\n</code></pre> <p>git alias references</p> <ul> <li> <p>The Ultimate Git Alias Setup</p> </li> <li> <p>Must Have Git Aliases</p> </li> <li> <p>Configure Git Alias</p> </li> </ul>"},{"location":"vcontroller/git/overview/#git-oneliners","title":"Git Oneliners","text":"<pre><code>git reset HEAD -- path/to/file -&gt; rm file from staged repo\n\ngit commit --amend -m 'created new files' --no-edit -&gt; modify recent commit\n\ngit reset --hard HEAD~1  -&gt; revert previous commit\n</code></pre>"},{"location":"vcontroller/git/overview/#references","title":"References","text":"<p>https://www.bogotobogo.com/cplusplus/Git/Git_GitHub_Express.php</p> <p>https://www.codementor.io/@alexershov/11-painful-git-interview-questions-and-answers-you-will-cry-on-lybbrqhvs`</p>"}]}